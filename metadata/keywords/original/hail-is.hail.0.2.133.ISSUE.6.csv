id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/6617:16048,Integrability,message,message,16048,"had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:16224,Integrability,message,message,16224,"put') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:16410,Integrability,message,message,16410,"pdate job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:16588,Integrability,message,message,16588,"ate_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod alr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:16747,Integrability,message,message,16747,"me"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1221"", ""message"": ""restarting ready and running jobs with pods not seen in k8s""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1225"", ""message"": ""restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:19100,Integrability,message,message,19100,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:4950,Performance,concurren,concurrent,4950,"eb_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:12681,Performance,concurren,concurrent,12681,"eduled'}],\n 'container_statuses': [{'container_id': None,\n 'image': 'alpine',\n 'image_id': '',\n 'last_state': {'running': None,\n 'terminated': None,\n 'waiting': None},\n 'name': 'main',\n 'ready': False,\n 'restart_count': 0,\n 'state': {'running': None,\n 'terminated': {'container_id': None,\n 'exit_code': 0,\n 'finished_at': None,\n 'message': None,\n 'reason': None,\n 'signal': None,\n 'started_at': None},\n 'waiting': None}}],\n 'host_ip': '10.128.0.56',\n 'init_container_statuses': None,\n 'message': None,\n 'nominated_node_name': None,\n 'phase': 'Pending',\n 'pod_ip': None,\n 'qos_class': 'Burstable',\n 'reason': None,\n 'start_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal())}}""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_clie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:19529,Performance,load,load,19529,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14756,Safety,timeout,timeout,14756,"quest; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14272,Security,Audit,Audit-Id,14272,"ient.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:17052,Security,Authoriz,Authorization,17052,"restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:5,Testability,log,logs,5,"[the logs for PR 6604, batch 2414, job 78, `test_batch`](https://github.com/hail-is/hail/files/3384019/job-78-log.txt). `test_batch` has been flaky lately. This is one particular failing PR. There seems to be an issue where the batch created by the following script hangs forever:. ```python; b4 = self.client.create_batch(); b4.create_job('alpine', ['sleep', '30']); b4 = b4.submit(); b4.cancel(); b4.wait(); b4s = b4.status(); assert b4s['complete'] and b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:110,Testability,log,log,110,"[the logs for PR 6604, batch 2414, job 78, `test_batch`](https://github.com/hail-is/hail/files/3384019/job-78-log.txt). `test_batch` has been flaky lately. This is one particular failing PR. There seems to be an issue where the batch created by the following script hangs forever:. ```python; b4 = self.client.create_batch(); b4.create_job('alpine', ['sleep', '30']); b4 = b4.submit(); b4.cancel(); b4.wait(); b4s = b4.status(); assert b4s['complete'] and b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:429,Testability,assert,assert,429,"[the logs for PR 6604, batch 2414, job 78, `test_batch`](https://github.com/hail-is/hail/files/3384019/job-78-log.txt). `test_batch` has been flaky lately. This is one particular failing PR. There seems to be an issue where the batch created by the following script hangs forever:. ```python; b4 = self.client.create_batch(); b4.create_job('alpine', ['sleep', '30']); b4 = b4.submit(); b4.cancel(); b4.wait(); b4s = b4.status(); assert b4s['complete'] and b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:587,Testability,log,logs,587,"[the logs for PR 6604, batch 2414, job 78, `test_batch`](https://github.com/hail-is/hail/files/3384019/job-78-log.txt). `test_batch` has been flaky lately. This is one particular failing PR. There seems to be an issue where the batch created by the following script hangs forever:. ```python; b4 = self.client.create_batch(); b4.create_job('alpine', ['sleep', '30']); b4 = b4.submit(); b4.cancel(); b4.wait(); b4s = b4.status(); assert b4s['complete'] and b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:900,Testability,log,log,900,"[the logs for PR 6604, batch 2414, job 78, `test_batch`](https://github.com/hail-is/hail/files/3384019/job-78-log.txt). `test_batch` has been flaky lately. This is one particular failing PR. There seems to be an issue where the batch created by the following script hangs forever:. ```python; b4 = self.client.create_batch(); b4.create_job('alpine', ['sleep', '30']); b4 = b4.submit(); b4.cancel(); b4.wait(); b4s = b4.status(); assert b4s['complete'] and b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:1453,Testability,log,log,1453,"nd b4s['state'] == 'cancelled', b4s; ```. This batch is assigned number 9. There is one job, its number is 1. Here's a snippet of the logs around when batch 9 is created:. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,933"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""create_jobs:986"", ""message"": ""created 1 jobs for batch 9""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,934"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:2146,Testability,log,log,2146,", ""first_request_line"": ""POST /api/v1alpha/batches/9/jobs/create HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,945"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/close HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/close HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,957"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""cancel:862"", ""message"": ""batch 9 cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,958"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1\"" 200 158 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""PATCH /api/v1alpha/batches/9/cancel HTTP/1.1"", ""response_status"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:3474,Testability,log,log,3474,"us"": 200, ""response_size"": 158, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,967"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,969"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""set_state:501"", ""message"": ""job (9, 1, 'main') changed state: Ready -> Cancelled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:4001,Testability,log,log,4001,"time"": ""2019-07-11 14:19:34,974"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""_delete_pvc:251"", ""message"": ""deleting persistent volume claim batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,976"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,977"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,985"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.4.199 [11/Jul/2019:14:19:34 +0000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:5047,Testability,log,log,5047,"000] \""GET /api/v1alpha/batches/9 HTTP/1.1\"" 200 279 \""-\"" \""Python/3.6 aiohttp/3.5.4\"""", ""remote_address"": ""10.32.4.199"", ""request_start_time"": ""[11/Jul/2019:14:19:34 +0000]"", ""first_request_line"": ""GET /api/v1alpha/batches/9 HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n '",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:6289,Testability,test,test,6289,""", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': None,\n 'automount_service_account_token': None,\n 'containers': [{'args': None,\n 'command': ['sleep', '30'],\n 'env': [{'name': 'POD_IP',\n 'value': None,\n 'value_from': {'config_map_key_ref': None,\n 'field_ref': {'api_version': 'v1',\n 'field_path': 'status.podIP'},\n 'resource_field_ref': None,\n 'secret_key_ref': None}},\n {'name': 'POD_NAME',\n 'value': None,\n 'value_from': {'config_map_key_ref': None,\n 'field_ref': {'api_version': 'v1',\n 'field_path': 'metadata.name'},\n 'resource_field_ref': None,\n 'secret_key_ref': None}}],\n 'env_from': None,\n 'image': 'alpine',\n 'image_pull_policy': 'Always',\n 'lifecycle': None,\n 'liveness_probe': None,\n 'name': 'main',\n 'ports': None,\n 'readiness_probe': None,\n 'resources': {'limits': None,\n 'requests': {'cpu': '100m',\n 'memory': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:7619,Testability,log,log,7619,"4-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': None,\n 'automount_service_account_token': None,\n 'containers': [{'args': None,\n 'command': ['sleep', '30'],\n 'env': [{'name': 'POD_IP',\n 'value': None,\n 'value_from': {'config_map_key_ref': None,\n 'field_ref': {'api_version': 'v1',\n 'field_path': 'status.podIP'},\n 'resource_field_ref': None,\n 'secret_key_ref': None}},\n {'name': 'POD_NAME',\n 'value': None,\n 'value_from': {'config_map_key_ref': None,\n 'field_ref': {'api_version': 'v1',\n 'field_path': 'metadata.name'},\n 'resource_field_ref': None,\n 'secret_key_ref': None}}],\n 'env_from': None,\n 'image': 'alpine',\n 'image_pull_policy': 'Always',\n 'lifecycle': None,\n 'liveness_probe': None,\n 'name': 'main',\n 'ports': None,\n 'readiness_probe': None,\n 'resources': {'limits': None,\n 'requests': {'cpu': '100m',\n 'memory': '500M'}},\n 'security_context': None,\n 'stdin': None,\n 'stdin_once': None,\n 'termination_message_path': '/dev/termination-log',\n 'termination_message_policy': 'File',\n 'tty': None,\n 'volume_devices': None,\n 'volume_mounts': [{'mount_path': '/gsa-key',\n 'mount_propagation': None,\n 'name': 'gsa-key',\n 'read_only': None,\n 'sub_path': None},\n {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',\n 'mount_propagation': None,\n 'name': 'default-token-brr2f',\n 'read_only': True,\n 'sub_path': None}],\n 'working_dir': None}],\n 'dns_config': None,\n 'dns_policy': 'ClusterFirst',\n 'enable_service_links': True,\n 'host_aliases': None,\n 'host_ipc': None,\n 'host_network': None,\n 'host_pid': None,\n 'hostname': None,\n 'image_pull_secrets': None,\n 'init_containers': None,\n 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-8hq5',\n 'node_selector': None,\n 'priority': 500000,\n 'priority_class_name': 'user',\n 'readiness_gates': None,\n 'restart_policy': 'Never',\n 'runtime_class_name': None,\n 'scheduler_nam",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:9982,Testability,test,test-gsa-key,9982,"effect': 'NoExecute',\n 'key': 'node.kubernetes.io/not-ready',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None},\n {'effect': 'NoExecute',\n 'key': 'node.kubernetes.io/unreachable',\n 'operator': 'Exists',\n 'toleration_seconds': 300,\n 'value': None}],\n 'volumes': [{'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_api': None,\n 'empty_dir': None,\n 'fc': None,\n 'flex_volume': None,\n 'flocker': None,\n 'gce_persistent_disk': None,\n 'git_repo': None,\n 'glusterfs': None,\n 'host_path': None,\n 'iscsi': None,\n 'name': 'gsa-key',\n 'nfs': None,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'test-gsa-key'},\n 'storageos': None,\n 'vsphere_volume': None},\n {'aws_elastic_block_store': None,\n 'azure_disk': None,\n 'azure_file': None,\n 'cephfs': None,\n 'cinder': None,\n 'config_map': None,\n 'downward_api': None,\n 'empty_dir': None,\n 'fc': None,\n 'flex_volume': None,\n 'flocker': None,\n 'gce_persistent_disk': None,\n 'git_repo': None,\n 'glusterfs': None,\n 'host_path': None,\n 'iscsi': None,\n 'name': 'default-token-brr2f',\n 'nfs': None,\n 'persistent_volume_claim': None,\n 'photon_persistent_disk': None,\n 'portworx_volume': None,\n 'projected': None,\n 'quobyte': None,\n 'rbd': None,\n 'scale_io': None,\n 'secret': {'default_mode': 420,\n 'items': None,\n 'optional': None,\n 'secret_name': 'default-token-brr2f'},\n 'storageos': None,\n 'vsphere_volume': None}]},\n 'status': {'conditions': [{'last_probe_time': None,\n 'last_transition_time': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'message': None,\n 'reason': None,\n 'status': 'True',\n 'type': 'Initialized'},\n {'last_probe_time': None,\n 'last_transition_tim",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14127,Testability,log,logs,14127,"ient.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:14741,Testability,test,tests,14741,"quest; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,924"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:591"", ""message"": ""no logs for batch-9-job-1-c8b9b2 due to previous error, rescheduling pod Error: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '6a744afc-154f-4b48-b4bb-51a15078d999', 'Content-Type': 'application/json', 'Date': 'Thu, 11 Jul 2019 14:19:39 GMT', 'Content-Length': '179'})\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""container \\\""main\\\"" in pod \\\""batch-9-job-1-c8b9b2\\\"" is terminated\"",\""reason\"":\""BadRequest\"",\""code\"":400}\n\n""}; ```. And finally, this k8s refresh loop sequence repeats until CI kills the tests due to a timeout. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,070"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1261"", ""message"": ""started k8s state refresh""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,085"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pods:1210"", ""message"": ""k8s had 3 pods""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,088"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 1, 'output') with pod batch-11-job-1-4f1118""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,090"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 2, 'output') with pod batch-11-job-2-ad1587""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (11, 3, 'output') with pod batch-11-job-3-d826dd""}; {",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:17029,Testability,test,test-batch,17029,"restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:17080,Testability,test,test-jwt,17080,"restarting job (9, 1, 'main')""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod None""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,093"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1148"", ""message"": ""job (9, 1, 'main') mark unscheduled""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,106"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_pvc:1237"", ""message"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:17600,Testability,log,log,17600,"e"": ""k8s had 3 pvcs""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:23:41,113"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""refresh_k8s_state:1264"", ""message"": ""k8s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the st",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:17782,Testability,log,log,17782,"s state refresh complete""}; ```. Because job 1's batch is cancelled, we never restart the job/pod. However, the batch is stuck in an incomplete state. Before this PR's namespace was destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `cre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:17945,Testability,log,log,17945,"as destroyed, I SSH'ed to the test_batch pod and looked at the batch's status:. ```; root@test-batch:/# curl -H ""Authorization: Bearer $(cat test-jwt/jwt)"" batch.pr-6604-default-ov5tgx24rrou/api/v1alpha/batches/9; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_un",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:18173,Testability,log,log,18173,"; {""id"": 9, ""state"": ""running"", ""complete"": false, ""closed"": 1, ""jobs"": [{""batch_id"": 9, ""job_id"": 1, ""state"": ""Running""}; ```. The batch is somehow in the state ""running"" and the only job is also in the; state ""Running"". Only two lines of code transition to the state 'Running'. They; both appear in the suffix of `Job._create_pod`:. ```python; pod, err = await app['k8s'].create_pod(body=pod_template); if err is not None:; if err.status == 409:; log.info(f'pod already exists for job {self.full_id}'); n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); return; traceback.print_tb(err.__traceback__); log.info(f'pod creation failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:19118,Testability,log,log,19118,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:19139,Testability,log,log,19139,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:19368,Testability,log,log,19368,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/issues/6617:19685,Testability,log,logs,19685,"n failed for job {self.full_id} '; f'with the following error: {err}'); return. n_updated = await db.jobs.update_record(*self.id, compare_items={'state': self._state}, state='Running'); if n_updated == 0:; log.warning(f'changing the state for job {self.full_id} failed due to the expected state {self._state} not in db'); ```. For either of these database updates to succeed, the thread of control must have; thought the `_state` was `Cancelled` or we moved through some intermediate; state. We continue under the assumption that we went directly to `Running`. Who calls `_create_pod`?. - `start_pod`, but it checks that the state is in `Ready`; - `mark_complete`, but that's only if there's a ""next task"", this job has only; one task. That leaves `create_if_ready` and `mark_unscheduled`. `create_if_ready` is only; called by methods that are triggered when a parent with children finishes. We; have no parent-child relationships here. By process of elimination, `mark_unscheduled` must be the culprit. But how?; `mark_unscheduled` is called when a pod is evicted or by the k8s update loop if; there exists no pod. In those cases a message a special log is printed. That log; appears later (because the pod is missing) but it does not appear during the; initial sequence of events. Let's set that aside and focus on the other path by; which `mark_unscheduled` is called: `mark_complete` when the pod log cannot be; retrieved. Proposed sequence of events:. - pod is created; - k8s sends an event that the pod is terminated (but without timing information); - we load a Job object from the db. the job is Pending, the batch is cancelled; - we enter `update_job_with_pod` then `mark_complete`; - we fail to retrieve the logs; - we try to mark unscheduled, but the batch is cancelled so we do nothing.; ...; - we realize a Pending pod was never created, we try to mark_unscheduled, but; the batch is cancelled, so we do nothing; ...; - we realize a Pending pod was never created, we try ... ad infinitum",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617
https://github.com/hail-is/hail/pull/6619:52,Deployability,update,update,52,"Contains user creation logic. Obvious improvements: update tests, add bit of logic to add resources to multiple namespaces (or define which resources should go in which namespace): right now gcp-sa-key needs to be in batch-pods, and (I think?) also default namespace. Has some duplicated database creation logic with auth. Please let me know what you'd like changed!. cc @cseed, @danking, @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6619
https://github.com/hail-is/hail/pull/6619:23,Testability,log,logic,23,"Contains user creation logic. Obvious improvements: update tests, add bit of logic to add resources to multiple namespaces (or define which resources should go in which namespace): right now gcp-sa-key needs to be in batch-pods, and (I think?) also default namespace. Has some duplicated database creation logic with auth. Please let me know what you'd like changed!. cc @cseed, @danking, @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6619
https://github.com/hail-is/hail/pull/6619:59,Testability,test,tests,59,"Contains user creation logic. Obvious improvements: update tests, add bit of logic to add resources to multiple namespaces (or define which resources should go in which namespace): right now gcp-sa-key needs to be in batch-pods, and (I think?) also default namespace. Has some duplicated database creation logic with auth. Please let me know what you'd like changed!. cc @cseed, @danking, @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6619
https://github.com/hail-is/hail/pull/6619:77,Testability,log,logic,77,"Contains user creation logic. Obvious improvements: update tests, add bit of logic to add resources to multiple namespaces (or define which resources should go in which namespace): right now gcp-sa-key needs to be in batch-pods, and (I think?) also default namespace. Has some duplicated database creation logic with auth. Please let me know what you'd like changed!. cc @cseed, @danking, @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6619
https://github.com/hail-is/hail/pull/6619:306,Testability,log,logic,306,"Contains user creation logic. Obvious improvements: update tests, add bit of logic to add resources to multiple namespaces (or define which resources should go in which namespace): right now gcp-sa-key needs to be in batch-pods, and (I think?) also default namespace. Has some duplicated database creation logic with auth. Please let me know what you'd like changed!. cc @cseed, @danking, @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6619
https://github.com/hail-is/hail/issues/6625:1844,Availability,Toler,Tolerations,1844,"ser=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2296,Availability,error,error,2296,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2555,Availability,Error,Error,2555,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:4318,Availability,toler,tolerations,4318,"-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:4441,Availability,toler,tolerationSeconds,4441,"Ref:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:4541,Availability,toler,tolerationSeconds,4541,"cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.101; phase: Pending; qosClass: Burstable; startTime: ""2019-07-12T17:17:15Z""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:990,Deployability,pipeline,pipeline,990,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:3396,Deployability,pipeline,pipeline,3396,"9; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; sched",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:151,Energy Efficiency,schedul,scheduled,151,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2052,Energy Efficiency,Schedul,Scheduled,2052,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2074,Energy Efficiency,schedul,scheduler,2074,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:4174,Energy Efficiency,schedul,schedulerName,4174,"-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:4197,Energy Efficiency,schedul,scheduler,4197,"-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2005,Integrability,Message,Message,2005,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2604,Integrability,bridg,bridge,2604,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:5015,Integrability,message,message,5015,"cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.101; phase: Pending; qosClass: Burstable; startTime: ""2019-07-12T17:17:15Z""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:5191,Integrability,message,message,5191,"cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.101; phase: Pending; qosClass: Burstable; startTime: ""2019-07-12T17:17:15Z""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2283,Modifiability,sandbox,sandbox,2283,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2342,Modifiability,sandbox,sandbox,2342,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:188,Safety,timeout,timeout,188,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:4208,Security,secur,securityContext,4208,"-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:893,Testability,test,test-gsa-key,893,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:939,Testability,test,test,939,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:966,Testability,test,test-,966,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:1249,Testability,test,test-gsa-key,1249,"job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:1267,Testability,test,test-gsa-key,1267,"job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:1476,Testability,test,test-gsa-key,1476,"ser=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:1550,Testability,test,test-gsa-key,1550,"ser=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2283,Testability,sandbox,sandbox,2283,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:2342,Testability,sandbox,sandbox,2342,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:3299,Testability,test,test-gsa-key,3299,"9; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; sched",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:3345,Testability,test,test,3345,"9; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; sched",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:3372,Testability,test,test-,3372,"9; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; sched",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:3767,Testability,log,log,3767,"tadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:3833,Testability,test,test-gsa-key,3833,"tadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:3853,Testability,test,test-gsa-key,3853,"tadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:4583,Testability,test,test-gsa-key,4583,"cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.101; phase: Pending; qosClass: Burstable; startTime: ""2019-07-12T17:17:15Z""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:4653,Testability,test,test-gsa-key,4653,"cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.101; phase: Pending; qosClass: Burstable; startTime: ""2019-07-12T17:17:15Z""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/issues/6625:36,Usability,clear,clear,36,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6625
https://github.com/hail-is/hail/pull/6628:25,Testability,test,tests,25,It's only used for a few tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6628
https://github.com/hail-is/hail/pull/6633:171,Availability,error,errors,171,It is currently possible to write a blocked index where the virtual file; offset is exactly ((REAL_FILE_OFFSET << 16) | BLOCK_SIZE). This is a bug; and leads to assertion errors when trying to seek to the appropriate row; because `off == end` for that index.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6633
https://github.com/hail-is/hail/pull/6633:161,Testability,assert,assertion,161,It is currently possible to write a blocked index where the virtual file; offset is exactly ((REAL_FILE_OFFSET << 16) | BLOCK_SIZE). This is a bug; and leads to assertion errors when trying to seek to the appropriate row; because `off == end` for that index.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6633
https://github.com/hail-is/hail/issues/6634:241,Availability,error,error,241,"hailctl dataproc start \; 	--max-idle 12h \; 	--init gs://gnomad-public/tools/inits/master-init.sh \; 	--packages slackclient==2.0.0,websocket-client,sklearn,tabulate,statsmodels,scikit-learn,hdbscan,matplotlib bw2. Fails with the following error:; ```; gcloud beta dataproc clusters create \; bw2 \; --image-version=1.4-debian9 \; --properties=spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g \; --initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh \; --metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:2146,Availability,ERROR,ERROR,2146,"0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/start.py"", line 195, in main; sp.check_call(cmd); File ""/usr/local/Cellar/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:4840,Availability,error,error,4840,"one-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_broadinstitute_org', '--max-idle=12h']' returned non-zero exit status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:5800,Availability,avail,available,5800,"t status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:5855,Deployability,install,install,5855,"g; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-clien",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:5865,Deployability,upgrade,upgrade,5865,"g; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-clien",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:6322,Deployability,install,install,6322,"py<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib')' returned non-zero exit status 1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 66, in <module>; safe_call(*command); File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 18, in safe_call; print(e.output).decode(); AttributeError: 'NoneType' object has no attribute 'decode'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:633,Energy Efficiency,monitor,monitoring,633,"hailctl dataproc start \; 	--max-idle 12h \; 	--init gs://gnomad-public/tools/inits/master-init.sh \; 	--packages slackclient==2.0.0,websocket-client,sklearn,tabulate,statsmodels,scikit-learn,hdbscan,matplotlib bw2. Fails with the following error:; ```; gcloud beta dataproc clusters create \; bw2 \; --image-version=1.4-debian9 \; --properties=spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g \; --initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh \; --metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:3735,Energy Efficiency,monitor,monitoring,3735,".exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/start.py"", line 195, in main; sp.check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'bw2', '--image-version=1.4-debian9', '--properties=spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g', '--initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh', '--metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:1983,Performance,perform,performance,1983,"|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:2044,Performance,perform,performance,2044,"onious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:2084,Performance,perform,performance,2084,"onious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output.; Traceback (most recent call last):; File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:1598,Safety,timeout,timeout,1598,"r.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g \; --initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh \; --metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bdd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:4700,Safety,timeout,timeout,4700,"er.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g', '--initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh', '--metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_broadinstitute_org', '--max-idle=12h']' returned non-zero exit status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclien",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:582,Testability,log,logging,582,"hailctl dataproc start \; 	--max-idle 12h \; 	--init gs://gnomad-public/tools/inits/master-init.sh \; 	--packages slackclient==2.0.0,websocket-client,sklearn,tabulate,statsmodels,scikit-learn,hdbscan,matplotlib bw2. Fails with the following error:; ```; gcloud beta dataproc clusters create \; bw2 \; --image-version=1.4-debian9 \; --properties=spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g \; --initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh \; --metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:1087,Testability,log,logger,1087,"s/master-init.sh \; 	--packages slackclient==2.0.0,websocket-client,sklearn,tabulate,statsmodels,scikit-learn,hdbscan,matplotlib bw2. Fails with the following error:; ```; gcloud beta dataproc clusters create \; bw2 \; --image-version=1.4-debian9 \; --properties=spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g \; --initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh \; --metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:3684,Testability,log,logging,3684,"Versions/3.7/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/start.py"", line 195, in main; sp.check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'bw2', '--image-version=1.4-debian9', '--properties=spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g', '--initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh', '--metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initializa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:4189,Testability,log,logger,4189,"framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'bw2', '--image-version=1.4-debian9', '--properties=spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g', '--initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh', '--metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_broadinstitute_org', '--max-idle=12h']' returned non-zero exit status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:4846,Testability,log,log,4846,"one-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_broadinstitute_org', '--max-idle=12h']' returned non-zero exit status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:5452,Testability,log,logger,5452,"local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_broadinstitute_org', '--max-idle=12h']' returned non-zero exit status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:6728,Testability,log,logger,6728,"py<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib')' returned non-zero exit status 1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 66, in <module>; safe_call(*command); File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 18, in safe_call; print(e.output).decode(); AttributeError: 'NoneType' object has no attribute 'decode'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:186,Usability,learn,learn,186,"hailctl dataproc start \; 	--max-idle 12h \; 	--init gs://gnomad-public/tools/inits/master-init.sh \; 	--packages slackclient==2.0.0,websocket-client,sklearn,tabulate,statsmodels,scikit-learn,hdbscan,matplotlib bw2. Fails with the following error:; ```; gcloud beta dataproc clusters create \; bw2 \; --image-version=1.4-debian9 \; --properties=spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g \; --initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh \; --metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:1230,Usability,learn,learn,1230,"r.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g \; --initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh \; --metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib \; --master-machine-type=n1-highmem-8 \; --master-boot-disk-size=100GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-8 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --labels=creator=weisburd_broadinstitute_org \; --max-idle=12h; Starting cluster 'bw2'...; Waiting on operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08].; Waiting for cluster creation operation...; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done.; ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/seqr-project/regions/global/operations/46f1d37d-798a-3fc0-8f70-eac304448a08] failed: Initialization action failed. Failed action 'gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py', see output in: gs://dataproc-d919bdd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:4332,Usability,learn,learn,4332,"er.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false,spark:spark.driver.memory=41g', '--initialization-actions=gs://hail-common/hailctl/dataproc/0.2.18/init_notebook.py,gs://gnomad-public/tools/inits/master-init.sh', '--metadata=^|||^WHEEL=gs://hail-common/hailctl/dataproc/0.2.18/hail-0.2.18-py3-none-any.whl|||PKGS=aiohttp|bokeh>1.1,<1.3|decorator<5|gcsfs==0.2.1|hurry.filesize==0.9|ipykernel<5|nest_asyncio|numpy<2|pandas>0.22,<0.24|parsimonious<0.9|PyJWT|python-json-logger==0.1.11|requests>=2.21.0,<2.21.1|scipy>1.2,<1.4|tabulate==0.8.3|slackclient==2.0.0|websocket-client|sklearn|tabulate|statsmodels|scikit-learn|hdbscan|matplotlib', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_broadinstitute_org', '--max-idle=12h']' returned non-zero exit status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclien",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:5622,Usability,learn,learn,5622,"=n1-standard-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--labels=creator=weisburd_broadinstitute_org', '--max-idle=12h']' returned non-zero exit status 1.; ```. Then looking at the error log; ```; $ gsutil cat gs://dataproc-d919bddb-bde3-4138-bbe1-e068dfa1e550-us/google-cloud-dataproc-metainfo/3ec45dcc-d901-4777-930c-23046e64a97d/bw2-m/dataproc-initialization-script-0_output; pip packages are ['setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6634:6898,Usability,learn,learn,6898,"py<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib']; b""Double requirement given: tabulate (already in tabulate==0.8.3, name='tabulate')\nYou are using pip version 10.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n""; Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 16, in safe_call; sp.check_output(args, stderr=sp.STDOUT); File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/opt/conda/default/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); subprocess.CalledProcessError: Command '('pip', 'install', 'setuptools', 'mkl<2020', 'ipywidgets<8', 'jupyter_console<5', 'nbconvert<6', 'notebook<6', 'qtconsole<5', 'jupyter', 'tornado<6', 'lxml<5', 'google-cloud==0.32.0', 'ipython<7', 'jgscm<0.2', 'jupyter-spark', 'aiohttp', 'bokeh>1.1,<1.3', 'decorator<5', 'gcsfs==0.2.1', 'hurry.filesize==0.9', 'ipykernel<5', 'nest_asyncio', 'numpy<2', 'pandas>0.22,<0.24', 'parsimonious<0.9', 'PyJWT', 'python-json-logger==0.1.11', 'requests>=2.21.0,<2.21.1', 'scipy>1.2,<1.4', 'tabulate==0.8.3', 'slackclient==2.0.0', 'websocket-client', 'sklearn', 'tabulate', 'statsmodels', 'scikit-learn', 'hdbscan', 'matplotlib')' returned non-zero exit status 1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 66, in <module>; safe_call(*command); File ""/etc/google-dataproc/startup-scripts/dataproc-initialization-script-0"", line 18, in safe_call; print(e.output).decode(); AttributeError: 'NoneType' object has no attribute 'decode'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634
https://github.com/hail-is/hail/issues/6635:28,Availability,error,error,28,"I'm repeatedly getting this error on when attempting to run vep GRCh38 on a dataproc cluster started using hailctl with 25 preemptible nodes, and otherwise default params.; ```; 2019-07-14 20:54:55 TaskSetManager: INFO: Finished task 1611.1 in stage 8.0 (TID 21696) in 49183 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1601/10000); 2019-07-14 20:54:57 TaskSetManager: INFO: Starting task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1, partition 1559, PROCESS_LOCAL, 8800 bytes); 2019-07-14 20:54:57 TaskSetManager: INFO: Finished task 1570.1 in stage 8.0 (TID 21697) in 45412 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1602/10000); 2019-07-14 20:55:04 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 1.; 2019-07-14 20:55:04 DAGScheduler: INFO: Executor lost: 1 (epoch 0); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-07-14 20:55:04 TransportClient: ERROR: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:994,Availability,ERROR,ERROR,994," repeatedly getting this error on when attempting to run vep GRCh38 on a dataproc cluster started using hailctl with 25 preemptible nodes, and otherwise default params.; ```; 2019-07-14 20:54:55 TaskSetManager: INFO: Finished task 1611.1 in stage 8.0 (TID 21696) in 49183 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1601/10000); 2019-07-14 20:54:57 TaskSetManager: INFO: Starting task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1, partition 1559, PROCESS_LOCAL, 8800 bytes); 2019-07-14 20:54:57 TaskSetManager: INFO: Finished task 1570.1 in stage 8.0 (TID 21697) in 45412 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1602/10000); 2019-07-14 20:55:04 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 1.; 2019-07-14 20:55:04 DAGScheduler: INFO: Executor lost: 1 (epoch 0); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-07-14 20:55:04 TransportClient: ERROR: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:3972,Availability,ERROR,ERROR,3972,"el.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1561.1 in stage 8.0 (TID 21701, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1541.2 in stage 8.0 (TID 21700, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:2044,Performance,concurren,concurrent,2044,"0.128.0.126:36044: java.nio.channels.ClosedChannelException; java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:2129,Performance,concurren,concurrent,2129,"ChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.writ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:2217,Performance,concurren,concurrent,2217,"Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerCo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:2302,Performance,concurren,concurrent,2302," BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.uti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:3310,Performance,concurren,concurrent,3310,"faultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:3405,Performance,concurren,concurrent,3405,"nsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): E",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:3573,Performance,concurren,concurrent,3573,"peline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1561.1 in stage 8.0 (T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:3670,Performance,concurren,concurrent,3670,"ndlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1561.1 in stage 8.0 (TID 21701, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:2416,Safety,safe,safeSetFailure,2416,"1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEven",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:3343,Safety,safe,safeExecute,3343,"22); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 YarnScheduler: ERROR: Lost executor 1 on bw2-sw-dp3j.c.seqr-project.internal: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1574.1 in stage 8.0 (TID 21699, bw2-sw-dp3j.c.seqr-project.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Slave lost; 2019-07-14 20:55:04 TaskSetManager: WARN: Lost task 1559.1 in stage 8.0 (TID 21702, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/issues/6635:2895,Security,access,access,2895,rg.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635
https://github.com/hail-is/hail/pull/6636:27,Energy Efficiency,monitor,monitoring,27,"We keep a JSON file in the monitoring directly that is a backup of our grafana dashboard, in case we ever lose the volume grafana is running on. This is me updating the backup.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6636
https://github.com/hail-is/hail/pull/6641:37,Modifiability,variab,variable,37,Ci is currently failing because this variable is not defined.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6641
https://github.com/hail-is/hail/pull/6644:39,Performance,load,loadX,39,"I've left all the instances of `region.loadX` untouched (and left the methods on the region object, but this should let us avoid piping through region objects when we just need to read something. (Broken out from #6580)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6644
https://github.com/hail-is/hail/pull/6644:123,Safety,avoid,avoid,123,"I've left all the instances of `region.loadX` untouched (and left the methods on the region object, but this should let us avoid piping through region objects when we just need to read something. (Broken out from #6580)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6644
https://github.com/hail-is/hail/pull/6645:35,Deployability,deploy,deploy,35,Forgot to authorize when I did dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6645
https://github.com/hail-is/hail/pull/6645:10,Security,authoriz,authorize,10,Forgot to authorize when I did dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6645
https://github.com/hail-is/hail/pull/6646:215,Safety,avoid,avoid,215,"Works pretty much like Read/Write, except serializes to/from a fixed number of byte array slots on the function object. Not *super* happy with this design, but I'm using it in the new TableMapRows implementation to avoid reading/writing all the aggregations to a file. (Broken out from #6580)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6646
https://github.com/hail-is/hail/issues/6647:161,Deployability,update,update,161,"currently we have to commit to hail every time the grafana dashboard changes. It's very easy to change it in the UI, it is unlikely that people will remember to update the GitHub. We need a better system. Example: https://github.com/hail-is/hail/pull/6636",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6647
https://github.com/hail-is/hail/issues/6648:107,Deployability,update,updated,107,Scorecard should use a readiness probe to prevent traffic from being sent to scorecard before it has fully updated itself and is ready to serve traffic. https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/. An HTTP readiness probe that hits `GET /` should be sufficient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6648
https://github.com/hail-is/hail/issues/6648:186,Modifiability,config,configure-pod-container,186,Scorecard should use a readiness probe to prevent traffic from being sent to scorecard before it has fully updated itself and is ready to serve traffic. https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/. An HTTP readiness probe that hits `GET /` should be sufficient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6648
https://github.com/hail-is/hail/issues/6648:210,Modifiability,config,configure-liveness-readiness-probes,210,Scorecard should use a readiness probe to prevent traffic from being sent to scorecard before it has fully updated itself and is ready to serve traffic. https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/. An HTTP readiness probe that hits `GET /` should be sufficient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6648
https://github.com/hail-is/hail/pull/6652:30,Integrability,interface,interface,30,Rewrite the staged aggregator interface to:; - allow primitive values in aggregator without associated region; - create Scala Region instance once and reuse by setting different underlying native Region object to avoid overhead of object creation. Also includes some interface changes to AggregatorState (formerly RVAState) to clean up the region dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6652
https://github.com/hail-is/hail/pull/6652:267,Integrability,interface,interface,267,Rewrite the staged aggregator interface to:; - allow primitive values in aggregator without associated region; - create Scala Region instance once and reuse by setting different underlying native Region object to avoid overhead of object creation. Also includes some interface changes to AggregatorState (formerly RVAState) to clean up the region dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6652
https://github.com/hail-is/hail/pull/6652:347,Integrability,depend,dependencies,347,Rewrite the staged aggregator interface to:; - allow primitive values in aggregator without associated region; - create Scala Region instance once and reuse by setting different underlying native Region object to avoid overhead of object creation. Also includes some interface changes to AggregatorState (formerly RVAState) to clean up the region dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6652
https://github.com/hail-is/hail/pull/6652:0,Modifiability,Rewrite,Rewrite,0,Rewrite the staged aggregator interface to:; - allow primitive values in aggregator without associated region; - create Scala Region instance once and reuse by setting different underlying native Region object to avoid overhead of object creation. Also includes some interface changes to AggregatorState (formerly RVAState) to clean up the region dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6652
https://github.com/hail-is/hail/pull/6652:213,Safety,avoid,avoid,213,Rewrite the staged aggregator interface to:; - allow primitive values in aggregator without associated region; - create Scala Region instance once and reuse by setting different underlying native Region object to avoid overhead of object creation. Also includes some interface changes to AggregatorState (formerly RVAState) to clean up the region dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6652
https://github.com/hail-is/hail/pull/6659:259,Availability,error,error,259,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6659
https://github.com/hail-is/hail/pull/6659:265,Integrability,message,message,265,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6659
https://github.com/hail-is/hail/pull/6659:390,Integrability,message,messages,390,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6659
https://github.com/hail-is/hail/pull/6659:53,Modifiability,config,config,53,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6659
https://github.com/hail-is/hail/pull/6659:105,Modifiability,config,config,105,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6659
https://github.com/hail-is/hail/pull/6659:189,Security,secur,security,189,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6659
https://github.com/hail-is/hail/pull/6659:386,Testability,log,log,386,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6659
https://github.com/hail-is/hail/pull/6660:24,Deployability,deploy,deploy,24,Part 1 of improving dev deploy usability. This PR:. - Moves definition of profiles out of CI so that new build steps can be added as part of development.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6660
https://github.com/hail-is/hail/pull/6660:31,Usability,usab,usability,31,Part 1 of improving dev deploy usability. This PR:. - Moves definition of profiles out of CI so that new build steps can be added as part of development.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6660
https://github.com/hail-is/hail/pull/6662:310,Integrability,interface,interface,310,This changes the signatures of native methods to pass in the pointer from Scala so that we don't have to call back into the JVM to retrieve it. Some other small changes: I put all the native methods together and removed some methods that weren't used and which I didn't think needed to be a part of the region interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6662
https://github.com/hail-is/hail/issues/6663:48,Availability,error,error,48,"randomly assigned Patrick. I expected to get an error like:; ```; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: <<empty key>>; Index Expressions: int32; ```; But instead got:; ```; # ipython; import hail Python 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.utils.ra; In [2]: t = hl.utils.range_table(10) . In [3]: t.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'idx': int32 ; ----------------------------------------; Key: ['idx']; ----------------------------------------. In [4]: t = t.key_by() . In [5]: t[t.idx] ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-5-a44cae835751> in <module>; ----> 1 t[t.idx]. /usr/local/lib/python3.7/site-packages/hail/table.py in __getitem__(self, item); 366 else:; 367 try:; --> 368 return self.index(*wrap_to_tuple(item)); 369 except TypeError as e:; 370 raise TypeError(f""Table.__getitem__: invalid index argument(s)\n"". /usr/local/lib/python3.7/site-packages/hail/table.py in index(self, all_matches, *exprs); 1530 """"""; 1531 try:; -> 1532 return self._index(*exprs, all_matches=all_matches); 1533 except TableIndexKeyError as err:; 1534 key_type, exprs = err.args. /usr/local/lib/python3.7/site-packages/hail/table.py in _index(self, all_matches, *exprs); 1554 ; 1555 is_interval = (len(exprs) == 1; -> 1556 and isinstance(self.key[0].dtype, hl.tinterval); 1557 and exprs[0].dtype == self.key[0].dtype.point_type); 1558 . </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-570> in __getitem__(self, item). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6663
https://github.com/hail-is/hail/issues/6663:1914,Integrability,wrap,wrapper,1914,"]: t = hl.utils.range_table(10) . In [3]: t.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'idx': int32 ; ----------------------------------------; Key: ['idx']; ----------------------------------------. In [4]: t = t.key_by() . In [5]: t[t.idx] ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-5-a44cae835751> in <module>; ----> 1 t[t.idx]. /usr/local/lib/python3.7/site-packages/hail/table.py in __getitem__(self, item); 366 else:; 367 try:; --> 368 return self.index(*wrap_to_tuple(item)); 369 except TypeError as e:; 370 raise TypeError(f""Table.__getitem__: invalid index argument(s)\n"". /usr/local/lib/python3.7/site-packages/hail/table.py in index(self, all_matches, *exprs); 1530 """"""; 1531 try:; -> 1532 return self._index(*exprs, all_matches=all_matches); 1533 except TableIndexKeyError as err:; 1534 key_type, exprs = err.args. /usr/local/lib/python3.7/site-packages/hail/table.py in _index(self, all_matches, *exprs); 1554 ; 1555 is_interval = (len(exprs) == 1; -> 1556 and isinstance(self.key[0].dtype, hl.tinterval); 1557 and exprs[0].dtype == self.key[0].dtype.point_type); 1558 . </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-570> in __getitem__(self, item). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in __getitem__(self, item); 1474 return self._get_field(item); 1475 else:; -> 1476 return self._get_field(self.dtype.fields[item]); 1477 ; 1478 def __iter__(self):. IndexError: tuple index out of range. In [6]: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6663
https://github.com/hail-is/hail/issues/6663:1965,Integrability,wrap,wrapper,1965,"]: t = hl.utils.range_table(10) . In [3]: t.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'idx': int32 ; ----------------------------------------; Key: ['idx']; ----------------------------------------. In [4]: t = t.key_by() . In [5]: t[t.idx] ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-5-a44cae835751> in <module>; ----> 1 t[t.idx]. /usr/local/lib/python3.7/site-packages/hail/table.py in __getitem__(self, item); 366 else:; 367 try:; --> 368 return self.index(*wrap_to_tuple(item)); 369 except TypeError as e:; 370 raise TypeError(f""Table.__getitem__: invalid index argument(s)\n"". /usr/local/lib/python3.7/site-packages/hail/table.py in index(self, all_matches, *exprs); 1530 """"""; 1531 try:; -> 1532 return self._index(*exprs, all_matches=all_matches); 1533 except TableIndexKeyError as err:; 1534 key_type, exprs = err.args. /usr/local/lib/python3.7/site-packages/hail/table.py in _index(self, all_matches, *exprs); 1554 ; 1555 is_interval = (len(exprs) == 1; -> 1556 and isinstance(self.key[0].dtype, hl.tinterval); 1557 and exprs[0].dtype == self.key[0].dtype.point_type); 1558 . </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-570> in __getitem__(self, item). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in __getitem__(self, item); 1474 return self._get_field(item); 1475 else:; -> 1476 return self._get_field(self.dtype.fields[item]); 1477 ; 1478 def __iter__(self):. IndexError: tuple index out of range. In [6]: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6663
https://github.com/hail-is/hail/issues/6663:2171,Integrability,wrap,wrapper,2171,"]: t = hl.utils.range_table(10) . In [3]: t.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'idx': int32 ; ----------------------------------------; Key: ['idx']; ----------------------------------------. In [4]: t = t.key_by() . In [5]: t[t.idx] ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-5-a44cae835751> in <module>; ----> 1 t[t.idx]. /usr/local/lib/python3.7/site-packages/hail/table.py in __getitem__(self, item); 366 else:; 367 try:; --> 368 return self.index(*wrap_to_tuple(item)); 369 except TypeError as e:; 370 raise TypeError(f""Table.__getitem__: invalid index argument(s)\n"". /usr/local/lib/python3.7/site-packages/hail/table.py in index(self, all_matches, *exprs); 1530 """"""; 1531 try:; -> 1532 return self._index(*exprs, all_matches=all_matches); 1533 except TableIndexKeyError as err:; 1534 key_type, exprs = err.args. /usr/local/lib/python3.7/site-packages/hail/table.py in _index(self, all_matches, *exprs); 1554 ; 1555 is_interval = (len(exprs) == 1; -> 1556 and isinstance(self.key[0].dtype, hl.tinterval); 1557 and exprs[0].dtype == self.key[0].dtype.point_type); 1558 . </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-570> in __getitem__(self, item). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in __getitem__(self, item); 1474 return self._get_field(item); 1475 else:; -> 1476 return self._get_field(self.dtype.fields[item]); 1477 ; 1478 def __iter__(self):. IndexError: tuple index out of range. In [6]: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6663
https://github.com/hail-is/hail/issues/6663:384,Modifiability,enhance,enhanced,384,"randomly assigned Patrick. I expected to get an error like:; ```; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: <<empty key>>; Index Expressions: int32; ```; But instead got:; ```; # ipython; import hail Python 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.utils.ra; In [2]: t = hl.utils.range_table(10) . In [3]: t.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'idx': int32 ; ----------------------------------------; Key: ['idx']; ----------------------------------------. In [4]: t = t.key_by() . In [5]: t[t.idx] ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-5-a44cae835751> in <module>; ----> 1 t[t.idx]. /usr/local/lib/python3.7/site-packages/hail/table.py in __getitem__(self, item); 366 else:; 367 try:; --> 368 return self.index(*wrap_to_tuple(item)); 369 except TypeError as e:; 370 raise TypeError(f""Table.__getitem__: invalid index argument(s)\n"". /usr/local/lib/python3.7/site-packages/hail/table.py in index(self, all_matches, *exprs); 1530 """"""; 1531 try:; -> 1532 return self._index(*exprs, all_matches=all_matches); 1533 except TableIndexKeyError as err:; 1534 key_type, exprs = err.args. /usr/local/lib/python3.7/site-packages/hail/table.py in _index(self, all_matches, *exprs); 1554 ; 1555 is_interval = (len(exprs) == 1; -> 1556 and isinstance(self.key[0].dtype, hl.tinterval); 1557 and exprs[0].dtype == self.key[0].dtype.point_type); 1558 . </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-570> in __getitem__(self, item). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6663
https://github.com/hail-is/hail/pull/6666:33,Safety,Safe,SafeRow,33,"fixes #6236. instead of calling `SafeRow` to manifest the entire row and then pick out the keys, create `UnsafeRow`s and then copy only the keys. added `RVD.toUnsafeRows`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6666
https://github.com/hail-is/hail/pull/6666:105,Safety,Unsafe,UnsafeRow,105,"fixes #6236. instead of calling `SafeRow` to manifest the entire row and then pick out the keys, create `UnsafeRow`s and then copy only the keys. added `RVD.toUnsafeRows`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6666
https://github.com/hail-is/hail/pull/6669:95,Testability,benchmark,benchmark,95,"When I timed the specific code block we hid behind the flag, it took; about 1.5 seconds on the benchmark dataset. This check scales linearly with the number of samples, though, which is not great.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6669
https://github.com/hail-is/hail/pull/6670:15,Deployability,deploy,deploy,15,"Currently, dev deploy always eventually returns a timeout in the terminal because it tries to wait for the entire batch deploy / tests to run before returning. Now it will just create the batch and return the batch number instead of waiting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6670
https://github.com/hail-is/hail/pull/6670:120,Deployability,deploy,deploy,120,"Currently, dev deploy always eventually returns a timeout in the terminal because it tries to wait for the entire batch deploy / tests to run before returning. Now it will just create the batch and return the batch number instead of waiting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6670
https://github.com/hail-is/hail/pull/6670:50,Safety,timeout,timeout,50,"Currently, dev deploy always eventually returns a timeout in the terminal because it tries to wait for the entire batch deploy / tests to run before returning. Now it will just create the batch and return the batch number instead of waiting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6670
https://github.com/hail-is/hail/pull/6670:129,Testability,test,tests,129,"Currently, dev deploy always eventually returns a timeout in the terminal because it tries to wait for the entire batch deploy / tests to run before returning. Now it will just create the batch and return the batch number instead of waiting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6670
https://github.com/hail-is/hail/issues/6673:71,Deployability,deploy,deploy,71,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/issues/6673:453,Deployability,deploy,deploy,453,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/issues/6673:272,Energy Efficiency,schedul,scheduler,272,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/issues/6673:28,Integrability,depend,depend,28,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/issues/6673:52,Integrability,depend,depends,52,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/issues/6673:420,Integrability,depend,depend,420,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/issues/6673:78,Testability,test,test,78,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/issues/6673:139,Testability,test,tests,139,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/issues/6673:282,Testability,test,test,282,"Currently, all cleanup jobs depend on `sink`, which depends on all the deploy/test jobs finishing. This creates a problem -- short-running tests with high resource requirements end up reserving those resources for the full duration of the *longest* job. For instance, the scheduler test takes 14 seconds, but ends up reserving 3 non-preemptible cores for up to 30 minutes!. This can be solved by making the cleanup jobs depend on all descendants of the deploy jobs they are intending to clean up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6673
https://github.com/hail-is/hail/pull/6678:97,Deployability,configurat,configuration,97,"CVE-2019-11245 is a vulnerability in k8s that causes some (all?); containers without a runAsUser configuration to run; as user id 0, i.e. root. Jupyter refuses to start as root.; This change enables Jupyter to start successfully.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6678
https://github.com/hail-is/hail/pull/6678:97,Modifiability,config,configuration,97,"CVE-2019-11245 is a vulnerability in k8s that causes some (all?); containers without a runAsUser configuration to run; as user id 0, i.e. root. Jupyter refuses to start as root.; This change enables Jupyter to start successfully.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6678
https://github.com/hail-is/hail/issues/6679:191,Availability,avail,available,191,https://github.com/hail-is/hail/pull/6678 addresses this vulnerability in our notebook service. A long term fix is to upgrade to a version of k8s where this vulnerability is fixed. 1.13.7 is available and I believe it addresses this vulnerability. We should do this as well.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6679
https://github.com/hail-is/hail/issues/6679:118,Deployability,upgrade,upgrade,118,https://github.com/hail-is/hail/pull/6678 addresses this vulnerability in our notebook service. A long term fix is to upgrade to a version of k8s where this vulnerability is fixed. 1.13.7 is available and I believe it addresses this vulnerability. We should do this as well.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6679
https://github.com/hail-is/hail/issues/6680:2,Energy Efficiency,monitor,monitor,2,A monitor of this type would have revealed that we were frequently oversubscribing non-preemptible nodes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6680
https://github.com/hail-is/hail/pull/6684:102,Deployability,deploy,deployed,102,Yeah that shell find is totally broken. I'll fix it properly later. The k8s API changed since we last deployed. I'm hand deploying this now anyway.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6684
https://github.com/hail-is/hail/pull/6684:121,Deployability,deploy,deploying,121,Yeah that shell find is totally broken. I'll fix it properly later. The k8s API changed since we last deployed. I'm hand deploying this now anyway.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6684
https://github.com/hail-is/hail/pull/6687:2,Modifiability,rewrite,rewrite,2,"- rewrite ArrayElements.initOp to take nested initOps as arguments directly; - pass in EmitTriplet instead of RVAVariable; get rid of RVAVariable; - StagedRegionValueAggregator -> StagedAggregator, since doesn't always use RVs; - refactor tests to be more extensible for new aggregators",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6687
https://github.com/hail-is/hail/pull/6687:230,Modifiability,refactor,refactor,230,"- rewrite ArrayElements.initOp to take nested initOps as arguments directly; - pass in EmitTriplet instead of RVAVariable; get rid of RVAVariable; - StagedRegionValueAggregator -> StagedAggregator, since doesn't always use RVs; - refactor tests to be more extensible for new aggregators",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6687
https://github.com/hail-is/hail/pull/6687:239,Testability,test,tests,239,"- rewrite ArrayElements.initOp to take nested initOps as arguments directly; - pass in EmitTriplet instead of RVAVariable; get rid of RVAVariable; - StagedRegionValueAggregator -> StagedAggregator, since doesn't always use RVs; - refactor tests to be more extensible for new aggregators",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6687
https://github.com/hail-is/hail/pull/6689:64,Testability,test,test-only,64,"- clean up PCRelate; - [x] use IR node; - [x] clean up unneeded/test-only functions; - [x] delete pyApply; - BlockMatrixToTableFunction; - [x] class; - [x] .execute; - [x] type info; - BlockMatrixToTableApply; - [x] class; - [x] .children; - [x] .copy; - [x] InferType; - [x] PruneDeadField -- *not sure if correct, needs to be looked at*; - [x] Interpretable, Compilable; - [x] Parser; - [x] Pretty; - [x] IRSuite; - [x] Interpret; - [x] Lower(?)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689
https://github.com/hail-is/hail/issues/6697:11,Modifiability,config,configure,11,"We need to configure Grafana with an SMTP server so that it can send us alert emails. I'd like to have alerts for things like disks getting dangerously close to filling up, abnormally high latency, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6697
https://github.com/hail-is/hail/issues/6697:189,Performance,latency,latency,189,"We need to configure Grafana with an SMTP server so that it can send us alert emails. I'd like to have alerts for things like disks getting dangerously close to filling up, abnormally high latency, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6697
https://github.com/hail-is/hail/pull/6698:37,Safety,avoid,avoid,37,Renamed agg->aggArgs in Interpret to avoid name collision.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6698
https://github.com/hail-is/hail/pull/6700:279,Deployability,update,updated,279,"Slowly getting rid of the regions being threading through non-allocating functions, part i + 1. This changes the method signature of CodeOrdering method signatures from f(region1, v1, region2, v2) to f(v1, v2). Some other function signatures (e.g. PInterval.loadStart) were also updated as necessary. No functionality has been changed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6700
https://github.com/hail-is/hail/pull/6700:258,Performance,load,loadStart,258,"Slowly getting rid of the regions being threading through non-allocating functions, part i + 1. This changes the method signature of CodeOrdering method signatures from f(region1, v1, region2, v2) to f(v1, v2). Some other function signatures (e.g. PInterval.loadStart) were also updated as necessary. No functionality has been changed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6700
https://github.com/hail-is/hail/issues/6701:102,Modifiability,config,configure,102,"Our CI service should really be logging a JSON format the way batch does. Easy to change, need to use configure logging in Gear.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6701
https://github.com/hail-is/hail/issues/6701:32,Testability,log,logging,32,"Our CI service should really be logging a JSON format the way batch does. Easy to change, need to use configure logging in Gear.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6701
https://github.com/hail-is/hail/issues/6701:112,Testability,log,logging,112,"Our CI service should really be logging a JSON format the way batch does. Easy to change, need to use configure logging in Gear.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6701
https://github.com/hail-is/hail/pull/6702:76,Testability,test,tests,76,Usages of `physicalType` remain in four places:. - in/around aggregators; - tests; - a few stray TableIR nodes; - at the boundary of encoding/decoding,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6702
https://github.com/hail-is/hail/pull/6703:63,Availability,error,errors,63,"If we miss an event, the refresh loop won't find it because it errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6703
https://github.com/hail-is/hail/issues/6707:20,Availability,ERROR,ERROR,20,"```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:29,378"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:42,418"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:49,707"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:871,Availability,ERROR,ERROR,871,"```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:29,378"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:42,418"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:49,707"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:1722,Availability,ERROR,ERROR,1722,"atch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:42,418"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:49,707"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:2983,Availability,ERROR,ERROR,2983,"ll last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__)\nTypeError: Object of type 'datetime' is not JSON serializable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:06,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:41,499"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:3834,Availability,ERROR,ERROR,3834,"""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__)\nTypeError: Object of type 'datetime' is not JSON serializable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:06,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:41,499"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:22:42,128"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:4685,Availability,ERROR,ERROR,4685,"atch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:41,499"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:22:42,128"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:23:31,398"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:5536,Availability,ERROR,ERROR,5536,"atch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:22:42,128"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:23:31,398"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:25:18,329"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:6387,Availability,ERROR,ERROR,6387,"atch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:23:31,398"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:25:18,329"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:7648,Availability,ERROR,ERROR,7648,"ll last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__)\nTypeError: Object of type 'datetime' is not JSON serializable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:26:01,694"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=Tru",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:134,Integrability,message,message,134,"```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:29,378"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:42,418"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:49,707"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:985,Integrability,message,message,985,"19-07-22 22:20:29,378"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:42,418"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:49,707"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:1836,Integrability,message,message,1836,"22 22:20:42,418"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:20:49,707"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:3097,Integrability,message,message,3097,"tch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__)\nTypeError: Object of type 'datetime' is not JSON serializable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:06,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:41,499"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:3948,Integrability,message,message,3948,"19-07-22 22:21:06,541"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:21:41,499"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:22:42,128"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:4799,Integrability,message,message,4799,"19-07-22 22:21:41,499"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:22:42,128"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:23:31,398"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:5650,Integrability,message,message,5650,"19-07-22 22:22:42,128"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:23:31,398"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:25:18,329"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:6501,Integrability,message,message,6501,"22 22:23:31,398"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 580, in mark_complete\n setup_container = pod.status.init_container_statuses[0]\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:25:18,329"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/issues/6707:7762,Integrability,message,message,7762,"tch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__)\nTypeError: Object of type 'datetime' is not JSON serializable""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-22 22:26:01,694"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1273"", ""message"": ""k8s event stream failed due to: Object of type 'datetime' is not JSON serializable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1271, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1256, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1234, in update_job_with_pod\n await job.mark_complete(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 608, in mark_complete\n json.dumps(pod_status.to_dict()))\n File \""/usr/lib/python3.6/json/__init__.py\"", line 231, in dumps\n return _default_encoder.encode(obj)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 257, in iterencode\n return _iterencode(o, 0)\n File \""/usr/lib/python3.6/json/encoder.py\"", line 180, in default\n o.__class__.__name__",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6707
https://github.com/hail-is/hail/pull/6708:61,Testability,log,log,61,I missed the `pod_status` thing in the rebase. The container log is just a mistake on my part.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6708
https://github.com/hail-is/hail/issues/6709:2650,Availability,Toler,Tolerations,2650,"e: ubuntu; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-cb568b8f204e/__TASK__2750/; /bin/sh -c ""sleep $(( 60 + (RANDOM % 20) ))""; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Image: gcr.io/hail-vdc/batch:w1eqo739af4d; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.sidecar; ; Requests:; cpu: 500m; Environment:; INSTANCE_ID: cd50b95a89914efb897965a5e982a29d; BATCH_ID: 101; JOB_ID: 2751; TOKEN: 90f13d; BATCH_BUCKET_NAME: hail-batch-3jmp5; COPY_OUTPUT_CMD: true; HAIL_POD_NAMESPACE: batch-pods; KUBERNETES_TIMEOUT_IN_SECONDS: 5.0; REFRESH_INTERVAL_IN_SECONDS: 300; POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: dking-gsa-key; Optional: false; batch-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: batch-gsa-key; Optional: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 21m default-scheduler Successfully assigned batch-pods/batch-101-job-2751-90f13d to gke-vdc-non-preemptible-pool-0106a51b-5znv; Warning OutOfcpu 21m kubelet, gke-vdc-non-preemptible-pool-0106a51b-5znv Node didn't have enough resource: cpu, requested: 600, used: 7371, capacity: 7910; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6709
https://github.com/hail-is/hail/issues/6709:1245,Deployability,pipeline,pipeline,1245,"19 18:19:58 -0400; Labels: app=batch-job; batch_id=101; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=2751; user=dking; uuid=f3bd48354c404bacb3f89273ba431457; Annotations: <none>; Status: Failed; Reason: OutOfcpu; Message: Pod Node didn't have enough resource: cpu, requested: 600, used: 7371, capacity: 7910; IP: ; Init Containers:; setup:; Image: google/cloud-sdk:237.0.0-alpine; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/101/2751/90f13d/container_results && exit 1; rm -rf /io/*; true; ; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Image: ubuntu; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-cb568b8f204e/__TASK__2750/; /bin/sh -c ""sleep $(( 60 + (RANDOM % 20) ))""; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Image: gcr.io/hail-vdc/batch:w1eqo739af4d; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.sidecar; ; Requests:; cpu: 500m; Environment:; INSTANCE_ID: cd50b95a89914efb897965a5e982a29d; BATCH_ID: 101; JOB_ID: 2751; TOKEN: 90f13d; BATCH_BUCKET_NAME: hail-batch-3jmp5; COPY_OUTPUT_CMD: true; HAIL_POD_NAMESPACE: batch-pods; KUBERNETES_TIMEOUT_IN_SECONDS: 5.0; REFRESH_INTERVAL_IN_SECONDS: 300; POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6709
https://github.com/hail-is/hail/issues/6709:1254,Deployability,pipeline,pipeline-,1254,"19 18:19:58 -0400; Labels: app=batch-job; batch_id=101; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=2751; user=dking; uuid=f3bd48354c404bacb3f89273ba431457; Annotations: <none>; Status: Failed; Reason: OutOfcpu; Message: Pod Node didn't have enough resource: cpu, requested: 600, used: 7371, capacity: 7910; IP: ; Init Containers:; setup:; Image: google/cloud-sdk:237.0.0-alpine; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/101/2751/90f13d/container_results && exit 1; rm -rf /io/*; true; ; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Image: ubuntu; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-cb568b8f204e/__TASK__2750/; /bin/sh -c ""sleep $(( 60 + (RANDOM % 20) ))""; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Image: gcr.io/hail-vdc/batch:w1eqo739af4d; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.sidecar; ; Requests:; cpu: 500m; Environment:; INSTANCE_ID: cd50b95a89914efb897965a5e982a29d; BATCH_ID: 101; JOB_ID: 2751; TOKEN: 90f13d; BATCH_BUCKET_NAME: hail-batch-3jmp5; COPY_OUTPUT_CMD: true; HAIL_POD_NAMESPACE: batch-pods; KUBERNETES_TIMEOUT_IN_SECONDS: 5.0; REFRESH_INTERVAL_IN_SECONDS: 300; POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6709
https://github.com/hail-is/hail/issues/6709:2858,Energy Efficiency,Schedul,Scheduled,2858,"e: ubuntu; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-cb568b8f204e/__TASK__2750/; /bin/sh -c ""sleep $(( 60 + (RANDOM % 20) ))""; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Image: gcr.io/hail-vdc/batch:w1eqo739af4d; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.sidecar; ; Requests:; cpu: 500m; Environment:; INSTANCE_ID: cd50b95a89914efb897965a5e982a29d; BATCH_ID: 101; JOB_ID: 2751; TOKEN: 90f13d; BATCH_BUCKET_NAME: hail-batch-3jmp5; COPY_OUTPUT_CMD: true; HAIL_POD_NAMESPACE: batch-pods; KUBERNETES_TIMEOUT_IN_SECONDS: 5.0; REFRESH_INTERVAL_IN_SECONDS: 300; POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: dking-gsa-key; Optional: false; batch-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: batch-gsa-key; Optional: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 21m default-scheduler Successfully assigned batch-pods/batch-101-job-2751-90f13d to gke-vdc-non-preemptible-pool-0106a51b-5znv; Warning OutOfcpu 21m kubelet, gke-vdc-non-preemptible-pool-0106a51b-5znv Node didn't have enough resource: cpu, requested: 600, used: 7371, capacity: 7910; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6709
https://github.com/hail-is/hail/issues/6709:2880,Energy Efficiency,schedul,scheduler,2880,"e: ubuntu; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-cb568b8f204e/__TASK__2750/; /bin/sh -c ""sleep $(( 60 + (RANDOM % 20) ))""; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Image: gcr.io/hail-vdc/batch:w1eqo739af4d; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.sidecar; ; Requests:; cpu: 500m; Environment:; INSTANCE_ID: cd50b95a89914efb897965a5e982a29d; BATCH_ID: 101; JOB_ID: 2751; TOKEN: 90f13d; BATCH_BUCKET_NAME: hail-batch-3jmp5; COPY_OUTPUT_CMD: true; HAIL_POD_NAMESPACE: batch-pods; KUBERNETES_TIMEOUT_IN_SECONDS: 5.0; REFRESH_INTERVAL_IN_SECONDS: 300; POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: dking-gsa-key; Optional: false; batch-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: batch-gsa-key; Optional: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 21m default-scheduler Successfully assigned batch-pods/batch-101-job-2751-90f13d to gke-vdc-non-preemptible-pool-0106a51b-5znv; Warning OutOfcpu 21m kubelet, gke-vdc-non-preemptible-pool-0106a51b-5znv Node didn't have enough resource: cpu, requested: 600, used: 7371, capacity: 7910; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6709
https://github.com/hail-is/hail/issues/6709:471,Integrability,Message,Message,471,"```; # k describe pod batch-101-job-2751-90f13d -n batch-pods; Name: batch-101-job-2751-90f13d; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-non-preemptible-pool-0106a51b-5znv/; Start Time: Mon, 22 Jul 2019 18:19:58 -0400; Labels: app=batch-job; batch_id=101; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=2751; user=dking; uuid=f3bd48354c404bacb3f89273ba431457; Annotations: <none>; Status: Failed; Reason: OutOfcpu; Message: Pod Node didn't have enough resource: cpu, requested: 600, used: 7371, capacity: 7910; IP: ; Init Containers:; setup:; Image: google/cloud-sdk:237.0.0-alpine; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/101/2751/90f13d/container_results && exit 1; rm -rf /io/*; true; ; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Image: ubuntu; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-cb568b8f204e/__TASK__2750/; /bin/sh -c ""sleep $(( 60 + (RANDOM % 20) ))""; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Image: gcr.io/hail-vdc/batch:w1eqo739af4d; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.sidecar; ; Requests:; cpu: 500m; Environment:; INSTANCE_ID: cd50b95a89914efb897965a5e982a29d; BATCH_ID: 101; JOB_ID: 2751; TOKEN: 90f13d; BATCH_BUCKET_NAME: hail-batch-3jmp5; COPY_OUTPUT_CMD: true; HAIL_POD_NAMESPACE: batch-pods; KUBERNETES_TIMEOUT_IN_SECONDS: 5.0; REFRE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6709
https://github.com/hail-is/hail/issues/6709:2811,Integrability,Message,Message,2811,"e: ubuntu; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-cb568b8f204e/__TASK__2750/; /bin/sh -c ""sleep $(( 60 + (RANDOM % 20) ))""; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Image: gcr.io/hail-vdc/batch:w1eqo739af4d; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.sidecar; ; Requests:; cpu: 500m; Environment:; INSTANCE_ID: cd50b95a89914efb897965a5e982a29d; BATCH_ID: 101; JOB_ID: 2751; TOKEN: 90f13d; BATCH_BUCKET_NAME: hail-batch-3jmp5; COPY_OUTPUT_CMD: true; HAIL_POD_NAMESPACE: batch-pods; KUBERNETES_TIMEOUT_IN_SECONDS: 5.0; REFRESH_INTERVAL_IN_SECONDS: 300; POD_NAME: batch-101-job-2751-90f13d (v1:metadata.name); Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: dking-gsa-key; Optional: false; batch-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: batch-gsa-key; Optional: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 21m default-scheduler Successfully assigned batch-pods/batch-101-job-2751-90f13d to gke-vdc-non-preemptible-pool-0106a51b-5znv; Warning OutOfcpu 21m kubelet, gke-vdc-non-preemptible-pool-0106a51b-5znv Node didn't have enough resource: cpu, requested: 600, used: 7371, capacity: 7910; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6709
https://github.com/hail-is/hail/pull/6713:49,Testability,log,logs,49,Addresses #6701 . This PR:. - Converts CI output logs to same format as batch by using the standard `hailtop.gear.configure_logging`. ; - Removes separate sending of ci logs to a local `ci.log` file on the ci pod. (Was anyone using this? I assume not).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6713
https://github.com/hail-is/hail/pull/6713:169,Testability,log,logs,169,Addresses #6701 . This PR:. - Converts CI output logs to same format as batch by using the standard `hailtop.gear.configure_logging`. ; - Removes separate sending of ci logs to a local `ci.log` file on the ci pod. (Was anyone using this? I assume not).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6713
https://github.com/hail-is/hail/pull/6713:189,Testability,log,log,189,Addresses #6701 . This PR:. - Converts CI output logs to same format as batch by using the standard `hailtop.gear.configure_logging`. ; - Removes separate sending of ci logs to a local `ci.log` file on the ci pod. (Was anyone using this? I assume not).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6713
https://github.com/hail-is/hail/issues/6715:674,Availability,avail,available,674,"- [x] DANN, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4341060/; - [x] CADD, http://europepmc.org/abstract/med/11384848; - [x] reference genomes 95, GRCh37 & 38., https://useast.ensembl.org/index.html; - [x] low complexity regions 95, GRCh37 & 38, https://useast.ensembl.org/index.html; - [x] Gencode (names and gene metadata) v19 GRCh37, v38 GRCh38, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3431492/, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3431492/; - [x] gnomAD LOF by gene, https://gnomad.broadinstitute.org/; - [x] gnomAD sites exomes & genomes (includes VEP, AF, etc.), https://gnomad.broadinstitute.org/; - [ ] MAF for 1KG, should be in the VCFs which are available via FTP here: http://www.internationalgenome.org/data; - [ ] VEP (?? need more information about what is requested); - [ ] which allele is ancestral (?? Elizabeth has more info?); - [X] clinvar for GRCh37 & 38; - [ ] LCR (?? is gnomAD sufficient?); - [ ] GERP or other sequence conservation score; - [ ] major exome platform capture areas (?? need more information); - [ ] All the missense annotation scores (from dbNSFP, ftp://dbnsfp.softgenetics.com/dbNSFP4.0a.zip)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6715
https://github.com/hail-is/hail/pull/6717:18,Integrability,rout,router,18,This resolves the router CrashLoopBackOff issue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6717
https://github.com/hail-is/hail/issues/6718:1842,Availability,Failure,Failure,1842,"notebook.py"", line 101, in <module>; user_table = Table(); File ""/notebook/table.py"", line 40, in __init__; self.connection_params = Table.get_secrets(); File ""/notebook/table.py"", line 24, in get_secrets; res = k8s.read_namespaced_secret('get-users', 'default'); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19441, in read_namespaced_secret; (data) = self.read_namespaced_secret_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19532, in read_namespaced_secret_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'b3dac876-d327-4129-84ca-30c0e1cb7e49', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Tue, 23 Jul 2019 18:36:54 GMT', 'Content-Length': '345'}); HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""secrets \""get-users\"" is forbidden: User \""system:serviceaccount:pr-6531-default-p84s51gcoauw:notebook\"" cannot get resource \""secrets\"" in API group \""\"" in the namespace \""default\"""",""reason"":""Forbidden"",""details"":{""name"":""get-users"",""kind"":""secrets""},""code"":403}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6718
https://github.com/hail-is/hail/issues/6718:1852,Integrability,message,message,1852,"notebook.py"", line 101, in <module>; user_table = Table(); File ""/notebook/table.py"", line 40, in __init__; self.connection_params = Table.get_secrets(); File ""/notebook/table.py"", line 24, in get_secrets; res = k8s.read_namespaced_secret('get-users', 'default'); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19441, in read_namespaced_secret; (data) = self.read_namespaced_secret_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19532, in read_namespaced_secret_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'b3dac876-d327-4129-84ca-30c0e1cb7e49', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Tue, 23 Jul 2019 18:36:54 GMT', 'Content-Length': '345'}); HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""secrets \""get-users\"" is forbidden: User \""system:serviceaccount:pr-6531-default-p84s51gcoauw:notebook\"" cannot get resource \""secrets\"" in API group \""\"" in the namespace \""default\"""",""reason"":""Forbidden"",""details"":{""name"":""get-users"",""kind"":""secrets""},""code"":403}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6718
https://github.com/hail-is/hail/issues/6718:1571,Security,Audit,Audit-Id,1571,"notebook.py"", line 101, in <module>; user_table = Table(); File ""/notebook/table.py"", line 40, in __init__; self.connection_params = Table.get_secrets(); File ""/notebook/table.py"", line 24, in get_secrets; res = k8s.read_namespaced_secret('get-users', 'default'); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19441, in read_namespaced_secret; (data) = self.read_namespaced_secret_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19532, in read_namespaced_secret_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'b3dac876-d327-4129-84ca-30c0e1cb7e49', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Tue, 23 Jul 2019 18:36:54 GMT', 'Content-Length': '345'}); HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""secrets \""get-users\"" is forbidden: User \""system:serviceaccount:pr-6531-default-p84s51gcoauw:notebook\"" cannot get resource \""secrets\"" in API group \""\"" in the namespace \""default\"""",""reason"":""Forbidden"",""details"":{""name"":""get-users"",""kind"":""secrets""},""code"":403}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6718
https://github.com/hail-is/hail/issues/6718:9,Testability,log,logs,9,"```; # k logs --tail=999999999 -n pr-6531-default-p84s51gcoauw notebook2-84cc9c889-8lsvc; Traceback (most recent call last):; File ""notebook.py"", line 101, in <module>; user_table = Table(); File ""/notebook/table.py"", line 40, in __init__; self.connection_params = Table.get_secrets(); File ""/notebook/table.py"", line 24, in get_secrets; res = k8s.read_namespaced_secret('get-users', 'default'); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19441, in read_namespaced_secret; (data) = self.read_namespaced_secret_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 19532, in read_namespaced_secret_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'b3dac876-d327-4129-84ca-30c0e1cb7e49', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Tue, 23 Jul 2019 18:36:54 GMT', 'Content-Length': '345'}); HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""secrets \""get-users\"" is forbidden: User \""system:serviceaccount:pr-6531-default-p84s51gcoauw:notebook\"" cannot get resource \""secrets\"" in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6718
https://github.com/hail-is/hail/issues/6720:807,Modifiability,Sandbox,Sandbox,807,"# SRE; - The Google SRE Books https://landing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:819,Modifiability,Portab,Portable,819,"# SRE; - The Google SRE Books https://landing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1753,Modifiability,Sandbox,Sandbox,1753,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1892,Modifiability,Sandbox,Sandboxed,1892,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:2009,Modifiability,sandbox,sandboxed-container-technologies,2009,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:717,Security,Authoriz,Authorization,717,"# SRE; - The Google SRE Books https://landing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:990,Security,XSS,XSS,990,"# SRE; - The Google SRE Books https://landing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1048,Security,XSS,XSS,1048,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1330,Security,Secur,Security,1330,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1371,Security,secur,security,1371,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1380,Security,secur,security,1380,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1401,Security,secur,security,1401,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1462,Security,secur,security-of-containers-,1462,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1604,Security,secur,secure-enough-a-k-a-subverting-posix-capabilities-,1604,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1771,Security,secur,security,1771,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:807,Testability,Sandbox,Sandbox,807,"# SRE; - The Google SRE Books https://landing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1753,Testability,Sandbox,Sandbox,1753,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1892,Testability,Sandbox,Sandboxed,1892,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:2009,Testability,sandbox,sandboxed-container-technologies,2009,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:195,Usability,Learn,Learning,195,"# SRE; - The Google SRE Books https://landing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:271,Usability,learn,learning,271,"# SRE; - The Google SRE Books https://landing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1091,Usability,Guid,Guide,1091,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/issues/6720:1299,Usability,guid,guide,1299,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720
https://github.com/hail-is/hail/pull/6730:260,Integrability,interface,interface,260,"Histograms are basically a group_by on the bin index. The implementation is a tad more complex than I would have hoped, but I think it will be nice to not have to maintain an extra aggregator. (Actually, I think if we break apart the array elements aggregator interface in the right way, hl.agg.hist might be written pretty naturally in terms of that. But that may need to wait until the old aggregators are gone.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6730
https://github.com/hail-is/hail/pull/6731:33,Deployability,deploy,deploy,33,Without this I cannot see my dev deploy batches.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6731
https://github.com/hail-is/hail/pull/6736:17,Availability,error,error,17,also changes the error message to display `<<<empty key>>>` as the list of keys. fixes #6663,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6736
https://github.com/hail-is/hail/pull/6736:23,Integrability,message,message,23,also changes the error message to display `<<<empty key>>>` as the list of keys. fixes #6663,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6736
https://github.com/hail-is/hail/issues/6737:316,Availability,error,error,316,"If a job is ""Running"", but the pod will always enter ""CrashLoopBackOff"" (due to a bad image), it will never finish. If a batch is deleted, the job is not marked cancelled so it is returned at the top of the refresh loop, but when the updated job is looked up using `undeleted` records, it is missing. This causes an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6737
https://github.com/hail-is/hail/issues/6737:234,Deployability,update,updated,234,"If a job is ""Running"", but the pod will always enter ""CrashLoopBackOff"" (due to a bad image), it will never finish. If a batch is deleted, the job is not marked cancelled so it is returned at the top of the refresh loop, but when the updated job is looked up using `undeleted` records, it is missing. This causes an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6737
https://github.com/hail-is/hail/issues/6738:89,Deployability,install,installer,89,"Kyle brought this up. `conda activate` doesn't actually work, unless you use the package installer, or manually append conda.sh to your path.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6738
https://github.com/hail-is/hail/pull/6739:95,Deployability,install,installation,95,"Miniconda definitely includes the necessary bash_profile modifications, and provides a smaller installation (no unneeded deps): 46.2MB for Mac cli installer, vs 435MB. Tried to make documentation a bit more readable (whitespace) and explicit. <img width=""713"" alt=""Screenshot 2019-07-25 15 05 59"" src=""https://user-images.githubusercontent.com/5543229/61901550-045a7f80-aeee-11e9-89a7-2dd8cc8a27ca.png"">. cc @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6739
https://github.com/hail-is/hail/pull/6739:147,Deployability,install,installer,147,"Miniconda definitely includes the necessary bash_profile modifications, and provides a smaller installation (no unneeded deps): 46.2MB for Mac cli installer, vs 435MB. Tried to make documentation a bit more readable (whitespace) and explicit. <img width=""713"" alt=""Screenshot 2019-07-25 15 05 59"" src=""https://user-images.githubusercontent.com/5543229/61901550-045a7f80-aeee-11e9-89a7-2dd8cc8a27ca.png"">. cc @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6739
https://github.com/hail-is/hail/pull/6740:209,Availability,error,errors,209,"The sources for agg transformations weren't being recorded correctly for transformations of hl.agg.count(). This wasn't a problem for e.g. a single-level hl.agg.group_by(key, hl.agg.count()), but was throwing errors when that was nested within other transformations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6740
https://github.com/hail-is/hail/pull/6743:118,Availability,failure,failures,118,Nested array element aggregations weren't working. This fixes it and adds a test for nested ArrayAggs. Caught by test failures from #6698.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6743
https://github.com/hail-is/hail/pull/6743:76,Testability,test,test,76,Nested array element aggregations weren't working. This fixes it and adds a test for nested ArrayAggs. Caught by test failures from #6698.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6743
https://github.com/hail-is/hail/pull/6743:113,Testability,test,test,113,Nested array element aggregations weren't working. This fixes it and adds a test for nested ArrayAggs. Caught by test failures from #6698.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6743
https://github.com/hail-is/hail/pull/6744:100,Usability,simpl,simplifies,100,"Focus on readability of the tutorial section. Removes Expressions from Tutorials (moves to How-To), simplifies Table tutorial (I don't think users need to care about Expressions at the outset, they should just know how to manipulate the major structures). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6744
https://github.com/hail-is/hail/pull/6746:86,Availability,alive,alive,86,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6746
https://github.com/hail-is/hail/pull/6746:187,Availability,alive,alive,187,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6746
https://github.com/hail-is/hail/pull/6746:209,Availability,alive,alive,209,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6746
https://github.com/hail-is/hail/pull/6746:141,Integrability,message,message,141,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6746
https://github.com/hail-is/hail/pull/6746:244,Integrability,message,message,244,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6746
https://github.com/hail-is/hail/pull/6746:419,Testability,Log,LogStore,419,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6746
https://github.com/hail-is/hail/pull/6746:462,Testability,log,logs,462,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6746
https://github.com/hail-is/hail/pull/6746:322,Usability,simpl,simpler,322,"Changes:; - four containers: setup container, main container, cleanup container, keep alive container; - cleanup container waits for an HTTP message from batch before cleaning up; - keep alive container stays alive until batch sends it an HTTP message (this prevents terminated pod GC); - split `mark_complete` into three simpler methods; - extract several parts of former `mark_complete` into named helper methods; - `LogStore.results_filename` is gone, if the logs are present, the pod has already been run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6746
https://github.com/hail-is/hail/issues/6747:672,Availability,error,error,672,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Hello, I am having problems to read vcf file using hail. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz'); py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply. : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6747
https://github.com/hail-is/hail/issues/6747:352,Deployability,install,installed,352,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Hello, I am having problems to read vcf file using hail. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz'); py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply. : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6747
https://github.com/hail-is/hail/issues/6747:491,Deployability,install,installed,491,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Hello, I am having problems to read vcf file using hail. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz'); py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply. : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6747
https://github.com/hail-is/hail/issues/6747:645,Integrability,protocol,protocol,645,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Hello, I am having problems to read vcf file using hail. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz'); py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply. : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6747
https://github.com/hail-is/hail/issues/6747:525,Performance,load,load,525,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Hello, I am having problems to read vcf file using hail. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz'); py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply. : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6747
https://github.com/hail-is/hail/issues/6750:159,Availability,ERROR,ERROR,159,"```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1263"", ""message"": ""received event ERROR None""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1266"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1264, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1248, in pod_changed\n job = await Job.from_k8s_labels(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 388, in from_k8s_labels\n batch_id = pod.metadata.labels['batch_id']\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:44,287"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.34.1 [26/Jul/2019:23:46:44 +0000] \""GET /healthcheck HTTP/1.1\"" 200 177 \""-\"" \""kube-probe/1.13+\"""", ""remote_address"": ""10.32.34.1"", ""request_start_time"": ""[26/Jul/2019:23:46:44 +0000]"", ""first_request_line"": ""GET /healthcheck HTTP/1.1"", ""response_status"": 200, ""response_size"": 177, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""kube-probe/1.13+""}}; ```. why even send this 🤷‍♀",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6750
https://github.com/hail-is/hail/issues/6750:188,Availability,ERROR,ERROR,188,"```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1263"", ""message"": ""received event ERROR None""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1266"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1264, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1248, in pod_changed\n job = await Job.from_k8s_labels(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 388, in from_k8s_labels\n batch_id = pod.metadata.labels['batch_id']\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:44,287"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.34.1 [26/Jul/2019:23:46:44 +0000] \""GET /healthcheck HTTP/1.1\"" 200 177 \""-\"" \""kube-probe/1.13+\"""", ""remote_address"": ""10.32.34.1"", ""request_start_time"": ""[26/Jul/2019:23:46:44 +0000]"", ""first_request_line"": ""GET /healthcheck HTTP/1.1"", ""response_status"": 200, ""response_size"": 177, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""kube-probe/1.13+""}}; ```. why even send this 🤷‍♀",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6750
https://github.com/hail-is/hail/issues/6750:133,Integrability,message,message,133,"```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1263"", ""message"": ""received event ERROR None""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1266"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1264, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1248, in pod_changed\n job = await Job.from_k8s_labels(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 388, in from_k8s_labels\n batch_id = pod.metadata.labels['batch_id']\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:44,287"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.34.1 [26/Jul/2019:23:46:44 +0000] \""GET /healthcheck HTTP/1.1\"" 200 177 \""-\"" \""kube-probe/1.13+\"""", ""remote_address"": ""10.32.34.1"", ""request_start_time"": ""[26/Jul/2019:23:46:44 +0000]"", ""first_request_line"": ""GET /healthcheck HTTP/1.1"", ""response_status"": 200, ""response_size"": 177, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""kube-probe/1.13+""}}; ```. why even send this 🤷‍♀",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6750
https://github.com/hail-is/hail/issues/6750:302,Integrability,message,message,302,"```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1263"", ""message"": ""received event ERROR None""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1266"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1264, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1248, in pod_changed\n job = await Job.from_k8s_labels(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 388, in from_k8s_labels\n batch_id = pod.metadata.labels['batch_id']\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:44,287"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.34.1 [26/Jul/2019:23:46:44 +0000] \""GET /healthcheck HTTP/1.1\"" 200 177 \""-\"" \""kube-probe/1.13+\"""", ""remote_address"": ""10.32.34.1"", ""request_start_time"": ""[26/Jul/2019:23:46:44 +0000]"", ""first_request_line"": ""GET /healthcheck HTTP/1.1"", ""response_status"": 200, ""response_size"": 177, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""kube-probe/1.13+""}}; ```. why even send this 🤷‍♀",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6750
https://github.com/hail-is/hail/issues/6750:1000,Integrability,message,message,1000,"```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1263"", ""message"": ""received event ERROR None""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1266"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1264, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1248, in pod_changed\n job = await Job.from_k8s_labels(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 388, in from_k8s_labels\n batch_id = pod.metadata.labels['batch_id']\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:44,287"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.34.1 [26/Jul/2019:23:46:44 +0000] \""GET /healthcheck HTTP/1.1\"" 200 177 \""-\"" \""kube-probe/1.13+\"""", ""remote_address"": ""10.32.34.1"", ""request_start_time"": ""[26/Jul/2019:23:46:44 +0000]"", ""first_request_line"": ""GET /healthcheck HTTP/1.1"", ""response_status"": 200, ""response_size"": 177, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""kube-probe/1.13+""}}; ```. why even send this 🤷‍♀",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6750
https://github.com/hail-is/hail/issues/6750:989,Testability,log,log,989,"```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1263"", ""message"": ""received event ERROR None""}; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-26 23:46:41,095"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1266"", ""message"": ""k8s event stream failed due to: 'NoneType' object is not subscriptable"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1264, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1248, in pod_changed\n job = await Job.from_k8s_labels(pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 388, in from_k8s_labels\n batch_id = pod.metadata.labels['batch_id']\nTypeError: 'NoneType' object is not subscriptable""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-26 23:46:44,287"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.34.1 [26/Jul/2019:23:46:44 +0000] \""GET /healthcheck HTTP/1.1\"" 200 177 \""-\"" \""kube-probe/1.13+\"""", ""remote_address"": ""10.32.34.1"", ""request_start_time"": ""[26/Jul/2019:23:46:44 +0000]"", ""first_request_line"": ""GET /healthcheck HTTP/1.1"", ""response_status"": 200, ""response_size"": 177, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""kube-probe/1.13+""}}; ```. why even send this 🤷‍♀",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6750
https://github.com/hail-is/hail/issues/6753:20,Availability,ERROR,ERROR,20,"```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:31:18,857"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1270"", ""message"": ""k8s event stream failed due to: "", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1268, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1253, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1195, in update_job_with_pod\n err = await app['k8s'].delete_pod(name=pod.metadata.name)\n File \""/usr/local/lib/python3.6/dist-packages/batch/k8s.py\"", line 46, in delete_pod\n assert name is not None\nAssertionError""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6753
https://github.com/hail-is/hail/issues/6753:134,Integrability,message,message,134,"```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:31:18,857"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1270"", ""message"": ""k8s event stream failed due to: "", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1268, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1253, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1195, in update_job_with_pod\n err = await app['k8s'].delete_pod(name=pod.metadata.name)\n File \""/usr/local/lib/python3.6/dist-packages/batch/k8s.py\"", line 46, in delete_pod\n assert name is not None\nAssertionError""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6753
https://github.com/hail-is/hail/issues/6753:731,Testability,assert,assert,731,"```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:31:18,857"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""kube_event_loop:1270"", ""message"": ""k8s event stream failed due to: "", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1268, in kube_event_loop\n await pod_changed(object)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1253, in pod_changed\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1195, in update_job_with_pod\n err = await app['k8s'].delete_pod(name=pod.metadata.name)\n File \""/usr/local/lib/python3.6/dist-packages/batch/k8s.py\"", line 46, in delete_pod\n assert name is not None\nAssertionError""}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6753
https://github.com/hail-is/hail/issues/6754:32,Availability,ERROR,ERROR,32,"batch log:. ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:35:40,389"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""polling_event_loop:1343"", ""message"": ""Could not poll due to exception: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 159, in _new_conn\n (self._dns_host, self.port), self.timeout, **extra_kw)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 80, in create_connection\n raise err\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 70, in create_connection\n sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 600, in urlopen\n chunked=chunked)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 354, in _make_request\n conn.request(method, url, **httplib_request_kw)\n File \""/usr/lib/python3.6/http/client.py\"", line 1239, in request\n self._send_request(method, url, body, headers, encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:2800,Availability,error,error,2800,"send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:2488,Energy Efficiency,adapt,adapters,2488,"nt.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n Fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:4427,Energy Efficiency,adapt,adapter,4427,": / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 594, in mark_complete\n requests.post(f'http://{pod.status.pod_ip}:5001/')\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 116, in post\n return request('post', url, data=data, json=json, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 60, in request\n return session.request(method=method, url=url, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 533, in request\n resp = self.send(prep, **send_kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 646, in send\n r = adapter.send(request, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 516, in send\n raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))""}; ```. I think this was probably already reaped by the other loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:4516,Energy Efficiency,adapt,adapters,4516,": / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 594, in mark_complete\n requests.post(f'http://{pod.status.pod_ip}:5001/')\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 116, in post\n return request('post', url, data=data, json=json, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 60, in request\n return session.request(method=method, url=url, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 533, in request\n resp = self.send(prep, **send_kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 646, in send\n r = adapter.send(request, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 516, in send\n raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))""}; ```. I think this was probably already reaped by the other loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:149,Integrability,message,message,149,"batch log:. ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:35:40,389"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""polling_event_loop:1343"", ""message"": ""Could not poll due to exception: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 159, in _new_conn\n (self._dns_host, self.port), self.timeout, **extra_kw)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 80, in create_connection\n raise err\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 70, in create_connection\n sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 600, in urlopen\n chunked=chunked)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 354, in _make_request\n conn.request(method, url, **httplib_request_kw)\n File \""/usr/lib/python3.6/http/client.py\"", line 1239, in request\n self._send_request(method, url, body, headers, encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:423,Integrability,rout,route,423,"batch log:. ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:35:40,389"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""polling_event_loop:1343"", ""message"": ""Could not poll due to exception: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 159, in _new_conn\n (self._dns_host, self.port), self.timeout, **extra_kw)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 80, in create_connection\n raise err\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 70, in create_connection\n sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 600, in urlopen\n chunked=chunked)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 354, in _make_request\n conn.request(method, url, **httplib_request_kw)\n File \""/usr/lib/python3.6/http/client.py\"", line 1239, in request\n self._send_request(method, url, body, headers, encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:916,Integrability,rout,route,916,"batch log:. ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:35:40,389"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""polling_event_loop:1343"", ""message"": ""Could not poll due to exception: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 159, in _new_conn\n (self._dns_host, self.port), self.timeout, **extra_kw)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 80, in create_connection\n raise err\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 70, in create_connection\n sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 600, in urlopen\n chunked=chunked)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 354, in _make_request\n conn.request(method, url, **httplib_request_kw)\n File \""/usr/lib/python3.6/http/client.py\"", line 1239, in request\n self._send_request(method, url, body, headers, encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:2308,Integrability,rout,route,2308,"http/client.py\"", line 1239, in request\n self._send_request(method, url, body, headers, encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:2488,Integrability,adapter,adapters,2488,"nt.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n Fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:3096,Integrability,rout,route,3096,"ew connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 594, in mark_complete\n requests.post(f'http://{pod.status.pod_ip}:5001/')\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 116, in post\n return request('post', url, data=data, json=json, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 60, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:4427,Integrability,adapter,adapter,4427,": / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 594, in mark_complete\n requests.post(f'http://{pod.status.pod_ip}:5001/')\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 116, in post\n return request('post', url, data=data, json=json, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 60, in request\n return session.request(method=method, url=url, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 533, in request\n resp = self.send(prep, **send_kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 646, in send\n r = adapter.send(request, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 516, in send\n raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))""}; ```. I think this was probably already reaped by the other loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:4516,Integrability,adapter,adapters,4516,": / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 594, in mark_complete\n requests.post(f'http://{pod.status.pod_ip}:5001/')\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 116, in post\n return request('post', url, data=data, json=json, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 60, in request\n return session.request(method=method, url=url, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 533, in request\n resp = self.send(prep, **send_kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 646, in send\n r = adapter.send(request, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 516, in send\n raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))""}; ```. I think this was probably already reaped by the other loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:4861,Integrability,rout,route,4861,": / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 594, in mark_complete\n requests.post(f'http://{pod.status.pod_ip}:5001/')\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 116, in post\n return request('post', url, data=data, json=json, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 60, in request\n return session.request(method=method, url=url, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 533, in request\n resp = self.send(prep, **send_kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 646, in send\n r = adapter.send(request, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 516, in send\n raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))""}; ```. I think this was probably already reaped by the other loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:2488,Modifiability,adapt,adapters,2488,"nt.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n Fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:4427,Modifiability,adapt,adapter,4427,": / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 594, in mark_complete\n requests.post(f'http://{pod.status.pod_ip}:5001/')\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 116, in post\n return request('post', url, data=data, json=json, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 60, in request\n return session.request(method=method, url=url, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 533, in request\n resp = self.send(prep, **send_kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 646, in send\n r = adapter.send(request, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 516, in send\n raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))""}; ```. I think this was probably already reaped by the other loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:4516,Modifiability,adapt,adapters,4516,": / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1296, in refresh_k8s_pods\n await update_job_with_pod(job, pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1229, in update_job_with_pod\n await job.mark_complete(success=True, pod=pod)\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 594, in mark_complete\n requests.post(f'http://{pod.status.pod_ip}:5001/')\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 116, in post\n return request('post', url, data=data, json=json, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/api.py\"", line 60, in request\n return session.request(method=method, url=url, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 533, in request\n resp = self.send(prep, **send_kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/sessions.py\"", line 646, in send\n r = adapter.send(request, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 516, in send\n raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))""}; ```. I think this was probably already reaped by the other loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:623,Safety,timeout,timeout,623,"batch log:. ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:35:40,389"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""polling_event_loop:1343"", ""message"": ""Could not poll due to exception: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 159, in _new_conn\n (self._dns_host, self.port), self.timeout, **extra_kw)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 80, in create_connection\n raise err\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 70, in create_connection\n sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 600, in urlopen\n chunked=chunked)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 354, in _make_request\n conn.request(method, url, **httplib_request_kw)\n File \""/usr/lib/python3.6/http/client.py\"", line 1239, in request\n self._send_request(method, url, body, headers, encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:2523,Safety,timeout,timeout,2523,"(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:2531,Safety,timeout,timeout,2531,"(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6754:6,Testability,log,log,6,"batch log:. ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-07-29 12:35:40,389"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""polling_event_loop:1343"", ""message"": ""Could not poll due to exception: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 159, in _new_conn\n (self._dns_host, self.port), self.timeout, **extra_kw)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 80, in create_connection\n raise err\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\"", line 70, in create_connection\n sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 600, in urlopen\n chunked=chunked)\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 354, in _make_request\n conn.request(method, url, **httplib_request_kw)\n File \""/usr/lib/python3.6/http/client.py\"", line 1239, in request\n self._send_request(method, url, body, headers, encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6754
https://github.com/hail-is/hail/issues/6756:391,Availability,error,errors,391,"> Tim Poterba: I think we need to kick up the local disk size requested when running VEP. > Tim Poterba: we were talking about this here https://discuss.hail.is/t/dataproc-workers-lost-after-intensive-task/1014/34. > Kyle Satterstrom: Interesting, thanks for pointing that out. Yeah, I had been using workers with 40GB boot disks (and no attached SSDs). I had looked in the log and seen the errors that said ""no space left on device"", but I guess I didn't really believe it would fill up all that space at once. I can try increasing the 40 to 100 and see if it helps. > Tim Poterba: that unblocked Kevin on VEP!. > Tim Poterba: and don't try to repartition, I think. > Kyle Satterstrom: Just an update -- I tried the same thing again with 100GB worker boot disks intead of 40GB, and it made more progress but ultimately failed again hail-20190726-1716-0.2.16-6da0d3571629.log. > Kyle Satterstrom: One more update -- I tried the same thing again but with a local SSD attached to each machine instead of increasing the boot disk size (so worker boot disks were 40 GB), and that worked!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6756
https://github.com/hail-is/hail/issues/6756:695,Deployability,update,update,695,"> Tim Poterba: I think we need to kick up the local disk size requested when running VEP. > Tim Poterba: we were talking about this here https://discuss.hail.is/t/dataproc-workers-lost-after-intensive-task/1014/34. > Kyle Satterstrom: Interesting, thanks for pointing that out. Yeah, I had been using workers with 40GB boot disks (and no attached SSDs). I had looked in the log and seen the errors that said ""no space left on device"", but I guess I didn't really believe it would fill up all that space at once. I can try increasing the 40 to 100 and see if it helps. > Tim Poterba: that unblocked Kevin on VEP!. > Tim Poterba: and don't try to repartition, I think. > Kyle Satterstrom: Just an update -- I tried the same thing again with 100GB worker boot disks intead of 40GB, and it made more progress but ultimately failed again hail-20190726-1716-0.2.16-6da0d3571629.log. > Kyle Satterstrom: One more update -- I tried the same thing again but with a local SSD attached to each machine instead of increasing the boot disk size (so worker boot disks were 40 GB), and that worked!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6756
https://github.com/hail-is/hail/issues/6756:906,Deployability,update,update,906,"> Tim Poterba: I think we need to kick up the local disk size requested when running VEP. > Tim Poterba: we were talking about this here https://discuss.hail.is/t/dataproc-workers-lost-after-intensive-task/1014/34. > Kyle Satterstrom: Interesting, thanks for pointing that out. Yeah, I had been using workers with 40GB boot disks (and no attached SSDs). I had looked in the log and seen the errors that said ""no space left on device"", but I guess I didn't really believe it would fill up all that space at once. I can try increasing the 40 to 100 and see if it helps. > Tim Poterba: that unblocked Kevin on VEP!. > Tim Poterba: and don't try to repartition, I think. > Kyle Satterstrom: Just an update -- I tried the same thing again with 100GB worker boot disks intead of 40GB, and it made more progress but ultimately failed again hail-20190726-1716-0.2.16-6da0d3571629.log. > Kyle Satterstrom: One more update -- I tried the same thing again but with a local SSD attached to each machine instead of increasing the boot disk size (so worker boot disks were 40 GB), and that worked!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6756
https://github.com/hail-is/hail/issues/6756:374,Testability,log,log,374,"> Tim Poterba: I think we need to kick up the local disk size requested when running VEP. > Tim Poterba: we were talking about this here https://discuss.hail.is/t/dataproc-workers-lost-after-intensive-task/1014/34. > Kyle Satterstrom: Interesting, thanks for pointing that out. Yeah, I had been using workers with 40GB boot disks (and no attached SSDs). I had looked in the log and seen the errors that said ""no space left on device"", but I guess I didn't really believe it would fill up all that space at once. I can try increasing the 40 to 100 and see if it helps. > Tim Poterba: that unblocked Kevin on VEP!. > Tim Poterba: and don't try to repartition, I think. > Kyle Satterstrom: Just an update -- I tried the same thing again with 100GB worker boot disks intead of 40GB, and it made more progress but ultimately failed again hail-20190726-1716-0.2.16-6da0d3571629.log. > Kyle Satterstrom: One more update -- I tried the same thing again but with a local SSD attached to each machine instead of increasing the boot disk size (so worker boot disks were 40 GB), and that worked!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6756
https://github.com/hail-is/hail/issues/6756:872,Testability,log,log,872,"> Tim Poterba: I think we need to kick up the local disk size requested when running VEP. > Tim Poterba: we were talking about this here https://discuss.hail.is/t/dataproc-workers-lost-after-intensive-task/1014/34. > Kyle Satterstrom: Interesting, thanks for pointing that out. Yeah, I had been using workers with 40GB boot disks (and no attached SSDs). I had looked in the log and seen the errors that said ""no space left on device"", but I guess I didn't really believe it would fill up all that space at once. I can try increasing the 40 to 100 and see if it helps. > Tim Poterba: that unblocked Kevin on VEP!. > Tim Poterba: and don't try to repartition, I think. > Kyle Satterstrom: Just an update -- I tried the same thing again with 100GB worker boot disks intead of 40GB, and it made more progress but ultimately failed again hail-20190726-1716-0.2.16-6da0d3571629.log. > Kyle Satterstrom: One more update -- I tried the same thing again but with a local SSD attached to each machine instead of increasing the boot disk size (so worker boot disks were 40 GB), and that worked!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6756
https://github.com/hail-is/hail/pull/6759:621,Performance,load,load,621,"The Encoder/Decoder methods weren't handling non-struct values correctly (this is mostly fine from an Encoder/Decoder perspective, since they don't handle non-struct/array values). I fixed this in #6727 because I wanted to encode arbitrary values, and changed EmitPackDecoder/EmitPackEncoder to handle arbitrary values so that I could test this. I also pulled out some more peripheral changes from that PR, mostly defining some (currently unused, untested) methods like `Code.orEmpty` and a version of `newMethod` that lets you give the method a real name for ease of debugging, as well as some more changes to move more load/store methods off of region instances and onto the Region object.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6759
https://github.com/hail-is/hail/pull/6759:335,Testability,test,test,335,"The Encoder/Decoder methods weren't handling non-struct values correctly (this is mostly fine from an Encoder/Decoder perspective, since they don't handle non-struct/array values). I fixed this in #6727 because I wanted to encode arbitrary values, and changed EmitPackDecoder/EmitPackEncoder to handle arbitrary values so that I could test this. I also pulled out some more peripheral changes from that PR, mostly defining some (currently unused, untested) methods like `Code.orEmpty` and a version of `newMethod` that lets you give the method a real name for ease of debugging, as well as some more changes to move more load/store methods off of region instances and onto the Region object.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6759
https://github.com/hail-is/hail/pull/6760:440,Usability,usab,usable,440,"Unfortunately, Google changed the way to connect to Spark UI, and now makes you either go through the YARN UI or use the Spark history page. This PR changes our Spark UI commands in `hailctl dataproc connect` to use the spark history page instead, since that works. The regular `spark-ui` command will show you running jobs, the `spark-history` command will show you finished jobs. I don't know what ui1 and ui2 were, but they're no longer usable so I removed them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6760
https://github.com/hail-is/hail/pull/6761:5,Deployability,update,update,5,Same update as on homepage.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6761
https://github.com/hail-is/hail/issues/6762:9,Deployability,install,installed,9,"Relevant installed library versions:; hail==0.2.18; numpy==1.17.0; pandas==0.23.4; pyspark==2.4.1; scipy==1.3.0. Code ran:; ```; import hail as hl. Stack trace:; AttributeError: module 'hail' has no attribute 'expr'. AttributeError Traceback (most recent call last); in engine; ----> 1 import hail as hl. /home/cdsw/.local/lib/python3.6/site-packages/hail/__init__.py in <module>(); 29 from .expr import *; 30 from .genetics import *; ---> 31 from .methods import *; 32 from . import genetics; 33 from . import methods. /home/cdsw/.local/lib/python3.6/site-packages/hail/methods/__init__.py in <module>(); 4 import_plink, read_matrix_table, read_table, get_vcf_metadata, import_vcf, import_vcfs, \; 5 index_bgen, import_matrix_table; ----> 6 from .statgen import skat, identity_by_descent, impute_sex, \; 7 genetic_relatedness_matrix, realized_relationship_matrix, pca, \; 8 hwe_normalized_pca, pc_relate, split_multi, filter_alleles, filter_alleles_hts, \. /home/cdsw/.local/lib/python3.6/site-packages/hail/methods/statgen.py in <module>(); 5 ; 6 import hail as hl; ----> 7 import hail.expr.aggregators as agg; 8 from hail.expr.expressions import *; 9 from hail.expr.types import *. AttributeError: module 'hail' has no attribute 'expr'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762
https://github.com/hail-is/hail/issues/6766:208,Availability,Error,Error,208,"```python; import hail as hl; hl.init(); hl.utils.get_1kg('data'); mt = hl.read_matrix_table('data/1kg.mt'); mt.entries().show(10); df = mt.entries().to_pandas(); ```. ```; Hail version: 0.2.18-08ec699f0fd4; Error summary: HailException: optimization changed type!; before: Table{global:Struct{},key:[],row:Struct{`locus.contig`:String,`locus.position`:Int32,alleles:Array[String],rsid:String,qual:Float64,filters:Array[String],`info.AC`:Array[Int32],`info.AF`:Array[Float64],`info.AN`:Int32,`info.BaseQRankSum`:Float64,`info.ClippingRankSum`:Float64,`info.DP`:Int32,`info.DS`:Boolean,`info.FS`:Float64,`info.HaplotypeScore`:Float64,`info.InbreedingCoeff`:Float64,`info.MLEAC`:Array[Int32],`info.MLEAF`:Array[Float64],`info.MQ`:Float64,`info.MQ0`:Int32,`info.MQRankSum`:Float64,`info.QD`:Float64,`info.ReadPosRankSum`:Float64,`info.set`:String,s:String,`GT.alleles`:Array[Int32],`GT.phased`:Boolean,AD:Array[Int32],DP:Int32,GQ:Int32,PL:Array[Int32]}}; after: Table{global:Struct{},key:[],row:Struct{`locus.contig`:String,`locus.position`:Int32,alleles:Array[String],rsid:String,qual:Float64,filters:Array[String],`info.AC`:Array[Int32],`info.AF`:Array[Float64],`info.AN`:Int32,`info.BaseQRankSum`:Float64,`info.ClippingRankSum`:Float64,`info.DP`:Int32,`info.DS`:Boolean,`info.FS`:Float64,`info.HaplotypeScore`:Float64,`info.InbreedingCoeff`:Float64,`info.MLEAC`:Array[Int32],`info.MLEAF`:Array[Float64],`info.MQ`:Float64,`info.MQ0`:Int32,`info.MQRankSum`:Float64,`info.QD`:Float64,`info.ReadPosRankSum`:Float64,`info.set`:String,s:String,`GT.alleles`:Array[Int32],`GT.phased`:Boolean,AD:Array[+Int32],DP:Int32,GQ:Int32,PL:Array[+Int32]}}; ```. Randomly assigned @catoverdrive, cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6766
https://github.com/hail-is/hail/issues/6766:238,Performance,optimiz,optimization,238,"```python; import hail as hl; hl.init(); hl.utils.get_1kg('data'); mt = hl.read_matrix_table('data/1kg.mt'); mt.entries().show(10); df = mt.entries().to_pandas(); ```. ```; Hail version: 0.2.18-08ec699f0fd4; Error summary: HailException: optimization changed type!; before: Table{global:Struct{},key:[],row:Struct{`locus.contig`:String,`locus.position`:Int32,alleles:Array[String],rsid:String,qual:Float64,filters:Array[String],`info.AC`:Array[Int32],`info.AF`:Array[Float64],`info.AN`:Int32,`info.BaseQRankSum`:Float64,`info.ClippingRankSum`:Float64,`info.DP`:Int32,`info.DS`:Boolean,`info.FS`:Float64,`info.HaplotypeScore`:Float64,`info.InbreedingCoeff`:Float64,`info.MLEAC`:Array[Int32],`info.MLEAF`:Array[Float64],`info.MQ`:Float64,`info.MQ0`:Int32,`info.MQRankSum`:Float64,`info.QD`:Float64,`info.ReadPosRankSum`:Float64,`info.set`:String,s:String,`GT.alleles`:Array[Int32],`GT.phased`:Boolean,AD:Array[Int32],DP:Int32,GQ:Int32,PL:Array[Int32]}}; after: Table{global:Struct{},key:[],row:Struct{`locus.contig`:String,`locus.position`:Int32,alleles:Array[String],rsid:String,qual:Float64,filters:Array[String],`info.AC`:Array[Int32],`info.AF`:Array[Float64],`info.AN`:Int32,`info.BaseQRankSum`:Float64,`info.ClippingRankSum`:Float64,`info.DP`:Int32,`info.DS`:Boolean,`info.FS`:Float64,`info.HaplotypeScore`:Float64,`info.InbreedingCoeff`:Float64,`info.MLEAC`:Array[Int32],`info.MLEAF`:Array[Float64],`info.MQ`:Float64,`info.MQ0`:Int32,`info.MQRankSum`:Float64,`info.QD`:Float64,`info.ReadPosRankSum`:Float64,`info.set`:String,s:String,`GT.alleles`:Array[Int32],`GT.phased`:Boolean,AD:Array[+Int32],DP:Int32,GQ:Int32,PL:Array[+Int32]}}; ```. Randomly assigned @catoverdrive, cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6766
https://github.com/hail-is/hail/pull/6768:40,Energy Efficiency,Schedul,SchedulerSuite,40,The PR adds support for skipping Scala `SchedulerSuite` unit tests by setting a `HAIL_TEST_SKIP_SCHEDULER` environment variable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6768
https://github.com/hail-is/hail/pull/6768:119,Modifiability,variab,variable,119,The PR adds support for skipping Scala `SchedulerSuite` unit tests by setting a `HAIL_TEST_SKIP_SCHEDULER` environment variable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6768
https://github.com/hail-is/hail/pull/6768:61,Testability,test,tests,61,The PR adds support for skipping Scala `SchedulerSuite` unit tests by setting a `HAIL_TEST_SKIP_SCHEDULER` environment variable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6768
https://github.com/hail-is/hail/pull/6769:36,Safety,safe,safe,36,Tests are approximate but should be safe.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6769
https://github.com/hail-is/hail/pull/6769:0,Testability,Test,Tests,0,Tests are approximate but should be safe.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6769
https://github.com/hail-is/hail/pull/6771:0,Testability,Test,Tests,0,Tests against a scala mutable.Set and a random batch of inserts.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6771
https://github.com/hail-is/hail/pull/6772:114,Availability,ping,ping,114,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:629,Availability,echo,echo,629,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:1062,Availability,down,down,1062,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:258,Deployability,pipeline,pipeline,258,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:272,Deployability,pipeline,pipeline,272,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:281,Deployability,Pipeline,Pipeline,281,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:300,Deployability,pipeline,pipeline,300,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:378,Deployability,pipeline,pipeline,378,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:1224,Deployability,deploy,deployments,1224,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:946,Safety,avoid,avoid,946,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:40,Testability,test,testing,40,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/pull/6772:1219,Testability,test,test,1219,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6772
https://github.com/hail-is/hail/issues/6773:382,Availability,down,down,382,"A number of people have observed the same behavior, but the prometheus team closes it as not a bug. It appears to be related to suddenly adding a very large number of metrics. This started happening when I started the scale tests, which keeps the k8s system operating with about 2000 pods, working through 30k pods over time. It's possible all the added pod information is bringing down prometheus. It appears k8s is restarting prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:652,Availability,Avail,Available,652,"A number of people have observed the same behavior, but the prometheus team closes it as not a bug. It appears to be related to suddenly adding a very large number of metrics. This started happening when I started the scale tests, which keeps the k8s system operating with about 2000 pods, working through 30k pods over time. It's possible all the added pod information is bringing down prometheus. It appears k8s is restarting prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:1868,Availability,recover,recover,1868," /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(L",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:2062,Availability,error,error,2062,"9% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=201",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:3401,Availability,repair,repair,3401,"e retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:3572,Availability,repair,repair,3572,"n=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:3743,Availability,repair,repair,3743,"t@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:3914,Availability,repair,repair,3914,"64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:4085,Availability,repair,repair,4085,"=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:4256,Availability,repair,repair,4256,"5:51.994Z caller=web.go:417 component=web msg=""Start listening for connections"" address=0.0.0.0:9090; level=info ts=2019-07-31T15:45:51.996Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:4427,Availability,repair,repair,4427,"tsdb msg=""found healthy block"" mint=1563105600000 maxt=1563170400000 ulid=01DFTDRJHCX1S9B0KPJTG8CRGW; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:4598,Availability,repair,repair,4598,"tsdb msg=""found healthy block"" mint=1563170400000 maxt=1563235200000 ulid=01DFWBK0336Z71ZCRRKS79T18P; level=info ts=2019-07-31T15:45:51.997Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:4769,Availability,repair,repair,4769,"tsdb msg=""found healthy block"" mint=1563235200000 maxt=1563300000000 ulid=01DFY9C92NRA1S7FDVHFRFMFPF; level=info ts=2019-07-31T15:45:51.998Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:4940,Availability,repair,repair,4940,"tsdb msg=""found healthy block"" mint=1563300000000 maxt=1563364800000 ulid=01DG075GN2MME91GM1DA5G3H07; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:5111,Availability,repair,repair,5111,"tsdb msg=""found healthy block"" mint=1563364800000 maxt=1563429600000 ulid=01DG24Z1SDJ7VXW96YYSY1FC8Y; level=info ts=2019-07-31T15:45:51.999Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:5282,Availability,repair,repair,5282,"tsdb msg=""found healthy block"" mint=1563429600000 maxt=1563494400000 ulid=01DG42SDMFEK1AJPRJ5YWKZFJ8; level=info ts=2019-07-31T15:45:52.000Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:5453,Availability,repair,repair,5453,"tsdb msg=""found healthy block"" mint=1563494400000 maxt=1563559200000 ulid=01DG60K1ADH2GGZ6ZHYVRQA7PQ; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:5624,Availability,repair,repair,5624,"tsdb msg=""found healthy block"" mint=1563559200000 maxt=1563624000000 ulid=01DG7YBCA5FFBKYXX7EADE91TP; level=info ts=2019-07-31T15:45:52.001Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:5795,Availability,repair,repair,5795,"tsdb msg=""found healthy block"" mint=1563624000000 maxt=1563688800000 ulid=01DG9W4WYEDBQ32Q112S7EPMEP; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:5966,Availability,repair,repair,5966,"tsdb msg=""found healthy block"" mint=1563688800000 maxt=1563753600000 ulid=01DGBSYJDGQ8NY58106XGFT7CS; level=info ts=2019-07-31T15:45:52.002Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:6137,Availability,repair,repair,6137,"tsdb msg=""found healthy block"" mint=1563753600000 maxt=1563818400000 ulid=01DGDQRCZ949B46BNYWP2S5F02; level=info ts=2019-07-31T15:45:52.003Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:6308,Availability,repair,repair,6308,"tsdb msg=""found healthy block"" mint=1563818400000 maxt=1563883200000 ulid=01DGFNHWFPXDCFKMJ45R22VST6; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:6479,Availability,repair,repair,6479,"info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019-07-31T15:45:52.020Z caller=web.go:461 component=web msg=""router prefix"" prefix=/monitoring/prometheus; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:6650,Availability,repair,repair,6650,"info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019-07-31T15:45:52.020Z caller=web.go:461 component=web msg=""router prefix"" prefix=/monitoring/prometheus; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:6821,Availability,repair,repair,6821,"info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019-07-31T15:45:52.020Z caller=web.go:461 component=web msg=""router prefix"" prefix=/monitoring/prometheus; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:6992,Availability,repair,repair,6992,"info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019-07-31T15:45:52.020Z caller=web.go:461 component=web msg=""router prefix"" prefix=/monitoring/prometheus; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:7163,Availability,repair,repair,7163,"info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019-07-31T15:45:52.020Z caller=web.go:461 component=web msg=""router prefix"" prefix=/monitoring/prometheus; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:1970,Energy Efficiency,monitor,monitoring,1970,"lv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:2295,Energy Efficiency,monitor,monitoring,2295,"st; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:7387,Energy Efficiency,monitor,monitoring,7387,"info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019-07-31T15:45:52.020Z caller=web.go:461 component=web msg=""router prefix"" prefix=/monitoring/prometheus; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:7364,Integrability,rout,router,7364,"info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563883200000 maxt=1563948000000 ulid=01DGHKFKSD0THQ0VWGY9MM01GG; level=info ts=2019-07-31T15:45:52.004Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1563948000000 maxt=1564012800000 ulid=01DGKH5AFC7KQ1CN0JE7AA3G6Y; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564012800000 maxt=1564077600000 ulid=01DGNEZMRJM9XKV1N0SA1Y9S3F; level=info ts=2019-07-31T15:45:52.005Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564077600000 maxt=1564142400000 ulid=01DGQCQYYCJDT3DQTVTBHS7N6G; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564142400000 maxt=1564207200000 ulid=01DGSAYGB5QEMACHZK7ZE89H70; level=info ts=2019-07-31T15:45:52.006Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564207200000 maxt=1564272000000 ulid=01DGV8AWDDHKPSN407Z08FBHVZ; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564272000000 maxt=1564336800000 ulid=01DGX64B6GVNNF4P09GB4YM3TV; level=info ts=2019-07-31T15:45:52.007Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564401600000 maxt=1564408800000 ulid=01DGZ3XMWQ04MNAJTWJNXGHNZ5; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564336800000 maxt=1564401600000 ulid=01DGZ426ED23NR5759BDAQM0H6; level=info ts=2019-07-31T15:45:52.008Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564408800000 maxt=1564416000000 ulid=01DGZASC8TRTFRM61J3MX4PHX4; level=info ts=2019-07-31T15:45:52.009Z caller=repair.go:59 component=tsdb msg=""found healthy block"" mint=1564416000000 maxt=1564423200000 ulid=01DGZHN38ENTPENE3MM35HVS42; level=info ts=2019-07-31T15:45:52.020Z caller=web.go:461 component=web msg=""router prefix"" prefix=/monitoring/prometheus; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:1576,Modifiability,config,config,1576," is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:546,Performance,load,load,546,"A number of people have observed the same behavior, but the prometheus team closes it as not a bug. It appears to be related to suddenly adding a very large number of metrics. This started happening when I started the scale tests, which keeps the k8s system operating with about 2000 pods, working through 30k pods over time. It's possible all the added pod information is bringing down prometheus. It appears k8s is restarting prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:1868,Safety,recover,recover,1868," /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(L",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:224,Testability,test,tests,224,"A number of people have observed the same behavior, but the prometheus team closes it as not a bug. It appears to be related to suddenly adding a very large number of metrics. This started happening when I started the scale tests, which keeps the k8s system operating with about 2000 pods, working through 30k pods over time. It's possible all the added pod information is bringing down prometheus. It appears k8s is restarting prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:922,Testability,log,log,922,"A number of people have observed the same behavior, but the prometheus team closes it as not a bug. It appears to be related to suddenly adding a very large number of metrics. This started happening when I started the scale tests, which keeps the k8s system operating with about 2000 pods, working through 30k pods over time. It's possible all the added pod information is bringing down prometheus. It appears k8s is restarting prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:1453,Testability,test,tests,1453,"prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/issues/6773:2287,Testability,log,logs,2287,"st; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecting to localhost:9090 (127.0.0.1:9090); wget: server returned error: HTTP/1.1 503 Service Unavailable; /prometheus $ ; ```. https://github.com/prometheus/prometheus/issues/5727#issuecomment-510818825; https://github.com/prometheus/prometheus/issues/4324#issuecomment-460243182. ```; # k logs -n monitoring prometheus-0 ; level=info ts=2019-07-31T15:45:51.990Z caller=main.go:286 msg=""no time or size retention was set so using the default time retention"" duration=15d; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:322 msg=""Starting Prometheus"" version=""(version=2.10.0, branch=HEAD, revision=d20e84d0fb64aff2f62a977adc8cfb656da4e286)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:323 build_context=""(go=go1.12.5, user=root@a49185acd9b0, date=20190525-12:28:13)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:324 host_details=""(Linux 4.14.127+ #1 SMP Tue Jun 18 18:32:10 PDT 2019 x86_64 prometheus-0 (none))""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:325 fd_limits=""(soft=1048576, hard=1048576)""; level=info ts=2019-07-31T15:45:51.991Z caller=main.go:326 vm_limits=""(soft=unlimited, hard=unlimited)""; level=info ts=2019-07-31T15:45:51.993Z caller=main.go:645 msg=""Starting TSDB ...""; level=info ts=2019-07-31T15:45:51.994Z caller=web.go:417 component=web msg=""Start listening ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773
https://github.com/hail-is/hail/pull/6777:45,Integrability,depend,dependencies,45,None is an unfortunate default for a list of dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6777
https://github.com/hail-is/hail/pull/6782:381,Integrability,depend,dependencies,381,"Small amount of preliminary work #6673. I made a PR trying to address this issue already, but ended up reverting it because it broke CI and we don't test that well. To minimize frustration, I'm making some initial changes here first. . Namely:. - Steps have an equality and hash method based solely on their name (I'm going to want to build a hash map of steps when coming up with dependencies for the cleanup jobs. ; - Cleanup now takes a list of parents, instead of just the sink job (useful for when there are more parents than just the sink job).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6782
https://github.com/hail-is/hail/pull/6782:274,Security,hash,hash,274,"Small amount of preliminary work #6673. I made a PR trying to address this issue already, but ended up reverting it because it broke CI and we don't test that well. To minimize frustration, I'm making some initial changes here first. . Namely:. - Steps have an equality and hash method based solely on their name (I'm going to want to build a hash map of steps when coming up with dependencies for the cleanup jobs. ; - Cleanup now takes a list of parents, instead of just the sink job (useful for when there are more parents than just the sink job).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6782
https://github.com/hail-is/hail/pull/6782:343,Security,hash,hash,343,"Small amount of preliminary work #6673. I made a PR trying to address this issue already, but ended up reverting it because it broke CI and we don't test that well. To minimize frustration, I'm making some initial changes here first. . Namely:. - Steps have an equality and hash method based solely on their name (I'm going to want to build a hash map of steps when coming up with dependencies for the cleanup jobs. ; - Cleanup now takes a list of parents, instead of just the sink job (useful for when there are more parents than just the sink job).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6782
https://github.com/hail-is/hail/pull/6782:149,Testability,test,test,149,"Small amount of preliminary work #6673. I made a PR trying to address this issue already, but ended up reverting it because it broke CI and we don't test that well. To minimize frustration, I'm making some initial changes here first. . Namely:. - Steps have an equality and hash method based solely on their name (I'm going to want to build a hash map of steps when coming up with dependencies for the cleanup jobs. ; - Cleanup now takes a list of parents, instead of just the sink job (useful for when there are more parents than just the sink job).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6782
https://github.com/hail-is/hail/pull/6783:82,Availability,error,error,82,"This fixes an issue where we cannot retrieve old batches for a PR. Addresses this error:; ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py\"", line 91, in wrapped\n context = await coro(*args)\n File \""/ci/ci.py\"", line 170, in get_batch\n j['duration'] = humanize.naturaldelta(datetime.timedelta(seconds=sum(j['duration'])))\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'""}; {""levelname"": ""INFO"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.0.177 [01/Aug/2019:15:59:17 +0000] \""GET /batches/584 HTTP/1.0\"" 500 315 \""-\"" \""Mozilla/5.0 (Macintosh;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6783
https://github.com/hail-is/hail/pull/6783:110,Availability,ERROR,ERROR,110,"This fixes an issue where we cannot retrieve old batches for a PR. Addresses this error:; ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py\"", line 91, in wrapped\n context = await coro(*args)\n File \""/ci/ci.py\"", line 170, in get_batch\n j['duration'] = humanize.naturaldelta(datetime.timedelta(seconds=sum(j['duration'])))\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'""}; {""levelname"": ""INFO"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.0.177 [01/Aug/2019:15:59:17 +0000] \""GET /batches/584 HTTP/1.0\"" 500 315 \""-\"" \""Mozilla/5.0 (Macintosh;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6783
https://github.com/hail-is/hail/pull/6783:239,Availability,Error,Error,239,"This fixes an issue where we cannot retrieve old batches for a PR. Addresses this error:; ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py\"", line 91, in wrapped\n context = await coro(*args)\n File \""/ci/ci.py\"", line 170, in get_batch\n j['duration'] = humanize.naturaldelta(datetime.timedelta(seconds=sum(j['duration'])))\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'""}; {""levelname"": ""INFO"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.0.177 [01/Aug/2019:15:59:17 +0000] \""GET /batches/584 HTTP/1.0\"" 500 315 \""-\"" \""Mozilla/5.0 (Macintosh;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6783
https://github.com/hail-is/hail/pull/6783:228,Integrability,message,message,228,"This fixes an issue where we cannot retrieve old batches for a PR. Addresses this error:; ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py\"", line 91, in wrapped\n context = await coro(*args)\n File \""/ci/ci.py\"", line 170, in get_batch\n j['duration'] = humanize.naturaldelta(datetime.timedelta(seconds=sum(j['duration'])))\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'""}; {""levelname"": ""INFO"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.0.177 [01/Aug/2019:15:59:17 +0000] \""GET /batches/584 HTTP/1.0\"" 500 315 \""-\"" \""Mozilla/5.0 (Macintosh;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6783
https://github.com/hail-is/hail/pull/6783:772,Integrability,wrap,wrapped,772,"This fixes an issue where we cannot retrieve old batches for a PR. Addresses this error:; ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py\"", line 91, in wrapped\n context = await coro(*args)\n File \""/ci/ci.py\"", line 170, in get_batch\n j['duration'] = humanize.naturaldelta(datetime.timedelta(seconds=sum(j['duration'])))\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'""}; {""levelname"": ""INFO"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.0.177 [01/Aug/2019:15:59:17 +0000] \""GET /batches/584 HTTP/1.0\"" 500 315 \""-\"" \""Mozilla/5.0 (Macintosh;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6783
https://github.com/hail-is/hail/pull/6783:1131,Integrability,message,message,1131,"This fixes an issue where we cannot retrieve old batches for a PR. Addresses this error:; ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py\"", line 91, in wrapped\n context = await coro(*args)\n File \""/ci/ci.py\"", line 170, in get_batch\n j['duration'] = humanize.naturaldelta(datetime.timedelta(seconds=sum(j['duration'])))\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'""}; {""levelname"": ""INFO"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.0.177 [01/Aug/2019:15:59:17 +0000] \""GET /batches/584 HTTP/1.0\"" 500 315 \""-\"" \""Mozilla/5.0 (Macintosh;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6783
https://github.com/hail-is/hail/pull/6783:1120,Testability,log,log,1120,"This fixes an issue where we cannot retrieve old batches for a PR. Addresses this error:; ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py\"", line 91, in wrapped\n context = await coro(*args)\n File \""/ci/ci.py\"", line 170, in get_batch\n j['duration'] = humanize.naturaldelta(datetime.timedelta(seconds=sum(j['duration'])))\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'""}; {""levelname"": ""INFO"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.0.177 [01/Aug/2019:15:59:17 +0000] \""GET /batches/584 HTTP/1.0\"" 500 315 \""-\"" \""Mozilla/5.0 (Macintosh;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6783
https://github.com/hail-is/hail/pull/6784:4,Safety,risk,risk,4,The risk is that a container image will eventually be pushed; to the container registry allowing this container to proceed.; We do not rely on this behavior (modulo the eventual consistency; of GCR).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6784
https://github.com/hail-is/hail/pull/6789:200,Safety,avoid,avoid,200,"- [x] Width of navbar doesn't match #body; * Caused by using em to determine max-width of a child where the parent (#body) had a font-size set differently from root (html). Fixed by using rem, and to avoid changing so many em's, removing font-size on #body. - [x] Better dropdown: width, box shadow, padding. - [x] Some apparently unnecessary styles. Before (narrow):; <img width=""677"" alt=""Screenshot 2019-08-01 17 03 06"" src=""https://user-images.githubusercontent.com/5543229/62328170-61bb7700-b480-11e9-838a-43229ee955c3.png"">. After (narrow):; <img width=""712"" alt=""Screenshot 2019-08-01 17 02 53"" src=""https://user-images.githubusercontent.com/5543229/62328172-641dd100-b480-11e9-9be1-f3ff67035cd0.png"">. (more views):; ; <img width=""894"" alt=""Screenshot 2019-08-01 17 37 00"" src=""https://user-images.githubusercontent.com/5543229/62329162-19518880-b483-11e9-9cfd-12ca8c1a52dc.png"">; <img width=""854"" alt=""Screenshot 2019-08-01 17 37 05"" src=""https://user-images.githubusercontent.com/5543229/62329163-19518880-b483-11e9-8a70-90faa3dcc685.png"">; <img width=""923"" alt=""Screenshot 2019-08-01 17 37 11"" src=""https://user-images.githubusercontent.com/5543229/62329164-19518880-b483-11e9-9871-0ce39c6c7c53.png"">. I did this quickly, so didn't set up local server. If anything becomes ugly I'll fix today. Also didn't test in IE, just chrome, safari, and I expect Firefox to be fine as well. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789
https://github.com/hail-is/hail/pull/6789:1317,Testability,test,test,1317,"- [x] Width of navbar doesn't match #body; * Caused by using em to determine max-width of a child where the parent (#body) had a font-size set differently from root (html). Fixed by using rem, and to avoid changing so many em's, removing font-size on #body. - [x] Better dropdown: width, box shadow, padding. - [x] Some apparently unnecessary styles. Before (narrow):; <img width=""677"" alt=""Screenshot 2019-08-01 17 03 06"" src=""https://user-images.githubusercontent.com/5543229/62328170-61bb7700-b480-11e9-838a-43229ee955c3.png"">. After (narrow):; <img width=""712"" alt=""Screenshot 2019-08-01 17 02 53"" src=""https://user-images.githubusercontent.com/5543229/62328172-641dd100-b480-11e9-9be1-f3ff67035cd0.png"">. (more views):; ; <img width=""894"" alt=""Screenshot 2019-08-01 17 37 00"" src=""https://user-images.githubusercontent.com/5543229/62329162-19518880-b483-11e9-9cfd-12ca8c1a52dc.png"">; <img width=""854"" alt=""Screenshot 2019-08-01 17 37 05"" src=""https://user-images.githubusercontent.com/5543229/62329163-19518880-b483-11e9-8a70-90faa3dcc685.png"">; <img width=""923"" alt=""Screenshot 2019-08-01 17 37 11"" src=""https://user-images.githubusercontent.com/5543229/62329164-19518880-b483-11e9-9871-0ce39c6c7c53.png"">. I did this quickly, so didn't set up local server. If anything becomes ugly I'll fix today. Also didn't test in IE, just chrome, safari, and I expect Firefox to be fine as well. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789
https://github.com/hail-is/hail/pull/6791:146,Availability,down,down,146,"This isn't needed any more, since setAggState() and newAggState() do the right thing when called. At least on my laptop, this brings the one test down to ; ```; Name	Mean	Median	StDev; matrix_table_many_aggs_col_wise	24.275	24.372	0.818; ```; vs with the flag disabled, which is ; ```; Name	Mean	Median	StDev; matrix_table_many_aggs_col_wise	30.147	29.706	1.868; ```; (oops)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6791
https://github.com/hail-is/hail/pull/6791:141,Testability,test,test,141,"This isn't needed any more, since setAggState() and newAggState() do the right thing when called. At least on my laptop, this brings the one test down to ; ```; Name	Mean	Median	StDev; matrix_table_many_aggs_col_wise	24.275	24.372	0.818; ```; vs with the flag disabled, which is ; ```; Name	Mean	Median	StDev; matrix_table_many_aggs_col_wise	30.147	29.706	1.868; ```; (oops)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6791
https://github.com/hail-is/hail/pull/6797:68,Security,authenticat,authentication,68,"It's a shame that we have to restart a whole job just because GCP's authentication servers hiccuped. This change retries the auth once if it fails after a random wait of [5, 10) seconds.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6797
https://github.com/hail-is/hail/pull/6799:70,Performance,queue,queue,70,"The separation makes that metric more accurate. Moreover, I check the queue before rescheduling. This saves a database call, a kubernetes call, and set churn (we remove then add back the job).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6799
https://github.com/hail-is/hail/pull/6801:8,Testability,test,tested,8,🐎 💨 . I tested this pretty heavily during my scale testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6801
https://github.com/hail-is/hail/pull/6801:51,Testability,test,testing,51,🐎 💨 . I tested this pretty heavily during my scale testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6801
https://github.com/hail-is/hail/pull/6802:43,Safety,safe,safe,43,"I think we can get up to 6k, but this is a safe limit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6802
https://github.com/hail-is/hail/pull/6803:10,Performance,load,load,10,"Added new load and extract files for additional datasets such as GERP++, variant_summary",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6803
https://github.com/hail-is/hail/pull/6804:27,Availability,error,errors,27,Two big changes. Catch any errors and release the semaphore. Restart failed workers in the concurrent worker pool.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6804
https://github.com/hail-is/hail/pull/6804:38,Deployability,release,release,38,Two big changes. Catch any errors and release the semaphore. Restart failed workers in the concurrent worker pool.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6804
https://github.com/hail-is/hail/pull/6804:91,Performance,concurren,concurrent,91,Two big changes. Catch any errors and release the semaphore. Restart failed workers in the concurrent worker pool.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6804
https://github.com/hail-is/hail/pull/6805:63,Deployability,deploy,deploys,63,I needed this when running PR tests. It is also useful for dev deploys. I also fixed a minor bug in install-editable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6805
https://github.com/hail-is/hail/pull/6805:100,Deployability,install,install-editable,100,I needed this when running PR tests. It is also useful for dev deploys. I also fixed a minor bug in install-editable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6805
https://github.com/hail-is/hail/pull/6805:30,Testability,test,tests,30,I needed this when running PR tests. It is also useful for dev deploys. I also fixed a minor bug in install-editable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6805
https://github.com/hail-is/hail/pull/6807:34,Deployability,release,release,34,"Without this fix, we almost never release the semaphore",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6807
https://github.com/hail-is/hail/pull/6808:25,Integrability,rout,router,25,- also add batch2 to the router,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6808
https://github.com/hail-is/hail/pull/6810:101,Availability,failure,failure,101,"We apparently did not exercise this code path much before I fixed batch to treat ImagePullBackOff as failure. Really confusing error message because `write_gs_file` returns `None` which is not iterable so you get a type error on line 565. The real issue is that you're trying to deconstruct a pair as `uri, err` and you received `None`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6810
https://github.com/hail-is/hail/pull/6810:127,Availability,error,error,127,"We apparently did not exercise this code path much before I fixed batch to treat ImagePullBackOff as failure. Really confusing error message because `write_gs_file` returns `None` which is not iterable so you get a type error on line 565. The real issue is that you're trying to deconstruct a pair as `uri, err` and you received `None`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6810
https://github.com/hail-is/hail/pull/6810:220,Availability,error,error,220,"We apparently did not exercise this code path much before I fixed batch to treat ImagePullBackOff as failure. Really confusing error message because `write_gs_file` returns `None` which is not iterable so you get a type error on line 565. The real issue is that you're trying to deconstruct a pair as `uri, err` and you received `None`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6810
https://github.com/hail-is/hail/pull/6810:133,Integrability,message,message,133,"We apparently did not exercise this code path much before I fixed batch to treat ImagePullBackOff as failure. Really confusing error message because `write_gs_file` returns `None` which is not iterable so you get a type error on line 565. The real issue is that you're trying to deconstruct a pair as `uri, err` and you received `None`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6810
https://github.com/hail-is/hail/pull/6813:276,Modifiability,refactor,refactoring,276,"This is currently just dead code, although it could conceivably be useful in the future; I want to remove this for now as it's pretty simple to add back in at a later date (follows the pattern of Serialize/Deserialize Aggs pretty much exactly) and makes for less code to keep refactoring as we optimize the aggregator stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6813
https://github.com/hail-is/hail/pull/6813:294,Performance,optimiz,optimize,294,"This is currently just dead code, although it could conceivably be useful in the future; I want to remove this for now as it's pretty simple to add back in at a later date (follows the pattern of Serialize/Deserialize Aggs pretty much exactly) and makes for less code to keep refactoring as we optimize the aggregator stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6813
https://github.com/hail-is/hail/pull/6813:134,Usability,simpl,simple,134,"This is currently just dead code, although it could conceivably be useful in the future; I want to remove this for now as it's pretty simple to add back in at a later date (follows the pattern of Serialize/Deserialize Aggs pretty much exactly) and makes for less code to keep refactoring as we optimize the aggregator stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6813
https://github.com/hail-is/hail/pull/6818:44,Deployability,release,release,44,"array_agg is broken for take, too :(. Can't release until we fix this bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6818
https://github.com/hail-is/hail/pull/6819:9,Performance,load,loadFields,9,"`builder.loadFields` just loads the offset of the data array, but it needs to do a deep copy otherwise all of the array aggragators will start off writing to (and overwriting) the same arraybuilder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6819
https://github.com/hail-is/hail/pull/6819:26,Performance,load,loads,26,"`builder.loadFields` just loads the offset of the data array, but it needs to do a deep copy otherwise all of the array aggragators will start off writing to (and overwriting) the same arraybuilder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6819
https://github.com/hail-is/hail/pull/6820:53,Availability,toler,toleration,53,"As discussed at team meeting today, filebeat needs a toleration to run on preemptibles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6820
https://github.com/hail-is/hail/pull/6821:10,Integrability,wrap,wrapping,10,"no method wrapping, but this way we can start to/we stop passing around the wrong method everywhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6821
https://github.com/hail-is/hail/issues/6823:54,Performance,queue,queueing,54,resolves some memory problems related to task results queueing up.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6823
https://github.com/hail-is/hail/pull/6824:173,Energy Efficiency,allocate,allocated,173,"I pulled out a flag to cache in java, but accidentally got rid of the thing it was actually doing. This should be fixed now; with a smaller test mt I'm seeing the number of allocated regions be consistent between combOps:. ```; ...; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6824
https://github.com/hail-is/hail/pull/6824:23,Performance,cache,cache,23,"I pulled out a flag to cache in java, but accidentally got rid of the thing it was actually doing. This should be fixed now; with a smaller test mt I'm seeing the number of allocated regions be consistent between combOps:. ```; ...; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6824
https://github.com/hail-is/hail/pull/6824:140,Testability,test,test,140,"I pulled out a flag to cache in java, but accidentally got rid of the thing it was actually doing. This should be fixed now; with a smaller test mt I'm seeing the number of allocated regions be consistent between combOps:. ```; ...; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6824
https://github.com/hail-is/hail/pull/6826:25,Modifiability,config,config,25,"The json output now has ""config"" and ""benchmarks"" top level fields",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6826
https://github.com/hail-is/hail/pull/6826:38,Testability,benchmark,benchmarks,38,"The json output now has ""config"" and ""benchmarks"" top level fields",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6826
https://github.com/hail-is/hail/issues/6827:568,Availability,Error,Error,568,"Test:. ```scala; @Test def testArrayLeftJoin() {; val l = Ref(genUID(), TInt32()); val r = Ref(genUID(), TInt32()); val left = ArrayRange(0, 10, 1); val right = MakeArray(Seq(2, 5, 8), TArray(TInt32())); assertEvalsTo(; ArrayLeftJoinDistinct(; left, right,; l.name, r.name,; l - r,; If(IsNA(r), l, 0)),; IndexedSeq(0, 1, 0, 3, 4, 0, 6, 7, 0, 9))(; ExecStrategy.javaOnly; ); }; ```. Fails with: `java.lang.IllegalStateException: Bytecode failed verification 1`. ```; Verify Output 2 for is/hail/codegen/generated/C1:; org.objectweb.asm.tree.analysis.AnalyzerException: Error at instruction 478: Argument 2: expected I, but found J; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6827
https://github.com/hail-is/hail/issues/6827:0,Testability,Test,Test,0,"Test:. ```scala; @Test def testArrayLeftJoin() {; val l = Ref(genUID(), TInt32()); val r = Ref(genUID(), TInt32()); val left = ArrayRange(0, 10, 1); val right = MakeArray(Seq(2, 5, 8), TArray(TInt32())); assertEvalsTo(; ArrayLeftJoinDistinct(; left, right,; l.name, r.name,; l - r,; If(IsNA(r), l, 0)),; IndexedSeq(0, 1, 0, 3, 4, 0, 6, 7, 0, 9))(; ExecStrategy.javaOnly; ); }; ```. Fails with: `java.lang.IllegalStateException: Bytecode failed verification 1`. ```; Verify Output 2 for is/hail/codegen/generated/C1:; org.objectweb.asm.tree.analysis.AnalyzerException: Error at instruction 478: Argument 2: expected I, but found J; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6827
https://github.com/hail-is/hail/issues/6827:18,Testability,Test,Test,18,"Test:. ```scala; @Test def testArrayLeftJoin() {; val l = Ref(genUID(), TInt32()); val r = Ref(genUID(), TInt32()); val left = ArrayRange(0, 10, 1); val right = MakeArray(Seq(2, 5, 8), TArray(TInt32())); assertEvalsTo(; ArrayLeftJoinDistinct(; left, right,; l.name, r.name,; l - r,; If(IsNA(r), l, 0)),; IndexedSeq(0, 1, 0, 3, 4, 0, 6, 7, 0, 9))(; ExecStrategy.javaOnly; ); }; ```. Fails with: `java.lang.IllegalStateException: Bytecode failed verification 1`. ```; Verify Output 2 for is/hail/codegen/generated/C1:; org.objectweb.asm.tree.analysis.AnalyzerException: Error at instruction 478: Argument 2: expected I, but found J; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6827
https://github.com/hail-is/hail/issues/6827:27,Testability,test,testArrayLeftJoin,27,"Test:. ```scala; @Test def testArrayLeftJoin() {; val l = Ref(genUID(), TInt32()); val r = Ref(genUID(), TInt32()); val left = ArrayRange(0, 10, 1); val right = MakeArray(Seq(2, 5, 8), TArray(TInt32())); assertEvalsTo(; ArrayLeftJoinDistinct(; left, right,; l.name, r.name,; l - r,; If(IsNA(r), l, 0)),; IndexedSeq(0, 1, 0, 3, 4, 0, 6, 7, 0, 9))(; ExecStrategy.javaOnly; ); }; ```. Fails with: `java.lang.IllegalStateException: Bytecode failed verification 1`. ```; Verify Output 2 for is/hail/codegen/generated/C1:; org.objectweb.asm.tree.analysis.AnalyzerException: Error at instruction 478: Argument 2: expected I, but found J; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6827
https://github.com/hail-is/hail/issues/6827:204,Testability,assert,assertEvalsTo,204,"Test:. ```scala; @Test def testArrayLeftJoin() {; val l = Ref(genUID(), TInt32()); val r = Ref(genUID(), TInt32()); val left = ArrayRange(0, 10, 1); val right = MakeArray(Seq(2, 5, 8), TArray(TInt32())); assertEvalsTo(; ArrayLeftJoinDistinct(; left, right,; l.name, r.name,; l - r,; If(IsNA(r), l, 0)),; IndexedSeq(0, 1, 0, 3, 4, 0, 6, 7, 0, 9))(; ExecStrategy.javaOnly; ); }; ```. Fails with: `java.lang.IllegalStateException: Bytecode failed verification 1`. ```; Verify Output 2 for is/hail/codegen/generated/C1:; org.objectweb.asm.tree.analysis.AnalyzerException: Error at instruction 478: Argument 2: expected I, but found J; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6827
https://github.com/hail-is/hail/pull/6831:0,Testability,Benchmark,Benchmarks,0,Benchmarks (not to be trusted anyway):. Master:. ```; [1/1] Running table_aggregate_counter...; run 1: 10.02; run 2: 9.72; run 3: 9.19; run 4: 9.30; run 5: 10.87; table_aggregate_counter	9.819	9.717	0.602; ```. PR:; ```; [1/1] Running table_aggregate_counter...; run 1: 9.74; run 2: 8.73; run 3: 8.30; run 4: 8.43; run 5: 10.30; table_aggregate_counter	9.099	8.728	0.787; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6831
https://github.com/hail-is/hail/pull/6832:112,Testability,test,testing,112,"We should have documentation about our expectations about versions of C compiler. Plus, we don't need R for dev testing anymore.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6832
https://github.com/hail-is/hail/pull/6833:256,Deployability,deploy,deployed,256,"You can run this as `python3 scale_test.py 120` to simulate 120 clients simultaneously creating notebooks. You have to delete the notebooks yourself using the admin account or manually killing them in k8s. This isn't actually used anywhere (notebook isn't deployed by CI), but I want it in our repository. At some point we should productionize notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6833
https://github.com/hail-is/hail/pull/6836:66,Deployability,deploy,deployment,66,"There is more work to do to get this unified with the modern hail deployment (note that I copied the logging setup here). Nevertheless, this brings asyncio to notebook leader, which enables it to handle a helluva lot more simultaneous users. Next steps are to get this regularly deployed again and to get it on to a normal hail docker image.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6836
https://github.com/hail-is/hail/pull/6836:279,Deployability,deploy,deployed,279,"There is more work to do to get this unified with the modern hail deployment (note that I copied the logging setup here). Nevertheless, this brings asyncio to notebook leader, which enables it to handle a helluva lot more simultaneous users. Next steps are to get this regularly deployed again and to get it on to a normal hail docker image.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6836
https://github.com/hail-is/hail/pull/6836:101,Testability,log,logging,101,"There is more work to do to get this unified with the modern hail deployment (note that I copied the logging setup here). Nevertheless, this brings asyncio to notebook leader, which enables it to handle a helluva lot more simultaneous users. Next steps are to get this regularly deployed again and to get it on to a normal hail docker image.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6836
https://github.com/hail-is/hail/pull/6841:467,Deployability,install,install-editable,467,"The docs on the new `export` method are pretty clear:; ```pycon; >>> mt.GT.export('gt.tsv'); >>> with open('gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus	alleles	0	1	2	3; 1:1	[""A"",""C""]	0/1	1/1	0/1	0/1; 1:2	[""A"",""C""]	1/1	0/1	1/1	1/1; 1:3	[""A"",""C""]	0/1	0/0	0/0	0/0; 1:4	[""A"",""C""]	1/1	1/1	0/0	1/1; ```. I also changed all the vds extensions to mt, added a new; dataset that is small enough to print the entire matrix,; and fixed a bug in `make install-editable`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841
https://github.com/hail-is/hail/pull/6841:47,Usability,clear,clear,47,"The docs on the new `export` method are pretty clear:; ```pycon; >>> mt.GT.export('gt.tsv'); >>> with open('gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus	alleles	0	1	2	3; 1:1	[""A"",""C""]	0/1	1/1	0/1	0/1; 1:2	[""A"",""C""]	1/1	0/1	1/1	1/1; 1:3	[""A"",""C""]	0/1	0/0	0/0	0/0; 1:4	[""A"",""C""]	1/1	1/1	0/0	1/1; ```. I also changed all the vds extensions to mt, added a new; dataset that is small enough to print the entire matrix,; and fixed a bug in `make install-editable`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841
https://github.com/hail-is/hail/pull/6842:51,Testability,benchmark,benchmark,51,Was causing segfaults in the read_with_index_p1000 benchmark. closes #6793,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6842
https://github.com/hail-is/hail/pull/6846:218,Integrability,depend,depending,218,"apparently gather runs things in the background even if; you do not await the gather task, which is different from; other awaitables (which simply do nothing), the result was; that batch nondeterministically succeeds, depending on; whether the tasks execute in the order of the program or not.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6846
https://github.com/hail-is/hail/pull/6846:140,Usability,simpl,simply,140,"apparently gather runs things in the background even if; you do not await the gather task, which is different from; other awaitables (which simply do nothing), the result was; that batch nondeterministically succeeds, depending on; whether the tasks execute in the order of the program or not.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6846
https://github.com/hail-is/hail/issues/6849:135,Integrability,depend,depends,135,"Issue known by several members of the dev team now -- if the first element combined is nan, the result is non-deterministically wrong (depends on the associativity of agg combination on the master)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6849
https://github.com/hail-is/hail/pull/6854:54,Availability,failure,failures,54,> pytest-instafail is a plugin for py.test that shows failures and errors instantly instead of waiting until the end of test session. https://github.com/pytest-dev/pytest-instafail/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6854
https://github.com/hail-is/hail/pull/6854:67,Availability,error,errors,67,> pytest-instafail is a plugin for py.test that shows failures and errors instantly instead of waiting until the end of test session. https://github.com/pytest-dev/pytest-instafail/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6854
https://github.com/hail-is/hail/pull/6854:24,Modifiability,plugin,plugin,24,> pytest-instafail is a plugin for py.test that shows failures and errors instantly instead of waiting until the end of test session. https://github.com/pytest-dev/pytest-instafail/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6854
https://github.com/hail-is/hail/pull/6854:38,Testability,test,test,38,> pytest-instafail is a plugin for py.test that shows failures and errors instantly instead of waiting until the end of test session. https://github.com/pytest-dev/pytest-instafail/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6854
https://github.com/hail-is/hail/pull/6854:120,Testability,test,test,120,> pytest-instafail is a plugin for py.test that shows failures and errors instantly instead of waiting until the end of test session. https://github.com/pytest-dev/pytest-instafail/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6854
https://github.com/hail-is/hail/issues/6855:0,Integrability,Interface,Interface,0,"Interface for `Call.one_hot_alleles` and `CallExpression.one_hot_alleles` are different. As far as I can tell, `one_hot_alleles` only needs to know how many different alleles there are and thus I think that both should use `n_alleles` as parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6855
https://github.com/hail-is/hail/pull/6856:722,Availability,failure,failure,722,"I think this may help users discover `group_by` and also help users; who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:2024,Availability,failure,failures,2024,"ch is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the aforementioned situation; - I changed ""Using Variants as Covariates"" in `guides/genetics.rst` because it seemed complicated and was broken by the aforementioned situation; - I removed `NOTEST` which was a custom pytest annotation that duplicates the functionality of `SKIP` (I changed all `NOTEST` annotations to `SKIP`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:1049,Deployability,install,installation,1049," who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:1114,Deployability,install,install,1114," who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:1126,Integrability,depend,dependencies,1126," who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:856,Modifiability,config,configurable,856,"I think this may help users discover `group_by` and also help users; who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:1171,Modifiability,config,configures,1171,"st_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the afore",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:1218,Modifiability,variab,variables,1218,"st_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the afore",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:1443,Testability,test,test,1443,"ch is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the aforementioned situation; - I changed ""Using Variants as Covariates"" in `guides/genetics.rst` because it seemed complicated and was broken by the aforementioned situation; - I removed `NOTEST` which was a custom pytest annotation that duplicates the functionality of `SKIP` (I changed all `NOTEST` annotations to `SKIP`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:1725,Testability,test,tests,1725,"ch is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the aforementioned situation; - I changed ""Using Variants as Covariates"" in `guides/genetics.rst` because it seemed complicated and was broken by the aforementioned situation; - I removed `NOTEST` which was a custom pytest annotation that duplicates the functionality of `SKIP` (I changed all `NOTEST` annotations to `SKIP`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:2019,Testability,test,test,2019,"ch is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the aforementioned situation; - I changed ""Using Variants as Covariates"" in `guides/genetics.rst` because it seemed complicated and was broken by the aforementioned situation; - I removed `NOTEST` which was a custom pytest annotation that duplicates the functionality of `SKIP` (I changed all `NOTEST` annotations to `SKIP`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/pull/6856:2284,Usability,guid,guides,2284,"ch is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the aforementioned situation; - I changed ""Using Variants as Covariates"" in `guides/genetics.rst` because it seemed complicated and was broken by the aforementioned situation; - I removed `NOTEST` which was a custom pytest annotation that duplicates the functionality of `SKIP` (I changed all `NOTEST` annotations to `SKIP`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6856
https://github.com/hail-is/hail/issues/6860:561,Availability,error,error,561,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:1163,Availability,error,error,1163,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:1815,Integrability,depend,dependencies,1815,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:944,Performance,load,loadInt,944,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:1344,Performance,load,loadInt,1344,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:576,Safety,detect,detected,576,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:14,Testability,test,tests,14,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:200,Testability,test,test,200,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:472,Testability,test,test,472,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:508,Testability,test,test,508,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:1277,Testability,log,log,1277,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/issues/6860:1538,Testability,stub,stub,1538,"the following tests causes a segfault:. ```python; def test_agg_table_take(self):; ht = hl.utils.range_table(10).annotate(x = 'a'); ht.aggregate(agg.take(ht.x, 2)); ```. *only* as long as you run the test `test_init_hail_context_twice` in the same execution, i.e. ```; hail/python $ pytest -k 'test_init_hail_context_twice or test_agg_table_take'; platform darwin -- Python 3.6.0, pytest-4.5.0, py-1.8.0, pluggy-0.12.0; collected 653 items / 651 deselected / 2 selected ; test/hail/test_context.py . [ 50%]; test/hail/expr/test_expr.py F [100%]. .... # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ac8bbe2, pid=92110, tid=0x0000000000013d03; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 5335 C1 is.hail.annotations.Region$.loadInt(J)I (5 bytes) @ 0x000000010ac8bbe2 [0x000000010ac8bb40+0xa2]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid92110.log; Compiled method (c1) 4061 5335 3 is.hail.annotations.Region$::loadInt (5 bytes); total in heap [0x000000010ac8b9d0,0x000000010ac8bd78] = 936; relocation [0x000000010ac8baf8,0x000000010ac8bb28] = 48; main code [0x000000010ac8bb40,0x000000010ac8bc60] = 288; stub code [0x000000010ac8bc60,0x000000010ac8bcf0] = 144; oops [0x000000010ac8bcf0,0x000000010ac8bcf8] = 8; metadata [0x000000010ac8bcf8,0x000000010ac8bd08] = 16; scopes data [0x000000010ac8bd08,0x000000010ac8bd30] = 40; scopes pcs [0x000000010ac8bd30,0x000000010ac8bd70] = 64; dependencies [0x000000010ac8bd70,0x000000010ac8bd78] = 8; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp. ....; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6860
https://github.com/hail-is/hail/pull/6863:148,Availability,down,down,148,"Not sure who to assign this to since it spans everything. I targeted the slowest test jobs. Currently CI's PR page timings are wrong. If you scroll down to ""Build History"" and click on a batch, that page has the right timings. (The CI PR page timings will be fixed by #6746",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6863
https://github.com/hail-is/hail/pull/6863:81,Testability,test,test,81,"Not sure who to assign this to since it spans everything. I targeted the slowest test jobs. Currently CI's PR page timings are wrong. If you scroll down to ""Build History"" and click on a batch, that page has the right timings. (The CI PR page timings will be fixed by #6746",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6863
https://github.com/hail-is/hail/pull/6867:820,Availability,down,down,820,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6867:308,Deployability,install,install-editable,308,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6867:449,Integrability,depend,depend,449,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6867:474,Integrability,depend,depend,474,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6867:632,Integrability,depend,depends,632,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6867:679,Integrability,depend,depend,679,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6867:809,Integrability,depend,dependency,809,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6867:459,Modifiability,variab,variables,459,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6867:553,Modifiability,variab,variable,553,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6867
https://github.com/hail-is/hail/pull/6873:20,Testability,log,log,20,Batch forgot how to log.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6873
https://github.com/hail-is/hail/pull/6874:429,Testability,test,test,429,"This PR:. - Implements IR Nodes `MakeNDArray` and `NDArrayShape` in the JVM emitter. First emitter task, so let me know if I'm doing things in a nonstandard way or making it more complicated than I ought to. I have two questions as well:. 1. Should I implement NDArray in the interpreter as I go? What is the result of MakeNDArray in Interpreter world? Just a row?; 2. In text_expr.py, I commented out a decorator that makes the test run only with cxx compile, since I want to test the jvm byte code version. Is there any way currently to run both cxx and jvm compile for a test? If not maybe I ought to add one?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874
https://github.com/hail-is/hail/pull/6874:477,Testability,test,test,477,"This PR:. - Implements IR Nodes `MakeNDArray` and `NDArrayShape` in the JVM emitter. First emitter task, so let me know if I'm doing things in a nonstandard way or making it more complicated than I ought to. I have two questions as well:. 1. Should I implement NDArray in the interpreter as I go? What is the result of MakeNDArray in Interpreter world? Just a row?; 2. In text_expr.py, I commented out a decorator that makes the test run only with cxx compile, since I want to test the jvm byte code version. Is there any way currently to run both cxx and jvm compile for a test? If not maybe I ought to add one?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874
https://github.com/hail-is/hail/pull/6874:574,Testability,test,test,574,"This PR:. - Implements IR Nodes `MakeNDArray` and `NDArrayShape` in the JVM emitter. First emitter task, so let me know if I'm doing things in a nonstandard way or making it more complicated than I ought to. I have two questions as well:. 1. Should I implement NDArray in the interpreter as I go? What is the result of MakeNDArray in Interpreter world? Just a row?; 2. In text_expr.py, I commented out a decorator that makes the test run only with cxx compile, since I want to test the jvm byte code version. Is there any way currently to run both cxx and jvm compile for a test? If not maybe I ought to add one?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874
https://github.com/hail-is/hail/pull/6877:43,Availability,failure,failure,43,"Try to give the user information about the failure. A follow up to this is to give a mode to ignore the error, and filter the variant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6877
https://github.com/hail-is/hail/pull/6877:104,Availability,error,error,104,"Try to give the user information about the failure. A follow up to this is to give a mode to ignore the error, and filter the variant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6877
https://github.com/hail-is/hail/pull/6878:43,Availability,Down,Downloading,43,"Spinning up a VM takes around two minutes. Downloading fresh container images; takes additional time, maybe a whole minute. The cost of timing out is high: an otherwise passing PR test run may fail; demanding a bump and delaying merging of said PR by fifteen to twenty minutes. The cost of waiting two more minutes is that a resource deadlock may last; two extra minutes. We address deadlocks by scaling up and limiting concurrent; PR tests to four.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6878
https://github.com/hail-is/hail/pull/6878:420,Performance,concurren,concurrent,420,"Spinning up a VM takes around two minutes. Downloading fresh container images; takes additional time, maybe a whole minute. The cost of timing out is high: an otherwise passing PR test run may fail; demanding a bump and delaying merging of said PR by fifteen to twenty minutes. The cost of waiting two more minutes is that a resource deadlock may last; two extra minutes. We address deadlocks by scaling up and limiting concurrent; PR tests to four.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6878
https://github.com/hail-is/hail/pull/6878:180,Testability,test,test,180,"Spinning up a VM takes around two minutes. Downloading fresh container images; takes additional time, maybe a whole minute. The cost of timing out is high: an otherwise passing PR test run may fail; demanding a bump and delaying merging of said PR by fifteen to twenty minutes. The cost of waiting two more minutes is that a resource deadlock may last; two extra minutes. We address deadlocks by scaling up and limiting concurrent; PR tests to four.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6878
https://github.com/hail-is/hail/pull/6878:435,Testability,test,tests,435,"Spinning up a VM takes around two minutes. Downloading fresh container images; takes additional time, maybe a whole minute. The cost of timing out is high: an otherwise passing PR test run may fail; demanding a bump and delaying merging of said PR by fifteen to twenty minutes. The cost of waiting two more minutes is that a resource deadlock may last; two extra minutes. We address deadlocks by scaling up and limiting concurrent; PR tests to four.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6878
https://github.com/hail-is/hail/issues/6880:1606,Availability,down,down,1606,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6880
https://github.com/hail-is/hail/issues/6880:1187,Modifiability,enhance,enhance,1187,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6880
https://github.com/hail-is/hail/issues/6880:1755,Performance,load,loading,1755,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6880
https://github.com/hail-is/hail/issues/6880:797,Testability,test,test,797,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6880
https://github.com/hail-is/hail/issues/6880:1328,Usability,simpl,simple,1328,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6880
https://github.com/hail-is/hail/issues/6881:474,Energy Efficiency,efficient,efficient,474,"There are three ways to represent a homogenous set of named values of type `T`.; 1. an `array<T>` and a `dict<str, int>`, the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/issues/6881:841,Energy Efficiency,efficient,efficient,841,"There are three ways to represent a homogenous set of named values of type `T`.; 1. an `array<T>` and a `dict<str, int>`, the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/issues/6881:1171,Performance,cache,cache-friendly,1171," the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by code duplication. ## Phase 2; Implement a new physical type that implements homogenous operations without code duplication.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/issues/6881:829,Security,access,access-time-efficient,829,"There are three ways to represent a homogenous set of named values of type `T`.; 1. an `array<T>` and a `dict<str, int>`, the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/issues/6881:1049,Security,access,accessing,1049,"a homogenous set of named values of type `T`.; 1. an `array<T>` and a `dict<str, int>`, the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by code duplication. ## Phase 2; Implem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/issues/6881:945,Testability,log,logarithmic,945,"There are three ways to represent a homogenous set of named values of type `T`.; 1. an `array<T>` and a `dict<str, int>`, the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/issues/6881:1031,Usability,user-friendly,user-friendly,1031,"a homogenous set of named values of type `T`.; 1. an `array<T>` and a `dict<str, int>`, the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by code duplication. ## Phase 2; Implem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/issues/6881:1093,Usability,user-friendly,user-friendly,1093,"nd a `dict<str, int>`, the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by code duplication. ## Phase 2; Implement a new physical type that implements homogenous operations wit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/issues/6881:1405,Usability,user-friendly,user-friendly,1405," the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by code duplication. ## Phase 2; Implement a new physical type that implements homogenous operations without code duplication.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6881
https://github.com/hail-is/hail/pull/6882:1315,Safety,safe,safe,1315,"The `MatrixEntriesTable` lowering rule was broken. It fails when the input `MatrixTable` has multiple key fields, which appear out of order in the row struct. `MatrixEntriesTable` has to do an `TableAggregateByKey`, which makes a table with row type `Struct{keyFields..., aggResult}`. In particular, it rearranges the key fields to the key order. Then the later; ```scala; mapRows('row.dropFields(toExplode).insertStruct('row (toExplode),; ordering = Some(x.typ.rowType.fieldNames.toFastIndexedSeq))); ```; fails, because it tries to put the key fields back in their original order. I've fixed this by changing the above line to a `mapRows(makeStruct(...))`, but I don't see any good reason for the restriction that `InsertFields` must preserve the relative ordering of old fields. Another fix, which I prefer, is to change the typecheck rule for `InsertFields` to; ```scala; case x@InsertFields(old, fields, fieldOrder) =>; fieldOrder.foreach { fds =>; val fieldsMap = scala.collection.mutable.Map(; old.typ.asInstanceOf[TStruct].fields.map(f => f.name -> -f.typ): _*); fieldsMap ++= fields.map { case (name, ir) => name -> ir.typ }. assert(fds.forall { f =>; fieldsMap.get(f).forall(_ == -x.typ.fieldType(f)); }); assert(fds.length == x.typ.size); ```; As far as I can tell, code generation for `InsertFields` is safe for this relaxed typechecking. @tpoterba Can you weigh in on the `InsertFields` semantics?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6882
https://github.com/hail-is/hail/pull/6882:1135,Testability,assert,assert,1135,"The `MatrixEntriesTable` lowering rule was broken. It fails when the input `MatrixTable` has multiple key fields, which appear out of order in the row struct. `MatrixEntriesTable` has to do an `TableAggregateByKey`, which makes a table with row type `Struct{keyFields..., aggResult}`. In particular, it rearranges the key fields to the key order. Then the later; ```scala; mapRows('row.dropFields(toExplode).insertStruct('row (toExplode),; ordering = Some(x.typ.rowType.fieldNames.toFastIndexedSeq))); ```; fails, because it tries to put the key fields back in their original order. I've fixed this by changing the above line to a `mapRows(makeStruct(...))`, but I don't see any good reason for the restriction that `InsertFields` must preserve the relative ordering of old fields. Another fix, which I prefer, is to change the typecheck rule for `InsertFields` to; ```scala; case x@InsertFields(old, fields, fieldOrder) =>; fieldOrder.foreach { fds =>; val fieldsMap = scala.collection.mutable.Map(; old.typ.asInstanceOf[TStruct].fields.map(f => f.name -> -f.typ): _*); fieldsMap ++= fields.map { case (name, ir) => name -> ir.typ }. assert(fds.forall { f =>; fieldsMap.get(f).forall(_ == -x.typ.fieldType(f)); }); assert(fds.length == x.typ.size); ```; As far as I can tell, code generation for `InsertFields` is safe for this relaxed typechecking. @tpoterba Can you weigh in on the `InsertFields` semantics?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6882
https://github.com/hail-is/hail/pull/6882:1216,Testability,assert,assert,1216,"The `MatrixEntriesTable` lowering rule was broken. It fails when the input `MatrixTable` has multiple key fields, which appear out of order in the row struct. `MatrixEntriesTable` has to do an `TableAggregateByKey`, which makes a table with row type `Struct{keyFields..., aggResult}`. In particular, it rearranges the key fields to the key order. Then the later; ```scala; mapRows('row.dropFields(toExplode).insertStruct('row (toExplode),; ordering = Some(x.typ.rowType.fieldNames.toFastIndexedSeq))); ```; fails, because it tries to put the key fields back in their original order. I've fixed this by changing the above line to a `mapRows(makeStruct(...))`, but I don't see any good reason for the restriction that `InsertFields` must preserve the relative ordering of old fields. Another fix, which I prefer, is to change the typecheck rule for `InsertFields` to; ```scala; case x@InsertFields(old, fields, fieldOrder) =>; fieldOrder.foreach { fds =>; val fieldsMap = scala.collection.mutable.Map(; old.typ.asInstanceOf[TStruct].fields.map(f => f.name -> -f.typ): _*); fieldsMap ++= fields.map { case (name, ir) => name -> ir.typ }. assert(fds.forall { f =>; fieldsMap.get(f).forall(_ == -x.typ.fieldType(f)); }); assert(fds.length == x.typ.size); ```; As far as I can tell, code generation for `InsertFields` is safe for this relaxed typechecking. @tpoterba Can you weigh in on the `InsertFields` semantics?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6882
https://github.com/hail-is/hail/issues/6883:218,Usability,clear,clear,218,Akhil is trying to run `ld_prune` and it fails on a matrix table with these dimensions:. 55k samples; 1.9 million variants; r_2 = 0.1. A run with r_2 = 0.2 succeeded. I'm concerned that ld_prune pervasively forgets to clear its,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6883
https://github.com/hail-is/hail/pull/6885:18,Availability,error,error,18,"this IR caused an error in the C++ emitter because `Streamify` ""skipped over"" the `ArraySort` and didn't streamify it properly (since ArraySort expects a stream as input, not an array); ```scala; ArrayMap(; ArraySort(; MakeArray(; Seq(I32(1), I32(2), I32(3)),; TArray(TInt32()))),; ""x"",; Ref(""x"", TInt32()) * 5); ```. this PR fixes this by hopefully making Streamify more straightforward",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6885
https://github.com/hail-is/hail/pull/6887:155,Security,expose,expose,155,"This puts a local filesystem option that is at parity with the remote option, and is triggered by the absence of a gs:// prefix. A question: do we want to expose an rm?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6887
https://github.com/hail-is/hail/issues/6889:123,Availability,error,erroring,123,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6889
https://github.com/hail-is/hail/issues/6889:318,Availability,ping,ping,318,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6889
https://github.com/hail-is/hail/issues/6889:1253,Modifiability,Rewrite,RewriteBottomUp,1253,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6889
https://github.com/hail-is/hail/issues/6889:1286,Modifiability,Rewrite,RewriteBottomUp,1286,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6889
https://github.com/hail-is/hail/issues/6889:1303,Modifiability,rewrite,rewrite,1303,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6889
https://github.com/hail-is/hail/issues/6889:1313,Modifiability,Rewrite,RewriteBottomUp,1313,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6889
https://github.com/hail-is/hail/issues/6889:1359,Modifiability,Rewrite,RewriteBottomUp,1359,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6889
https://github.com/hail-is/hail/issues/6889:1382,Modifiability,Rewrite,RewriteBottomUp,1382,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6889
https://github.com/hail-is/hail/pull/6891:114,Safety,avoid,avoid,114,"I like the != syntax but it is not supported by the Make shipped; with recent OS X versions, so I think we should avoid it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6891
https://github.com/hail-is/hail/pull/6892:0,Testability,test,testing,0,testing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892
https://github.com/hail-is/hail/pull/6896:52,Availability,down,downcasts,52,This should hopefully save on computing things like downcasts for hom-refs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6896
https://github.com/hail-is/hail/pull/6897:42,Availability,redundant,redundant,42,Fixes #6888. Also simplify code to remove redundant checks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6897
https://github.com/hail-is/hail/pull/6897:42,Safety,redund,redundant,42,Fixes #6888. Also simplify code to remove redundant checks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6897
https://github.com/hail-is/hail/pull/6897:18,Usability,simpl,simplify,18,Fixes #6888. Also simplify code to remove redundant checks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6897
https://github.com/hail-is/hail/issues/6898:40,Testability,log,log,40,https://ci.hail.is/batches/9894/jobs/30/log. ```; /usr/local/lib/python3.6/dist-packages/sphinx/events.py:76: RemovedInSphinx30Warning: autodoc_default_flags is now deprecated. Please use autodoc_default_options instead.; results.append(callback(*args)); /mm42biwzygro/python/hail/docs/_templates/layout.html:60: RemovedInSphinx30Warning: To modify script_files in the theme is deprecated. Please insert a <script> tag directly in your theme instead.; {%- block linktags %}; /mm42biwzygro/python/hail/docs/_templates/layout.html:72: RemovedInSphinx30Warning: To modify script_files in the theme is deprecated. Please insert a <script> tag directly in your theme instead.; {%- if hasdoc('copyright') %}; /usr/local/lib/python3.6/dist-packages/sphinx_rtd_theme/search.html:20: RemovedInSphinx30Warning: To modify script_files in the theme is deprecated. Please insert a <script> tag directly in your theme instead.; {{ super() }}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6898
https://github.com/hail-is/hail/pull/6904:177,Deployability,update,update,177,"Updating some docs since I found spots that were out of date and I got really confused trying to refresh myself on what it did just now. Also, I think the hom_ref case needs to update the GQ if the PL is recalculated, so I did that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6904
https://github.com/hail-is/hail/issues/6905:115,Deployability,pipeline,pipelines,115,"Hail's optimizer should be smart enough to push `TableFilter` into a `TableExplode`. Consider these two equivalent pipelines on *tiny* data, a ten-by-ten matrix. ```; import hail as hl; mt = hl.balding_nichols_model(3, 10, 10); t = mt.entries(); t.filter(t.GT.is_hom_ref()).export('foo.tsv'); ```; ```; foo.tsv; merge time: 45.459ms; ```. ```; import hail as hl; mt = hl.balding_nichols_model(3,10,10); mt.filter_entries(mt.GT.is_hom_ref()).entries().export('foo2.tsv'); ```; ```; foo2.tsv; merge time: 23.856ms; ```. This will likely also require improving Hail's filter movement. I observed a `TableFilter` getting stuck behind a `TableMapGlobals`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6905
https://github.com/hail-is/hail/issues/6905:7,Performance,optimiz,optimizer,7,"Hail's optimizer should be smart enough to push `TableFilter` into a `TableExplode`. Consider these two equivalent pipelines on *tiny* data, a ten-by-ten matrix. ```; import hail as hl; mt = hl.balding_nichols_model(3, 10, 10); t = mt.entries(); t.filter(t.GT.is_hom_ref()).export('foo.tsv'); ```; ```; foo.tsv; merge time: 45.459ms; ```. ```; import hail as hl; mt = hl.balding_nichols_model(3,10,10); mt.filter_entries(mt.GT.is_hom_ref()).entries().export('foo2.tsv'); ```; ```; foo2.tsv; merge time: 23.856ms; ```. This will likely also require improving Hail's filter movement. I observed a `TableFilter` getting stuck behind a `TableMapGlobals`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6905
https://github.com/hail-is/hail/pull/6906:549,Testability,log,logic,549,"changed the signature of the `deepCopy` functions that take source and destination parameters, replacing the source parameter with a ""value"" parameter. this way you can pass in primitive values to `deepCopy` and it will call `storePrimitive`. the reasoning for this is that most applications of `deepCopy` want this behavior, previously having to match over the type to determine if its a primitive. now they can just call `deepCopy` with any PType. @patrick-schultz suggested this change in #6858, so if this gets merged i can simplify some of the logic in that PR as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6906
https://github.com/hail-is/hail/pull/6906:528,Usability,simpl,simplify,528,"changed the signature of the `deepCopy` functions that take source and destination parameters, replacing the source parameter with a ""value"" parameter. this way you can pass in primitive values to `deepCopy` and it will call `storePrimitive`. the reasoning for this is that most applications of `deepCopy` want this behavior, previously having to match over the type to determine if its a primitive. now they can just call `deepCopy` with any PType. @patrick-schultz suggested this change in #6858, so if this gets merged i can simplify some of the logic in that PR as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6906
https://github.com/hail-is/hail/issues/6907:534,Availability,error,error,534,"I think this is probably a bug, since we write out the reference genome and seem to support it just fine in Scala. I want to be able to do e.g.:. ```; >>> import hail as hl; >>> rg = hl.ReferenceGenome(""foo"", ['a', 'b'], {'a': 4, 'b': 6}); >>> t = hl.utils.range_table(10); >>> t = t.annotate(locus=hl.locus_from_global_position(t.idx, reference_genome='foo')); >>> t.write('test.t'); ```. and then, in a separate instance of hail, do:. ```; >>> import hail as hl; >>> t = hl.read_table('test.t'); ```. Currently, I get the following error:; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/Users/wang/code/hail/hail/python/hail/expr/type_parsing.py"", line 80, in visit_locus; return hl.tlocus(gr); File ""<decorator-gen-56>"", line 2, in __init__; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 512, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 56, in check; return tc.check(x, caller, param); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 303, in check; return f(tc.check(x, caller, param)); File ""/Users/wang/code/hail/hail/python/hail/genetics/reference_genome.py"", line 10, in <lambda>; reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type); File ""/Users/wang/code/hail/hail/python/hail/context.py"", line 362, in get_reference; return ReferenceGenome._references[name]; KeyError: 'foo'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in read_table; File ""/Users/wang/code/h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6907
https://github.com/hail-is/hail/issues/6907:966,Integrability,wrap,wrapper,966," we write out the reference genome and seem to support it just fine in Scala. I want to be able to do e.g.:. ```; >>> import hail as hl; >>> rg = hl.ReferenceGenome(""foo"", ['a', 'b'], {'a': 4, 'b': 6}); >>> t = hl.utils.range_table(10); >>> t = t.annotate(locus=hl.locus_from_global_position(t.idx, reference_genome='foo')); >>> t.write('test.t'); ```. and then, in a separate instance of hail, do:. ```; >>> import hail as hl; >>> t = hl.read_table('test.t'); ```. Currently, I get the following error:; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/Users/wang/code/hail/hail/python/hail/expr/type_parsing.py"", line 80, in visit_locus; return hl.tlocus(gr); File ""<decorator-gen-56>"", line 2, in __init__; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 512, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 56, in check; return tc.check(x, caller, param); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 303, in check; return f(tc.check(x, caller, param)); File ""/Users/wang/code/hail/hail/python/hail/genetics/reference_genome.py"", line 10, in <lambda>; reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type); File ""/Users/wang/code/hail/hail/python/hail/context.py"", line 362, in get_reference; return ReferenceGenome._references[name]; KeyError: 'foo'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in read_table; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6907
https://github.com/hail-is/hail/issues/6907:2056,Integrability,wrap,wrapper,2056,"on/hail/typecheck/check.py"", line 512, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 56, in check; return tc.check(x, caller, param); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 303, in check; return f(tc.check(x, caller, param)); File ""/Users/wang/code/hail/hail/python/hail/genetics/reference_genome.py"", line 10, in <lambda>; reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type); File ""/Users/wang/code/hail/hail/python/hail/context.py"", line 362, in get_reference; return ReferenceGenome._references[name]; KeyError: 'foo'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in read_table; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/methods/impex.py"", line 2196, in read_table; return Table(TableRead(tr, False)); File ""/Users/wang/code/hail/hail/python/hail/table.py"", line 336, in __init__; self._type = self._tir.typ; File ""/Users/wang/code/hail/hail/python/hail/ir/base_ir.py"", line 142, in typ; self._compute_type(); File ""/Users/wang/code/hail/hail/python/hail/ir/table_ir.py"", line 201, in _compute_type; self._type = Env.backend().table_type(self); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 120, in table_type; return ttable._from_java(jir.typ()); File ""/Users/wang/code/hail/hail/python/hail/expr/table_type.py"", line 11, in _from_java; dtype(jtt.rowType().toString()),; File ""/Users/wang/code/hail/hail/python/hail/expr/types.py"", line 110, in dtype; return type_node_visitor.visit(tree); File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/anacond",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6907
https://github.com/hail-is/hail/issues/6907:6467,Integrability,wrap,wrapper,6467,"anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in <listcomp>; return method(node, [self.visit(n) for n in node]); File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 227, in visit; reraise(VisitationError, VisitationError(exc, exc_class, node), tb); File ""/anaconda3/lib/python3.6/site-packages/six.py"", line 692, in reraise; raise value.with_traceback(tb); File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/Users/wang/code/hail/hail/python/hail/expr/type_parsing.py"", line 80, in visit_locus; return hl.tlocus(gr); File ""<decorator-gen-56>"", line 2, in __init__; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 512, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 56, in check; return tc.check(x, caller, param); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 303, in check; return f(tc.check(x, caller, param)); File ""/Users/wang/code/hail/hail/python/hail/genetics/reference_genome.py"", line 10, in <lambda>; reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type); File ""/Users/wang/code/hail/hail/python/hail/context.py"", line 362, in get_reference; return ReferenceGenome._references[name]; parsimonious.exceptions.VisitationError: KeyError: 'foo'. Parse tree:; <Node called ""locus"" matching ""locus<foo>""> <-- *** We were here. ***; <Node matching ""locus"">; <Node matching ""locus"">; <RegexNode called ""_"" matching """">; <Node matching ""<"">; <Node called ""identifi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6907
https://github.com/hail-is/hail/issues/6907:375,Testability,test,test,375,"I think this is probably a bug, since we write out the reference genome and seem to support it just fine in Scala. I want to be able to do e.g.:. ```; >>> import hail as hl; >>> rg = hl.ReferenceGenome(""foo"", ['a', 'b'], {'a': 4, 'b': 6}); >>> t = hl.utils.range_table(10); >>> t = t.annotate(locus=hl.locus_from_global_position(t.idx, reference_genome='foo')); >>> t.write('test.t'); ```. and then, in a separate instance of hail, do:. ```; >>> import hail as hl; >>> t = hl.read_table('test.t'); ```. Currently, I get the following error:; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/Users/wang/code/hail/hail/python/hail/expr/type_parsing.py"", line 80, in visit_locus; return hl.tlocus(gr); File ""<decorator-gen-56>"", line 2, in __init__; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 512, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 56, in check; return tc.check(x, caller, param); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 303, in check; return f(tc.check(x, caller, param)); File ""/Users/wang/code/hail/hail/python/hail/genetics/reference_genome.py"", line 10, in <lambda>; reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type); File ""/Users/wang/code/hail/hail/python/hail/context.py"", line 362, in get_reference; return ReferenceGenome._references[name]; KeyError: 'foo'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in read_table; File ""/Users/wang/code/h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6907
https://github.com/hail-is/hail/issues/6907:488,Testability,test,test,488,"I think this is probably a bug, since we write out the reference genome and seem to support it just fine in Scala. I want to be able to do e.g.:. ```; >>> import hail as hl; >>> rg = hl.ReferenceGenome(""foo"", ['a', 'b'], {'a': 4, 'b': 6}); >>> t = hl.utils.range_table(10); >>> t = t.annotate(locus=hl.locus_from_global_position(t.idx, reference_genome='foo')); >>> t.write('test.t'); ```. and then, in a separate instance of hail, do:. ```; >>> import hail as hl; >>> t = hl.read_table('test.t'); ```. Currently, I get the following error:; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/Users/wang/code/hail/hail/python/hail/expr/type_parsing.py"", line 80, in visit_locus; return hl.tlocus(gr); File ""<decorator-gen-56>"", line 2, in __init__; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 512, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 56, in check; return tc.check(x, caller, param); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 303, in check; return f(tc.check(x, caller, param)); File ""/Users/wang/code/hail/hail/python/hail/genetics/reference_genome.py"", line 10, in <lambda>; reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type); File ""/Users/wang/code/hail/hail/python/hail/context.py"", line 362, in get_reference; return ReferenceGenome._references[name]; KeyError: 'foo'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in read_table; File ""/Users/wang/code/h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6907
https://github.com/hail-is/hail/pull/6908:65,Availability,error,error,65,"Caveats:; * the copy-to-GS at the end is crashing without a good error message,; but probably permissions, even though I've given my service account; access to that bucket.; * this runs all benchmarks in replicate. We should split them up; in a randomized (deterministic?) way so that the wall time is; shorter.; * needs to dump into a database instead of json files on GS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6908
https://github.com/hail-is/hail/pull/6908:71,Integrability,message,message,71,"Caveats:; * the copy-to-GS at the end is crashing without a good error message,; but probably permissions, even though I've given my service account; access to that bucket.; * this runs all benchmarks in replicate. We should split them up; in a randomized (deterministic?) way so that the wall time is; shorter.; * needs to dump into a database instead of json files on GS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6908
https://github.com/hail-is/hail/pull/6908:150,Security,access,access,150,"Caveats:; * the copy-to-GS at the end is crashing without a good error message,; but probably permissions, even though I've given my service account; access to that bucket.; * this runs all benchmarks in replicate. We should split them up; in a randomized (deterministic?) way so that the wall time is; shorter.; * needs to dump into a database instead of json files on GS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6908
https://github.com/hail-is/hail/pull/6908:190,Testability,benchmark,benchmarks,190,"Caveats:; * the copy-to-GS at the end is crashing without a good error message,; but probably permissions, even though I've given my service account; access to that bucket.; * this runs all benchmarks in replicate. We should split them up; in a randomized (deterministic?) way so that the wall time is; shorter.; * needs to dump into a database instead of json files on GS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6908
https://github.com/hail-is/hail/pull/6912:113,Integrability,interface,interface,113,"Tests pass. Have an adjustment to the assertPType to make the check deep on PStruct and PArray . Happy to adjust interface, add additional assertions. The todo comments will all go, just to raise a few questions I had. Also, I preferred to pass false to the constructors, instead of relying on default, because this is more explicitly indicating intentions, and defaults only auto-populate for those using IDEs. Similarly, I explicitly checked booleans, but I suspect the preference is not that. cc @cseed, @chrisvittal, @patrick-schultz, @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6912
https://github.com/hail-is/hail/pull/6912:0,Testability,Test,Tests,0,"Tests pass. Have an adjustment to the assertPType to make the check deep on PStruct and PArray . Happy to adjust interface, add additional assertions. The todo comments will all go, just to raise a few questions I had. Also, I preferred to pass false to the constructors, instead of relying on default, because this is more explicitly indicating intentions, and defaults only auto-populate for those using IDEs. Similarly, I explicitly checked booleans, but I suspect the preference is not that. cc @cseed, @chrisvittal, @patrick-schultz, @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6912
https://github.com/hail-is/hail/pull/6912:38,Testability,assert,assertPType,38,"Tests pass. Have an adjustment to the assertPType to make the check deep on PStruct and PArray . Happy to adjust interface, add additional assertions. The todo comments will all go, just to raise a few questions I had. Also, I preferred to pass false to the constructors, instead of relying on default, because this is more explicitly indicating intentions, and defaults only auto-populate for those using IDEs. Similarly, I explicitly checked booleans, but I suspect the preference is not that. cc @cseed, @chrisvittal, @patrick-schultz, @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6912
https://github.com/hail-is/hail/pull/6912:139,Testability,assert,assertions,139,"Tests pass. Have an adjustment to the assertPType to make the check deep on PStruct and PArray . Happy to adjust interface, add additional assertions. The todo comments will all go, just to raise a few questions I had. Also, I preferred to pass false to the constructors, instead of relying on default, because this is more explicitly indicating intentions, and defaults only auto-populate for those using IDEs. Similarly, I explicitly checked booleans, but I suspect the preference is not that. cc @cseed, @chrisvittal, @patrick-schultz, @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6912
https://github.com/hail-is/hail/pull/6915:19,Deployability,deploy,deploy,19,Simplified the dev deploy interface: just specify fully qualified branch (user/repo:branch) and a list of steps (instead of profile) which are transitively closed over dependencies. Pick up namespace from the user database.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6915
https://github.com/hail-is/hail/pull/6915:26,Integrability,interface,interface,26,Simplified the dev deploy interface: just specify fully qualified branch (user/repo:branch) and a list of steps (instead of profile) which are transitively closed over dependencies. Pick up namespace from the user database.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6915
https://github.com/hail-is/hail/pull/6915:168,Integrability,depend,dependencies,168,Simplified the dev deploy interface: just specify fully qualified branch (user/repo:branch) and a list of steps (instead of profile) which are transitively closed over dependencies. Pick up namespace from the user database.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6915
https://github.com/hail-is/hail/pull/6915:0,Usability,Simpl,Simplified,0,Simplified the dev deploy interface: just specify fully qualified branch (user/repo:branch) and a list of steps (instead of profile) which are transitively closed over dependencies. Pick up namespace from the user database.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6915
https://github.com/hail-is/hail/pull/6916:337,Integrability,depend,depend,337,"We were getting warnings of the form:. `DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working`. because Python has moved abstract collections like `Iterable` from `collections` to `collections.abc`. We still get some warnings from packages we depend on, but I've addressed all instances of this I saw warnings for in our hail python code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6916
https://github.com/hail-is/hail/pull/6918:239,Deployability,configurat,configuration,239,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918
https://github.com/hail-is/hail/pull/6918:60,Integrability,rout,router,60,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918
https://github.com/hail-is/hail/pull/6918:173,Integrability,rout,router,173,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918
https://github.com/hail-is/hail/pull/6918:239,Modifiability,config,configuration,239,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918
https://github.com/hail-is/hail/pull/6918:37,Security,authenticat,authentication,37,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918
https://github.com/hail-is/hail/pull/6918:130,Security,access,access,130,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918
https://github.com/hail-is/hail/pull/6918:149,Security,encrypt,encryption,149,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918
https://github.com/hail-is/hail/pull/6918:111,Testability,test,test,111,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918
https://github.com/hail-is/hail/pull/6919:47,Deployability,update,update-hail-version,47,"Running the command `hailctl dataproc modify --update-hail-version` will update the version of hail running on the cluster to the version you're currently running locally. So if user has a cluster already and we tell them to update hail, they can just do:; ```; pip install hail -U; hailctl dataproc modify my-cluster --update-hail-version; ```. This PR resolves #6674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6919
https://github.com/hail-is/hail/pull/6919:73,Deployability,update,update,73,"Running the command `hailctl dataproc modify --update-hail-version` will update the version of hail running on the cluster to the version you're currently running locally. So if user has a cluster already and we tell them to update hail, they can just do:; ```; pip install hail -U; hailctl dataproc modify my-cluster --update-hail-version; ```. This PR resolves #6674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6919
https://github.com/hail-is/hail/pull/6919:225,Deployability,update,update,225,"Running the command `hailctl dataproc modify --update-hail-version` will update the version of hail running on the cluster to the version you're currently running locally. So if user has a cluster already and we tell them to update hail, they can just do:; ```; pip install hail -U; hailctl dataproc modify my-cluster --update-hail-version; ```. This PR resolves #6674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6919
https://github.com/hail-is/hail/pull/6919:266,Deployability,install,install,266,"Running the command `hailctl dataproc modify --update-hail-version` will update the version of hail running on the cluster to the version you're currently running locally. So if user has a cluster already and we tell them to update hail, they can just do:; ```; pip install hail -U; hailctl dataproc modify my-cluster --update-hail-version; ```. This PR resolves #6674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6919
https://github.com/hail-is/hail/pull/6919:320,Deployability,update,update-hail-version,320,"Running the command `hailctl dataproc modify --update-hail-version` will update the version of hail running on the cluster to the version you're currently running locally. So if user has a cluster already and we tell them to update hail, they can just do:; ```; pip install hail -U; hailctl dataproc modify my-cluster --update-hail-version; ```. This PR resolves #6674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6919
https://github.com/hail-is/hail/pull/6920:4,Deployability,release,releaseJar,4,Our releaseJar example should not use Spark version 2.3.0 when we don't actually want users to use that version,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6920
https://github.com/hail-is/hail/pull/6923:796,Integrability,depend,dependencies,796,"Our build system is a bit of a tangled mess. There's `build.yaml`, `build.gradle`, `hail/Makefile`, and `src/main/c/Makefile`. In this PR, I've tried to minimize `build.gradle`'s role to just JVM compilation and JAR production. I've removed a number of shell scripts that were previously gluing together build steps. I've either removed the need for that glue or incorporated it into `hail/Makefile`. I did not attempt to unify `src/main/c/Makefile` with `hail/Makefile`. I started to unify `build.yaml` and `hail/Makefile` but it grew into a bigger project than I want to tackle. A couple things I expect to raise questions:. Q. Where did COMPILE_NATIVES go?; A: I moved tightly around native-lib, which is the only rule it affects. Q: Why did you remove env/SHORT_REVISION from $(SHADOW_JAR)'s dependencies?; A: It is not an immediate dependency, it's transitive through the $(SCALA_BUILD_INFO). Q: Why did you remove `define properties ...`?; A: AFAIK, this had no effect and was unused. Q: Why inline $(DATE)?; A: Obviated the need for a comment explaining why we didn't make it an `env_var`. Q: Why no parallelism in pytest and doctest?; A: Hail uses all cores by default. Getting parallelism right in pytest and doctest would require a bit of smarts. I'm doubt its worth the effort since we have CI and you can test specific tests. Q: What the hell is `pgradle` and the `+` prefix?; A: This uses the [Make job server](https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html#POSIX-Jobserver) to cooperatively obey parallelism limits (`--jobs`). It uses a pair of pipes to claim and return *extra* CPU reservations (we implicitly have one core). Hail's JVM tests use two core clusters, so I set PARALLELISM to half the number of cores we could claim (with a minimum of one). I did not use this for shadowJar because there is no parallelism to be had. Q: How did you fix references.html?; A: `build.yaml` and `hail/Makefile` both use `www/Makefile` which now correctly finds all the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923
https://github.com/hail-is/hail/pull/6923:837,Integrability,depend,dependency,837,"Our build system is a bit of a tangled mess. There's `build.yaml`, `build.gradle`, `hail/Makefile`, and `src/main/c/Makefile`. In this PR, I've tried to minimize `build.gradle`'s role to just JVM compilation and JAR production. I've removed a number of shell scripts that were previously gluing together build steps. I've either removed the need for that glue or incorporated it into `hail/Makefile`. I did not attempt to unify `src/main/c/Makefile` with `hail/Makefile`. I started to unify `build.yaml` and `hail/Makefile` but it grew into a bigger project than I want to tackle. A couple things I expect to raise questions:. Q. Where did COMPILE_NATIVES go?; A: I moved tightly around native-lib, which is the only rule it affects. Q: Why did you remove env/SHORT_REVISION from $(SHADOW_JAR)'s dependencies?; A: It is not an immediate dependency, it's transitive through the $(SCALA_BUILD_INFO). Q: Why did you remove `define properties ...`?; A: AFAIK, this had no effect and was unused. Q: Why inline $(DATE)?; A: Obviated the need for a comment explaining why we didn't make it an `env_var`. Q: Why no parallelism in pytest and doctest?; A: Hail uses all cores by default. Getting parallelism right in pytest and doctest would require a bit of smarts. I'm doubt its worth the effort since we have CI and you can test specific tests. Q: What the hell is `pgradle` and the `+` prefix?; A: This uses the [Make job server](https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html#POSIX-Jobserver) to cooperatively obey parallelism limits (`--jobs`). It uses a pair of pipes to claim and return *extra* CPU reservations (we implicitly have one core). Hail's JVM tests use two core clusters, so I set PARALLELISM to half the number of cores we could claim (with a minimum of one). I did not use this for shadowJar because there is no parallelism to be had. Q: How did you fix references.html?; A: `build.yaml` and `hail/Makefile` both use `www/Makefile` which now correctly finds all the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923
https://github.com/hail-is/hail/pull/6923:1317,Testability,test,test,1317,"e`'s role to just JVM compilation and JAR production. I've removed a number of shell scripts that were previously gluing together build steps. I've either removed the need for that glue or incorporated it into `hail/Makefile`. I did not attempt to unify `src/main/c/Makefile` with `hail/Makefile`. I started to unify `build.yaml` and `hail/Makefile` but it grew into a bigger project than I want to tackle. A couple things I expect to raise questions:. Q. Where did COMPILE_NATIVES go?; A: I moved tightly around native-lib, which is the only rule it affects. Q: Why did you remove env/SHORT_REVISION from $(SHADOW_JAR)'s dependencies?; A: It is not an immediate dependency, it's transitive through the $(SCALA_BUILD_INFO). Q: Why did you remove `define properties ...`?; A: AFAIK, this had no effect and was unused. Q: Why inline $(DATE)?; A: Obviated the need for a comment explaining why we didn't make it an `env_var`. Q: Why no parallelism in pytest and doctest?; A: Hail uses all cores by default. Getting parallelism right in pytest and doctest would require a bit of smarts. I'm doubt its worth the effort since we have CI and you can test specific tests. Q: What the hell is `pgradle` and the `+` prefix?; A: This uses the [Make job server](https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html#POSIX-Jobserver) to cooperatively obey parallelism limits (`--jobs`). It uses a pair of pipes to claim and return *extra* CPU reservations (we implicitly have one core). Hail's JVM tests use two core clusters, so I set PARALLELISM to half the number of cores we could claim (with a minimum of one). I did not use this for shadowJar because there is no parallelism to be had. Q: How did you fix references.html?; A: `build.yaml` and `hail/Makefile` both use `www/Makefile` which now correctly finds all the markdown files instead of a limited list. Q: Why did you change conftest.py?; A: It is now location independent, so you can run doctests using `python3 setup.py pytest ...`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923
https://github.com/hail-is/hail/pull/6923:1331,Testability,test,tests,1331,"e`'s role to just JVM compilation and JAR production. I've removed a number of shell scripts that were previously gluing together build steps. I've either removed the need for that glue or incorporated it into `hail/Makefile`. I did not attempt to unify `src/main/c/Makefile` with `hail/Makefile`. I started to unify `build.yaml` and `hail/Makefile` but it grew into a bigger project than I want to tackle. A couple things I expect to raise questions:. Q. Where did COMPILE_NATIVES go?; A: I moved tightly around native-lib, which is the only rule it affects. Q: Why did you remove env/SHORT_REVISION from $(SHADOW_JAR)'s dependencies?; A: It is not an immediate dependency, it's transitive through the $(SCALA_BUILD_INFO). Q: Why did you remove `define properties ...`?; A: AFAIK, this had no effect and was unused. Q: Why inline $(DATE)?; A: Obviated the need for a comment explaining why we didn't make it an `env_var`. Q: Why no parallelism in pytest and doctest?; A: Hail uses all cores by default. Getting parallelism right in pytest and doctest would require a bit of smarts. I'm doubt its worth the effort since we have CI and you can test specific tests. Q: What the hell is `pgradle` and the `+` prefix?; A: This uses the [Make job server](https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html#POSIX-Jobserver) to cooperatively obey parallelism limits (`--jobs`). It uses a pair of pipes to claim and return *extra* CPU reservations (we implicitly have one core). Hail's JVM tests use two core clusters, so I set PARALLELISM to half the number of cores we could claim (with a minimum of one). I did not use this for shadowJar because there is no parallelism to be had. Q: How did you fix references.html?; A: `build.yaml` and `hail/Makefile` both use `www/Makefile` which now correctly finds all the markdown files instead of a limited list. Q: Why did you change conftest.py?; A: It is now location independent, so you can run doctests using `python3 setup.py pytest ...`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923
https://github.com/hail-is/hail/pull/6923:1677,Testability,test,tests,1677,"e`'s role to just JVM compilation and JAR production. I've removed a number of shell scripts that were previously gluing together build steps. I've either removed the need for that glue or incorporated it into `hail/Makefile`. I did not attempt to unify `src/main/c/Makefile` with `hail/Makefile`. I started to unify `build.yaml` and `hail/Makefile` but it grew into a bigger project than I want to tackle. A couple things I expect to raise questions:. Q. Where did COMPILE_NATIVES go?; A: I moved tightly around native-lib, which is the only rule it affects. Q: Why did you remove env/SHORT_REVISION from $(SHADOW_JAR)'s dependencies?; A: It is not an immediate dependency, it's transitive through the $(SCALA_BUILD_INFO). Q: Why did you remove `define properties ...`?; A: AFAIK, this had no effect and was unused. Q: Why inline $(DATE)?; A: Obviated the need for a comment explaining why we didn't make it an `env_var`. Q: Why no parallelism in pytest and doctest?; A: Hail uses all cores by default. Getting parallelism right in pytest and doctest would require a bit of smarts. I'm doubt its worth the effort since we have CI and you can test specific tests. Q: What the hell is `pgradle` and the `+` prefix?; A: This uses the [Make job server](https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html#POSIX-Jobserver) to cooperatively obey parallelism limits (`--jobs`). It uses a pair of pipes to claim and return *extra* CPU reservations (we implicitly have one core). Hail's JVM tests use two core clusters, so I set PARALLELISM to half the number of cores we could claim (with a minimum of one). I did not use this for shadowJar because there is no parallelism to be had. Q: How did you fix references.html?; A: `build.yaml` and `hail/Makefile` both use `www/Makefile` which now correctly finds all the markdown files instead of a limited list. Q: Why did you change conftest.py?; A: It is now location independent, so you can run doctests using `python3 setup.py pytest ...`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923
https://github.com/hail-is/hail/pull/6925:4,Deployability,deploy,deploy,4,"Dev deploy is currently broken, this should hopefully fix. Issue is that we were treating the name of a step like it was a step object. I am calling the argument `requested_step_names` now to help keep track of the distinction. . @jigold @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6925
https://github.com/hail-is/hail/pull/6927:995,Availability,error,error,995,"This change removes previous infrastructure for building generated C++; code. The previous infrastructure would write two files, a cpp file and; a makefile, then run make to build the shared object. This change gets rid of all that in favor of a pipe-fork-exec model,; using the ability of clang++/g++ to read from stdin via `-x <LANG>` and; `-` arguments. Some notes:. * We still invoke the shell to find JAVA_HOME if it is not defined. We; do this in a similar way to what we do in the makefiles.; * Because of the odd signatures of the `exec` family of functions, we; use a `const_cast` to discard the appropriate qualifiers. This is safe; because we only do it after forking, and only to exec, and never use; that data after the call to `execvp`.; * We ignore `SIGPIPE`, as it is raised when the process tries to write; to a pipe where the other end is closed. Not doing this could crash; hail, and means that the child process died before we wrote all of the; c++ source to the pipe, other error handling will catch what actually; went wrong, rather than being unable to write to the pipe.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6927
https://github.com/hail-is/hail/pull/6927:637,Safety,safe,safe,637,"This change removes previous infrastructure for building generated C++; code. The previous infrastructure would write two files, a cpp file and; a makefile, then run make to build the shared object. This change gets rid of all that in favor of a pipe-fork-exec model,; using the ability of clang++/g++ to read from stdin via `-x <LANG>` and; `-` arguments. Some notes:. * We still invoke the shell to find JAVA_HOME if it is not defined. We; do this in a similar way to what we do in the makefiles.; * Because of the odd signatures of the `exec` family of functions, we; use a `const_cast` to discard the appropriate qualifiers. This is safe; because we only do it after forking, and only to exec, and never use; that data after the call to `execvp`.; * We ignore `SIGPIPE`, as it is raised when the process tries to write; to a pipe where the other end is closed. Not doing this could crash; hail, and means that the child process died before we wrote all of the; c++ source to the pipe, other error handling will catch what actually; went wrong, rather than being unable to write to the pipe.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6927
https://github.com/hail-is/hail/pull/6928:351,Deployability,configurat,configuration,351,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/pull/6928:490,Deployability,deploy,deployed,490,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/pull/6928:455,Energy Efficiency,monitor,monitoring,455,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/pull/6928:20,Integrability,rout,routing,20,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/pull/6928:63,Integrability,rout,routes,63,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/pull/6928:97,Integrability,rout,router,97,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/pull/6928:141,Integrability,rout,router,141,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/pull/6928:338,Integrability,rout,router,338,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/pull/6928:351,Modifiability,config,configuration,351,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928
https://github.com/hail-is/hail/issues/6929:1312,Availability,recover,recover,1312,"re are two ordering operations in hail: `Table.order_by`, and `MatrixTable.choose_cols`. Of the three keying operations, only `Table.key_by` and `MatrixTable.key_rows_by` enforce ordering. The ordering of the columns of a matrix table has no relationship to the keys of the columns of a MatrixTable. **NB**: when a table is created from the columns of a matrix table (`MatrixTable.cols`), the table is sorted by the keys (there exists an implicit `Table.key_by`). Moreover, we guarantee and document (in `MatrixTable.cols`) that when the matrix table has a zero-length column key, the table's ordering is given by matrix table columns' ordering. According to ""Ordering and keys in relational objects"", all sorts (whether triggered by an order_by or a key_by) are unstable. A common user operation is to export or collect a field of a relational object. Sometimes users do not want the keys of an expression exported or collected. In this situation, the user requires that the data is sorted in a sensible way (otherwise they cannot recover which item came from which key). Hail internally guarantees (but does not guarantee to our users or document) that localizing operations (take, collect, and show) and `export` produce data in the ordering of the relational object. For example:. ```; In [38]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx).show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+; ```. # Ordering and Library Developers. On occasion, a user may have a table of unknown ordering and keying. For example, the implementor of `Expression.collect` (e.g. `mt.GT.collect()`). In this situation, it is desirable to be able to remove keys without modifying the order. In particular, the values should appear in the same order that they appear in the relational object (for a table, in the order of the rows; for a matrix table, ordered first by the row and then by the column). For example, the multiplication table for 0 to 2:. ```; In [39",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6929
https://github.com/hail-is/hail/issues/6929:6325,Modifiability,inherit,inherited,6325,"by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.key_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+; ```. # Dealing With It. In practice, this following will remove the key on a table without changing the ordering imposed by previous order-changing operations. (NB: ""latent"" ordering inherited from a file [see aforementioned family id, sample id example] is not guaranteed to be preserved by this though, in practice, it often is). ```python; def unkey(t):; if len(t.key) != 0:; t = t.order_by(t.key); return t; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6929
https://github.com/hail-is/hail/issues/6929:4939,Performance,Optimiz,Optimizer,4939," then data must be sorted by the key and the ordering of rows with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6929
https://github.com/hail-is/hail/issues/6929:4970,Performance,optimiz,optimizer,4970," with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.key_by().show(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6929
https://github.com/hail-is/hail/issues/6929:1312,Safety,recover,recover,1312,"re are two ordering operations in hail: `Table.order_by`, and `MatrixTable.choose_cols`. Of the three keying operations, only `Table.key_by` and `MatrixTable.key_rows_by` enforce ordering. The ordering of the columns of a matrix table has no relationship to the keys of the columns of a MatrixTable. **NB**: when a table is created from the columns of a matrix table (`MatrixTable.cols`), the table is sorted by the keys (there exists an implicit `Table.key_by`). Moreover, we guarantee and document (in `MatrixTable.cols`) that when the matrix table has a zero-length column key, the table's ordering is given by matrix table columns' ordering. According to ""Ordering and keys in relational objects"", all sorts (whether triggered by an order_by or a key_by) are unstable. A common user operation is to export or collect a field of a relational object. Sometimes users do not want the keys of an expression exported or collected. In this situation, the user requires that the data is sorted in a sensible way (otherwise they cannot recover which item came from which key). Hail internally guarantees (but does not guarantee to our users or document) that localizing operations (take, collect, and show) and `export` produce data in the ordering of the relational object. For example:. ```; In [38]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx).show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+; ```. # Ordering and Library Developers. On occasion, a user may have a table of unknown ordering and keying. For example, the implementor of `Expression.collect` (e.g. `mt.GT.collect()`). In this situation, it is desirable to be able to remove keys without modifying the order. In particular, the values should appear in the same order that they appear in the relational object (for a table, in the order of the rows; for a matrix table, ordered first by the row and then by the column). For example, the multiplication table for 0 to 2:. ```; In [39",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6929
https://github.com/hail-is/hail/issues/6929:4051,Safety,safe,safe,4051," 0, 0, 2, 1, 0, 4, 2, 0]; ```. A library developer may want to remove keys while preserving order so as to implement the above methods. Because all sorts in Hail are unstable, this is a delicate feat. There are two cases: zero-length key, non-zero-length key. When the key is of zero-length, the data may be sorted in some unknown and arbitrary order. Consider for example:. ```; In [45]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().collect() ; ...: ; Out[45]: [Struct(col_idx=1), Struct(col_idx=2), Struct(col_idx=0)]; ```. Or importing data with no key. In this case it is crucial to *not* call `order_by()` or `key_by()` because both permit hail to arbitrarily reorder the entire dataset (we are unstably sorting by an empty key, ergo, all values are equal). When the key is of non-zero-length, then data must be sorted by the key and the ordering of rows with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6929
https://github.com/hail-is/hail/issues/6930:1078,Performance,perform,performance,1078,"There is no hail operation that removes keys and guarantees no change to order (see https://github.com/hail-is/hail/issues/6929). In `BaseExpression._to_table`, we use the following expression which works in practice but is not guaranteed to work.; ```python; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return ..., entries.select(name); ```. In particular:; - `order_by(*ds.row_key)` does not guarantee that entries from the same row are appear in the order of the corresponding columns; - `order_by(*ds.row_key)` does not guarantee that entries from a pair of rows with equivalent keys appear in the order of the corresponding rows. There exists an operation that guarantees this:; ```python; ridx = Env.get_uid(); cidx = Env.get_uid(); entries = ds.key_cols_by().add_row_index(ridx).add_col_index(cidx).entries(); entries = entries.order_by(ridx, cidx); return ..., entries.select(name); ```; But this requires a scan to produce the row index and then a scan to verify the table rows are in the correct order. We consider this unacceptable performance. Instead, we rely on the above expression and add the following tests which we hope will alert us if we accidentally change the behavior:. ```; rmt33 = hl.utils.range_matrix_table(3, 3, n_partitions=2). mt = rmt33.choose_cols([1, 2, 0]); assert mt.col.collect() == [hl.Struct(col_idx=x) for x in [1, 2, 0]]. mt = rmt33.key_rows_by(rowkey=-mt.row_idx); assert mt.row.collect() == [hl.Struct(row_idx=x) for x in [2, 1, 0]]. mt = rmt33.annotate_entries(; x=(rmt33.row_idx + 1) * (rmt33.col_idx + 1)); mt = mt.key_rows_by(rowkey=-mt.row_idx); mt = mt.choose_cols([2, 1, 0]); assert mt.x.collect() == [9, 6, 3, 6, 4, 2, 3, 2, 1]. t = hl.utils.range_table(3); t = t.key_by(-t.idx); assert t.idx.collect() == [2, 1, 0]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6930
https://github.com/hail-is/hail/issues/6930:1154,Testability,test,tests,1154,"There is no hail operation that removes keys and guarantees no change to order (see https://github.com/hail-is/hail/issues/6929). In `BaseExpression._to_table`, we use the following expression which works in practice but is not guaranteed to work.; ```python; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return ..., entries.select(name); ```. In particular:; - `order_by(*ds.row_key)` does not guarantee that entries from the same row are appear in the order of the corresponding columns; - `order_by(*ds.row_key)` does not guarantee that entries from a pair of rows with equivalent keys appear in the order of the corresponding rows. There exists an operation that guarantees this:; ```python; ridx = Env.get_uid(); cidx = Env.get_uid(); entries = ds.key_cols_by().add_row_index(ridx).add_col_index(cidx).entries(); entries = entries.order_by(ridx, cidx); return ..., entries.select(name); ```; But this requires a scan to produce the row index and then a scan to verify the table rows are in the correct order. We consider this unacceptable performance. Instead, we rely on the above expression and add the following tests which we hope will alert us if we accidentally change the behavior:. ```; rmt33 = hl.utils.range_matrix_table(3, 3, n_partitions=2). mt = rmt33.choose_cols([1, 2, 0]); assert mt.col.collect() == [hl.Struct(col_idx=x) for x in [1, 2, 0]]. mt = rmt33.key_rows_by(rowkey=-mt.row_idx); assert mt.row.collect() == [hl.Struct(row_idx=x) for x in [2, 1, 0]]. mt = rmt33.annotate_entries(; x=(rmt33.row_idx + 1) * (rmt33.col_idx + 1)); mt = mt.key_rows_by(rowkey=-mt.row_idx); mt = mt.choose_cols([2, 1, 0]); assert mt.x.collect() == [9, 6, 3, 6, 4, 2, 3, 2, 1]. t = hl.utils.range_table(3); t = t.key_by(-t.idx); assert t.idx.collect() == [2, 1, 0]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6930
https://github.com/hail-is/hail/issues/6930:1328,Testability,assert,assert,1328,"There is no hail operation that removes keys and guarantees no change to order (see https://github.com/hail-is/hail/issues/6929). In `BaseExpression._to_table`, we use the following expression which works in practice but is not guaranteed to work.; ```python; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return ..., entries.select(name); ```. In particular:; - `order_by(*ds.row_key)` does not guarantee that entries from the same row are appear in the order of the corresponding columns; - `order_by(*ds.row_key)` does not guarantee that entries from a pair of rows with equivalent keys appear in the order of the corresponding rows. There exists an operation that guarantees this:; ```python; ridx = Env.get_uid(); cidx = Env.get_uid(); entries = ds.key_cols_by().add_row_index(ridx).add_col_index(cidx).entries(); entries = entries.order_by(ridx, cidx); return ..., entries.select(name); ```; But this requires a scan to produce the row index and then a scan to verify the table rows are in the correct order. We consider this unacceptable performance. Instead, we rely on the above expression and add the following tests which we hope will alert us if we accidentally change the behavior:. ```; rmt33 = hl.utils.range_matrix_table(3, 3, n_partitions=2). mt = rmt33.choose_cols([1, 2, 0]); assert mt.col.collect() == [hl.Struct(col_idx=x) for x in [1, 2, 0]]. mt = rmt33.key_rows_by(rowkey=-mt.row_idx); assert mt.row.collect() == [hl.Struct(row_idx=x) for x in [2, 1, 0]]. mt = rmt33.annotate_entries(; x=(rmt33.row_idx + 1) * (rmt33.col_idx + 1)); mt = mt.key_rows_by(rowkey=-mt.row_idx); mt = mt.choose_cols([2, 1, 0]); assert mt.x.collect() == [9, 6, 3, 6, 4, 2, 3, 2, 1]. t = hl.utils.range_table(3); t = t.key_by(-t.idx); assert t.idx.collect() == [2, 1, 0]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6930
https://github.com/hail-is/hail/issues/6930:1442,Testability,assert,assert,1442,"There is no hail operation that removes keys and guarantees no change to order (see https://github.com/hail-is/hail/issues/6929). In `BaseExpression._to_table`, we use the following expression which works in practice but is not guaranteed to work.; ```python; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return ..., entries.select(name); ```. In particular:; - `order_by(*ds.row_key)` does not guarantee that entries from the same row are appear in the order of the corresponding columns; - `order_by(*ds.row_key)` does not guarantee that entries from a pair of rows with equivalent keys appear in the order of the corresponding rows. There exists an operation that guarantees this:; ```python; ridx = Env.get_uid(); cidx = Env.get_uid(); entries = ds.key_cols_by().add_row_index(ridx).add_col_index(cidx).entries(); entries = entries.order_by(ridx, cidx); return ..., entries.select(name); ```; But this requires a scan to produce the row index and then a scan to verify the table rows are in the correct order. We consider this unacceptable performance. Instead, we rely on the above expression and add the following tests which we hope will alert us if we accidentally change the behavior:. ```; rmt33 = hl.utils.range_matrix_table(3, 3, n_partitions=2). mt = rmt33.choose_cols([1, 2, 0]); assert mt.col.collect() == [hl.Struct(col_idx=x) for x in [1, 2, 0]]. mt = rmt33.key_rows_by(rowkey=-mt.row_idx); assert mt.row.collect() == [hl.Struct(row_idx=x) for x in [2, 1, 0]]. mt = rmt33.annotate_entries(; x=(rmt33.row_idx + 1) * (rmt33.col_idx + 1)); mt = mt.key_rows_by(rowkey=-mt.row_idx); mt = mt.choose_cols([2, 1, 0]); assert mt.x.collect() == [9, 6, 3, 6, 4, 2, 3, 2, 1]. t = hl.utils.range_table(3); t = t.key_by(-t.idx); assert t.idx.collect() == [2, 1, 0]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6930
https://github.com/hail-is/hail/issues/6930:1661,Testability,assert,assert,1661,"There is no hail operation that removes keys and guarantees no change to order (see https://github.com/hail-is/hail/issues/6929). In `BaseExpression._to_table`, we use the following expression which works in practice but is not guaranteed to work.; ```python; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return ..., entries.select(name); ```. In particular:; - `order_by(*ds.row_key)` does not guarantee that entries from the same row are appear in the order of the corresponding columns; - `order_by(*ds.row_key)` does not guarantee that entries from a pair of rows with equivalent keys appear in the order of the corresponding rows. There exists an operation that guarantees this:; ```python; ridx = Env.get_uid(); cidx = Env.get_uid(); entries = ds.key_cols_by().add_row_index(ridx).add_col_index(cidx).entries(); entries = entries.order_by(ridx, cidx); return ..., entries.select(name); ```; But this requires a scan to produce the row index and then a scan to verify the table rows are in the correct order. We consider this unacceptable performance. Instead, we rely on the above expression and add the following tests which we hope will alert us if we accidentally change the behavior:. ```; rmt33 = hl.utils.range_matrix_table(3, 3, n_partitions=2). mt = rmt33.choose_cols([1, 2, 0]); assert mt.col.collect() == [hl.Struct(col_idx=x) for x in [1, 2, 0]]. mt = rmt33.key_rows_by(rowkey=-mt.row_idx); assert mt.row.collect() == [hl.Struct(row_idx=x) for x in [2, 1, 0]]. mt = rmt33.annotate_entries(; x=(rmt33.row_idx + 1) * (rmt33.col_idx + 1)); mt = mt.key_rows_by(rowkey=-mt.row_idx); mt = mt.choose_cols([2, 1, 0]); assert mt.x.collect() == [9, 6, 3, 6, 4, 2, 3, 2, 1]. t = hl.utils.range_table(3); t = t.key_by(-t.idx); assert t.idx.collect() == [2, 1, 0]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6930
https://github.com/hail-is/hail/issues/6930:1766,Testability,assert,assert,1766,"There is no hail operation that removes keys and guarantees no change to order (see https://github.com/hail-is/hail/issues/6929). In `BaseExpression._to_table`, we use the following expression which works in practice but is not guaranteed to work.; ```python; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return ..., entries.select(name); ```. In particular:; - `order_by(*ds.row_key)` does not guarantee that entries from the same row are appear in the order of the corresponding columns; - `order_by(*ds.row_key)` does not guarantee that entries from a pair of rows with equivalent keys appear in the order of the corresponding rows. There exists an operation that guarantees this:; ```python; ridx = Env.get_uid(); cidx = Env.get_uid(); entries = ds.key_cols_by().add_row_index(ridx).add_col_index(cidx).entries(); entries = entries.order_by(ridx, cidx); return ..., entries.select(name); ```; But this requires a scan to produce the row index and then a scan to verify the table rows are in the correct order. We consider this unacceptable performance. Instead, we rely on the above expression and add the following tests which we hope will alert us if we accidentally change the behavior:. ```; rmt33 = hl.utils.range_matrix_table(3, 3, n_partitions=2). mt = rmt33.choose_cols([1, 2, 0]); assert mt.col.collect() == [hl.Struct(col_idx=x) for x in [1, 2, 0]]. mt = rmt33.key_rows_by(rowkey=-mt.row_idx); assert mt.row.collect() == [hl.Struct(row_idx=x) for x in [2, 1, 0]]. mt = rmt33.annotate_entries(; x=(rmt33.row_idx + 1) * (rmt33.col_idx + 1)); mt = mt.key_rows_by(rowkey=-mt.row_idx); mt = mt.choose_cols([2, 1, 0]); assert mt.x.collect() == [9, 6, 3, 6, 4, 2, 3, 2, 1]. t = hl.utils.range_table(3); t = t.key_by(-t.idx); assert t.idx.collect() == [2, 1, 0]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6930
https://github.com/hail-is/hail/pull/6933:7,Availability,checkpoint,checkpointing,7,Adding checkpointing as a quality of life improvement for block matrices.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6933
https://github.com/hail-is/hail/pull/6935:180,Security,access,access,180,"So we can pull out the necessary reference genomes (in python) to execute a given IR, or to parse a given type, and pass them in to the Scala execution context which will not have access to the reference genomes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6935
https://github.com/hail-is/hail/pull/6936:10,Integrability,rout,router,10,plus some router config fixes. Mainly I'm PR'ing this so I can use the cached base image when testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6936
https://github.com/hail-is/hail/pull/6936:17,Modifiability,config,config,17,plus some router config fixes. Mainly I'm PR'ing this so I can use the cached base image when testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6936
https://github.com/hail-is/hail/pull/6936:71,Performance,cache,cached,71,plus some router config fixes. Mainly I'm PR'ing this so I can use the cached base image when testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6936
https://github.com/hail-is/hail/pull/6936:94,Testability,test,testing,94,plus some router config fixes. Mainly I'm PR'ing this so I can use the cached base image when testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6936
https://github.com/hail-is/hail/pull/6940:69,Deployability,update,updated,69,"Previously the mtime of hail/prebuilt/lib/**/*.{so,dylib}, would be; updated during `reset-prebuilt` which would cause the prebuilts to be; newer than libhail, as such they wouldn't be copied. Also use `$(dir file)` rather than `$(basename file)`, as it gives the; directory whereas basename gives the path minus the extension.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6940
https://github.com/hail-is/hail/pull/6941:465,Safety,avoid,avoid,465,"We already statically know the return type of a given expression in Python (without needing to consult the function registry), so there's no need to ask our function registry to re-derive that information. The change here adds a return type field to all of our Apply nodes, and changes the function registry to unify over `argTypes :+ retType` when looking up the correct function without attempting to infer what the return type needs to be. This will allow us to avoid registering a set of LocusFunctions and LiftoverFunctions per reference genome/liftover, since the return type does not need to be inferred and we can just treat them as any other function with type-specific information. This actually probably means we can remove all of the predefined functions from the (python) function registry, and just reserve that for user-defined functions. I haven't done that here for the sake of keeping this change pretty minimal, but I think it would be a reasonable thing to do.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6941
https://github.com/hail-is/hail/pull/6942:99,Testability,test,tests,99,"Spicy meatball. This will be hard to review, and I realize that. I've included quite comprehensive tests to help make that easier.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942
https://github.com/hail-is/hail/pull/6943:136,Availability,error,errors,136,"Do a tree reduce instead of a linear reduce. This means that the java; stack depth is log2(N) instead of N, and prevents stack overflow errors; when unioning hundreds of tables together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6943
https://github.com/hail-is/hail/pull/6943:10,Energy Efficiency,reduce,reduce,10,"Do a tree reduce instead of a linear reduce. This means that the java; stack depth is log2(N) instead of N, and prevents stack overflow errors; when unioning hundreds of tables together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6943
https://github.com/hail-is/hail/pull/6943:37,Energy Efficiency,reduce,reduce,37,"Do a tree reduce instead of a linear reduce. This means that the java; stack depth is log2(N) instead of N, and prevents stack overflow errors; when unioning hundreds of tables together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6943
https://github.com/hail-is/hail/pull/6946:120,Modifiability,refactor,refactored,120,"This was left over from the old AST parsing stuff and I don't think it's used anymore (except in one test, which I just refactored).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6946
https://github.com/hail-is/hail/pull/6946:101,Testability,test,test,101,"This was left over from the old AST parsing stuff and I don't think it's used anymore (except in one test, which I just refactored).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6946
https://github.com/hail-is/hail/pull/6950:12,Testability,benchmark,benchmarks,12,"#6915 broke benchmarks by removing these import lines, which had the; side effect of adding benchmarks to a registry.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6950
https://github.com/hail-is/hail/pull/6950:92,Testability,benchmark,benchmarks,92,"#6915 broke benchmarks by removing these import lines, which had the; side effect of adding benchmarks to a registry.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6950
https://github.com/hail-is/hail/issues/6952:30,Modifiability,rewrite,rewriter,30,"Issues to address. - [ ] Type rewriter for CastRename: https://github.com/hail-is/hail/pull/6912/files/5257ca5eead8da3c470932e61944c715a5293913#r317669438. - [x] NA, Die: https://github.com/hail-is/hail/pull/6912#discussion_r317669761, https://github.com/hail-is/hail/pull/6912/files#r317671912. - [x] MakeArray: https://github.com/hail-is/hail/pull/6912#discussion_r317670311; - [x] Literal: walk values: https://github.com/hail-is/hail/pull/6990#discussion_r323810812 ; * Literal appears done, marking completed, but lets verify @tpoterba . - [x] MakeTuple: same as MakeArray; - [x] Coalesce: https://github.com/hail-is/hail/pull/6912#discussion_r317670382; - [x] If; - [ ] Upcast pass in Emit (and Interpret). cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6952
https://github.com/hail-is/hail/pull/6954:227,Testability,log,logic,227,"Threads an environment through the type parser to handle reference genomes. Currently we're just using the default, which looks at the global ReferenceGenome.references; this needs to be threaded through the type serialization logic before we can get rid of that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6954
https://github.com/hail-is/hail/pull/6956:21,Availability,Error,Error,21,It was failing with 'Error file not found:' for me. Putting all the; arguments on one line fixed it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956
https://github.com/hail-is/hail/issues/6957:94,Availability,error,error,94,"MakeNDArray should return a missing NDArray if shape or data are missing, but should raise an error if any elements of the shape or data are missing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6957
https://github.com/hail-is/hail/pull/6961:6,Availability,failure,failure,6,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6961:45,Availability,failure,failure,45,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6961:140,Availability,failure,failure,140,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6961:176,Availability,failure,failure,176,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6961:216,Availability,failure,failure,216,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6961:308,Availability,alive,alive,308,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6961:155,Energy Efficiency,schedul,scheduled,155,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6961:275,Testability,log,logs,275,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6961:348,Testability,log,logs,348,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6961
https://github.com/hail-is/hail/pull/6967:103,Availability,down,down,103,"On a test of 1-2 partitions with 5000 samples, this takes the second stage of a densify from 2 minutes down to 1.4 minutes (only loading GT).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6967
https://github.com/hail-is/hail/pull/6967:129,Performance,load,loading,129,"On a test of 1-2 partitions with 5000 samples, this takes the second stage of a densify from 2 minutes down to 1.4 minutes (only loading GT).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6967
https://github.com/hail-is/hail/pull/6967:5,Testability,test,test,5,"On a test of 1-2 partitions with 5000 samples, this takes the second stage of a densify from 2 minutes down to 1.4 minutes (only loading GT).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6967
https://github.com/hail-is/hail/pull/6972:107,Deployability,update,update,107,"- introduce job logging methods that automatically include id, state, and pod name; - add `reap_job` state update logic into `new_state`, use `new_state` in `reap_job`; - remove unused function `refresh_parents_and_maybe_create`, which lead to more dead code which was also removed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6972
https://github.com/hail-is/hail/pull/6972:16,Testability,log,logging,16,"- introduce job logging methods that automatically include id, state, and pod name; - add `reap_job` state update logic into `new_state`, use `new_state` in `reap_job`; - remove unused function `refresh_parents_and_maybe_create`, which lead to more dead code which was also removed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6972
https://github.com/hail-is/hail/pull/6972:114,Testability,log,logic,114,"- introduce job logging methods that automatically include id, state, and pod name; - add `reap_job` state update logic into `new_state`, use `new_state` in `reap_job`; - remove unused function `refresh_parents_and_maybe_create`, which lead to more dead code which was also removed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6972
https://github.com/hail-is/hail/pull/6973:61,Testability,log,log,61,"- add pod_status page and add link to all places we link the log; - make historical batches look the same (use emoji, colors, etc.) as the current batch for a pr; - link back to the batch from a job; - link to the GitHub PR page from the CI PR page; - banish tabs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6973
https://github.com/hail-is/hail/issues/6974:177,Integrability,interface,interface,177,"An NDArray's physical type is currently always a `PNDArray`, which has its data in a `PArray`, which is indexed by an integer. So maximum array length is `Max_Int`. The current interface to make an ndarray allows passing in an arbitrary python list, which could be of any length. Things like matrix multiplies could also produce overly large ndarrays. Need to have a runtime check on size.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6974
https://github.com/hail-is/hail/pull/6976:118,Availability,error,error,118,"* add simulated BGEN file based on distributions in real data; * add import, info score, filter benchmarks; * fix bad error message in `export_bgen`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6976
https://github.com/hail-is/hail/pull/6976:124,Integrability,message,message,124,"* add simulated BGEN file based on distributions in real data; * add import, info score, filter benchmarks; * fix bad error message in `export_bgen`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6976
https://github.com/hail-is/hail/pull/6976:96,Testability,benchmark,benchmarks,96,"* add simulated BGEN file based on distributions in real data; * add import, info score, filter benchmarks; * fix bad error message in `export_bgen`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6976
https://github.com/hail-is/hail/issues/6979:240,Availability,error,error,240,"`pheno_file = p.read_input_group(**{'gz': pheno_path})` works if `pheno_path` is a string. But if it's a ResourceFile object (don't ask how I arrived at that), the pipeline still submits, but the localizing files step fails without obvious error (localizes the other files and then dies silently).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6979
https://github.com/hail-is/hail/issues/6979:164,Deployability,pipeline,pipeline,164,"`pheno_file = p.read_input_group(**{'gz': pheno_path})` works if `pheno_path` is a string. But if it's a ResourceFile object (don't ask how I arrived at that), the pipeline still submits, but the localizing files step fails without obvious error (localizes the other files and then dies silently).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6979
https://github.com/hail-is/hail/pull/6981:592,Deployability,update,updated,592,"Stacked on #6980 . **Problem 1**: functions like `min`/`max` need to do a length check,; preventing us from implementing them in a single primitive fold. **Problem 2**: functions like `mean` use a struct as the accumulator,; leading to allocation (!) every element. **Solution**: make it possible to have multiple primitive; accumulators. This is `ArrayFold2`. The node is different from `ArrayFold` in that it:; * has as sequence of accumulators, not just one; * has a sequence of seq ops, one for each accumulator. Each of these; sequence ops can see all the accumulators, and will see the updated; value from sequence operations with a smaller index.; * has a result op, which is a function from accumulators to result. By changing `min`/`max` to use ArrayFold2 and inlining these functions,; we can get a reasonable speedup on `split_multi_hts`:. #6980 (this PR's parent):. ```; 2019-09-03 07:11:16,374: INFO: burn in: 42.33s; 2019-09-03 07:11:56,085: INFO: run 1: 39.71s; 2019-09-03 07:12:34,916: INFO: run 2: 38.83s; 2019-09-03 07:13:14,087: INFO: run 3: 39.17s; ```. PR:; ```; 2019-09-03 07:32:10,416: INFO: burn in: 38.03s; 2019-09-03 07:32:39,237: INFO: run 1: 28.82s; 2019-09-03 07:33:07,778: INFO: run 2: 28.50s; 2019-09-03 07:33:35,997: INFO: run 3: 28.21s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6981
https://github.com/hail-is/hail/issues/6982:85,Availability,error,error,85,"Hello, I have installed hail using pycharm not from github. Now getting the attached error. My command was:; ```; import hail as hl; hl.init(); import os; from hail.plot import show; [hail.err.txt](https://github.com/hail-is/hail/files/3570397/hail.err.txt). from pprint import pprint; hl.plot.output_notebook(). hl.utils.get_1kg('data/'); hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). mt = hl.read_matrix_table('data/1kg.mt'); mt.rows().select().show(5); ```; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6982
https://github.com/hail-is/hail/issues/6982:14,Deployability,install,installed,14,"Hello, I have installed hail using pycharm not from github. Now getting the attached error. My command was:; ```; import hail as hl; hl.init(); import os; from hail.plot import show; [hail.err.txt](https://github.com/hail-is/hail/files/3570397/hail.err.txt). from pprint import pprint; hl.plot.output_notebook(). hl.utils.get_1kg('data/'); hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). mt = hl.read_matrix_table('data/1kg.mt'); mt.rows().select().show(5); ```; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6982
https://github.com/hail-is/hail/pull/6983:249,Performance,load,load,249,"This PR:. - Introduces mapping over NDArrays in JVM emitter. ; - Introduces the `NDArrayEmitter` class, mimicking the cxx analogue. This class is used as the basis of our `NDArray` deforesting efforts (see `emitLoops`); - Fixes a bug in `PStruct`'s load field function. ; - Introduces useful helper functions on `PNDArray`: `numElements` and `makeDefaultStrides`. Ready for review",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6983
https://github.com/hail-is/hail/pull/6987:1799,Availability,reliab,reliable,1799,"ue, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:2261,Availability,error,error,2261,"e. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty slow!? Splitting into multiple methods allowed me to interrogate where time was spent. The answer is ""method4"" which is `parseEntries`. This code includes the loop, the srvb state management, the Region manipulation, and checking for the missing value. I'd be surprised its the checking for missing value because I delegate to `String.regionMatches` for the heavy lifting and that does not show up in the profiler. I'm left to conclude that either srvb state management or writing/rea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:1638,Integrability,message,message,1638,": m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:2641,Integrability,wrap,wraps,2641," | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty slow!? Splitting into multiple methods allowed me to interrogate where time was spent. The answer is ""method4"" which is `parseEntries`. This code includes the loop, the srvb state management, the Region manipulation, and checking for the missing value. I'd be surprised its the checking for missing value because I delegate to `String.regionMatches` for the heavy lifting and that does not show up in the profiler. I'm left to conclude that either srvb state management or writing/reading to regions is expensive.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:56,Performance,perform,performance,56,"I staged `import_matrix_table` and achieved substantial performance improvements. A few changes were necessary:; - `FunctionBuilder` now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists. The main change is in `ImportMatrix.scala` which is both staged and based on scanning the string rather than using `String.split`. The approach is essentially a simplified, staged version of `import_vcf`. I benchmarked the change with:; ```; In [2]: %%time ; ...: import hail as hl ; ...: m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Asi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:1443,Performance,perform,performance,1443,"h is essentially a simplified, staged version of `import_vcf`. I benchmarked the change with:; ```; In [2]: %%time ; ...: import hail as hl ; ...: m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:1808,Performance,perform,performance,1808,"ue, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:1930,Performance,perform,performance,1930,"ws() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty slow!? Splitting into multiple methods allowed me to interrogate where time was spent. The answer is ""method4"" which is `pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:580,Testability,benchmark,benchmarked,580,"I staged `import_matrix_table` and achieved substantial performance improvements. A few changes were necessary:; - `FunctionBuilder` now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists. The main change is in `ImportMatrix.scala` which is both staged and based on scanning the string rather than using `String.split`. The approach is essentially a simplified, staged version of `import_vcf`. I benchmarked the change with:; ```; In [2]: %%time ; ...: import hail as hl ; ...: m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Asi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:534,Usability,simpl,simplified,534,"I staged `import_matrix_table` and achieved substantial performance improvements. A few changes were necessary:; - `FunctionBuilder` now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists. The main change is in `ImportMatrix.scala` which is both staged and based on scanning the string rather than using `String.split`. The approach is essentially a simplified, staged version of `import_vcf`. I benchmarked the change with:; ```; In [2]: %%time ; ...: import hail as hl ; ...: m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Asi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6987:2687,Usability,clear,clear,2687," | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Aside: using the staged stuff is hard, especially when using multiple methods. A couple thoughts:; - Because SRVB generates a fresh SRVB for arrays and structs, you must thread the srvb through your code gen rather than using a single field, this is annoying and error prone; - `init` is not a first class thing in `FunctionBuilder` and I've arguably made the whole situation more ugly by exposing `addInitInstructions`. Without the ability to place code in the constructor, it is hard to coordinate work between multiple methods.; - When using lots of methods, there's a lot of bookkeeping. I would like a way to define a ""staged class"" that wraps up some of the boilerplate. Not totally clear what I want here, just less boilerplate. Aside2: This is still pretty slow!? Splitting into multiple methods allowed me to interrogate where time was spent. The answer is ""method4"" which is `parseEntries`. This code includes the loop, the srvb state management, the Region manipulation, and checking for the missing value. I'd be surprised its the checking for missing value because I delegate to `String.regionMatches` for the heavy lifting and that does not show up in the profiler. I'm left to conclude that either srvb state management or writing/reading to regions is expensive.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987
https://github.com/hail-is/hail/pull/6989:10,Testability,benchmark,benchmark,10,"Running a benchmark on densify now, will let y'all know how it looks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6989
https://github.com/hail-is/hail/pull/6990:520,Availability,down,down,520,"The first series addressing https://github.com/hail-is/hail/issues/6952. If we like this, will implement Coalesce Node in a similar manner. As part of this NA node changes, so that elements of collections are set to required (effectively a hoop when taking the boolean and of requireness on element types of non-NA nodes). Implemented and tested for every collection type, besides PNDArray, because we currently don't support arrays of NDArray. This also fixes the ToDict node inference, which requires the union of top-down and bottom-up element requiredeness inference. cc @patrick-schultz @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990
https://github.com/hail-is/hail/pull/6990:339,Testability,test,tested,339,"The first series addressing https://github.com/hail-is/hail/issues/6952. If we like this, will implement Coalesce Node in a similar manner. As part of this NA node changes, so that elements of collections are set to required (effectively a hoop when taking the boolean and of requireness on element types of non-NA nodes). Implemented and tested for every collection type, besides PNDArray, because we currently don't support arrays of NDArray. This also fixes the ToDict node inference, which requires the union of top-down and bottom-up element requiredeness inference. cc @patrick-schultz @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990
https://github.com/hail-is/hail/issues/6995:318,Availability,checkpoint,checkpoint,318,"If a many-partitions heavily-filtered matrix table is converted to a block matrix with `write_from_entry_expr`, parallelism is lost and kills performance. In the extreme case, imagine a MT with 4096 partitions, each with 1M rows, which are filtered to 1 row. There will be 1-way parallelism in the write. . We need to checkpoint the intermediate matrix if the loss of parallelism is above some threshold.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6995
https://github.com/hail-is/hail/issues/6995:142,Performance,perform,performance,142,"If a many-partitions heavily-filtered matrix table is converted to a block matrix with `write_from_entry_expr`, parallelism is lost and kills performance. In the extreme case, imagine a MT with 4096 partitions, each with 1M rows, which are filtered to 1 row. There will be 1-way parallelism in the write. . We need to checkpoint the intermediate matrix if the loss of parallelism is above some threshold.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6995
https://github.com/hail-is/hail/pull/6996:28,Deployability,install,installed,28,"Apparently Hail needs to be installed even to build the docs without tests? I don't know why but I ran into this today. Seems wrong that we need it installed to build the docs, but 🤷‍♂",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6996
https://github.com/hail-is/hail/pull/6996:148,Deployability,install,installed,148,"Apparently Hail needs to be installed even to build the docs without tests? I don't know why but I ran into this today. Seems wrong that we need it installed to build the docs, but 🤷‍♂",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6996
https://github.com/hail-is/hail/pull/6996:69,Testability,test,tests,69,"Apparently Hail needs to be installed even to build the docs without tests? I don't know why but I ran into this today. Seems wrong that we need it installed to build the docs, but 🤷‍♂",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6996
https://github.com/hail-is/hail/pull/6997:333,Performance,cache,cache,333,"I didn't bust js yet because there are some external libraries (MathJax) that cary their own version strings and I don't want to break them. In testing seems safe, although I admit the regex isn't the most specific. In the worst case I believe we would append an unnecessary version string, which shouldn't break anything (just will cache the browser to reload the css instead of using cache)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6997
https://github.com/hail-is/hail/pull/6997:386,Performance,cache,cache,386,"I didn't bust js yet because there are some external libraries (MathJax) that cary their own version strings and I don't want to break them. In testing seems safe, although I admit the regex isn't the most specific. In the worst case I believe we would append an unnecessary version string, which shouldn't break anything (just will cache the browser to reload the css instead of using cache)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6997
https://github.com/hail-is/hail/pull/6997:158,Safety,safe,safe,158,"I didn't bust js yet because there are some external libraries (MathJax) that cary their own version strings and I don't want to break them. In testing seems safe, although I admit the regex isn't the most specific. In the worst case I believe we would append an unnecessary version string, which shouldn't break anything (just will cache the browser to reload the css instead of using cache)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6997
https://github.com/hail-is/hail/pull/6997:144,Testability,test,testing,144,"I didn't bust js yet because there are some external libraries (MathJax) that cary their own version strings and I don't want to break them. In testing seems safe, although I admit the regex isn't the most specific. In the worst case I believe we would append an unnecessary version string, which shouldn't break anything (just will cache the browser to reload the css instead of using cache)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6997
https://github.com/hail-is/hail/pull/7005:23,Deployability,deploy,deploy,23,"We might need to force deploy this, but let's see if it just works first.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7005
https://github.com/hail-is/hail/issues/7008:110,Availability,error,error,110,"Version:; Apache Spark version 2.4.3; Hail version 0.2.19-c6ec8b76eb26. When exporting a table, the following error occurs when running on a yarn cluster. It does not occur when running locally. Any suggestions on this?. ```; Container exited with a non-zero exit code 127. Error file: prelaunch.err.; Last 4096 bytes of prelaunch.err :; Last 4096 bytes of stderr :; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/291/__spark_libs__4347827829503170766.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.0-292/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/09/06 15:54:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/09/06 15:54:30 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; /usr/java/default/bin/java: symbol lookup error: /data01/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616_0098/container_e2451_1565788829616_0098_01_000011/tmp/jniloader3452911880890326608netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. ```; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008
https://github.com/hail-is/hail/issues/7008:274,Availability,Error,Error,274,"Version:; Apache Spark version 2.4.3; Hail version 0.2.19-c6ec8b76eb26. When exporting a table, the following error occurs when running on a yarn cluster. It does not occur when running locally. Any suggestions on this?. ```; Container exited with a non-zero exit code 127. Error file: prelaunch.err.; Last 4096 bytes of prelaunch.err :; Last 4096 bytes of stderr :; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/291/__spark_libs__4347827829503170766.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.0-292/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/09/06 15:54:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/09/06 15:54:30 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; /usr/java/default/bin/java: symbol lookup error: /data01/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616_0098/container_e2451_1565788829616_0098_01_000011/tmp/jniloader3452911880890326608netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. ```; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008
https://github.com/hail-is/hail/issues/7008:1242,Availability,error,error,1242,"Version:; Apache Spark version 2.4.3; Hail version 0.2.19-c6ec8b76eb26. When exporting a table, the following error occurs when running on a yarn cluster. It does not occur when running locally. Any suggestions on this?. ```; Container exited with a non-zero exit code 127. Error file: prelaunch.err.; Last 4096 bytes of prelaunch.err :; Last 4096 bytes of stderr :; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/291/__spark_libs__4347827829503170766.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.0-292/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/09/06 15:54:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/09/06 15:54:30 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; /usr/java/default/bin/java: symbol lookup error: /data01/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616_0098/container_e2451_1565788829616_0098_01_000011/tmp/jniloader3452911880890326608netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. ```; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008
https://github.com/hail-is/hail/issues/7008:960,Performance,load,load,960,"Version:; Apache Spark version 2.4.3; Hail version 0.2.19-c6ec8b76eb26. When exporting a table, the following error occurs when running on a yarn cluster. It does not occur when running locally. Any suggestions on this?. ```; Container exited with a non-zero exit code 127. Error file: prelaunch.err.; Last 4096 bytes of prelaunch.err :; Last 4096 bytes of stderr :; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/291/__spark_libs__4347827829503170766.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.0-292/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/09/06 15:54:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/09/06 15:54:30 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; /usr/java/default/bin/java: symbol lookup error: /data01/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616_0098/container_e2451_1565788829616_0098_01_000011/tmp/jniloader3452911880890326608netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. ```; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008
https://github.com/hail-is/hail/issues/7008:1191,Performance,load,loaded,1191,"Version:; Apache Spark version 2.4.3; Hail version 0.2.19-c6ec8b76eb26. When exporting a table, the following error occurs when running on a yarn cluster. It does not occur when running locally. Any suggestions on this?. ```; Container exited with a non-zero exit code 127. Error file: prelaunch.err.; Last 4096 bytes of prelaunch.err :; Last 4096 bytes of stderr :; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/291/__spark_libs__4347827829503170766.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.0-292/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/09/06 15:54:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/09/06 15:54:30 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; /usr/java/default/bin/java: symbol lookup error: /data01/hadoop/yarn/local/usercache/farrell/appcache/application_1565788829616_0098/container_e2451_1565788829616_0098_01_000011/tmp/jniloader3452911880890326608netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. ```; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008
https://github.com/hail-is/hail/pull/7009:57,Deployability,integrat,integrates,57,"Stacked on #7000. Adds a new IR renderer in Python which integrates a CSE pass. It would be easy to argue that a CSE pass should be separate from the renderer. But we can't easily make the Python IR mutable, because a given IR tree might be used in multiple larger IR (which is exactly what this pass is taking advantage of!) so mutation which depends on the larger context won't work. So rather than rebuild the entire IR every time we print, I decided for now this is best integrated into the renderer. I think longer term this should be ported to scala as a full CSE pass (which first does hash-consing/value-numbering to find all repeated subexpressions). This is not a simple algorithm, but I did my best to make it understandable. If anything feels harder to follow than it should be, I'd like to try to improve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009
https://github.com/hail-is/hail/pull/7009:475,Deployability,integrat,integrated,475,"Stacked on #7000. Adds a new IR renderer in Python which integrates a CSE pass. It would be easy to argue that a CSE pass should be separate from the renderer. But we can't easily make the Python IR mutable, because a given IR tree might be used in multiple larger IR (which is exactly what this pass is taking advantage of!) so mutation which depends on the larger context won't work. So rather than rebuild the entire IR every time we print, I decided for now this is best integrated into the renderer. I think longer term this should be ported to scala as a full CSE pass (which first does hash-consing/value-numbering to find all repeated subexpressions). This is not a simple algorithm, but I did my best to make it understandable. If anything feels harder to follow than it should be, I'd like to try to improve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009
https://github.com/hail-is/hail/pull/7009:57,Integrability,integrat,integrates,57,"Stacked on #7000. Adds a new IR renderer in Python which integrates a CSE pass. It would be easy to argue that a CSE pass should be separate from the renderer. But we can't easily make the Python IR mutable, because a given IR tree might be used in multiple larger IR (which is exactly what this pass is taking advantage of!) so mutation which depends on the larger context won't work. So rather than rebuild the entire IR every time we print, I decided for now this is best integrated into the renderer. I think longer term this should be ported to scala as a full CSE pass (which first does hash-consing/value-numbering to find all repeated subexpressions). This is not a simple algorithm, but I did my best to make it understandable. If anything feels harder to follow than it should be, I'd like to try to improve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009
https://github.com/hail-is/hail/pull/7009:344,Integrability,depend,depends,344,"Stacked on #7000. Adds a new IR renderer in Python which integrates a CSE pass. It would be easy to argue that a CSE pass should be separate from the renderer. But we can't easily make the Python IR mutable, because a given IR tree might be used in multiple larger IR (which is exactly what this pass is taking advantage of!) so mutation which depends on the larger context won't work. So rather than rebuild the entire IR every time we print, I decided for now this is best integrated into the renderer. I think longer term this should be ported to scala as a full CSE pass (which first does hash-consing/value-numbering to find all repeated subexpressions). This is not a simple algorithm, but I did my best to make it understandable. If anything feels harder to follow than it should be, I'd like to try to improve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009
https://github.com/hail-is/hail/pull/7009:475,Integrability,integrat,integrated,475,"Stacked on #7000. Adds a new IR renderer in Python which integrates a CSE pass. It would be easy to argue that a CSE pass should be separate from the renderer. But we can't easily make the Python IR mutable, because a given IR tree might be used in multiple larger IR (which is exactly what this pass is taking advantage of!) so mutation which depends on the larger context won't work. So rather than rebuild the entire IR every time we print, I decided for now this is best integrated into the renderer. I think longer term this should be ported to scala as a full CSE pass (which first does hash-consing/value-numbering to find all repeated subexpressions). This is not a simple algorithm, but I did my best to make it understandable. If anything feels harder to follow than it should be, I'd like to try to improve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009
https://github.com/hail-is/hail/pull/7009:593,Security,hash,hash-consing,593,"Stacked on #7000. Adds a new IR renderer in Python which integrates a CSE pass. It would be easy to argue that a CSE pass should be separate from the renderer. But we can't easily make the Python IR mutable, because a given IR tree might be used in multiple larger IR (which is exactly what this pass is taking advantage of!) so mutation which depends on the larger context won't work. So rather than rebuild the entire IR every time we print, I decided for now this is best integrated into the renderer. I think longer term this should be ported to scala as a full CSE pass (which first does hash-consing/value-numbering to find all repeated subexpressions). This is not a simple algorithm, but I did my best to make it understandable. If anything feels harder to follow than it should be, I'd like to try to improve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009
https://github.com/hail-is/hail/pull/7009:674,Usability,simpl,simple,674,"Stacked on #7000. Adds a new IR renderer in Python which integrates a CSE pass. It would be easy to argue that a CSE pass should be separate from the renderer. But we can't easily make the Python IR mutable, because a given IR tree might be used in multiple larger IR (which is exactly what this pass is taking advantage of!) so mutation which depends on the larger context won't work. So rather than rebuild the entire IR every time we print, I decided for now this is best integrated into the renderer. I think longer term this should be ported to scala as a full CSE pass (which first does hash-consing/value-numbering to find all repeated subexpressions). This is not a simple algorithm, but I did my best to make it understandable. If anything feels harder to follow than it should be, I'd like to try to improve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009
https://github.com/hail-is/hail/issues/7016:3597,Availability,alive,alive,3597,from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment:; COPY_OUTPUT_CMD: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); keep-alive:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5001/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.keep_alive_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 1m; Environment: <none>; Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Conditions:; Type Status; Initialized False ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: wang-gsa-key; Optional: false; batch-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: batch-gsa-key; Optional: false; batch-12728-job-287-742170:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:4743,Availability,Toler,Tolerations,4743,"eac; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); keep-alive:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5001/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.keep_alive_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 1m; Environment: <none>; Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Conditions:; Type Status; Initialized False ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: wang-gsa-key; Optional: false; batch-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: batch-gsa-key; Optional: false; batch-12728-job-287-742170:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:7655,Availability,alive,alive,7655,"e-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:9633,Availability,toler,tolerations,9633,"oud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:9756,Availability,toler,tolerationSeconds,9756,"-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:9856,Availability,toler,tolerationSeconds,9856,"f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: cleanup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: keep-alive; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/broad-ctsa/benchmark_wang:latest; imageID: """"; lastState: {}; name: main; ready: false; r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:10581,Availability,alive,alive,10581,"f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: cleanup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: keep-alive; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/broad-ctsa/benchmark_wang:latest; imageID: """"; lastState: {}; name: main; ready: false; r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:10774,Availability,alive,alive,10774,"f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: cleanup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: keep-alive; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/broad-ctsa/benchmark_wang:latest; imageID: """"; lastState: {}; name: main; ready: false; r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:11218,Availability,alive,alive,11218,"perator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: cleanup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: keep-alive; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/broad-ctsa/benchmark_wang:latest; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; hostIP: 10.128.0.160; initContainerStatuses:; - image: google/cloud-sdk:237.0.0-alpine; imageID: """"; lastState: {}; name: setup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; phase: Pending; qosClass: Burstable; startTime: 2019-09-05T19:15:42Z; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:1267,Deployability,pipeline,pipeline,1267,"-287-742170; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-non-preemptible-pool-0106a51b-qz7f/10.128.0.160; Start Time: Thu, 05 Sep 2019 15:15:42 -0400; Labels: app=batch-job; batch_id=12728; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=287; user=wang; uuid=ca985fd90f9d46968ab9c480af9c931c; Annotations: <none>; Status: Pending; IP: ; Init Containers:; setup:; Container ID: ; Image: google/cloud-sdk:237.0.0-alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData)); gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:1276,Deployability,pipeline,pipeline-,1276,"-287-742170; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-non-preemptible-pool-0106a51b-qz7f/10.128.0.160; Start Time: Thu, 05 Sep 2019 15:15:42 -0400; Labels: app=batch-job; batch_id=12728; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=287; user=wang; uuid=ca985fd90f9d46968ab9c480af9c931c; Annotations: <none>; Status: Pending; IP: ; Init Containers:; setup:; Container ID: ; Image: google/cloud-sdk:237.0.0-alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData)); gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:1346,Deployability,pipeline,pipeline,1346,"-287-742170; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-non-preemptible-pool-0106a51b-qz7f/10.128.0.160; Start Time: Thu, 05 Sep 2019 15:15:42 -0400; Labels: app=batch-job; batch_id=12728; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=287; user=wang; uuid=ca985fd90f9d46968ab9c480af9c931c; Annotations: <none>; Status: Pending; IP: ; Init Containers:; setup:; Container ID: ; Image: google/cloud-sdk:237.0.0-alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData)); gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:1355,Deployability,pipeline,pipeline-,1355,"-287-742170; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-non-preemptible-pool-0106a51b-qz7f/10.128.0.160; Start Time: Thu, 05 Sep 2019 15:15:42 -0400; Labels: app=batch-job; batch_id=12728; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=287; user=wang; uuid=ca985fd90f9d46968ab9c480af9c931c; Annotations: <none>; Status: Pending; IP: ; Init Containers:; setup:; Container ID: ; Image: google/cloud-sdk:237.0.0-alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData)); gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:1400,Deployability,pipeline,pipeline,1400,"-287-742170; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-non-preemptible-pool-0106a51b-qz7f/10.128.0.160; Start Time: Thu, 05 Sep 2019 15:15:42 -0400; Labels: app=batch-job; batch_id=12728; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=287; user=wang; uuid=ca985fd90f9d46968ab9c480af9c931c; Annotations: <none>; Status: Pending; IP: ; Init Containers:; setup:; Container ID: ; Image: google/cloud-sdk:237.0.0-alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData)); gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:1409,Deployability,pipeline,pipeline-,1409,"-287-742170; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-non-preemptible-pool-0106a51b-qz7f/10.128.0.160; Start Time: Thu, 05 Sep 2019 15:15:42 -0400; Labels: app=batch-job; batch_id=12728; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=287; user=wang; uuid=ca985fd90f9d46968ab9c480af9c931c; Annotations: <none>; Status: Pending; IP: ; Init Containers:; setup:; Container ID: ; Image: google/cloud-sdk:237.0.0-alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/sh; -c; ; set -ex; (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData)); gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchm",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:1949,Deployability,pipeline,pipeline,1949,/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 50,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:1958,Deployability,pipeline,pipeline-,1958,/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 50,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:2019,Deployability,pipeline,pipeline,2019,/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 50,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:2028,Deployability,pipeline,pipeline-,2028,/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 50,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:2095,Deployability,pipeline,pipeline,2095,/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 50,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:2104,Deployability,pipeline,pipeline-,2104,/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 50,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:3261,Deployability,pipeline,pipeline,3261,benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment:; COPY_OUTPUT_CMD: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); keep-alive:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5001/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.keep_alive_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 1m; Environment: <none>; Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Conditions:; Type Status; Initialized False ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume popula,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:3270,Deployability,pipeline,pipeline-,3270,benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment:; COPY_OUTPUT_CMD: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); keep-alive:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5001/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.keep_alive_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 1m; Environment: <none>; Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Conditions:; Type Status; Initialized False ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume popula,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:3334,Deployability,pipeline,pipeline,3334,benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment:; COPY_OUTPUT_CMD: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); keep-alive:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5001/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.keep_alive_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 1m; Environment: <none>; Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Conditions:; Type Status; Initialized False ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume popula,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:3343,Deployability,pipeline,pipeline-,3343,benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment:; COPY_OUTPUT_CMD: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); keep-alive:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5001/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.keep_alive_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 1m; Environment: <none>; Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Conditions:; Type Status; Initialized False ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume popula,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5488,Deployability,pipeline,pipeline,5488," Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5497,Deployability,pipeline,pipeline-,5497," Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5558,Deployability,pipeline,pipeline,5558," Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5567,Deployability,pipeline,pipeline-,5567," Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5635,Deployability,pipeline,pipeline,5635," Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5644,Deployability,pipeline,pipeline-,5644," Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:6843,Deployability,pipeline,pipeline,6843,"rces.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/ter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:6852,Deployability,pipeline,pipeline-,6852,"rces.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/ter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:6917,Deployability,pipeline,pipeline,6917,"rces.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/ter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:6926,Deployability,pipeline,pipeline-,6926,"rces.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/ter",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:8700,Deployability,pipeline,pipeline,8700,"tch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:8709,Deployability,pipeline,pipeline-,8709,"tch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:8779,Deployability,pipeline,pipeline,8779,"tch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:8788,Deployability,pipeline,pipeline-,8788,"tch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:8834,Deployability,pipeline,pipeline,8834,"tch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:8843,Deployability,pipeline,pipeline-,8843,"tch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:9471,Energy Efficiency,schedul,schedulerName,9471,"oud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:9494,Energy Efficiency,schedul,scheduler,9494,"oud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:7090,Integrability,protocol,protocol,7090,"ry: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:7693,Integrability,protocol,protocol,7693,"e-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:10330,Integrability,message,message,10330,"f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: cleanup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: keep-alive; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/broad-ctsa/benchmark_wang:latest; imageID: """"; lastState: {}; name: main; ready: false; r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:10520,Integrability,message,message,10520,"f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: cleanup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: keep-alive; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/broad-ctsa/benchmark_wang:latest; imageID: """"; lastState: {}; name: main; ready: false; r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:10713,Integrability,message,message,10713,"f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with unready status: [main cleanup keep-alive]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: cleanup; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imageID: """"; lastState: {}; name: keep-alive; ready: false; restartCount: 0; state:; waiting:; reason: PodInitializing; - image: gcr.io/broad-ctsa/benchmark_wang:latest; imageID: """"; lastState: {}; name: main; ready: false; r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:9505,Security,secur,securityContext,9505,"oud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; claimName: batch-12728-job-287-742170; - name: batch-output-pod-token-8pkmz; secret:; defaultMode: 420; secretName: batch-output-pod-token-8pkmz; status:; conditions:; - lastProbeTime: null; lastTransitionTime: 2019-09-05T19:15:42Z; message: 'containers with incomplete status: [setup]'; reason: ContainersNotInitialized; status: ""False""; type: Initialized; - lastProbeTime: null; lastTransitionTime:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:2171,Testability,benchmark,benchmark-resources,2171,/742170/container_logs && exit 1; rm -rf /io/*; set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 50,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:2215,Testability,benchmark,benchmark-resources,2215,-key-file=/gsa-key/privateKeyData)) && mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment:; COPY_OUTPUT_CMD: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:2257,Testability,benchmark,benchmark,2257,dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment:; COPY_OUTPUT_CMD: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TAS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:2317,Testability,benchmark,benchmark-resources,2317,dd4e66d/__TASK__0/0731f9a3 /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment: <none>; Mounts:; /batch-gsa-key from batch-gsa-key (rw); /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); Containers:; main:; Container ID: ; Image: gcr.io/broad-ctsa/benchmark_wang:latest; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources -t read_with_index_p1000; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 2; memory: 7G; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-12728-job-287-742170 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-12728-job-287-742170 (rw); /var/run/secrets/kubernetes.io/serviceaccount from batch-output-pod-token-8pkmz (ro); cleanup:; Container ID: ; Image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; Image ID: ; Port: 5000/TCP; Host Port: 0/TCP; Command:; /bin/sh; -c; ; set -ex; python3 -m batch.cleanup_sidecar; ; State: Waiting; Reason: PodInitializing; Ready: False; Restart Count: 0; Requests:; cpu: 500m; Environment:; COPY_OUTPUT_CMD: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TAS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5712,Testability,benchmark,benchmark-resources,5712," Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-12728-job-287-742170; ReadOnly: false; batch-output-pod-token-8pkmz:; Type: Secret (a volume populated by a Secret); SecretName: batch-output-pod-token-8pkmz; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5756,Testability,benchmark,benchmark-resources,5756,"node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events: <none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5799,Testability,benchmark,benchmark,5799,"<none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:5859,Testability,benchmark,benchmark-resources,5859,"<none>; ```; ```; # k get pods -n batch-pods batch-12728-job-287-742170 -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: 2019-09-05T19:12:22Z; labels:; app: batch-job; batch_id: ""12728""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""287""; user: wang; uuid: ca985fd90f9d46968ab9c480af9c931c; name: batch-12728-job-287-742170; namespace: batch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:6246,Testability,log,log,6246,"atch-pods; resourceVersion: ""116541360""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-12728-job-287-742170; uid: 1681dd05-d011-11e9-92a9-42010a800041; spec:; containers:; - command:; - /bin/bash; - -c; - set -e; mkdir -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/; __RESOURCE_FILE__286=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac;; __RESOURCE_FILE__0=/io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; mv; ${__RESOURCE_FILE__0} benchmark-resources.tar.gz && time tar -xvf benchmark-resources.tar.gz; && hailctl dev benchmark run -v -o ${__RESOURCE_FILE__286} -n 5 --data-dir benchmark-resources; -t read_with_index_p1000; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/broad-ctsa/benchmark_wang:latest; imagePullPolicy: Always; name: main; resources:; requests:; cpu: ""2""; memory: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-ke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:7180,Testability,log,log,7180,"ry: 7G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.cleanup_sidecar\n ""; env:; - name: COPY_OUTPUT_CMD; value: set -ex; (gcloud -q auth activate-service-account --key-file=/gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:7781,Testability,log,log,7781,"e-account; --key-file=/gsa-key/privateKeyData)) && gsutil -m cp -R /io/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__286/8926feac; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: cleanup; ports:; - containerPort: 5000; protocol: TCP; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; - command:; - /bin/sh; - -c; - ""\n set -ex\n python3 -m batch.keep_alive_sidecar\n ""; image: gcr.io/hail-vdc/batch:s32fqwbuz8nv; imagePullPolicy: IfNotPresent; name: keep-alive; ports:; - containerPort: 5001; protocol: TCP; resources:; requests:; cpu: 1m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; initContainers:; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/issues/7016:9048,Testability,log,log,9048,":; - command:; - /bin/sh; - -c; - ""\n set -ex\n (gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData; || (sleep $(( 5 + (RANDOM % 5) )); gcloud -q auth activate-service-account --key-file=/batch-gsa-key/privateKeyData))\n; \ gsutil -q stat gs://hail-batch-3jmp5/cd50b95a89914efb897965a5e982a29d/12728/287/742170/container_logs; && exit 1\n rm -rf /io/*\n set -ex; (gcloud -q auth activate-service-account; --key-file=/gsa-key/privateKeyData || (sleep $(( 5 + (RANDOM % 5) )); gcloud; -q auth activate-service-account --key-file=/gsa-key/privateKeyData)) && mkdir; -p /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0; gsutil -m cp -R gs://hail-wang-ukps2/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3; /io/pipeline/pipeline-1cac3dd4e66d/__TASK__0/0731f9a3\n ""; image: google/cloud-sdk:237.0.0-alpine; imagePullPolicy: IfNotPresent; name: setup; resources:; requests:; cpu: 500m; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /batch-gsa-key; name: batch-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-12728-job-287-742170; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: batch-output-pod-token-8pkmz; readOnly: true; nodeName: gke-vdc-non-preemptible-pool-0106a51b-qz7f; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: batch-output-pod; serviceAccountName: batch-output-pod; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: wang-gsa-key; - name: batch-gsa-key; secret:; defaultMode: 420; secretName: batch-gsa-key; - name: batch-12728-job-287-742170; persistentVolumeClaim:; cla",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016
https://github.com/hail-is/hail/pull/7018:74,Deployability,pipeline,pipeline,74,"In noticed this was missing when I tried to run the benchmarks, which use pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7018
https://github.com/hail-is/hail/pull/7018:52,Testability,benchmark,benchmarks,52,"In noticed this was missing when I tried to run the benchmarks, which use pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7018
https://github.com/hail-is/hail/pull/7020:203,Testability,log,log,203,I think this improves on the experience of receiving a KeyError; when the default namespace has no tokens defined. Now the user; will recieve a NoTokenFileFound exception that indicates the; user should log in with `hailctl auth login`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7020
https://github.com/hail-is/hail/pull/7020:229,Testability,log,login,229,I think this improves on the experience of receiving a KeyError; when the default namespace has no tokens defined. Now the user; will recieve a NoTokenFileFound exception that indicates the; user should log in with `hailctl auth login`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7020
https://github.com/hail-is/hail/pull/7024:475,Availability,down,downloads,475,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024
https://github.com/hail-is/hail/pull/7024:691,Availability,down,downloads,691,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024
https://github.com/hail-is/hail/pull/7024:277,Deployability,configurat,configuration,277,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024
https://github.com/hail-is/hail/pull/7024:535,Deployability,install,installed,535,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024
https://github.com/hail-is/hail/pull/7024:277,Modifiability,config,configuration,277,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024
https://github.com/hail-is/hail/pull/7024:1139,Performance,perform,perform,1139,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024
https://github.com/hail-is/hail/pull/7024:1159,Performance,multi-thread,multi-threaded,1159,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024
https://github.com/hail-is/hail/pull/7024:627,Security,checksum,checksums,627,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024
https://github.com/hail-is/hail/pull/7026:158,Energy Efficiency,reduce,reduce,158,"1. log should include job id not job; 2. `client_session` is only used for k8s-internal requests to worker pods, so; use a very aggressive timeout of 10s; 3. reduce refresh delay to two minutes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7026
https://github.com/hail-is/hail/pull/7026:139,Safety,timeout,timeout,139,"1. log should include job id not job; 2. `client_session` is only used for k8s-internal requests to worker pods, so; use a very aggressive timeout of 10s; 3. reduce refresh delay to two minutes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7026
https://github.com/hail-is/hail/pull/7026:3,Testability,log,log,3,"1. log should include job id not job; 2. `client_session` is only used for k8s-internal requests to worker pods, so; use a very aggressive timeout of 10s; 3. reduce refresh delay to two minutes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7026
https://github.com/hail-is/hail/issues/7030:120,Integrability,depend,depend,120,"Hail strives to be deterministic wrt a seed, but partitioning can interfere with this. As such, the doctests should not depend on data that is randomly generated at doctest time. Instead, we should randomly generate data once and store it in a checked-in file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7030
https://github.com/hail-is/hail/pull/7035:151,Availability,error,error,151,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7035
https://github.com/hail-is/hail/pull/7035:309,Deployability,configurat,configuration,309,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7035
https://github.com/hail-is/hail/pull/7035:263,Modifiability,config,config,263,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7035
https://github.com/hail-is/hail/pull/7035:309,Modifiability,config,configuration,309,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7035
https://github.com/hail-is/hail/pull/7035:490,Modifiability,config,config,490,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7035
https://github.com/hail-is/hail/pull/7035:473,Performance,load,load,473,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7035
https://github.com/hail-is/hail/pull/7035:196,Security,authenticat,authentication,196,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7035
https://github.com/hail-is/hail/issues/7044:296,Availability,error,error,296,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. An error arises when executing operations/actions using field entries. For intance executing `mt.AD.show()` would give the following: . ### Error No.1: ; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 700 type_pprinters=self.type_printers,; 701 deferred_pprinters=self.deferred_printers); --> 702 printer.pretty(obj); 703 printer.flush(); 704 return stream.getvalue(). /usr/local/lib/python3.6/site-packages/IPython/lib/pretty.py in pretty(self, obj); 400 if cls is not object \; 401 and callable(cls.__dict__.get('__repr__')):; --> 402 return _repr_pprint(obj, self, cycle); 403 ; 404 return _default_pprint(obj, self, cycle). /usr/local/lib/python3.6/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle); 695 """"""A pprint that just redirects to the normal repr function.""""""; 696 # Find newlines and replace them with p.break_(); --> 697 output = repr(obj); 698 for idx,output_line in enumerate(output.splitlines()):; 699 if idx:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in __repr__(self); 2521 ; 2522 def __repr__(self):; -> 2523 return self.__str__(); 2524 ; 2525 def _repr_html_(self):. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in __str__(self); 2515 ; 2516 def __str__(self):; -> 2517 s = self.table_show.__str__(); 2518 if self.displayed_n_cols != self.actual_n_cols:; 2519 s += f""showing the first { self.displayed_n_cols } of { self.actual_n_cols } columns"". /usr/local/lib/python3.6/site-packages/hail/table.py in __str__(self); 1241 ; 1242 def __str__(self):; -> 1243 ret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:433,Availability,Error,Error,433,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. An error arises when executing operations/actions using field entries. For intance executing `mt.AD.show()` would give the following: . ### Error No.1: ; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 700 type_pprinters=self.type_printers,; 701 deferred_pprinters=self.deferred_printers); --> 702 printer.pretty(obj); 703 printer.flush(); 704 return stream.getvalue(). /usr/local/lib/python3.6/site-packages/IPython/lib/pretty.py in pretty(self, obj); 400 if cls is not object \; 401 and callable(cls.__dict__.get('__repr__')):; --> 402 return _repr_pprint(obj, self, cycle); 403 ; 404 return _default_pprint(obj, self, cycle). /usr/local/lib/python3.6/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle); 695 """"""A pprint that just redirects to the normal repr function.""""""; 696 # Find newlines and replace them with p.break_(); --> 697 output = repr(obj); 698 for idx,output_line in enumerate(output.splitlines()):; 699 if idx:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in __repr__(self); 2521 ; 2522 def __repr__(self):; -> 2523 return self.__str__(); 2524 ; 2525 def _repr_html_(self):. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in __str__(self); 2515 ; 2516 def __str__(self):; -> 2517 s = self.table_show.__str__(); 2518 if self.displayed_n_cols != self.actual_n_cols:; 2519 s += f""showing the first { self.displayed_n_cols } of { self.actual_n_cols } columns"". /usr/local/lib/python3.6/site-packages/hail/table.py in __str__(self); 1241 ; 1242 def __str__(self):; -> 1243 ret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:4822,Availability,Error,Error,4822,", **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 times, most recent failure: Lost task 15.3 in stage 16.0 (TID 178, ip-172-31-1-20.ec2.internal, executor 4): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:5177,Availability,failure,failure,5177,"f, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 times, most recent failure: Lost task 15.3 in stage 16.0 (TID 178, ip-172-31-1-20.ec2.internal, executor 4): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:5236,Availability,failure,failure,5236,"hc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 times, most recent failure: Lost task 15.3 in stage 16.0 (TID 178, ip-172-31-1-20.ec2.internal, executor 4): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:5371,Availability,error,error,5371," /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 times, most recent failure: Lost task 15.3 in stage 16.0 (TID 178, ip-172-31-1-20.ec2.internal, executor 4): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:17094,Availability,error,error,17094,ge.scala:596); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at or,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:26179,Availability,Error,Error,26179,"oint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in _repr_html_(self); 1256 ; 1257 def _repr_html_(self):; -> 1258 return self._html_str(); 1259 ; 1260 def _ascii_str(self):. /usr/local/lib/python3.6/site-packages/hail/table.py in _html_str(self); 1342 types = self.types; 1343 ; -> 1344 rows, has_more, d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:26250,Availability,Error,Error,26250,".iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in _repr_html_(self); 1256 ; 1257 def _repr_html_(self):; -> 1258 return self._html_str(); 1259 ; 1260 def _ascii_str(self):. /usr/local/lib/python3.6/site-packages/hail/table.py in _html_str(self); 1342 types = self.types; 1343 ; -> 1344 rows, has_more, dtype = self.data(); 1345 fields = list(dtype); 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:29807,Availability,Error,Error,29807,", **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:30159,Availability,failure,failure,30159,"(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:30217,Availability,failure,failure,30217,"()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:30352,Availability,error,error,30352,"s']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:42075,Availability,error,error,42075,ge.scala:596); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:7); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at or,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:51160,Availability,Error,Error,51160,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:7436,Energy Efficiency,schedul,scheduler,7436,titionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:7516,Energy Efficiency,schedul,scheduler,7516,uteOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:7596,Energy Efficiency,schedul,scheduler,7596,ala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:11842,Energy Efficiency,schedul,scheduler,11842,ion.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:11882,Energy Efficiency,schedul,scheduler,11882,ala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:11981,Energy Efficiency,schedul,scheduler,11981,ike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12079,Energy Efficiency,schedul,scheduler,12079,; 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12333,Energy Efficiency,schedul,scheduler,12333,nceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12414,Energy Efficiency,schedul,scheduler,12414,anceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12520,Energy Efficiency,schedul,scheduler,12520,ala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12670,Energy Efficiency,schedul,scheduler,12670, org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12759,Energy Efficiency,schedul,scheduler,12759,(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12857,Energy Efficiency,schedul,scheduler,12857,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:222); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12953,Energy Efficiency,schedul,scheduler,12953,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:222); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1227); 	at is.hail.rvd.RVD.coalesce(RVD.scala:281); 	at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:13118,Energy Efficiency,schedul,scheduler,13118,.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:222); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1227); 	at is.hail.rvd.RVD.coalesce(RVD.scala:281); 	at is.hail.expr.ir.TableRepartition.execute(TableIR.scala:482); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:929); 	at is.hail.expr.ir.TableRename.execute(TableIR.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:19159,Energy Efficiency,schedul,scheduler,19159,titionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:19239,Energy Efficiency,schedul,scheduler,19239,uteOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:19319,Energy Efficiency,schedul,scheduler,19319,ala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:25500,Energy Efficiency,schedul,scheduler,25500,"titionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:25580,Energy Efficiency,schedul,scheduler,25580,"uteOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:25660,Energy Efficiency,schedul,scheduler,25660,"ala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:32417,Energy Efficiency,schedul,scheduler,32417,titionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:32497,Energy Efficiency,schedul,scheduler,32497,uteOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:32577,Energy Efficiency,schedul,scheduler,32577,ala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:36823,Energy Efficiency,schedul,scheduler,36823,ion.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:36863,Energy Efficiency,schedul,scheduler,36863,ala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:36962,Energy Efficiency,schedul,scheduler,36962,ike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37060,Energy Efficiency,schedul,scheduler,37060,; 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37314,Energy Efficiency,schedul,scheduler,37314,nceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37395,Energy Efficiency,schedul,scheduler,37395,anceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37501,Energy Efficiency,schedul,scheduler,37501,ala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37651,Energy Efficiency,schedul,scheduler,37651, org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37740,Energy Efficiency,schedul,scheduler,37740,(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37838,Energy Efficiency,schedul,scheduler,37838,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:222); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37934,Energy Efficiency,schedul,scheduler,37934,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:222); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1227); 	at is.hail.rvd.RVD.coalesce(RVD.scala:281); 	at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:38099,Energy Efficiency,schedul,scheduler,38099,.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:222); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1227); 	at is.hail.rvd.RVD.coalesce(RVD.scala:281); 	at is.hail.expr.ir.TableRepartition.execute(TableIR.scala:482); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:929); 	at is.hail.expr.ir.TableRename.execute(TableIR.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:44140,Energy Efficiency,schedul,scheduler,44140,titionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:44220,Energy Efficiency,schedul,scheduler,44220,uteOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:44300,Energy Efficiency,schedul,scheduler,44300,ala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:50481,Energy Efficiency,schedul,scheduler,50481,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:50561,Energy Efficiency,schedul,scheduler,50561,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:50641,Energy Efficiency,schedul,scheduler,50641,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:2939,Integrability,wrap,wrapper,2939,"lib/python3.6/site-packages/hail/table.py in _ascii_str(self); 1268 return s; 1269 ; -> 1270 rows, has_more, dtype = self.data(); 1271 fields = list(dtype); 1272 trunc_fields = [trunc(f) for f in fields]. /usr/local/lib/python3.6/site-packages/hail/table.py in data(self); 1251 row_dtype = t.row.dtype; 1252 t = t.select(**{k: Table._hl_format(v, self.truncate) for (k, v) in t.row.items()}); -> 1253 rows, has_more = t._take_n(self.n); 1254 self._data = (rows, has_more, row_dtype); 1255 return self._data. /usr/local/lib/python3.6/site-packages/hail/table.py in _take_n(self, n); 1372 has_more = False; 1373 else:; -> 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:2990,Integrability,wrap,wrapper,2990,"lib/python3.6/site-packages/hail/table.py in _ascii_str(self); 1268 return s; 1269 ; -> 1270 rows, has_more, dtype = self.data(); 1271 fields = list(dtype); 1272 trunc_fields = [trunc(f) for f in fields]. /usr/local/lib/python3.6/site-packages/hail/table.py in data(self); 1251 row_dtype = t.row.dtype; 1252 t = t.select(**{k: Table._hl_format(v, self.truncate) for (k, v) in t.row.items()}); -> 1253 rows, has_more = t._take_n(self.n); 1254 self._data = (rows, has_more, row_dtype); 1255 return self._data. /usr/local/lib/python3.6/site-packages/hail/table.py in _take_n(self, n); 1372 has_more = False; 1373 else:; -> 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:3196,Integrability,wrap,wrapper,3196,"lib/python3.6/site-packages/hail/table.py in _ascii_str(self); 1268 return s; 1269 ; -> 1270 rows, has_more, dtype = self.data(); 1271 fields = list(dtype); 1272 trunc_fields = [trunc(f) for f in fields]. /usr/local/lib/python3.6/site-packages/hail/table.py in data(self); 1251 row_dtype = t.row.dtype; 1252 t = t.select(**{k: Table._hl_format(v, self.truncate) for (k, v) in t.row.items()}); -> 1253 rows, has_more = t._take_n(self.n); 1254 self._data = (rows, has_more, row_dtype); 1255 return self._data. /usr/local/lib/python3.6/site-packages/hail/table.py in _take_n(self, n); 1372 has_more = False; 1373 else:; -> 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:3558,Integrability,wrap,wrapper,3558," 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:3609,Integrability,wrap,wrapper,3609," 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:3815,Integrability,wrap,wrapper,3815," 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:27924,Integrability,wrap,wrapper,27924,"f _ascii_str(self):. /usr/local/lib/python3.6/site-packages/hail/table.py in _html_str(self); 1342 types = self.types; 1343 ; -> 1344 rows, has_more, dtype = self.data(); 1345 fields = list(dtype); 1346 . /usr/local/lib/python3.6/site-packages/hail/table.py in data(self); 1251 row_dtype = t.row.dtype; 1252 t = t.select(**{k: Table._hl_format(v, self.truncate) for (k, v) in t.row.items()}); -> 1253 rows, has_more = t._take_n(self.n); 1254 self._data = (rows, has_more, row_dtype); 1255 return self._data. /usr/local/lib/python3.6/site-packages/hail/table.py in _take_n(self, n); 1372 has_more = False; 1373 else:; -> 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:27975,Integrability,wrap,wrapper,27975,"f _ascii_str(self):. /usr/local/lib/python3.6/site-packages/hail/table.py in _html_str(self); 1342 types = self.types; 1343 ; -> 1344 rows, has_more, dtype = self.data(); 1345 fields = list(dtype); 1346 . /usr/local/lib/python3.6/site-packages/hail/table.py in data(self); 1251 row_dtype = t.row.dtype; 1252 t = t.select(**{k: Table._hl_format(v, self.truncate) for (k, v) in t.row.items()}); -> 1253 rows, has_more = t._take_n(self.n); 1254 self._data = (rows, has_more, row_dtype); 1255 return self._data. /usr/local/lib/python3.6/site-packages/hail/table.py in _take_n(self, n); 1372 has_more = False; 1373 else:; -> 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:28181,Integrability,wrap,wrapper,28181,"f _ascii_str(self):. /usr/local/lib/python3.6/site-packages/hail/table.py in _html_str(self); 1342 types = self.types; 1343 ; -> 1344 rows, has_more, dtype = self.data(); 1345 fields = list(dtype); 1346 . /usr/local/lib/python3.6/site-packages/hail/table.py in data(self); 1251 row_dtype = t.row.dtype; 1252 t = t.select(**{k: Table._hl_format(v, self.truncate) for (k, v) in t.row.items()}); -> 1253 rows, has_more = t._take_n(self.n); 1254 self._data = (rows, has_more, row_dtype); 1255 return self._data. /usr/local/lib/python3.6/site-packages/hail/table.py in _take_n(self, n); 1372 has_more = False; 1373 else:; -> 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:28543,Integrability,wrap,wrapper,28543," 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:28594,Integrability,wrap,wrapper,28594," 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:28800,Integrability,wrap,wrapper,28800," 1374 rows = self.take(n + 1); 1375 has_more = len(rows) > n; 1376 rows = rows[:n]. </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1029> in take(self, n, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:4215,Performance,load,loads,4215,"python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 tim",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:7881,Performance,concurren,concurrent,7881,2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:7966,Performance,concurren,concurrent,7966,he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:19604,Performance,concurren,concurrent,19604,2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:19689,Performance,concurren,concurrent,19689,he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:25945,Performance,concurren,concurrent,25945,"2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:26030,Performance,concurren,concurrent,26030,"he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in _repr_html_(self); 1256 ; 1257 def _repr_html_(self):; -> 1258 return self._html_str(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:29200,Performance,load,loads,29200,"python3.6/site-packages/hail/table.py in take(self, n, _localize); 2011 """"""; 2012 ; -> 2013 return self.head(n).collect(_localize); 2014 ; 2015 @typecheck_method(n=int). </usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1023> in collect(self, _localize). /usr/local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kwargs):; 584 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 585 return __original_func(*args_, **kwargs_); 586 ; 587 return wrapper. /usr/local/lib/python3.6/site-packages/hail/table.py in collect(self, _localize); 1825 e = construct_expr(ir, hl.tarray(self.row.dtype)); 1826 if _localize:; -> 1827 return Env.backend().execute(e._ir); 1828 else:; 1829 return e. /usr/local/lib/python3.6/site-packages/hail/backend/backend.py in execute(self, ir, timed); 106 ; 107 def execute(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:32862,Performance,concurren,concurrent,32862,2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:32947,Performance,concurren,concurrent,32947,he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:44585,Performance,concurren,concurrent,44585,2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:44670,Performance,concurren,concurrent,44670,he.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.scala:1443),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:50926,Performance,concurren,concurrent,50926,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:51011,Performance,concurren,concurrent,51011,tor(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:5156,Safety,abort,aborted,5156,"f, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37. ```. ### Traces No. 1: . ```java ; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 16.0 failed 4 times, most recent failure: Lost task 15.3 in stage 16.0 (TID 178, ip-172-31-1-20.ec2.internal, executor 4): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12013,Safety,abort,abortStage,12013,ala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12111,Safety,abort,abortStage,12111,.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:12356,Safety,abort,abortStage,12356,ion.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:30138,Safety,abort,aborted,30138,"(self, ir, timed=False):; --> 108 result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); 109 value = ir.typ._from_json(result['value']); 110 timings = result['timings']. /usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. /usr/local/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 223 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 224 'Hail version: %s\n'; --> 225 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 226 except pyspark.sql.utils.CapturedException as e:; 227 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NoSuchElementException: key not found: GRCh37; ```. ### Traces No.2:; ```java; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 220, ip-172-31-2-255.ec2.internal, executor 2): org.json4s.package$MappingException: unknown error; 	at org.json4s.Extraction$.extract(Extraction.scala:43); 	at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21); 	at is.hail.io.index.IndexReader$.readMetadata(IndexReader.scala:65); 	at is.hail.io.index.IndexReader.<init>(IndexReader.scala:90); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.io.index.IndexReaderBuilder$$anonfun$withDecoders$1.apply(IndexReader.scala:50); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:879); 	at is.hail.HailContext$$anon$3$$anonfun$20.apply(HailContext.scala:877); 	at scala.Option.map(Option.scala:146); 	at is.hail.HailContext$$anon$3.compute(HailContext.scala:877); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:36994,Safety,abort,abortStage,36994,ala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37092,Safety,abort,abortStage,37092,.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/issues/7044:37337,Safety,abort,abortStage,37337,ion.scala:546); 	at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:597); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:400); 	at org.json4s.Extraction$$anonfun$extract$6.apply(Extraction.scala:392); 	at org.json4s.Extraction$.customOrElse(Extraction.scala:606); 	at org.json4s.Extraction$.extract(Extraction.scala:392); 	at org.json4s.Extraction$.extract(Extraction.scala:39); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2039); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2027); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2026); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2026); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2260); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2209); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2198); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7044
https://github.com/hail-is/hail/pull/7045:373,Deployability,install,installed,373,"The image _BUILT target didn't depend on everything in the folders. I don't know how to make it work. `%` doesn't seem to get substituted inside a Make `$(...)` command, so I don't know how to `find` all the dependencies. I just made it always run `docker`. I renamed the hail image because I use `hail` locally for an image that doesn't contain a notebook. It's just hail installed in ubuntu. The `--ip` is apparently necessary for python2 Jupyter Notebook (which is used by the isia image).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7045
https://github.com/hail-is/hail/pull/7045:31,Integrability,depend,depend,31,"The image _BUILT target didn't depend on everything in the folders. I don't know how to make it work. `%` doesn't seem to get substituted inside a Make `$(...)` command, so I don't know how to `find` all the dependencies. I just made it always run `docker`. I renamed the hail image because I use `hail` locally for an image that doesn't contain a notebook. It's just hail installed in ubuntu. The `--ip` is apparently necessary for python2 Jupyter Notebook (which is used by the isia image).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7045
https://github.com/hail-is/hail/pull/7045:208,Integrability,depend,dependencies,208,"The image _BUILT target didn't depend on everything in the folders. I don't know how to make it work. `%` doesn't seem to get substituted inside a Make `$(...)` command, so I don't know how to `find` all the dependencies. I just made it always run `docker`. I renamed the hail image because I use `hail` locally for an image that doesn't contain a notebook. It's just hail installed in ubuntu. The `--ip` is apparently necessary for python2 Jupyter Notebook (which is used by the isia image).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7045
https://github.com/hail-is/hail/pull/7046:14,Testability,test,tested,14,Fixes #7044. (tested on cluster),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7046
https://github.com/hail-is/hail/pull/7054:19,Deployability,release,release,19,Add change log for release 0.2.22,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7054
https://github.com/hail-is/hail/pull/7054:11,Testability,log,log,11,Add change log for release 0.2.22,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7054
https://github.com/hail-is/hail/pull/7055:15,Integrability,interface,interface,15,"## `JoinPoint` interface for nontrivial, type-safe control flow in the JVM backend. This PR implements a ""join-point"" abstraction for the JVM backend. Join-points are a primitive; control flow construct that allow sophisticated forms of branching to be implemented in a type safe; way, without having to directly manipulate labels and jumps at the JVM bytecode level. Importantly,; they will enable the kinds of branching needed by stream-deforestation techniques that; @patrick-schultz and I have been discussing, for which while loops and if's were not sufficient. We've also discussed plans for implementing join points as a feature in the IR. ### Notable examples:. * Implementation of `whileLoop` (emits bytecode identical to current version):; ```scala; def whileLoop(cond: Code[Boolean], body: Code[Unit]): Code[Unit] =; JoinPoint.CallCC[Unit] { (jb, break) =>; val continue = jb.joinPoint(); val loopBody = jb.joinPoint(); continue.define { _ => JoinPoint.mux(cond, loopBody, break) }; loopBody.define { _ => Code(body, continue(())) }; continue(()); }; ```. * Mutual recursion:; ```scala; def parity(; n: Code[Int],; even: Code[Ctrl],; odd: Code[Ctrl]; ): Code[Ctrl] = {; val isEven = jb.joinPoint[Code[Int]](mb); val isOdd = jb.joinPoint[Code[Int]](mb); isEven.define { i => (i ceq 0).mux(even, isOdd(i - 1)) }; isOdd.define { i => (i ceq 0).mux(odd, isEven(i - 1)) }; isEven(n); }; ```. ### Classes of interest (tl;dr). - `JoinPoint` - Non-returning function. Used to implement control flow in a type-safe, functional way.; - `ParameterPack` - Trait used for tuple deforesting. Allows join-points to be provided multiple; arguments.; - `JoinPointBuilder` - Used to define new join points.; - `CallCC` - Entry-point for expressions with complex control flow. Provides a `JoinPointBuilder`; and a `JoinPoint` to return a value from the expression. ### `JoinPoint`. A `JoinPoint[A]` acts like a non-returning function with an argument of type `A`. The type of an; applied join point is `Code[C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055
https://github.com/hail-is/hail/pull/7055:46,Safety,safe,safe,46,"## `JoinPoint` interface for nontrivial, type-safe control flow in the JVM backend. This PR implements a ""join-point"" abstraction for the JVM backend. Join-points are a primitive; control flow construct that allow sophisticated forms of branching to be implemented in a type safe; way, without having to directly manipulate labels and jumps at the JVM bytecode level. Importantly,; they will enable the kinds of branching needed by stream-deforestation techniques that; @patrick-schultz and I have been discussing, for which while loops and if's were not sufficient. We've also discussed plans for implementing join points as a feature in the IR. ### Notable examples:. * Implementation of `whileLoop` (emits bytecode identical to current version):; ```scala; def whileLoop(cond: Code[Boolean], body: Code[Unit]): Code[Unit] =; JoinPoint.CallCC[Unit] { (jb, break) =>; val continue = jb.joinPoint(); val loopBody = jb.joinPoint(); continue.define { _ => JoinPoint.mux(cond, loopBody, break) }; loopBody.define { _ => Code(body, continue(())) }; continue(()); }; ```. * Mutual recursion:; ```scala; def parity(; n: Code[Int],; even: Code[Ctrl],; odd: Code[Ctrl]; ): Code[Ctrl] = {; val isEven = jb.joinPoint[Code[Int]](mb); val isOdd = jb.joinPoint[Code[Int]](mb); isEven.define { i => (i ceq 0).mux(even, isOdd(i - 1)) }; isOdd.define { i => (i ceq 0).mux(odd, isEven(i - 1)) }; isEven(n); }; ```. ### Classes of interest (tl;dr). - `JoinPoint` - Non-returning function. Used to implement control flow in a type-safe, functional way.; - `ParameterPack` - Trait used for tuple deforesting. Allows join-points to be provided multiple; arguments.; - `JoinPointBuilder` - Used to define new join points.; - `CallCC` - Entry-point for expressions with complex control flow. Provides a `JoinPointBuilder`; and a `JoinPoint` to return a value from the expression. ### `JoinPoint`. A `JoinPoint[A]` acts like a non-returning function with an argument of type `A`. The type of an; applied join point is `Code[C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055
https://github.com/hail-is/hail/pull/7055:275,Safety,safe,safe,275,"## `JoinPoint` interface for nontrivial, type-safe control flow in the JVM backend. This PR implements a ""join-point"" abstraction for the JVM backend. Join-points are a primitive; control flow construct that allow sophisticated forms of branching to be implemented in a type safe; way, without having to directly manipulate labels and jumps at the JVM bytecode level. Importantly,; they will enable the kinds of branching needed by stream-deforestation techniques that; @patrick-schultz and I have been discussing, for which while loops and if's were not sufficient. We've also discussed plans for implementing join points as a feature in the IR. ### Notable examples:. * Implementation of `whileLoop` (emits bytecode identical to current version):; ```scala; def whileLoop(cond: Code[Boolean], body: Code[Unit]): Code[Unit] =; JoinPoint.CallCC[Unit] { (jb, break) =>; val continue = jb.joinPoint(); val loopBody = jb.joinPoint(); continue.define { _ => JoinPoint.mux(cond, loopBody, break) }; loopBody.define { _ => Code(body, continue(())) }; continue(()); }; ```. * Mutual recursion:; ```scala; def parity(; n: Code[Int],; even: Code[Ctrl],; odd: Code[Ctrl]; ): Code[Ctrl] = {; val isEven = jb.joinPoint[Code[Int]](mb); val isOdd = jb.joinPoint[Code[Int]](mb); isEven.define { i => (i ceq 0).mux(even, isOdd(i - 1)) }; isOdd.define { i => (i ceq 0).mux(odd, isEven(i - 1)) }; isEven(n); }; ```. ### Classes of interest (tl;dr). - `JoinPoint` - Non-returning function. Used to implement control flow in a type-safe, functional way.; - `ParameterPack` - Trait used for tuple deforesting. Allows join-points to be provided multiple; arguments.; - `JoinPointBuilder` - Used to define new join points.; - `CallCC` - Entry-point for expressions with complex control flow. Provides a `JoinPointBuilder`; and a `JoinPoint` to return a value from the expression. ### `JoinPoint`. A `JoinPoint[A]` acts like a non-returning function with an argument of type `A`. The type of an; applied join point is `Code[C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055
https://github.com/hail-is/hail/pull/7055:1512,Safety,safe,safe,1512,"n discussing, for which while loops and if's were not sufficient. We've also discussed plans for implementing join points as a feature in the IR. ### Notable examples:. * Implementation of `whileLoop` (emits bytecode identical to current version):; ```scala; def whileLoop(cond: Code[Boolean], body: Code[Unit]): Code[Unit] =; JoinPoint.CallCC[Unit] { (jb, break) =>; val continue = jb.joinPoint(); val loopBody = jb.joinPoint(); continue.define { _ => JoinPoint.mux(cond, loopBody, break) }; loopBody.define { _ => Code(body, continue(())) }; continue(()); }; ```. * Mutual recursion:; ```scala; def parity(; n: Code[Int],; even: Code[Ctrl],; odd: Code[Ctrl]; ): Code[Ctrl] = {; val isEven = jb.joinPoint[Code[Int]](mb); val isOdd = jb.joinPoint[Code[Int]](mb); isEven.define { i => (i ceq 0).mux(even, isOdd(i - 1)) }; isOdd.define { i => (i ceq 0).mux(odd, isEven(i - 1)) }; isEven(n); }; ```. ### Classes of interest (tl;dr). - `JoinPoint` - Non-returning function. Used to implement control flow in a type-safe, functional way.; - `ParameterPack` - Trait used for tuple deforesting. Allows join-points to be provided multiple; arguments.; - `JoinPointBuilder` - Used to define new join points.; - `CallCC` - Entry-point for expressions with complex control flow. Provides a `JoinPointBuilder`; and a `JoinPoint` to return a value from the expression. ### `JoinPoint`. A `JoinPoint[A]` acts like a non-returning function with an argument of type `A`. The type of an; applied join point is `Code[Ctrl]`, which indicates that this code does some sort of control flow; (like a jump, or a loop), instead of returning a value. Under the hood, `JoinPoint`s are implemented; with a label and a `GOTO` instruction. ```scala; def example1(j: JoinPoint[Code[Int]]): Code[Ctrl] = Code(; j(3),; ""this line is never reached"".println,; j(4)); ```. ### `ParameterPack`. The trait `ParameterPack[A]` says that the type `A` is comprised of a list of `Code[T]`s, such that the structure of that list is statically k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055
https://github.com/hail-is/hail/pull/7055:3644,Security,access,access,3644,", instead of returning a value. Under the hood, `JoinPoint`s are implemented; with a label and a `GOTO` instruction. ```scala; def example1(j: JoinPoint[Code[Int]]): Code[Ctrl] = Code(; j(3),; ""this line is never reached"".println,; j(4)); ```. ### `ParameterPack`. The trait `ParameterPack[A]` says that the type `A` is comprised of a list of `Code[T]`s, such that the structure of that list is statically known. While this is generally useful for representing deforested tuples, it is specifically useful here because it allows `JoinPoint`s to take multiple arguments as a tuple of the individual arguments. This ""tuple"" will be represented at runtime by pushing or popping multiple values from the JVM argument stack. ```scala; // case class JoinPoint[A: ParameterPack] ( .... ); def example2(j: JoinPoint[(Code[Int], Code[Boolean], Code[Int])]): Code[Ctrl] =; j((8, true, 9)); /* LDC 8; * LDC 1; * LDC 9; * GOTO j */; ```. ### `JoinPointBuilder`. The only way to create new join-points is via a `JoinPointBuilder` object, using the `joinPoint[A]` method (where `A` is the desired argument type). Then the body of the join-point can be provided by calling `define`. ```scala; val mb: MethodBuilder; // method builder required to create locals to store join point arguments; def example3(jb: JoinPointBuilder, exit: JoinPoint[Code[Int]]): JoinPoint[Code[Int]] = {; val j = jb.joinPoint[Code[Int]](mb); j.define { n => exit(n + 1) }; j; }; ```. ### `CallCC`. The only way to obtain a `JoinPointBuilder` is through a `CallCC`. `CallCC` also provides access to; the ""current continuation"" -- a join-point which takes an argument that will become the final result; of the entire `CallCC` expression. In fact, the only way to return a value from a `CallCC` is by; eventually calling this join-point. ```scala; val example4: Code[Boolean] =; JoinPoint.CallCC { (jb: JoinPointBuilder, ret: JoinPoint[Code[Boolean]]) =>; const(false).mux(; ret(false),; ret(true)); }; // running 'example4' gives 'true'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055
https://github.com/hail-is/hail/pull/7059:37,Performance,perform,performance,37,"This doesn't actually hugely improve performance:. Master:; ```; 2019-09-12 16:25:06,282: INFO: [1/1] Running import_bgen_force_count_all...; 2019-09-12 16:26:26,427: INFO: burn in: 80.14s; 2019-09-12 16:27:46,816: INFO: run 1: 80.39s; 2019-09-12 16:29:10,637: INFO: run 2: 83.82s; 2019-09-12 16:30:33,742: INFO: run 3: 83.11s; 2019-09-12 16:31:53,968: INFO: run 4: 80.22s; 2019-09-12 16:33:18,855: INFO: run 5: 84.89s; ```. PR:; ```; 2019-09-12 16:46:20,424: INFO: [1/1] Running import_bgen_force_count_all...; 2019-09-12 16:47:39,550: INFO: burn in: 79.12s; 2019-09-12 16:48:57,259: INFO: run 1: 77.71s; 2019-09-12 16:50:15,457: INFO: run 2: 78.20s; 2019-09-12 16:51:32,472: INFO: run 3: 77.01s; 2019-09-12 16:52:49,160: INFO: run 4: 76.68s; 2019-09-12 16:54:05,504: INFO: run 5: 76.34s; ```. However, it does make the generated much easier to profile, and; would make much more of difference if we support BGEN 1.3 with ZSTD.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7059
https://github.com/hail-is/hail/pull/7061:154,Safety,safe,safer,154,"cc: @tpoterba . The main and cleanup containers run in parallel and thus should not be summed but max'ed (cleanup should always be longer, but this seems safer). I'm not sure why we return None if a duration is present in the dictionary but set to None (as opposed to missing from the dictionary), but I preserved that behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7061
https://github.com/hail-is/hail/pull/7062:687,Integrability,rout,router,687,"A few things. - `app.secret_key` was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; - set the default image to isia; - fix the URL redirect when you go to `notebook.hail.is` with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use `with_query`. What I did URL encoded the `?` so it wasn't treated as a query parameter it was treated as a file with a literal `?` in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7062
https://github.com/hail-is/hail/pull/7062:699,Integrability,rout,router,699,"A few things. - `app.secret_key` was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; - set the default image to isia; - fix the URL redirect when you go to `notebook.hail.is` with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use `with_query`. What I did URL encoded the `?` so it wasn't treated as a query parameter it was treated as a file with a literal `?` in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7062
https://github.com/hail-is/hail/pull/7062:706,Integrability,rout,router,706,"A few things. - `app.secret_key` was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; - set the default image to isia; - fix the URL redirect when you go to `notebook.hail.is` with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use `with_query`. What I did URL encoded the `?` so it wasn't treated as a query parameter it was treated as a file with a literal `?` in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7062
https://github.com/hail-is/hail/pull/7062:1088,Integrability,depend,depends,1088,"A few things. - `app.secret_key` was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; - set the default image to isia; - fix the URL redirect when you go to `notebook.hail.is` with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use `with_query`. What I did URL encoded the `?` so it wasn't treated as a query parameter it was treated as a file with a literal `?` in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7062
https://github.com/hail-is/hail/pull/7062:793,Testability,log,login,793,"A few things. - `app.secret_key` was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; - set the default image to isia; - fix the URL redirect when you go to `notebook.hail.is` with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use `with_query`. What I did URL encoded the `?` so it wasn't treated as a query parameter it was treated as a file with a literal `?` in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7062
https://github.com/hail-is/hail/issues/7063:657,Availability,error,error,657,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7063
https://github.com/hail-is/hail/issues/7063:663,Integrability,message,message,663,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7063
https://github.com/hail-is/hail/issues/7063:1096,Performance,cache,cache,1096,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7063
https://github.com/hail-is/hail/issues/7063:1320,Performance,cache,cache,1320,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7063
https://github.com/hail-is/hail/issues/7063:229,Testability,test,test,229,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7063
https://github.com/hail-is/hail/issues/7063:706,Testability,test,test,706,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7063
https://github.com/hail-is/hail/issues/7063:1165,Testability,test,test,1165,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7063
https://github.com/hail-is/hail/pull/7064:169,Deployability,deploy,deployed,169,"Use web_common in ci, batch and scorecard for common design (e.g. header) and styling. The styling is clearly not ideal, this is intended to be a work in progress. Hand-deployed to checked batch and scorecard UI are still workable. Not easy to do with CI. Converted scorecard to aiohttp and gidgethub. Will remove dependencies in separate PR so I don't want to wait for all the images to rebuild. Can't quite get rid of Flask yet as it is used in callback tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064
https://github.com/hail-is/hail/pull/7064:314,Integrability,depend,dependencies,314,"Use web_common in ci, batch and scorecard for common design (e.g. header) and styling. The styling is clearly not ideal, this is intended to be a work in progress. Hand-deployed to checked batch and scorecard UI are still workable. Not easy to do with CI. Converted scorecard to aiohttp and gidgethub. Will remove dependencies in separate PR so I don't want to wait for all the images to rebuild. Can't quite get rid of Flask yet as it is used in callback tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064
https://github.com/hail-is/hail/pull/7064:456,Testability,test,tests,456,"Use web_common in ci, batch and scorecard for common design (e.g. header) and styling. The styling is clearly not ideal, this is intended to be a work in progress. Hand-deployed to checked batch and scorecard UI are still workable. Not easy to do with CI. Converted scorecard to aiohttp and gidgethub. Will remove dependencies in separate PR so I don't want to wait for all the images to rebuild. Can't quite get rid of Flask yet as it is used in callback tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064
https://github.com/hail-is/hail/pull/7064:102,Usability,clear,clearly,102,"Use web_common in ci, batch and scorecard for common design (e.g. header) and styling. The styling is clearly not ideal, this is intended to be a work in progress. Hand-deployed to checked batch and scorecard UI are still workable. Not easy to do with CI. Converted scorecard to aiohttp and gidgethub. Will remove dependencies in separate PR so I don't want to wait for all the images to rebuild. Can't quite get rid of Flask yet as it is used in callback tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064
https://github.com/hail-is/hail/pull/7065:675,Integrability,rout,router,675,"A few things. - app.secret_key was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; set the default image to isia; - fix the URL redirect when you go to notebook.hail.is with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use with_query. What I did URL encoded the ? so it wasn't treated as a query parameter it was treated as a file with a literal ? in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7065
https://github.com/hail-is/hail/pull/7065:687,Integrability,rout,router,687,"A few things. - app.secret_key was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; set the default image to isia; - fix the URL redirect when you go to notebook.hail.is with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use with_query. What I did URL encoded the ? so it wasn't treated as a query parameter it was treated as a file with a literal ? in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7065
https://github.com/hail-is/hail/pull/7065:694,Integrability,rout,router,694,"A few things. - app.secret_key was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; set the default image to isia; - fix the URL redirect when you go to notebook.hail.is with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use with_query. What I did URL encoded the ? so it wasn't treated as a query parameter it was treated as a file with a literal ? in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7065
https://github.com/hail-is/hail/pull/7065:1076,Integrability,depend,depends,1076,"A few things. - app.secret_key was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; set the default image to isia; - fix the URL redirect when you go to notebook.hail.is with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use with_query. What I did URL encoded the ? so it wasn't treated as a query parameter it was treated as a file with a literal ? in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7065
https://github.com/hail-is/hail/pull/7065:781,Testability,log,login,781,"A few things. - app.secret_key was never used, it was an old hold over from the flask style of doing HTTP session cookies. When I translated notebook, I accidentally made it generate a new secret key every time it started up. This PR fixes it to use the secret key in the notebook secret.; set the default image to isia; - fix the URL redirect when you go to notebook.hail.is with a preexisting session. I screwed this up when I switched to aiohttp. You're not supposed to put query parameters in yourself, you're supposed to use with_query. What I did URL encoded the ? so it wasn't treated as a query parameter it was treated as a file with a literal ? in the name.; - the router (see router/router.nginx.conf.in) treats 502 and 504 as if the pod was dead which redirects to the login page and deletes the pod and service if they exist, if the user does not have a valid service name in their cookie, something went wrong with the cookie and we should kill everything and start fresh, returning 502 has this effect.; - set isia image to latest one (I build this locally, it depends on some very large files, notebook1 is going away soon otherwise we'd have to have a better solution to this).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7065
https://github.com/hail-is/hail/pull/7066:36,Performance,cache,cached,36,Fixes #7063. I forgot to change the cached methods in Emit to account for return type when I changed it to allow a single function to specify multiple return types.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7066
https://github.com/hail-is/hail/issues/7068:579,Availability,Error,Error,579,"- [ ] Signature is wrong for load_references_from_dataset; * abstractMethod returns []. This doesn't do anything (`load_references_from_dataset` still needs to be implemented), and is also not what the concrete implementation of SparkBackend returns (which is a string).; * In general I think we should have type annotations on these methods, to document the expected inputs, outputs. - [ ] ServiceBackend needs to implement load_references_from_dataset. - [ ] ServiceBackend constructor is wrong. states it takes `deploy_config`, but backend.py passes a string (apiserver_url). Error generated is `str has no method base_url`. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7068
https://github.com/hail-is/hail/pull/7069:954,Integrability,depend,depending,954,"This PR:. - Introduces `NDArrayReindex`, which allows the use of the `transpose` on hail `NDArrayExpressions`. ; - Introduces `NDArrayMap2`, which allows operations like elementwise +, -, /, *, etc. This has full broadcasting support to match numpys. ; - Fixes `NDArrayShape`, which was not properly implemented. The only reason `NDArrayShape` seemed to work was because calling `NDArrayShape` directly on a `MakeNDArray` or on a `NDArrayMap1` was always simplified away to just return the tuple. ; - Removed the Simplify rule that transformed `NDArrayShape(MakeNDArray(..., shape))` to `shape`, since that was resulting in a type change, as `NDArrayShape` always returns a required tuple and `shape` was not a required tuple.; - Changes `ndarrays_eq` in the tests to just go through `eval` instead of a write then read, since write and read don't exist yet. . Things I'm not happy with:; - The rule for emitting `NDArrayReindex` is written twice, since depending on the situation one or the other is computationally cheaper. Seems like there should be a version of `NDArrayEmitter` that allows us to use an already existing array as the data. . Might add a few more tests, but code is here for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7069
https://github.com/hail-is/hail/pull/7069:759,Testability,test,tests,759,"This PR:. - Introduces `NDArrayReindex`, which allows the use of the `transpose` on hail `NDArrayExpressions`. ; - Introduces `NDArrayMap2`, which allows operations like elementwise +, -, /, *, etc. This has full broadcasting support to match numpys. ; - Fixes `NDArrayShape`, which was not properly implemented. The only reason `NDArrayShape` seemed to work was because calling `NDArrayShape` directly on a `MakeNDArray` or on a `NDArrayMap1` was always simplified away to just return the tuple. ; - Removed the Simplify rule that transformed `NDArrayShape(MakeNDArray(..., shape))` to `shape`, since that was resulting in a type change, as `NDArrayShape` always returns a required tuple and `shape` was not a required tuple.; - Changes `ndarrays_eq` in the tests to just go through `eval` instead of a write then read, since write and read don't exist yet. . Things I'm not happy with:; - The rule for emitting `NDArrayReindex` is written twice, since depending on the situation one or the other is computationally cheaper. Seems like there should be a version of `NDArrayEmitter` that allows us to use an already existing array as the data. . Might add a few more tests, but code is here for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7069
https://github.com/hail-is/hail/pull/7069:1167,Testability,test,tests,1167,"This PR:. - Introduces `NDArrayReindex`, which allows the use of the `transpose` on hail `NDArrayExpressions`. ; - Introduces `NDArrayMap2`, which allows operations like elementwise +, -, /, *, etc. This has full broadcasting support to match numpys. ; - Fixes `NDArrayShape`, which was not properly implemented. The only reason `NDArrayShape` seemed to work was because calling `NDArrayShape` directly on a `MakeNDArray` or on a `NDArrayMap1` was always simplified away to just return the tuple. ; - Removed the Simplify rule that transformed `NDArrayShape(MakeNDArray(..., shape))` to `shape`, since that was resulting in a type change, as `NDArrayShape` always returns a required tuple and `shape` was not a required tuple.; - Changes `ndarrays_eq` in the tests to just go through `eval` instead of a write then read, since write and read don't exist yet. . Things I'm not happy with:; - The rule for emitting `NDArrayReindex` is written twice, since depending on the situation one or the other is computationally cheaper. Seems like there should be a version of `NDArrayEmitter` that allows us to use an already existing array as the data. . Might add a few more tests, but code is here for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7069
https://github.com/hail-is/hail/pull/7069:455,Usability,simpl,simplified,455,"This PR:. - Introduces `NDArrayReindex`, which allows the use of the `transpose` on hail `NDArrayExpressions`. ; - Introduces `NDArrayMap2`, which allows operations like elementwise +, -, /, *, etc. This has full broadcasting support to match numpys. ; - Fixes `NDArrayShape`, which was not properly implemented. The only reason `NDArrayShape` seemed to work was because calling `NDArrayShape` directly on a `MakeNDArray` or on a `NDArrayMap1` was always simplified away to just return the tuple. ; - Removed the Simplify rule that transformed `NDArrayShape(MakeNDArray(..., shape))` to `shape`, since that was resulting in a type change, as `NDArrayShape` always returns a required tuple and `shape` was not a required tuple.; - Changes `ndarrays_eq` in the tests to just go through `eval` instead of a write then read, since write and read don't exist yet. . Things I'm not happy with:; - The rule for emitting `NDArrayReindex` is written twice, since depending on the situation one or the other is computationally cheaper. Seems like there should be a version of `NDArrayEmitter` that allows us to use an already existing array as the data. . Might add a few more tests, but code is here for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7069
https://github.com/hail-is/hail/pull/7069:513,Usability,Simpl,Simplify,513,"This PR:. - Introduces `NDArrayReindex`, which allows the use of the `transpose` on hail `NDArrayExpressions`. ; - Introduces `NDArrayMap2`, which allows operations like elementwise +, -, /, *, etc. This has full broadcasting support to match numpys. ; - Fixes `NDArrayShape`, which was not properly implemented. The only reason `NDArrayShape` seemed to work was because calling `NDArrayShape` directly on a `MakeNDArray` or on a `NDArrayMap1` was always simplified away to just return the tuple. ; - Removed the Simplify rule that transformed `NDArrayShape(MakeNDArray(..., shape))` to `shape`, since that was resulting in a type change, as `NDArrayShape` always returns a required tuple and `shape` was not a required tuple.; - Changes `ndarrays_eq` in the tests to just go through `eval` instead of a write then read, since write and read don't exist yet. . Things I'm not happy with:; - The rule for emitting `NDArrayReindex` is written twice, since depending on the situation one or the other is computationally cheaper. Seems like there should be a version of `NDArrayEmitter` that allows us to use an already existing array as the data. . Might add a few more tests, but code is here for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7069
https://github.com/hail-is/hail/pull/7071:66,Availability,error,error,66,Makes it possible to proceed past the duplicate-multiallelic-loci error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7071
https://github.com/hail-is/hail/pull/7073:158,Modifiability,rewrite,rewrite,158,"At some point we started optimizing the MakeStruct to a SelectFields,; which is great, but not if it breaks important optimizations like the; avoid-a-shuffle rewrite rule!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7073
https://github.com/hail-is/hail/pull/7073:25,Performance,optimiz,optimizing,25,"At some point we started optimizing the MakeStruct to a SelectFields,; which is great, but not if it breaks important optimizations like the; avoid-a-shuffle rewrite rule!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7073
https://github.com/hail-is/hail/pull/7073:118,Performance,optimiz,optimizations,118,"At some point we started optimizing the MakeStruct to a SelectFields,; which is great, but not if it breaks important optimizations like the; avoid-a-shuffle rewrite rule!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7073
https://github.com/hail-is/hail/pull/7073:142,Safety,avoid,avoid-a-shuffle,142,"At some point we started optimizing the MakeStruct to a SelectFields,; which is great, but not if it breaks important optimizations like the; avoid-a-shuffle rewrite rule!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7073
https://github.com/hail-is/hail/pull/7078:30,Testability,test,tests,30,"Currently, the notebook scale tests are broken by this. Aiohttp [has a bug in 3.5](https://github.com/aio-libs/aiohttp/issues/3700) that incorrectly handles cookies in 302 redirects. The master commit was cherry-picked into 3.6.0.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7078
https://github.com/hail-is/hail/issues/7080:17,Deployability,install,installed,17,"When Hail is pip-installed, you cannot use it with an already constructed SparkContext, unless said SparkContext's class path includes the hail JAR file. It is somewhat annoying to find the hail JAR location and add that to the class path. The primary reason users want to pass an already constructed SparkContext is to specify some configuration parameters. `hl.init` should take a `conf` as either a `SparkConf` or a `dict`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080
https://github.com/hail-is/hail/issues/7080:333,Deployability,configurat,configuration,333,"When Hail is pip-installed, you cannot use it with an already constructed SparkContext, unless said SparkContext's class path includes the hail JAR file. It is somewhat annoying to find the hail JAR location and add that to the class path. The primary reason users want to pass an already constructed SparkContext is to specify some configuration parameters. `hl.init` should take a `conf` as either a `SparkConf` or a `dict`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080
https://github.com/hail-is/hail/issues/7080:333,Modifiability,config,configuration,333,"When Hail is pip-installed, you cannot use it with an already constructed SparkContext, unless said SparkContext's class path includes the hail JAR file. It is somewhat annoying to find the hail JAR location and add that to the class path. The primary reason users want to pass an already constructed SparkContext is to specify some configuration parameters. `hl.init` should take a `conf` as either a `SparkConf` or a `dict`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080
https://github.com/hail-is/hail/pull/7092:65,Modifiability,rewrite,rewrite,65,"This consisted almost entirely of `region` -> `Region`. I had to rewrite some things to replace `appendBinary` and `appendString`, but there shouldn't be any other substantive changes in here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7092
https://github.com/hail-is/hail/pull/7096:85,Testability,test,test,85,"Following up on the last PR, that didn't actually solve the general problem. See the test at the bottom for an example of an IR that wasn't matching the rule.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7096
https://github.com/hail-is/hail/pull/7097:581,Deployability,deploy,deployed,581,"Pushing pixels around. Summary of changes:; - make links blue, with underline :hover, :active; - added data-table class for striped, hover-highlighting tables, dark background headers, use all over; - put open_in_new on links that open new tabs; - made the search bars a bit wider (40%); - make Github links all be <title> #<number>, where number is a lighter gray; - format attributes of repos and batches in CI tighter; - added batch labels on PR page; - made artifacts link clickable (long overdue). @tpoterba I think this will fix most of your complaints from earlier today. I deployed just CI by hand to verify it works, looks good. I'm going to leave off styling for a bit after this and work on workshops in notebook2 and support many jobs in batch (paging, search, etc.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7097
https://github.com/hail-is/hail/pull/7101:46,Deployability,deploy,deploying,46,I need to make sure this all works when ci is deploying in a test namespace.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7101
https://github.com/hail-is/hail/pull/7101:61,Testability,test,test,61,I need to make sure this all works when ci is deploying in a test namespace.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7101
https://github.com/hail-is/hail/pull/7105:344,Performance,perform,performance,344,"* add --metric argument to choose between best, median, ...; * add --min-time option to set minimum time for benchmark inclusion.; This lets us write benchmarks for things we expect to be fast, but where there's enough variance we don't really think the exact number is very meaningful in the current benchmarking system. They will still catch performance regressions, though!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7105
https://github.com/hail-is/hail/pull/7105:109,Testability,benchmark,benchmark,109,"* add --metric argument to choose between best, median, ...; * add --min-time option to set minimum time for benchmark inclusion.; This lets us write benchmarks for things we expect to be fast, but where there's enough variance we don't really think the exact number is very meaningful in the current benchmarking system. They will still catch performance regressions, though!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7105
https://github.com/hail-is/hail/pull/7105:150,Testability,benchmark,benchmarks,150,"* add --metric argument to choose between best, median, ...; * add --min-time option to set minimum time for benchmark inclusion.; This lets us write benchmarks for things we expect to be fast, but where there's enough variance we don't really think the exact number is very meaningful in the current benchmarking system. They will still catch performance regressions, though!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7105
https://github.com/hail-is/hail/pull/7105:301,Testability,benchmark,benchmarking,301,"* add --metric argument to choose between best, median, ...; * add --min-time option to set minimum time for benchmark inclusion.; This lets us write benchmarks for things we expect to be fast, but where there's enough variance we don't really think the exact number is very meaningful in the current benchmarking system. They will still catch performance regressions, though!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7105
https://github.com/hail-is/hail/issues/7107:4,Energy Efficiency,reduce,reducer,4,The reducer should be parameterized as a keyword-only arg. `filter_missing` should become a keyword-only arg.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7107
https://github.com/hail-is/hail/issues/7107:22,Modifiability,parameteriz,parameterized,22,The reducer should be parameterized as a keyword-only arg. `filter_missing` should become a keyword-only arg.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7107
https://github.com/hail-is/hail/pull/7108:26,Energy Efficiency,allocate,allocates,26,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:85,Energy Efficiency,allocate,allocated,85,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:338,Performance,perform,performance,338,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:124,Safety,avoid,avoid,124,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:516,Safety,unsafe,unsafe,516,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:765,Testability,benchmark,benchmark,765,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:804,Testability,benchmark,benchmark,804,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:1214,Testability,benchmark,benchmark,1214,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:1280,Testability,benchmark,benchmarks,1280,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7108:1163,Usability,Simpl,Simple,1163,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108
https://github.com/hail-is/hail/pull/7110:482,Availability,down,down,482,"The main goal here was to flatten out the aggregator states in the tuple of aggregator states, so that we could e.g. inline the value of a prevnonnull aggregator instead of storing a pointer to the state. The big change that I made was in creating a `TupleAggregatorState` that knows its own offset so that when we initialize a state, we can initialize the value offset directly from the value of the container. (The previous StateContainer got renamed `StateTuple` and was slimmed down accordingly.) I think I eventually want `TupleAggregatorState` to implement the `AggregatorState` interface; I haven't pushed it there yet because I haven't needed to, but I think it would fit a lot better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7110
https://github.com/hail-is/hail/pull/7110:585,Integrability,interface,interface,585,"The main goal here was to flatten out the aggregator states in the tuple of aggregator states, so that we could e.g. inline the value of a prevnonnull aggregator instead of storing a pointer to the state. The big change that I made was in creating a `TupleAggregatorState` that knows its own offset so that when we initialize a state, we can initialize the value offset directly from the value of the container. (The previous StateContainer got renamed `StateTuple` and was slimmed down accordingly.) I think I eventually want `TupleAggregatorState` to implement the `AggregatorState` interface; I haven't pushed it there yet because I haven't needed to, but I think it would fit a lot better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7110
https://github.com/hail-is/hail/pull/7112:839,Availability,reliab,reliable,839,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1180,Availability,error,error,1180,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1225,Availability,avail,available,1225,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1283,Deployability,integrat,integrated,1283,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1588,Deployability,update,update,1588,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:705,Energy Efficiency,monitor,monitor,705,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:251,Integrability,message,message,251,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:600,Integrability,depend,dependencies,600,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1283,Integrability,integrat,integrated,1283,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1303,Integrability,message,message,1303,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1633,Integrability,message,message,1633,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:854,Modifiability,refactor,refactored,854,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:985,Performance,load,load,985,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1005,Performance,load,load,1005,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:990,Testability,test,test,990,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1010,Testability,test,test,1010,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:1171,Testability,test,test,1171,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:185,Usability,simpl,simple,185,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:822,Usability,simpl,simpler,822,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/pull/7112:943,Usability,simpl,simpler,943,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112
https://github.com/hail-is/hail/issues/7117:202,Deployability,pipeline,pipeline,202,The remove_tmpdir job fails with e.g.:. ```; Activated service account credentials for: [my-service-account@hail-vdc.iam.gserviceaccount.com]; CommandException: No URLs matched: gs://my-service-account/pipeline/pipeline-947753d9ef82; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7117
https://github.com/hail-is/hail/issues/7117:211,Deployability,pipeline,pipeline-,211,The remove_tmpdir job fails with e.g.:. ```; Activated service account credentials for: [my-service-account@hail-vdc.iam.gserviceaccount.com]; CommandException: No URLs matched: gs://my-service-account/pipeline/pipeline-947753d9ef82; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7117
https://github.com/hail-is/hail/pull/7118:30,Deployability,install,install,30,"* don't require HAIL_WHEEL to install; * version doesn't actually mean version benchmarked, so that's now a param; * Fix pip in install target",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7118
https://github.com/hail-is/hail/pull/7118:128,Deployability,install,install,128,"* don't require HAIL_WHEEL to install; * version doesn't actually mean version benchmarked, so that's now a param; * Fix pip in install target",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7118
https://github.com/hail-is/hail/pull/7118:79,Testability,benchmark,benchmarked,79,"* don't require HAIL_WHEEL to install; * version doesn't actually mean version benchmarked, so that's now a param; * Fix pip in install target",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7118
https://github.com/hail-is/hail/pull/7119:132,Testability,test,testing,132,"I left a parameter on the StagedBTree implementation to be able to adjust the number of elements a node can hold, but we were never testing this (we currently never use anything except the default). I'm modifying the BTree test to run on trees of size 2, 3, 5, 6, and 22; 2 is the default, and 22 is the check for when the max number of possible elements doesn't exceed half the size of the node. The others are mainly just to check that both even and odd numbers work correctly for a size that we'd conceivably want to use.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7119
https://github.com/hail-is/hail/pull/7119:223,Testability,test,test,223,"I left a parameter on the StagedBTree implementation to be able to adjust the number of elements a node can hold, but we were never testing this (we currently never use anything except the default). I'm modifying the BTree test to run on trees of size 2, 3, 5, 6, and 22; 2 is the default, and 22 is the check for when the max number of possible elements doesn't exceed half the size of the node. The others are mainly just to check that both even and odd numbers work correctly for a size that we'd conceivably want to use.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7119
https://github.com/hail-is/hail/pull/7120:1243,Availability,error,errors,1243,"This PR brings the `Streamify` pass back to the JVM emitter. Streamify is useful because it make explicit *in the structure of the IR*, which nodes are stream producers and which nodes are stream consumers. In particular, the following nodes **produce** streams:; ```; ReadPartition; MakeStream; StreamRange; ToStream. ArrayMap; ArrayFilter; ArrayFlatMap; ArrayScan; ArrayAggScan; ArrayLeftJoinDistinct. Let /* sometimes */; ```; And the following nodes **consume** streams (`#` indicates which arguments are streams):; ```; ToArray(#); ToDict(#); ToSet(#); GroupByKey(#) ; ArraySort(#, -, -, -); ArrayFold(#, -, -, -, -); ArrayFold2(#, -, -, -, -); ArrayFor(#, -, -); ArrayAgg(#, -, -); CollectDistributedArray(#, -, -, -, -). ArrayMap(#, -, -); ArrayFilter(#, -, -); ArrayFlatMap(#, -, #); ArrayScan(#, -, -, -, -); ArrayAggScan(#, -, -); ArrayLeftJoinDistinct(#, #, -, -, -, -). Let(-, -, #) /* sometimes */; ```. Thus, `Emit` may make better assumptions about the IR it is walking over. `emitArrayIterator` only deals with stream producers, and all stream consumers must call `emitArrayIterator` on their stream arguments. Additionally:; - `Streamify` was fixed to materialize less arrays than it used to (there were also bugs that caused errors on certain IR; see tests); - The producer `ToStream` may assume that its argument is a container.; - The IR nodes `ArrayRange` and `MakeArray` do not make their way to `Emit`.; - I'm not sure what the purpose of `{T,P}Streamable` is, I think all of its functionality is already covered by `{T,P}Iterable`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7120
https://github.com/hail-is/hail/pull/7120:1269,Testability,test,tests,1269,"This PR brings the `Streamify` pass back to the JVM emitter. Streamify is useful because it make explicit *in the structure of the IR*, which nodes are stream producers and which nodes are stream consumers. In particular, the following nodes **produce** streams:; ```; ReadPartition; MakeStream; StreamRange; ToStream. ArrayMap; ArrayFilter; ArrayFlatMap; ArrayScan; ArrayAggScan; ArrayLeftJoinDistinct. Let /* sometimes */; ```; And the following nodes **consume** streams (`#` indicates which arguments are streams):; ```; ToArray(#); ToDict(#); ToSet(#); GroupByKey(#) ; ArraySort(#, -, -, -); ArrayFold(#, -, -, -, -); ArrayFold2(#, -, -, -, -); ArrayFor(#, -, -); ArrayAgg(#, -, -); CollectDistributedArray(#, -, -, -, -). ArrayMap(#, -, -); ArrayFilter(#, -, -); ArrayFlatMap(#, -, #); ArrayScan(#, -, -, -, -); ArrayAggScan(#, -, -); ArrayLeftJoinDistinct(#, #, -, -, -, -). Let(-, -, #) /* sometimes */; ```. Thus, `Emit` may make better assumptions about the IR it is walking over. `emitArrayIterator` only deals with stream producers, and all stream consumers must call `emitArrayIterator` on their stream arguments. Additionally:; - `Streamify` was fixed to materialize less arrays than it used to (there were also bugs that caused errors on certain IR; see tests); - The producer `ToStream` may assume that its argument is a container.; - The IR nodes `ArrayRange` and `MakeArray` do not make their way to `Emit`.; - I'm not sure what the purpose of `{T,P}Streamable` is, I think all of its functionality is already covered by `{T,P}Iterable`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7120
https://github.com/hail-is/hail/issues/7122:199,Integrability,wrap,wrap,199,"Currently, we only offer `hl._ndarray`, which takes a python list. Based on the contents of that list, and inner lists, it determines the dimensions. Problematically, the IR that gets generated will wrap every single number in the NDArray in an IR Node. . To resolve this, we should at least support:. - [x] Numpy NDArray literals; - [x] Make an ndarray from a hail array. Right now needs to be a python iterable or numpy array.; - [x] Natural generator functions, like `zeros`, `ones`, `full`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7122
https://github.com/hail-is/hail/pull/7125:54,Modifiability,config,config,54,"Now we try, in order:; ```; $XDG_CONFIG_HOME/hail; ~/.config/hail; ~/.hail; ```. The [XDG Base Directory Specification] is a freedesktop spec inteded to; define where applications should look for files they need to run. [XDG Base Directory Specification]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. If ~/.hail already exists on your system, this should not break you as; long as $XDG_CONFIG_HOME/hail or ~/.config/hail also do not exist. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125
https://github.com/hail-is/hail/pull/7125:442,Modifiability,config,config,442,"Now we try, in order:; ```; $XDG_CONFIG_HOME/hail; ~/.config/hail; ~/.hail; ```. The [XDG Base Directory Specification] is a freedesktop spec inteded to; define where applications should look for files they need to run. [XDG Base Directory Specification]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. If ~/.hail already exists on your system, this should not break you as; long as $XDG_CONFIG_HOME/hail or ~/.config/hail also do not exist. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125
https://github.com/hail-is/hail/pull/7127:264,Testability,benchmark,benchmark,264,"- each node in the BTree holds 6 elements; - the agg containers are stored inline in each tree element instead of by reference; - unrolled the recursive calls to `get` into a while loop (I'm not actually super sure why this helps, but it seems to have sped up the benchmark.). Current master vs this PR:; ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_counter 86.6% 9.107 7.887; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7127
https://github.com/hail-is/hail/pull/7128:298,Availability,redundant,redundant,298,"Turns out we didn't support `--dry-run` on `dataproc connect`. I'm not totally satisfied with this, as the command that gets printed out isn't runnable, because it will lack quotes around the `--ssh-flag=-D 1000` part. I tried adding the quotes into the command, thinking it would work but just be redundant, but I couldn't get it to work. I suppose I could have the printing logic go through the list and replace that bit with a quoted version. Let me know what you think.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7128
https://github.com/hail-is/hail/pull/7128:298,Safety,redund,redundant,298,"Turns out we didn't support `--dry-run` on `dataproc connect`. I'm not totally satisfied with this, as the command that gets printed out isn't runnable, because it will lack quotes around the `--ssh-flag=-D 1000` part. I tried adding the quotes into the command, thinking it would work but just be redundant, but I couldn't get it to work. I suppose I could have the printing logic go through the list and replace that bit with a quoted version. Let me know what you think.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7128
https://github.com/hail-is/hail/pull/7128:376,Testability,log,logic,376,"Turns out we didn't support `--dry-run` on `dataproc connect`. I'm not totally satisfied with this, as the command that gets printed out isn't runnable, because it will lack quotes around the `--ssh-flag=-D 1000` part. I tried adding the quotes into the command, thinking it would work but just be redundant, but I couldn't get it to work. I suppose I could have the printing logic go through the list and replace that bit with a quoted version. Let me know what you think.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7128
https://github.com/hail-is/hail/issues/7132:158,Availability,avail,available,158,"May not be implemented in aiohttp. Flag prevents cookie from being carried with cross-origin request. Lax allows cookie to be carried with GET requests. Only available in relatively recent browsers. With CSRF protection in place not strictly necessary, but an added layer of protection, for low cost",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7132
https://github.com/hail-is/hail/pull/7133:5,Deployability,install,install-wheel,5,The `install-wheel` Make target was renamed to `install` in hail-is/hail@346fb67aa5f943c42981025823876272ee222183 but the documentation for building Hail still refers to the old name.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7133
https://github.com/hail-is/hail/pull/7133:48,Deployability,install,install,48,The `install-wheel` Make target was renamed to `install` in hail-is/hail@346fb67aa5f943c42981025823876272ee222183 but the documentation for building Hail still refers to the old name.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7133
https://github.com/hail-is/hail/issues/7135:556,Availability,error,error,556,"`ArrayScan` is implemented in such a way that a scan on an empty array will read some uninitialized memory:. ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.23-7f06f94534d5; >>> hl.eval(hl.empty_array(hl.tint32).scan(lambda x,y: x + y, 0)); [0]; >>> hl.eval(hl.array([1, 2, 3]).scan(lambda x,y: x + y, 0)); [0, 1, 3, 6]; >>> hl.eval(hl.empty_array(hl.tint32).scan(lambda x,y: x + y, 0)); [643629112]; >>> hl.eval(hl.empty_array(hl.tarray(hl.tint32)).scan(lambda x,y: y, hl.empty_array(hl.tint32))); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ef85ded, pid=49261, tid=0x0000000000009903; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x585ded]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid49261.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. This is because `ArrayScan` claims to have `len+1` elements, where `len` is the length of the inner stream. However, it will only call the consumer continuation during the `addElements` loop of the inner stream. Thus, if the inner stream is empty, the consumer continuation is never called. So the resulting effect is that we return an initialized array with length 1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7135
https://github.com/hail-is/hail/issues/7135:1077,Availability,error,error,1077,"`ArrayScan` is implemented in such a way that a scan on an empty array will read some uninitialized memory:. ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.23-7f06f94534d5; >>> hl.eval(hl.empty_array(hl.tint32).scan(lambda x,y: x + y, 0)); [0]; >>> hl.eval(hl.array([1, 2, 3]).scan(lambda x,y: x + y, 0)); [0, 1, 3, 6]; >>> hl.eval(hl.empty_array(hl.tint32).scan(lambda x,y: x + y, 0)); [643629112]; >>> hl.eval(hl.empty_array(hl.tarray(hl.tint32)).scan(lambda x,y: y, hl.empty_array(hl.tint32))); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ef85ded, pid=49261, tid=0x0000000000009903; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x585ded]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid49261.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. This is because `ArrayScan` claims to have `len+1` elements, where `len` is the length of the inner stream. However, it will only call the consumer continuation during the `addElements` loop of the inner stream. Thus, if the inner stream is empty, the consumer continuation is never called. So the resulting effect is that we return an initialized array with length 1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7135
https://github.com/hail-is/hail/issues/7135:571,Safety,detect,detected,571,"`ArrayScan` is implemented in such a way that a scan on an empty array will read some uninitialized memory:. ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.23-7f06f94534d5; >>> hl.eval(hl.empty_array(hl.tint32).scan(lambda x,y: x + y, 0)); [0]; >>> hl.eval(hl.array([1, 2, 3]).scan(lambda x,y: x + y, 0)); [0, 1, 3, 6]; >>> hl.eval(hl.empty_array(hl.tint32).scan(lambda x,y: x + y, 0)); [643629112]; >>> hl.eval(hl.empty_array(hl.tarray(hl.tint32)).scan(lambda x,y: y, hl.empty_array(hl.tint32))); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ef85ded, pid=49261, tid=0x0000000000009903; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x585ded]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid49261.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. This is because `ArrayScan` claims to have `len+1` elements, where `len` is the length of the inner stream. However, it will only call the consumer continuation during the `addElements` loop of the inner stream. Thus, if the inner stream is empty, the consumer continuation is never called. So the resulting effect is that we return an initialized array with length 1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7135
https://github.com/hail-is/hail/issues/7135:1191,Testability,log,log,1191,"`ArrayScan` is implemented in such a way that a scan on an empty array will read some uninitialized memory:. ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.23-7f06f94534d5; >>> hl.eval(hl.empty_array(hl.tint32).scan(lambda x,y: x + y, 0)); [0]; >>> hl.eval(hl.array([1, 2, 3]).scan(lambda x,y: x + y, 0)); [0, 1, 3, 6]; >>> hl.eval(hl.empty_array(hl.tint32).scan(lambda x,y: x + y, 0)); [643629112]; >>> hl.eval(hl.empty_array(hl.tarray(hl.tint32)).scan(lambda x,y: y, hl.empty_array(hl.tint32))); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010ef85ded, pid=49261, tid=0x0000000000009903; #; # JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x585ded]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mturner/Documents/hail/hail/python/hs_err_pid49261.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. This is because `ArrayScan` claims to have `len+1` elements, where `len` is the length of the inner stream. However, it will only call the consumer continuation during the `addElements` loop of the inner stream. Thus, if the inner stream is empty, the consumer continuation is never called. So the resulting effect is that we return an initialized array with length 1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7135
https://github.com/hail-is/hail/issues/7136:93,Deployability,install,installation,93,"I'm running hail version 0.2.23 and python 3.7.4 in a conda environment as described in the [installation instructions for mac os](https://hail.is/docs/0.2/getting_started.html#installing-hail-on-mac-os-x-or-gnu-linux-with-pip). The tutorials worked fine when I ran them from the command line using ipython, but when I launched a jupyter notebook, I kept getting a `ModuleNotFoundError` when running `import hail as hl`. I solved the problem by first running `conda install jupyter` in my virtual environment. Am i the only one who had this issue? If not, please consider adding this extra step to the installation instructions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136
https://github.com/hail-is/hail/issues/7136:177,Deployability,install,installing-hail-on-mac-os-x-or-gnu-linux-with-pip,177,"I'm running hail version 0.2.23 and python 3.7.4 in a conda environment as described in the [installation instructions for mac os](https://hail.is/docs/0.2/getting_started.html#installing-hail-on-mac-os-x-or-gnu-linux-with-pip). The tutorials worked fine when I ran them from the command line using ipython, but when I launched a jupyter notebook, I kept getting a `ModuleNotFoundError` when running `import hail as hl`. I solved the problem by first running `conda install jupyter` in my virtual environment. Am i the only one who had this issue? If not, please consider adding this extra step to the installation instructions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136
https://github.com/hail-is/hail/issues/7136:466,Deployability,install,install,466,"I'm running hail version 0.2.23 and python 3.7.4 in a conda environment as described in the [installation instructions for mac os](https://hail.is/docs/0.2/getting_started.html#installing-hail-on-mac-os-x-or-gnu-linux-with-pip). The tutorials worked fine when I ran them from the command line using ipython, but when I launched a jupyter notebook, I kept getting a `ModuleNotFoundError` when running `import hail as hl`. I solved the problem by first running `conda install jupyter` in my virtual environment. Am i the only one who had this issue? If not, please consider adding this extra step to the installation instructions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136
https://github.com/hail-is/hail/issues/7136:602,Deployability,install,installation,602,"I'm running hail version 0.2.23 and python 3.7.4 in a conda environment as described in the [installation instructions for mac os](https://hail.is/docs/0.2/getting_started.html#installing-hail-on-mac-os-x-or-gnu-linux-with-pip). The tutorials worked fine when I ran them from the command line using ipython, but when I launched a jupyter notebook, I kept getting a `ModuleNotFoundError` when running `import hail as hl`. I solved the problem by first running `conda install jupyter` in my virtual environment. Am i the only one who had this issue? If not, please consider adding this extra step to the installation instructions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136
https://github.com/hail-is/hail/pull/7138:63,Deployability,pipeline,pipelines,63,Eases memory pressure during large `annotate_cols` aggregation pipelines.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7138
https://github.com/hail-is/hail/pull/7139:22,Performance,optimiz,optimization,22,I want to use this in optimization.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7139
https://github.com/hail-is/hail/pull/7143:28,Performance,perform,performance,28,No detectable difference in performance:. ```; $ hail-bench compare /tmp/before.json /tmp/after2.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; matrix_table_decode_and_count 101.5% 4.216 4.278; ----------------------; Geometric mean: 101.5%; Simple mean: 101.5%; Median: 101.5%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7143
https://github.com/hail-is/hail/pull/7143:3,Safety,detect,detectable,3,No detectable difference in performance:. ```; $ hail-bench compare /tmp/before.json /tmp/after2.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; matrix_table_decode_and_count 101.5% 4.216 4.278; ----------------------; Geometric mean: 101.5%; Simple mean: 101.5%; Median: 101.5%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7143
https://github.com/hail-is/hail/pull/7143:253,Usability,Simpl,Simple,253,No detectable difference in performance:. ```; $ hail-bench compare /tmp/before.json /tmp/after2.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; matrix_table_decode_and_count 101.5% 4.216 4.278; ----------------------; Geometric mean: 101.5%; Simple mean: 101.5%; Median: 101.5%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7143
https://github.com/hail-is/hail/pull/7145:554,Availability,error,errors,554,"Summary of changes:; - add a / index page to the workshop service with some chipper content. FYI @tpoterba, feel free to change if you don't like.; - make csrf token session-based; - add common render_template function to web_common that handles csrf and jinja2 rendering. This is necessary because the header has a logout button (potentially) so every page needs make sure the csrf is set.; - added a toplevel make check target; - fixed a forwarding bug: Host: $updated_host needs to get set when proxying to the notebook itself or you get cross-origin errors in the notebook. Things I have left to do:; - make the notebook non-clickable when it isn't ready; - write up a UI testing playbook; - link to notebook/workshop-admin somewhere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145
https://github.com/hail-is/hail/pull/7145:316,Testability,log,logout,316,"Summary of changes:; - add a / index page to the workshop service with some chipper content. FYI @tpoterba, feel free to change if you don't like.; - make csrf token session-based; - add common render_template function to web_common that handles csrf and jinja2 rendering. This is necessary because the header has a logout button (potentially) so every page needs make sure the csrf is set.; - added a toplevel make check target; - fixed a forwarding bug: Host: $updated_host needs to get set when proxying to the notebook itself or you get cross-origin errors in the notebook. Things I have left to do:; - make the notebook non-clickable when it isn't ready; - write up a UI testing playbook; - link to notebook/workshop-admin somewhere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145
https://github.com/hail-is/hail/pull/7145:676,Testability,test,testing,676,"Summary of changes:; - add a / index page to the workshop service with some chipper content. FYI @tpoterba, feel free to change if you don't like.; - make csrf token session-based; - add common render_template function to web_common that handles csrf and jinja2 rendering. This is necessary because the header has a logout button (potentially) so every page needs make sure the csrf is set.; - added a toplevel make check target; - fixed a forwarding bug: Host: $updated_host needs to get set when proxying to the notebook itself or you get cross-origin errors in the notebook. Things I have left to do:; - make the notebook non-clickable when it isn't ready; - write up a UI testing playbook; - link to notebook/workshop-admin somewhere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145
https://github.com/hail-is/hail/pull/7146:552,Availability,error,error,552,"`BlockMatrix.filterCols` and `BlockMatrix.filterRows` has a bug where the smaller blocks located on the edge of the matrix are not always correctly handled when they're of a smaller size than the rest of the blocks. The source of this problem was the fact that `BlockMatrix.scala` line 1515 below, which was calling `blockDims(split.index)`. This was a problem because `split.index` is a partition index, not a block index. The fix was to convert it to a block index and then call `blockDims`. The rest of this PR is some white space fixes, a spelling error, and an additional test case for BlockMatrix filtering that would fail in current master because of this bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7146
https://github.com/hail-is/hail/pull/7146:577,Testability,test,test,577,"`BlockMatrix.filterCols` and `BlockMatrix.filterRows` has a bug where the smaller blocks located on the edge of the matrix are not always correctly handled when they're of a smaller size than the rest of the blocks. The source of this problem was the fact that `BlockMatrix.scala` line 1515 below, which was calling `blockDims(split.index)`. This was a problem because `split.index` is a partition index, not a block index. The fix was to convert it to a block index and then call `blockDims`. The rest of this PR is some white space fixes, a spelling error, and an additional test case for BlockMatrix filtering that would fail in current master because of this bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7146
https://github.com/hail-is/hail/issues/7147:22,Energy Efficiency,Power,Powered,22,"The links on the Hail-Powered Science page don't seem to be live on the website:. https://hail.is/references.html. But they do display correctly in markdown, however, on GitHub:. https://github.com/hail-is/hail/blob/master/hail/www/references.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7147
https://github.com/hail-is/hail/pull/7154:318,Testability,test,test,318,"FYI @tpoterba you can see this in action in my namespace (for the moment). @akotlar and I agree this is not ideal, but it is done and better than nothing. I will continue to play with alternatives and make a PR when I have something better. Remaining tasks:; - make notebook like non-clickable until ready; - write up test playbook. @akotlar let me know if/when you get tired of these, I'll send them to Dan or Jackie. Probably good for them to engage with this stuff, too.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7154
https://github.com/hail-is/hail/pull/7155:616,Deployability,deploy,deployed,616,"Summary of changes:; - change notebook states to Scheduling, Initializing, Ready (Running was too suggestion of being ready); - remove ""a_notebook"" notebook name, replace with ""Creating Notebook..."" or ""Open Notebook[open_in_new]"" link; - change nb-state-container styling, blue/underline link on top, but the whole container still acts as a link; - made material icons bold. I'm not sure if I like this, it is bordering on cartoonish.; - make checkmark green to match other success/ok coloring using green (e.g. messages); - un-bold pod name, which is really only interested for us (maybe we should remove?). It is deployed in my namespace if you want to play with it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7155
https://github.com/hail-is/hail/pull/7155:49,Energy Efficiency,Schedul,Scheduling,49,"Summary of changes:; - change notebook states to Scheduling, Initializing, Ready (Running was too suggestion of being ready); - remove ""a_notebook"" notebook name, replace with ""Creating Notebook..."" or ""Open Notebook[open_in_new]"" link; - change nb-state-container styling, blue/underline link on top, but the whole container still acts as a link; - made material icons bold. I'm not sure if I like this, it is bordering on cartoonish.; - make checkmark green to match other success/ok coloring using green (e.g. messages); - un-bold pod name, which is really only interested for us (maybe we should remove?). It is deployed in my namespace if you want to play with it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7155
https://github.com/hail-is/hail/pull/7155:454,Energy Efficiency,green,green,454,"Summary of changes:; - change notebook states to Scheduling, Initializing, Ready (Running was too suggestion of being ready); - remove ""a_notebook"" notebook name, replace with ""Creating Notebook..."" or ""Open Notebook[open_in_new]"" link; - change nb-state-container styling, blue/underline link on top, but the whole container still acts as a link; - made material icons bold. I'm not sure if I like this, it is bordering on cartoonish.; - make checkmark green to match other success/ok coloring using green (e.g. messages); - un-bold pod name, which is really only interested for us (maybe we should remove?). It is deployed in my namespace if you want to play with it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7155
https://github.com/hail-is/hail/pull/7155:501,Energy Efficiency,green,green,501,"Summary of changes:; - change notebook states to Scheduling, Initializing, Ready (Running was too suggestion of being ready); - remove ""a_notebook"" notebook name, replace with ""Creating Notebook..."" or ""Open Notebook[open_in_new]"" link; - change nb-state-container styling, blue/underline link on top, but the whole container still acts as a link; - made material icons bold. I'm not sure if I like this, it is bordering on cartoonish.; - make checkmark green to match other success/ok coloring using green (e.g. messages); - un-bold pod name, which is really only interested for us (maybe we should remove?). It is deployed in my namespace if you want to play with it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7155
https://github.com/hail-is/hail/pull/7155:513,Integrability,message,messages,513,"Summary of changes:; - change notebook states to Scheduling, Initializing, Ready (Running was too suggestion of being ready); - remove ""a_notebook"" notebook name, replace with ""Creating Notebook..."" or ""Open Notebook[open_in_new]"" link; - change nb-state-container styling, blue/underline link on top, but the whole container still acts as a link; - made material icons bold. I'm not sure if I like this, it is bordering on cartoonish.; - make checkmark green to match other success/ok coloring using green (e.g. messages); - un-bold pod name, which is really only interested for us (maybe we should remove?). It is deployed in my namespace if you want to play with it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7155
https://github.com/hail-is/hail/pull/7156:235,Availability,error,error,235,"FYI @tpoterba. I have a few pending PRs now. I will run through this when they are all in. If anything else changes, we should probably do it again a couple days before the workshop and fix issues. Remaining workshop todo items:; - My error handling is too aggressive and frozen pod (2.B) won't work right now; - Still need to add memory/CPU settings to the workshop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7156
https://github.com/hail-is/hail/pull/7157:69,Testability,log,login,69,"Changes:; - add cpu/memory field to workshops; - simplified workshop login logic a bit: workshops_session is just workshop name, token and guest user id, userdata for workshops is id and workshop, workshop auth decorator verifies workshop is valid to before creating userdata,; - make sure to check active everywhere,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7157
https://github.com/hail-is/hail/pull/7157:75,Testability,log,logic,75,"Changes:; - add cpu/memory field to workshops; - simplified workshop login logic a bit: workshops_session is just workshop name, token and guest user id, userdata for workshops is id and workshop, workshop auth decorator verifies workshop is valid to before creating userdata,; - make sure to check active everywhere,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7157
https://github.com/hail-is/hail/pull/7157:49,Usability,simpl,simplified,49,"Changes:; - add cpu/memory field to workshops; - simplified workshop login logic a bit: workshops_session is just workshop name, token and guest user id, userdata for workshops is id and workshop, workshop auth decorator verifies workshop is valid to before creating userdata,; - make sure to check active everywhere,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7157
https://github.com/hail-is/hail/pull/7158:0,Availability,Error,Error,0,"Error page was too aggressive: it would delete any pod even if there was just a temporary network hiccup. Error page means a connection to the notebook failed. That means the state isn't Ready anymore, it is at most initializing (but maybe the pod is gone). In that case, probe k8s to figure out the current state, but don't probe the notebook, let /notebook call /wait to poll for the notebook to come back up (or report whatever state is there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7158
https://github.com/hail-is/hail/pull/7158:106,Availability,Error,Error,106,"Error page was too aggressive: it would delete any pod even if there was just a temporary network hiccup. Error page means a connection to the notebook failed. That means the state isn't Ready anymore, it is at most initializing (but maybe the pod is gone). In that case, probe k8s to figure out the current state, but don't probe the notebook, let /notebook call /wait to poll for the notebook to come back up (or report whatever state is there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7158
https://github.com/hail-is/hail/pull/7160:140,Integrability,message,message,140,Use set_message in CI and Batch to give users feedback after performing POST operations. I added set_message while working on notebook. Any message set will show up at the top of the next page loaded styled according to the message type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7160
https://github.com/hail-is/hail/pull/7160:224,Integrability,message,message,224,Use set_message in CI and Batch to give users feedback after performing POST operations. I added set_message while working on notebook. Any message set will show up at the top of the next page loaded styled according to the message type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7160
https://github.com/hail-is/hail/pull/7160:61,Performance,perform,performing,61,Use set_message in CI and Batch to give users feedback after performing POST operations. I added set_message while working on notebook. Any message set will show up at the top of the next page loaded styled according to the message type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7160
https://github.com/hail-is/hail/pull/7160:193,Performance,load,loaded,193,Use set_message in CI and Batch to give users feedback after performing POST operations. I added set_message while working on notebook. Any message set will show up at the top of the next page loaded styled according to the message type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7160
https://github.com/hail-is/hail/pull/7160:46,Usability,feedback,feedback,46,Use set_message in CI and Batch to give users feedback after performing POST operations. I added set_message while working on notebook. Any message set will show up at the top of the next page loaded styled according to the message type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7160
https://github.com/hail-is/hail/pull/7162:113,Availability,failure,failures,113,"Mostly small, straightforward stuff. /auth must only return 2xx, 401 or 403, or nginx returns 500. Redirect auth failures connecting to instance to /error, too. Changed ""Create/Open Notebook"" to ""Launch/Open Jupyter"" and associated language throughout. I'll run through the whole test playbook again after these go in. Note to self, some improvements to consider:; - Validate image, memory, cpu values in workshop-admin. Right now, if you enter invalid values, you get a 500 on launch Jupyter with invalid pod spec.; - Could change notebook.hail.is/notebook URL to notebook.hail.is/jupyter now.; - A background loop to kill any notebook workers associated to inactive workshops. Then if you just inactivate the workshop at the end, everything gets cleaned up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162
https://github.com/hail-is/hail/pull/7162:149,Availability,error,error,149,"Mostly small, straightforward stuff. /auth must only return 2xx, 401 or 403, or nginx returns 500. Redirect auth failures connecting to instance to /error, too. Changed ""Create/Open Notebook"" to ""Launch/Open Jupyter"" and associated language throughout. I'll run through the whole test playbook again after these go in. Note to self, some improvements to consider:; - Validate image, memory, cpu values in workshop-admin. Right now, if you enter invalid values, you get a 500 on launch Jupyter with invalid pod spec.; - Could change notebook.hail.is/notebook URL to notebook.hail.is/jupyter now.; - A background loop to kill any notebook workers associated to inactive workshops. Then if you just inactivate the workshop at the end, everything gets cleaned up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162
https://github.com/hail-is/hail/pull/7162:367,Security,Validat,Validate,367,"Mostly small, straightforward stuff. /auth must only return 2xx, 401 or 403, or nginx returns 500. Redirect auth failures connecting to instance to /error, too. Changed ""Create/Open Notebook"" to ""Launch/Open Jupyter"" and associated language throughout. I'll run through the whole test playbook again after these go in. Note to self, some improvements to consider:; - Validate image, memory, cpu values in workshop-admin. Right now, if you enter invalid values, you get a 500 on launch Jupyter with invalid pod spec.; - Could change notebook.hail.is/notebook URL to notebook.hail.is/jupyter now.; - A background loop to kill any notebook workers associated to inactive workshops. Then if you just inactivate the workshop at the end, everything gets cleaned up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162
https://github.com/hail-is/hail/pull/7162:280,Testability,test,test,280,"Mostly small, straightforward stuff. /auth must only return 2xx, 401 or 403, or nginx returns 500. Redirect auth failures connecting to instance to /error, too. Changed ""Create/Open Notebook"" to ""Launch/Open Jupyter"" and associated language throughout. I'll run through the whole test playbook again after these go in. Note to self, some improvements to consider:; - Validate image, memory, cpu values in workshop-admin. Right now, if you enter invalid values, you get a 500 on launch Jupyter with invalid pod spec.; - Could change notebook.hail.is/notebook URL to notebook.hail.is/jupyter now.; - A background loop to kill any notebook workers associated to inactive workshops. Then if you just inactivate the workshop at the end, everything gets cleaned up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162
https://github.com/hail-is/hail/issues/7166:18,Availability,ERROR,ERROR,18,"`; {""levelname"": ""ERROR"", ""asctime"": ""2019-09-30 19:09:54,555"", ""filename"": ""ci.py"", ""funcNameAndLine"": ""update_loop:315"", ""message"": ""hail-is/hail:master update failed due to exception"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 929, in _wrap_create_connection\n await self._loop.create_connection(*args, **kwargs))\n File \""uvloop/loop.pyx\"", line 1904, in create_connection\n File \""uvloop/loop.pyx\"", line 1883, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 85",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/issues/7166:155,Deployability,update,update,155,"`; {""levelname"": ""ERROR"", ""asctime"": ""2019-09-30 19:09:54,555"", ""filename"": ""ci.py"", ""funcNameAndLine"": ""update_loop:315"", ""message"": ""hail-is/hail:master update failed due to exception"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 929, in _wrap_create_connection\n await self._loop.create_connection(*args, **kwargs))\n File \""uvloop/loop.pyx\"", line 1904, in create_connection\n File \""uvloop/loop.pyx\"", line 1883, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 85",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/issues/7166:799,Deployability,update,update,799,"`; {""levelname"": ""ERROR"", ""asctime"": ""2019-09-30 19:09:54,555"", ""filename"": ""ci.py"", ""funcNameAndLine"": ""update_loop:315"", ""message"": ""hail-is/hail:master update failed due to exception"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 929, in _wrap_create_connection\n await self._loop.create_connection(*args, **kwargs))\n File \""uvloop/loop.pyx\"", line 1904, in create_connection\n File \""uvloop/loop.pyx\"", line 1883, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 85",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/issues/7166:888,Deployability,update,update,888,"`; {""levelname"": ""ERROR"", ""asctime"": ""2019-09-30 19:09:54,555"", ""filename"": ""ci.py"", ""funcNameAndLine"": ""update_loop:315"", ""message"": ""hail-is/hail:master update failed due to exception"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 929, in _wrap_create_connection\n await self._loop.create_connection(*args, **kwargs))\n File \""uvloop/loop.pyx\"", line 1904, in create_connection\n File \""uvloop/loop.pyx\"", line 1883, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 85",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/issues/7166:2646,Deployability,deploy,deployed,2646,"back (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 859, in _create_connection\n req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 997, in _create_direct_connection\n raise last_exc\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 979, in _create_direct_connection\n req=req, client_error=client_error)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 936, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.default:80 ssl:None [Connection refused]""}; `. This happens every time CI is deployed, as far as I know.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/issues/7166:124,Integrability,message,message,124,"`; {""levelname"": ""ERROR"", ""asctime"": ""2019-09-30 19:09:54,555"", ""filename"": ""ci.py"", ""funcNameAndLine"": ""update_loop:315"", ""message"": ""hail-is/hail:master update failed due to exception"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 929, in _wrap_create_connection\n await self._loop.create_connection(*args, **kwargs))\n File \""uvloop/loop.pyx\"", line 1904, in create_connection\n File \""uvloop/loop.pyx\"", line 1883, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 85",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/issues/7166:1746,Safety,timeout,timeout,1746,"back (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 859, in _create_connection\n req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 997, in _create_direct_connection\n raise last_exc\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 979, in _create_direct_connection\n req=req, client_error=client_error)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 936, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.default:80 ssl:None [Connection refused]""}; `. This happens every time CI is deployed, as far as I know.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/issues/7166:1913,Safety,timeout,timeout,1913,"back (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 859, in _create_connection\n req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 997, in _create_direct_connection\n raise last_exc\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 979, in _create_direct_connection\n req=req, client_error=client_error)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 936, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.default:80 ssl:None [Connection refused]""}; `. This happens every time CI is deployed, as far as I know.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/issues/7166:2041,Safety,timeout,timeout,2041,"back (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/ci/ci.py\"", line 311, in update_loop\n await wb.update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 518, in update\n await self._update(app)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 540, in _update\n await self._update_batch(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 652, in _update_batch\n await self._update_deploy(batch_client)\n File \""/usr/local/lib/python3.6/dist-packages/ci/github.py\"", line 609, in _update_deploy\n 'target_branch': self.branch.short_str()\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 476, in list_batches\n batches = await self._get('/api/v1alpha/batches', params=params)\n File \""/usr/local/lib/python3.6/dist-packages/hailtop/batch_client/aioclient.py\"", line 455, in _get\n self.url + path, params=params, headers=self._headers)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py\"", line 484, in _request\n timeout=real_timeout\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 523, in connect\n proto = await self._create_connection(req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 859, in _create_connection\n req, traces, timeout)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 997, in _create_direct_connection\n raise last_exc\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 979, in _create_direct_connection\n req=req, client_error=client_error)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/connector.py\"", line 936, in _wrap_create_connection\n raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.default:80 ssl:None [Connection refused]""}; `. This happens every time CI is deployed, as far as I know.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7166
https://github.com/hail-is/hail/pull/7167:0,Deployability,Update,Update,0,Update notebook scale test to reflect new location of workshop service (moved notebook.hail.is/workshop => workshop.hail.is). Just ran it with 10 notebooks against production and it worked fine.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7167
https://github.com/hail-is/hail/pull/7167:22,Testability,test,test,22,Update notebook scale test to reflect new location of workshop service (moved notebook.hail.is/workshop => workshop.hail.is). Just ran it with 10 notebooks against production and it worked fine.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7167
https://github.com/hail-is/hail/issues/7173:184,Energy Efficiency,reduce,reduce,184,"We do not have a strategy for folds on arbitrary types, but for primitive types the answer is clear: represent the value as a JVM primitive. The Python syntax should mirror [functools.reduce](https://docs.python.org/3/library/functools.html#functools.reduce):. ```python; hl.agg.reduce(lambda acc, x: hl.bit_or(acc, x), t.bit_string, 0L); ```. The implementation should not box the primitive values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7173
https://github.com/hail-is/hail/issues/7173:251,Energy Efficiency,reduce,reduce,251,"We do not have a strategy for folds on arbitrary types, but for primitive types the answer is clear: represent the value as a JVM primitive. The Python syntax should mirror [functools.reduce](https://docs.python.org/3/library/functools.html#functools.reduce):. ```python; hl.agg.reduce(lambda acc, x: hl.bit_or(acc, x), t.bit_string, 0L); ```. The implementation should not box the primitive values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7173
https://github.com/hail-is/hail/issues/7173:279,Energy Efficiency,reduce,reduce,279,"We do not have a strategy for folds on arbitrary types, but for primitive types the answer is clear: represent the value as a JVM primitive. The Python syntax should mirror [functools.reduce](https://docs.python.org/3/library/functools.html#functools.reduce):. ```python; hl.agg.reduce(lambda acc, x: hl.bit_or(acc, x), t.bit_string, 0L); ```. The implementation should not box the primitive values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7173
https://github.com/hail-is/hail/issues/7173:94,Usability,clear,clear,94,"We do not have a strategy for folds on arbitrary types, but for primitive types the answer is clear: represent the value as a JVM primitive. The Python syntax should mirror [functools.reduce](https://docs.python.org/3/library/functools.html#functools.reduce):. ```python; hl.agg.reduce(lambda acc, x: hl.bit_or(acc, x), t.bit_string, 0L); ```. The implementation should not box the primitive values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7173
https://github.com/hail-is/hail/pull/7174:230,Availability,ERROR,ERRORNOTFOUND,230,"This causes issues when starting interactive sessions on clusters. Before, I get lots of output like this:; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: ERRORNOTFOUND. SPARKMONITOR_LISTENER: Exception creating socket:java.lang.NumberFormatException: For input string: ""ERRORNOTFOUND"". SPARKMONITOR_LISTENER: Application Started: application_1569946119076_0001 ...Start Time: 1569946336092. SPARKMONITOR_LISTENER: Exception sending socket message:java.lang.NullPointerException. After:; <nothing>. I also tested to make sure the monitor still worked in a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7174
https://github.com/hail-is/hail/pull/7174:346,Availability,ERROR,ERRORNOTFOUND,346,"This causes issues when starting interactive sessions on clusters. Before, I get lots of output like this:; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: ERRORNOTFOUND. SPARKMONITOR_LISTENER: Exception creating socket:java.lang.NumberFormatException: For input string: ""ERRORNOTFOUND"". SPARKMONITOR_LISTENER: Application Started: application_1569946119076_0001 ...Start Time: 1569946336092. SPARKMONITOR_LISTENER: Exception sending socket message:java.lang.NullPointerException. After:; <nothing>. I also tested to make sure the monitor still worked in a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7174
https://github.com/hail-is/hail/pull/7174:605,Energy Efficiency,monitor,monitor,605,"This causes issues when starting interactive sessions on clusters. Before, I get lots of output like this:; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: ERRORNOTFOUND. SPARKMONITOR_LISTENER: Exception creating socket:java.lang.NumberFormatException: For input string: ""ERRORNOTFOUND"". SPARKMONITOR_LISTENER: Application Started: application_1569946119076_0001 ...Start Time: 1569946336092. SPARKMONITOR_LISTENER: Exception sending socket message:java.lang.NullPointerException. After:; <nothing>. I also tested to make sure the monitor still worked in a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7174
https://github.com/hail-is/hail/pull/7174:515,Integrability,message,message,515,"This causes issues when starting interactive sessions on clusters. Before, I get lots of output like this:; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: ERRORNOTFOUND. SPARKMONITOR_LISTENER: Exception creating socket:java.lang.NumberFormatException: For input string: ""ERRORNOTFOUND"". SPARKMONITOR_LISTENER: Application Started: application_1569946119076_0001 ...Start Time: 1569946336092. SPARKMONITOR_LISTENER: Exception sending socket message:java.lang.NullPointerException. After:; <nothing>. I also tested to make sure the monitor still worked in a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7174
https://github.com/hail-is/hail/pull/7174:581,Testability,test,tested,581,"This causes issues when starting interactive sessions on clusters. Before, I get lots of output like this:; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: ERRORNOTFOUND. SPARKMONITOR_LISTENER: Exception creating socket:java.lang.NumberFormatException: For input string: ""ERRORNOTFOUND"". SPARKMONITOR_LISTENER: Application Started: application_1569946119076_0001 ...Start Time: 1569946336092. SPARKMONITOR_LISTENER: Exception sending socket message:java.lang.NullPointerException. After:; <nothing>. I also tested to make sure the monitor still worked in a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7174
https://github.com/hail-is/hail/pull/7176:10,Testability,benchmark,benchmark,10,the 1b_1k benchmark goes from 100s to 0.12s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7176
https://github.com/hail-is/hail/pull/7178:1521,Availability,error,error,1521,"dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(t2.key); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32; Index Expressions: int32, int32; ```. This new, private, method facilitates the annotation db which users expect to automatically find a compatible prefix of the key to use as an indexer. ---. Dice came up @chrisvittal, but I'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:1527,Availability,echo,echos,1527,"dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(t2.key); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32; Index Expressions: int32, int32; ```. This new, private, method facilitates the annotation db which users expect to automatically find a compatible prefix of the key to use as an indexer. ---. Dice came up @chrisvittal, but I'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:1539,Availability,ERROR,ERROR,1539,"dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(t2.key); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32; Index Expressions: int32, int32; ```. This new, private, method facilitates the annotation db which users expect to automatically find a compatible prefix of the key to use as an indexer. ---. Dice came up @chrisvittal, but I'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:1769,Availability,error,error,1769,"fy JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(t2.key); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32; Index Expressions: int32, int32; ```. This new, private, method facilitates the annotation db which users expect to automatically find a compatible prefix of the key to use as an indexer. ---. Dice came up @chrisvittal, but I'm curious to hear @tpoterba 's feedback as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:150,Deployability,configurat,configuration,150,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:240,Deployability,configurat,configuration,240,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:298,Deployability,deploy,deploy,298,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:345,Deployability,configurat,configuration,345,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:639,Deployability,deploy,deployment,639,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:671,Deployability,configurat,configuration,671,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:461,Energy Efficiency,reduce,reduce,461,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:150,Modifiability,config,configuration,150,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:240,Modifiability,config,configuration,240,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:345,Modifiability,config,configuration,345,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:671,Modifiability,config,configuration,671,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:369,Testability,test,testing,369,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:568,Testability,log,logic,568,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:979,Testability,test,test,979,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:1736,Testability,log,logic,1736,"fy JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(t2.key); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32; Index Expressions: int32, int32; ```. This new, private, method facilitates the annotation db which users expect to automatically find a compatible prefix of the key to use as an indexer. ---. Dice came up @chrisvittal, but I'm curious to hear @tpoterba 's feedback as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:12,Usability,simpl,simplifies,12,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:556,Usability,Simpl,Simplify,556,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7178:2545,Usability,feedback,feedback,2545,"fy JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(t2.key); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32; Index Expressions: int32, int32; ```. This new, private, method facilitates the annotation db which users expect to automatically find a compatible prefix of the key to use as an indexer. ---. Dice came up @chrisvittal, but I'm curious to hear @tpoterba 's feedback as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178
https://github.com/hail-is/hail/pull/7196:223,Availability,down,down,223,"Another attempt at dropdown menus. I like this better, too, for the reasons you described. Changes:; - change dropdowns to ""caret"" style, with a triangle on the top of the dropdown that points to the header item it dropped down from; - move monitoring links into their own dropdown. To issues I'm not totally happy with:; - Monitoring can't be clicked on, so it is grayed out, but styling matches hover styling for active header items; - To center the triangle under the header item, I had to measure the width of the header items in the browser first. It would be nice to do this from within CSS, but I don't know how to do that: the caret and the header item are in different parts of the DOM, and I don't know how to communicate the width of the header item to the left property of the caret. It is deployed in my namespace if you want to take a look.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7196
https://github.com/hail-is/hail/pull/7196:802,Deployability,deploy,deployed,802,"Another attempt at dropdown menus. I like this better, too, for the reasons you described. Changes:; - change dropdowns to ""caret"" style, with a triangle on the top of the dropdown that points to the header item it dropped down from; - move monitoring links into their own dropdown. To issues I'm not totally happy with:; - Monitoring can't be clicked on, so it is grayed out, but styling matches hover styling for active header items; - To center the triangle under the header item, I had to measure the width of the header items in the browser first. It would be nice to do this from within CSS, but I don't know how to do that: the caret and the header item are in different parts of the DOM, and I don't know how to communicate the width of the header item to the left property of the caret. It is deployed in my namespace if you want to take a look.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7196
https://github.com/hail-is/hail/pull/7196:241,Energy Efficiency,monitor,monitoring,241,"Another attempt at dropdown menus. I like this better, too, for the reasons you described. Changes:; - change dropdowns to ""caret"" style, with a triangle on the top of the dropdown that points to the header item it dropped down from; - move monitoring links into their own dropdown. To issues I'm not totally happy with:; - Monitoring can't be clicked on, so it is grayed out, but styling matches hover styling for active header items; - To center the triangle under the header item, I had to measure the width of the header items in the browser first. It would be nice to do this from within CSS, but I don't know how to do that: the caret and the header item are in different parts of the DOM, and I don't know how to communicate the width of the header item to the left property of the caret. It is deployed in my namespace if you want to take a look.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7196
https://github.com/hail-is/hail/pull/7196:324,Energy Efficiency,Monitor,Monitoring,324,"Another attempt at dropdown menus. I like this better, too, for the reasons you described. Changes:; - change dropdowns to ""caret"" style, with a triangle on the top of the dropdown that points to the header item it dropped down from; - move monitoring links into their own dropdown. To issues I'm not totally happy with:; - Monitoring can't be clicked on, so it is grayed out, but styling matches hover styling for active header items; - To center the triangle under the header item, I had to measure the width of the header items in the browser first. It would be nice to do this from within CSS, but I don't know how to do that: the caret and the header item are in different parts of the DOM, and I don't know how to communicate the width of the header item to the left property of the caret. It is deployed in my namespace if you want to take a look.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7196
https://github.com/hail-is/hail/pull/7197:30,Testability,test,test,30,"I need to write another scala test, but it's working as expected in; large test cases in benchmarks. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 57.8% 57.617 33.278; table_aggregate_downsample_dense 30.0% 127.079 38.119; ----------------------; Geometric mean: 41.6%; Simple mean: 43.9%; Median: 43.9%; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7197
https://github.com/hail-is/hail/pull/7197:75,Testability,test,test,75,"I need to write another scala test, but it's working as expected in; large test cases in benchmarks. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 57.8% 57.617 33.278; table_aggregate_downsample_dense 30.0% 127.079 38.119; ----------------------; Geometric mean: 41.6%; Simple mean: 43.9%; Median: 43.9%; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7197
https://github.com/hail-is/hail/pull/7197:89,Testability,benchmark,benchmarks,89,"I need to write another scala test, but it's working as expected in; large test cases in benchmarks. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 57.8% 57.617 33.278; table_aggregate_downsample_dense 30.0% 127.079 38.119; ----------------------; Geometric mean: 41.6%; Simple mean: 43.9%; Median: 43.9%; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7197
https://github.com/hail-is/hail/pull/7197:319,Usability,Simpl,Simple,319,"I need to write another scala test, but it's working as expected in; large test cases in benchmarks. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 57.8% 57.617 33.278; table_aggregate_downsample_dense 30.0% 127.079 38.119; ----------------------; Geometric mean: 41.6%; Simple mean: 43.9%; Median: 43.9%; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7197
https://github.com/hail-is/hail/pull/7199:38,Availability,avail,available,38,"Most of the functionality was already available in EmitFunctionBuilder, but Compile() didn't make it available. This PR creates a `PrintWriter` during assertEvalsTo if you set the `jvm_bytecode_dump` flag to a file path you want the bytecode to be written to. Example:; ```scala; HailContext.setFlag(""jvm_bytecode_dump"", ""arr_filter_bytecode.java""); assertEvalsTo(ArrayFilter(a, ""x"",; ApplyComparisonOp(LT(TInt32()), Ref(""x"", TInt32()), I32(6))), FastIndexedSeq(3)); HailContext.setFlag(""jvm_bytecode_dump"", null); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7199
https://github.com/hail-is/hail/pull/7199:101,Availability,avail,available,101,"Most of the functionality was already available in EmitFunctionBuilder, but Compile() didn't make it available. This PR creates a `PrintWriter` during assertEvalsTo if you set the `jvm_bytecode_dump` flag to a file path you want the bytecode to be written to. Example:; ```scala; HailContext.setFlag(""jvm_bytecode_dump"", ""arr_filter_bytecode.java""); assertEvalsTo(ArrayFilter(a, ""x"",; ApplyComparisonOp(LT(TInt32()), Ref(""x"", TInt32()), I32(6))), FastIndexedSeq(3)); HailContext.setFlag(""jvm_bytecode_dump"", null); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7199
https://github.com/hail-is/hail/pull/7199:151,Testability,assert,assertEvalsTo,151,"Most of the functionality was already available in EmitFunctionBuilder, but Compile() didn't make it available. This PR creates a `PrintWriter` during assertEvalsTo if you set the `jvm_bytecode_dump` flag to a file path you want the bytecode to be written to. Example:; ```scala; HailContext.setFlag(""jvm_bytecode_dump"", ""arr_filter_bytecode.java""); assertEvalsTo(ArrayFilter(a, ""x"",; ApplyComparisonOp(LT(TInt32()), Ref(""x"", TInt32()), I32(6))), FastIndexedSeq(3)); HailContext.setFlag(""jvm_bytecode_dump"", null); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7199
https://github.com/hail-is/hail/pull/7199:350,Testability,assert,assertEvalsTo,350,"Most of the functionality was already available in EmitFunctionBuilder, but Compile() didn't make it available. This PR creates a `PrintWriter` during assertEvalsTo if you set the `jvm_bytecode_dump` flag to a file path you want the bytecode to be written to. Example:; ```scala; HailContext.setFlag(""jvm_bytecode_dump"", ""arr_filter_bytecode.java""); assertEvalsTo(ArrayFilter(a, ""x"",; ApplyComparisonOp(LT(TInt32()), Ref(""x"", TInt32()), I32(6))), FastIndexedSeq(3)); HailContext.setFlag(""jvm_bytecode_dump"", null); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7199
https://github.com/hail-is/hail/pull/7201:26,Testability,benchmark,benchmarks,26,"Permits the big aggregate benchmarks to run...slowly:. ```; 2019-10-05 10:58:08,110: INFO: [1/2] Running table_big_aggregate_compilation...; 2019-10-05 10:58:24,685: INFO: burn in: 16.57s; 2019-10-05 10:58:28,664: INFO: run 1: 3.98s; 2019-10-05 10:58:32,558: INFO: run 2: 3.89s; 2019-10-05 10:58:36,332: INFO: run 3: 3.77s; 2019-10-05 10:58:36,335: INFO: [2/2] Running table_big_aggregate_compile_and_execute...; 2019-10-05 10:58:42,972: INFO: burn in: 6.64s; 2019-10-05 10:58:48,677: INFO: run 1: 5.71s; 2019-10-05 10:58:54,358: INFO: run 2: 5.68s; 2019-10-05 10:59:00,091: INFO: run 3: 5.73s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7201
https://github.com/hail-is/hail/pull/7203:55,Deployability,update,update,55,switch storage class to ssd for a bit of a speed bump; update to R 3.5+ for file format compat; added README with rough instructions for updating the browser. This is deployed running against updated data from Duncan.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7203
https://github.com/hail-is/hail/pull/7203:167,Deployability,deploy,deployed,167,switch storage class to ssd for a bit of a speed bump; update to R 3.5+ for file format compat; added README with rough instructions for updating the browser. This is deployed running against updated data from Duncan.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7203
https://github.com/hail-is/hail/pull/7203:192,Deployability,update,updated,192,switch storage class to ssd for a bit of a speed bump; update to R 3.5+ for file format compat; added README with rough instructions for updating the browser. This is deployed running against updated data from Duncan.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7203
https://github.com/hail-is/hail/pull/7207:201,Usability,simpl,simply,201,"`switch` can either be used with `Code[T]`s or `JoinPoint[Unit]`s. The former resemble's Java's `switch` by executing the code in the matched case. The latter in some cases generates less bytecode; it simply jumps to the join point for the matched case. ```scala; Code.switch[T](target: Code[Int], dflt: Code[T], cases: Seq[Code[T]]): Code[T]; JoinPoint.switch(target: Code[Int], dflt: JoinPoint[Unit], cases: Seq[JoinPoint[Unit]]): Code[Ctrl]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7207
https://github.com/hail-is/hail/pull/7208:45,Availability,error,errors,45,"Added batch2_check step in build, fixed lint errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7208
https://github.com/hail-is/hail/pull/7209:174,Safety,avoid,avoid,174,This PR adds the ability to use `hl.literal` on numpy arrays. It also changes the behavior of `hl._ndarray` when called on a numpy ndarray so it just goes through literal to avoid creating an IR node for each element of tensor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7209
https://github.com/hail-is/hail/pull/7210:204,Energy Efficiency,schedul,scheduler,204,"Stacked on: https://github.com/hail-is/hail/pull/7208. Make app, db and v1 not global variables in batch.py. I'm doing this because I want to break Batch, Job into separate files and use them in both the scheduler and the front end. To do that, they need to be parameterized by the app (or the services that are carried on the app).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7210
https://github.com/hail-is/hail/pull/7210:86,Modifiability,variab,variables,86,"Stacked on: https://github.com/hail-is/hail/pull/7208. Make app, db and v1 not global variables in batch.py. I'm doing this because I want to break Batch, Job into separate files and use them in both the scheduler and the front end. To do that, they need to be parameterized by the app (or the services that are carried on the app).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7210
https://github.com/hail-is/hail/pull/7210:261,Modifiability,parameteriz,parameterized,261,"Stacked on: https://github.com/hail-is/hail/pull/7208. Make app, db and v1 not global variables in batch.py. I'm doing this because I want to break Batch, Job into separate files and use them in both the scheduler and the front end. To do that, they need to be parameterized by the app (or the services that are carried on the app).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7210
https://github.com/hail-is/hail/pull/7214:23,Safety,timeout,timeouts,23,"This change introduces timeouts to `benchmark run`. This makes it easier; to run benchmarks on old versions of Hail, which might have extremely; slow times for certain benchmarks (block matrix multiply, looking at you)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7214
https://github.com/hail-is/hail/pull/7214:36,Testability,benchmark,benchmark,36,"This change introduces timeouts to `benchmark run`. This makes it easier; to run benchmarks on old versions of Hail, which might have extremely; slow times for certain benchmarks (block matrix multiply, looking at you)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7214
https://github.com/hail-is/hail/pull/7214:81,Testability,benchmark,benchmarks,81,"This change introduces timeouts to `benchmark run`. This makes it easier; to run benchmarks on old versions of Hail, which might have extremely; slow times for certain benchmarks (block matrix multiply, looking at you)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7214
https://github.com/hail-is/hail/pull/7214:168,Testability,benchmark,benchmarks,168,"This change introduces timeouts to `benchmark run`. This makes it easier; to run benchmarks on old versions of Hail, which might have extremely; slow times for certain benchmarks (block matrix multiply, looking at you)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7214
https://github.com/hail-is/hail/pull/7215:34,Availability,failure,failures,34,"Retry ALL requests with temporary failures. We might want to add more exceptions/status codes, but these seem like a good first cut. Also, don't cancel a batch that failed to submit. It shouldn't be cancellable unless it was closed (in case the submit block in question would have succeeded). At some point we should have something that cleans up unclosed batches after a while (e.g. 24hrs).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7215
https://github.com/hail-is/hail/pull/7218:99,Deployability,deploy,deploy,99,"Changes:; - created batch2 Hail service account with the correct google permissions; - fixed ""make deploy""; - updated UI code to be compatible with latest changes; Hand deployed to verify it is all working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7218
https://github.com/hail-is/hail/pull/7218:110,Deployability,update,updated,110,"Changes:; - created batch2 Hail service account with the correct google permissions; - fixed ""make deploy""; - updated UI code to be compatible with latest changes; Hand deployed to verify it is all working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7218
https://github.com/hail-is/hail/pull/7218:169,Deployability,deploy,deployed,169,"Changes:; - created batch2 Hail service account with the correct google permissions; - fixed ""make deploy""; - updated UI code to be compatible with latest changes; Hand deployed to verify it is all working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7218
https://github.com/hail-is/hail/pull/7219:88,Modifiability,rewrite,rewrite,88,Fixed inlining of ArrayAgg nodes without aggs in the body. Also added; simplify rule to rewrite these nodes to just the body.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7219
https://github.com/hail-is/hail/pull/7219:71,Usability,simpl,simplify,71,Fixed inlining of ArrayAgg nodes without aggs in the body. Also added; simplify rule to rewrite these nodes to just the body.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7219
https://github.com/hail-is/hail/issues/7221:152,Testability,test,tests,152,"A missing ndarray should return a missing ndarray on any operation, e.g.:. ```; hl.null(hl.tndarray(hl.tfloat, 1)).map(lambda x: x * 2); ```. - [ ] add tests; - [ ] fix `NDArrayEmitter`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7221
https://github.com/hail-is/hail/pull/7226:19,Testability,test,testing,19,"Going to need some testing, will assign when it is passing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7226
https://github.com/hail-is/hail/pull/7228:70,Integrability,interface,interface,70,"- [x] ~~Stacked on #7207 (needed for `MakeArray`)~~. Implements a new interface `EmitStream` for staged ""pull"" based streams. This design differs significantly from the current `ArrayIteratorTriplet` design; such ""push"" based streams are incapable of deforested operations that operate on multiple sub-streams, notably joins or zips. A (pull) stream has an internal state, which can be used to generate new elements by a `step` operation. This state is initialized by passing an initial parameter to an `init` operation, which will determine the initial state. You can think of a stream as allowing elements to be computed ""on-demand"", as opposed to the old design where the entire stream is consumed at once. This PR just contains the most simple streams (`ArrayRange`, `MakeArray`, `ArrayMap`, `ArrayFilter`). I'd like to get comments on it before merging my other stream implementations. So far I have all of the other streams implemented, except for `ReadPartition` and `If` (it turns out that `If` is tricky). **Interface**. `init` can return in two possible ways:; - `Missing` - the stream is actually entirely missing; this usually happens if one of the parameters to a stream is missing (e.g. `ArrayRange(0, NA, 1) = NA`); - `State(s0)` - the stream has started; its initial state is `s0`. `step` can return in three possible ways as well:; - `EOS` - we've reached the End Of Stream, there are no more elements left.; - `Skip(s1)` - this iteration didn't produce an element, you must try stepping again with state `s1` (see ""design notes"").; - `Yield(elt, s1)` - the stream computed an element `elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/pull/7228:1017,Integrability,Interface,Interface,1017,"207 (needed for `MakeArray`)~~. Implements a new interface `EmitStream` for staged ""pull"" based streams. This design differs significantly from the current `ArrayIteratorTriplet` design; such ""push"" based streams are incapable of deforested operations that operate on multiple sub-streams, notably joins or zips. A (pull) stream has an internal state, which can be used to generate new elements by a `step` operation. This state is initialized by passing an initial parameter to an `init` operation, which will determine the initial state. You can think of a stream as allowing elements to be computed ""on-demand"", as opposed to the old design where the entire stream is consumed at once. This PR just contains the most simple streams (`ArrayRange`, `MakeArray`, `ArrayMap`, `ArrayFilter`). I'd like to get comments on it before merging my other stream implementations. So far I have all of the other streams implemented, except for `ReadPartition` and `If` (it turns out that `If` is tricky). **Interface**. `init` can return in two possible ways:; - `Missing` - the stream is actually entirely missing; this usually happens if one of the parameters to a stream is missing (e.g. `ArrayRange(0, NA, 1) = NA`); - `State(s0)` - the stream has started; its initial state is `s0`. `step` can return in three possible ways as well:; - `EOS` - we've reached the End Of Stream, there are no more elements left.; - `Skip(s1)` - this iteration didn't produce an element, you must try stepping again with state `s1` (see ""design notes"").; - `Yield(elt, s1)` - the stream computed an element `elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implement",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/pull/7228:2067,Modifiability,parameteriz,parameterized,2067,"can return in two possible ways:; - `Missing` - the stream is actually entirely missing; this usually happens if one of the parameters to a stream is missing (e.g. `ArrayRange(0, NA, 1) = NA`); - `State(s0)` - the stream has started; its initial state is `s0`. `step` can return in three possible ways as well:; - `EOS` - we've reached the End Of Stream, there are no more elements left.; - `Skip(s1)` - this iteration didn't produce an element, you must try stepping again with state `s1` (see ""design notes"").; - `Yield(elt, s1)` - the stream computed an element `elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/pull/7228:2626,Modifiability,Parameteriz,Parameterized,2626,"elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple functions. Instead of `init` ""returning"" `Missing`/`Start(s0)` (which is impossible, since these are compile time data structures; they won't exist during JVM runtime), init takes a continuation `k: Init[S] => Code[Ctrl]`, which it must call with one of these values. To use `init`, you can simply provide a pattern matching lambda, which closely resembles the syntax you would normally use to pattern match on a returned value, e.g. (from `contMap`):; ```scala; self.init(mb, jb, param) {; case Missing => k(Missing); case Start(s) => Code(setup, k(Start(s))); };",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/pull/7228:2662,Modifiability,Parameteriz,Parameterized,2662,"r simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple functions. Instead of `init` ""returning"" `Missing`/`Start(s0)` (which is impossible, since these are compile time data structures; they won't exist during JVM runtime), init takes a continuation `k: Init[S] => Code[Ctrl]`, which it must call with one of these values. To use `init`, you can simply provide a pattern matching lambda, which closely resembles the syntax you would normally use to pattern match on a returned value, e.g. (from `contMap`):; ```scala; self.init(mb, jb, param) {; case Missing => k(Missing); case Start(s) => Code(setup, k(Start(s))); }; ```; This technique actually cleans up the implementation significantly, especially moving forward to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/pull/7228:741,Usability,simpl,simple,741,"- [x] ~~Stacked on #7207 (needed for `MakeArray`)~~. Implements a new interface `EmitStream` for staged ""pull"" based streams. This design differs significantly from the current `ArrayIteratorTriplet` design; such ""push"" based streams are incapable of deforested operations that operate on multiple sub-streams, notably joins or zips. A (pull) stream has an internal state, which can be used to generate new elements by a `step` operation. This state is initialized by passing an initial parameter to an `init` operation, which will determine the initial state. You can think of a stream as allowing elements to be computed ""on-demand"", as opposed to the old design where the entire stream is consumed at once. This PR just contains the most simple streams (`ArrayRange`, `MakeArray`, `ArrayMap`, `ArrayFilter`). I'd like to get comments on it before merging my other stream implementations. So far I have all of the other streams implemented, except for `ReadPartition` and `If` (it turns out that `If` is tricky). **Interface**. `init` can return in two possible ways:; - `Missing` - the stream is actually entirely missing; this usually happens if one of the parameters to a stream is missing (e.g. `ArrayRange(0, NA, 1) = NA`); - `State(s0)` - the stream has started; its initial state is `s0`. `step` can return in three possible ways as well:; - `EOS` - we've reached the End Of Stream, there are no more elements left.; - `Skip(s1)` - this iteration didn't produce an element, you must try stepping again with state `s1` (see ""design notes"").; - `Yield(elt, s1)` - the stream computed an element `elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/pull/7228:1707,Usability,simpl,simplifying,1707,"his PR just contains the most simple streams (`ArrayRange`, `MakeArray`, `ArrayMap`, `ArrayFilter`). I'd like to get comments on it before merging my other stream implementations. So far I have all of the other streams implemented, except for `ReadPartition` and `If` (it turns out that `If` is tricky). **Interface**. `init` can return in two possible ways:; - `Missing` - the stream is actually entirely missing; this usually happens if one of the parameters to a stream is missing (e.g. `ArrayRange(0, NA, 1) = NA`); - `State(s0)` - the stream has started; its initial state is `s0`. `step` can return in three possible ways as well:; - `EOS` - we've reached the End Of Stream, there are no more elements left.; - `Skip(s1)` - this iteration didn't produce an element, you must try stepping again with state `s1` (see ""design notes"").; - `Yield(elt, s1)` - the stream computed an element `elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiate",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/pull/7228:2000,Usability,simpl,simplify,2000,"can return in two possible ways:; - `Missing` - the stream is actually entirely missing; this usually happens if one of the parameters to a stream is missing (e.g. `ArrayRange(0, NA, 1) = NA`); - `State(s0)` - the stream has started; its initial state is `s0`. `step` can return in three possible ways as well:; - `EOS` - we've reached the End Of Stream, there are no more elements left.; - `Skip(s1)` - this iteration didn't produce an element, you must try stepping again with state `s1` (see ""design notes"").; - `Yield(elt, s1)` - the stream computed an element `elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/pull/7228:3330,Usability,simpl,simply,3330,"tation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple functions. Instead of `init` ""returning"" `Missing`/`Start(s0)` (which is impossible, since these are compile time data structures; they won't exist during JVM runtime), init takes a continuation `k: Init[S] => Code[Ctrl]`, which it must call with one of these values. To use `init`, you can simply provide a pattern matching lambda, which closely resembles the syntax you would normally use to pattern match on a returned value, e.g. (from `contMap`):; ```scala; self.init(mb, jb, param) {; case Missing => k(Missing); case Start(s) => Code(setup, k(Start(s))); }; ```; This technique actually cleans up the implementation significantly, especially moving forward to more complicated streams.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7228
https://github.com/hail-is/hail/issues/7242:428,Testability,test,test,428,"roughly:. ```python; In [77]: hl.utils.range_matrix_table(2, 1, n_partitions=2).annotate_entries(x=1).filter_rows(False).x.export('/tmp/foo.tsv.gz') ; 2019-10-10 08:59:22 Hail: INFO: merging 1 files totalling 30...; 2019-10-10 08:59:22 Hail: INFO: while writing:; /tmp/foo.tsv.gz; merge time: 12.641ms. In [78]: hl.import_matrix_table('/tmp/foo.tsv.gz')._force_count_rows(); ```. Also, hail should actually work, currently this test will fail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7242
https://github.com/hail-is/hail/pull/7257:165,Testability,test,tests,165,"`order_by` supports sort fields. Eventually `order_by` will use the shuffler. To support this `CodeOrdering` needs to support sort fields. I didn't actually add any tests. The Shuffler will eventually land and use this to do non-standard orderings. There's a lot of noise due to threading SortField's everywhere, but the implementation of `CodeOrdering.reverse` and `PType.codeOrdering` are actually quite simple. I also cleaned the code that I touched by abstracting over lt, lteq, gt, gteq.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7257
https://github.com/hail-is/hail/pull/7257:406,Usability,simpl,simple,406,"`order_by` supports sort fields. Eventually `order_by` will use the shuffler. To support this `CodeOrdering` needs to support sort fields. I didn't actually add any tests. The Shuffler will eventually land and use this to do non-standard orderings. There's a lot of noise due to threading SortField's everywhere, but the implementation of `CodeOrdering.reverse` and `PType.codeOrdering` are actually quite simple. I also cleaned the code that I touched by abstracting over lt, lteq, gt, gteq.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7257
https://github.com/hail-is/hail/pull/7259:17,Usability,simpl,simpler,17,Cosmetic. Seemed simpler overall to not take an RV to mutate. No one used this in a smart way anyway.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7259
https://github.com/hail-is/hail/pull/7261:164,Availability,down,down,164,"Stacked on #7260 . Also set wire and memory spec to LZ4Fast. It was a bit annoying to make serialization work with a shared super class, so if anything looks funny down there that's probably why.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7261
https://github.com/hail-is/hail/pull/7270:10,Performance,load,loads,10,"front_end loads them from batch2.front_end:. > setup_aiohttp_jinja2(app, 'batch.front_end'). added tests to check the ui pages load",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7270
https://github.com/hail-is/hail/pull/7270:127,Performance,load,load,127,"front_end loads them from batch2.front_end:. > setup_aiohttp_jinja2(app, 'batch.front_end'). added tests to check the ui pages load",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7270
https://github.com/hail-is/hail/pull/7270:99,Testability,test,tests,99,"front_end loads them from batch2.front_end:. > setup_aiohttp_jinja2(app, 'batch.front_end'). added tests to check the ui pages load",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7270
https://github.com/hail-is/hail/issues/7276:78,Integrability,inject,injects,78,"Investigate whether possible to invert menu inclusion, such that each project injects its own menu alongside a common menu section (e.g login).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7276
https://github.com/hail-is/hail/issues/7276:78,Security,inject,injects,78,"Investigate whether possible to invert menu inclusion, such that each project injects its own menu alongside a common menu section (e.g login).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7276
https://github.com/hail-is/hail/issues/7276:136,Testability,log,login,136,"Investigate whether possible to invert menu inclusion, such that each project injects its own menu alongside a common menu section (e.g login).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7276
https://github.com/hail-is/hail/pull/7279:65,Availability,error,error,65,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7279:113,Availability,failure,failures,113,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7279:143,Availability,error,errors,143,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7279:441,Availability,failure,failures,441,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7279:288,Integrability,message,message,288,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7279:75,Safety,timeout,timeout,75,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7279:368,Safety,timeout,timeout,368,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7279:84,Testability,log,log,84,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7279:387,Testability,log,logging,387,"retry in get_userinfo; retry on ClientOSError (client connection error) on timeout; log errno to diagnose future failures. Some of the current errors are ClientOSErrors (ClientConnectionErrors, actually) with strerror ""Connect call failed"" but that doesn't correspond to a standard errno message returned by perror/os.strerror as far as I can tell. So I'm retrying on timeout (good) and logging the errno so we can diagnose future transient failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7279
https://github.com/hail-is/hail/pull/7280:193,Performance,cache,cache,193,"I ended up restructuring the summarizing to make the formatting easier. I also ended up putting the summary stuff on the expression; the vague goal is to allow a call to `table.summarize()` to cache all the summaries of the (nested) row fields, so that subsequent calls to e.g. `table.locus.summarize()` should be free.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7280
https://github.com/hail-is/hail/pull/7283:24,Testability,test,test,24,"Need this to be able to test batch2 with non-production databases. Unfortunately, I have no way of testing whether this works with a test database until it merges...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7283
https://github.com/hail-is/hail/pull/7283:99,Testability,test,testing,99,"Need this to be able to test batch2 with non-production databases. Unfortunately, I have no way of testing whether this works with a test database until it merges...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7283
https://github.com/hail-is/hail/pull/7283:133,Testability,test,test,133,"Need this to be able to test batch2 with non-production databases. Unfortunately, I have no way of testing whether this works with a test database until it merges...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7283
https://github.com/hail-is/hail/pull/7284:150,Availability,error,error,150,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:234,Availability,error,error,234,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:638,Availability,error,error,638,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:659,Availability,recover,recover,659,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:700,Availability,outage,outage,700,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:1151,Availability,failure,failures,1151,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:1339,Availability,error,errors,1339,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:163,Safety,timeout,timeout,163,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:659,Safety,recover,recover,659,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7284:1236,Testability,log,logging,1236,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7284
https://github.com/hail-is/hail/pull/7287:34,Availability,toler,tolerate,34,"Copy pattern from other services: tolerate preemptibles, min 3 replicas, auto-scale up to 10.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7287
https://github.com/hail-is/hail/pull/7288:196,Security,hash,hash,196,"This should fully address the scroll-to artifact on docs, which had the item that was scrolled to (specified by a #id after the url) was hidden by the navbar. ### Background; Browsers interpret a hash after a url as being an element selector (id) to move the top of the page to. The browser will set the top of the browser to the top of the element, which is determined by its height, padding, and box-sizing rule (which determines whether padding and borders are taken into account when calculating the element's height and width). *The margin on the element is not taken into account*. This is incompatible with fixed navbars, because now the the top of the element should also take into account the height of the nabber. ### Solution; 2 solutions, one for the case where the element has a transparent background, and one for the case that it doesn't. In the transparent case (`.section`), we add padding to the element that is larger than the default by the height of the navbar, and a negative margin that is the negative height of the navbar. For `.section`, which doesn't have any padding, we simply use the height of the navbar. In the colored-background case (`dt`, the function blocks, which have 6px of padding, a blue background, and darker blue top-border) we need to use a different solution, because that padding will be the color of the background, making the element appear much taller than expected. The solution is to use a tall transparent border instead, along with the setting `background-clip: padding-box`. To handle the border we use a pseudo element that is absolutely positioned.; - background-clip is widely supported: https://caniuse.com/#search=background-clip. ### Before; <img width=""697"" alt=""Screenshot 2019-10-12 14 34 42"" src=""https://user-images.githubusercontent.com/5543229/66706171-960fa700-ecfd-11e9-9fa0-17a05da486a2.png"">. <img width=""701"" alt=""Screenshot 2019-10-12 14 34 53"" src=""https://user-images.githubusercontent.com/5543229/66706173-9740d400-ecfd-11e9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7288
https://github.com/hail-is/hail/pull/7288:1099,Usability,simpl,simply,1099," to (specified by a #id after the url) was hidden by the navbar. ### Background; Browsers interpret a hash after a url as being an element selector (id) to move the top of the page to. The browser will set the top of the browser to the top of the element, which is determined by its height, padding, and box-sizing rule (which determines whether padding and borders are taken into account when calculating the element's height and width). *The margin on the element is not taken into account*. This is incompatible with fixed navbars, because now the the top of the element should also take into account the height of the nabber. ### Solution; 2 solutions, one for the case where the element has a transparent background, and one for the case that it doesn't. In the transparent case (`.section`), we add padding to the element that is larger than the default by the height of the navbar, and a negative margin that is the negative height of the navbar. For `.section`, which doesn't have any padding, we simply use the height of the navbar. In the colored-background case (`dt`, the function blocks, which have 6px of padding, a blue background, and darker blue top-border) we need to use a different solution, because that padding will be the color of the background, making the element appear much taller than expected. The solution is to use a tall transparent border instead, along with the setting `background-clip: padding-box`. To handle the border we use a pseudo element that is absolutely positioned.; - background-clip is widely supported: https://caniuse.com/#search=background-clip. ### Before; <img width=""697"" alt=""Screenshot 2019-10-12 14 34 42"" src=""https://user-images.githubusercontent.com/5543229/66706171-960fa700-ecfd-11e9-9fa0-17a05da486a2.png"">. <img width=""701"" alt=""Screenshot 2019-10-12 14 34 53"" src=""https://user-images.githubusercontent.com/5543229/66706173-9740d400-ecfd-11e9-9fa5-5fd4bb23fc54.png"">. ### After. <img width=""700"" alt=""Screenshot 2019-10-12 14 35 03"" src",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7288
https://github.com/hail-is/hail/issues/7298:71,Deployability,install,install,71,"Hello? Anyone who can help for Hail 0.2 on Azure DataBrick?. After pip install lots of problems came out.... can't find Java Package , import hail.plot , hl.init(). According to document. https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html#create-a-hail-cluster. I've pip install hail. set ENABLE_HAIL=true in Cluster Environment Setting. However. import hail as hl; hl.init(sc, idempotent=True); ; AttributeError: module 'hail' has no attribute 'init'. Also another document. https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html. import hail as hl; import hail.expr.aggregators as agg; hl.init(sc, idempotent=True). ModuleNotFoundError: No module named 'hail.expr'. Anyone can give a solution?; Thanks a lot !!!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7298
https://github.com/hail-is/hail/issues/7298:294,Deployability,install,install,294,"Hello? Anyone who can help for Hail 0.2 on Azure DataBrick?. After pip install lots of problems came out.... can't find Java Package , import hail.plot , hl.init(). According to document. https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html#create-a-hail-cluster. I've pip install hail. set ENABLE_HAIL=true in Cluster Environment Setting. However. import hail as hl; hl.init(sc, idempotent=True); ; AttributeError: module 'hail' has no attribute 'init'. Also another document. https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html. import hail as hl; import hail.expr.aggregators as agg; hl.init(sc, idempotent=True). ModuleNotFoundError: No module named 'hail.expr'. Anyone can give a solution?; Thanks a lot !!!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7298
https://github.com/hail-is/hail/issues/7299:285,Availability,down,downgrade,285,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299
https://github.com/hail-is/hail/issues/7299:462,Availability,down,downgrades,462,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299
https://github.com/hail-is/hail/issues/7299:17,Deployability,install,installing,17,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299
https://github.com/hail-is/hail/issues/7299:268,Deployability,install,install,268,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299
https://github.com/hail-is/hail/issues/7299:76,Integrability,depend,dependencies,76,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299
https://github.com/hail-is/hail/issues/7299:369,Integrability,depend,dependencies,369,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299
https://github.com/hail-is/hail/issues/7299:520,Integrability,depend,dependencies,520,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299
https://github.com/hail-is/hail/pull/7300:66,Testability,assert,assert,66,"This might give us a bit more insight into the 500. However, this assert shouldn't fail because we're querying on the primary key which is necessarily unique.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7300
https://github.com/hail-is/hail/pull/7301:112,Deployability,update,update,112,"In response to #7299. It would be nice to get to a more automated way of doing this, but for now we should just update this assuming it passes all tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7301
https://github.com/hail-is/hail/pull/7301:147,Testability,test,tests,147,"In response to #7299. It would be nice to get to a more automated way of doing this, but for now we should just update this assuming it passes all tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7301
https://github.com/hail-is/hail/pull/7310:139,Deployability,update,updated,139,The jinja2 templates got moved from batch2/templates to batch2/front_end/templates but the manifest file to copy the template files wasn't updated.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7310
https://github.com/hail-is/hail/pull/7313:243,Deployability,update,update,243,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:190,Integrability,interface,interface,190,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:265,Integrability,rout,route,265,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:866,Integrability,depend,dependence,866,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:945,Integrability,rout,routine,945,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:86,Security,validat,validator,86,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:709,Security,validat,validator,709,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:1041,Security,validat,validator,1041,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:1083,Security,validat,validation,1083,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:1028,Testability,benchmark,benchmark,1028,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7313:581,Usability,clear,clearer,581,"Changes:; - create a custom job spec schema for what a job means to us; - hand-rolled validator; - use in bath_client, /jobs/create endpoints in batch, batch2; - slightly changed create_job interface around volumes, docker socket and secrets, update usage; - wrote route to convert this to a k8s pod spec, use when actually creating jobs. The secret has a namespace, but it is ignored by the servers. Eventually, batch should be able to pull secrets from wherever, but needs to enforce permissions on who can use what secrets. This was a long-standing issue that I think now has a clearer path. We can get rid of the mount docker socket option by making the worker support a build (rather than run) task. The validator should really go in the server code, but it needs to be shared between batch and batch2 for now. Plan is to push this through batch2 to remove the dependence on the k8s pod serialization. When that's done, the job to pod spec routine can go into batch (and go away when CI uses batch2). Will be interested to benchmark my validator vs. the previous cerberus + k8s validation/serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7313
https://github.com/hail-is/hail/pull/7319:311,Testability,test,test,311,"Resolves #6957 . Basically none of the NDArray stuff currently supports missingess, so this is the beginning of adding that support. The IR nodes that rely on the `NDArrayEmitter` are more complicated, but here's two easy ones to start. This fixes MakeNDArray and NDArrayShape's handling of missingness, with a test for each. It also adds some test for NDArrayRef with missingess, which was already handled but not tested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7319
https://github.com/hail-is/hail/pull/7319:344,Testability,test,test,344,"Resolves #6957 . Basically none of the NDArray stuff currently supports missingess, so this is the beginning of adding that support. The IR nodes that rely on the `NDArrayEmitter` are more complicated, but here's two easy ones to start. This fixes MakeNDArray and NDArrayShape's handling of missingness, with a test for each. It also adds some test for NDArrayRef with missingess, which was already handled but not tested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7319
https://github.com/hail-is/hail/pull/7319:415,Testability,test,tested,415,"Resolves #6957 . Basically none of the NDArray stuff currently supports missingess, so this is the beginning of adding that support. The IR nodes that rely on the `NDArrayEmitter` are more complicated, but here's two easy ones to start. This fixes MakeNDArray and NDArrayShape's handling of missingness, with a test for each. It also adds some test for NDArrayRef with missingess, which was already handled but not tested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7319
https://github.com/hail-is/hail/pull/7326:83,Deployability,deploy,deployed,83,on pod not container. Scheduling will be a bit messed up until this goes in (I had deployed to hopefully speed up the testing).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7326
https://github.com/hail-is/hail/pull/7326:22,Energy Efficiency,Schedul,Scheduling,22,on pod not container. Scheduling will be a bit messed up until this goes in (I had deployed to hopefully speed up the testing).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7326
https://github.com/hail-is/hail/pull/7326:118,Testability,test,testing,118,on pod not container. Scheduling will be a bit messed up until this goes in (I had deployed to hopefully speed up the testing).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7326
https://github.com/hail-is/hail/pull/7327:29,Integrability,rout,router-services,29,Add missing batch2-driver to router-services,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7327
https://github.com/hail-is/hail/pull/7331:423,Testability,test,tests,423,"Changes:; - Remove the per-job callback. Nobody is relying on this.; - The batch callback should send batch metadata, not job metadata. This is already what the CI is expecting (I think the only callback user).; - Switch to aiohttp for the callback, not threading/requests. @tpoterba this will speed up merges in the CI by a few minutes. Now the callback is ignored and its the periodic poll loop that notices the finished tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7331
https://github.com/hail-is/hail/pull/7332:334,Testability,test,testing,334,"Changes:; - batch2: send job_spec to worker instead of kubernetes pod spec; - batch2: create containers on worker from job_spec; - move job_spec_to_k8s_pod_spec to batch, only used there now; - simplified volume handling somewhat: Volume now means docker volume, cspec has list of volume mounts. As usual, will probably need a bit of testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7332
https://github.com/hail-is/hail/pull/7332:194,Usability,simpl,simplified,194,"Changes:; - batch2: send job_spec to worker instead of kubernetes pod spec; - batch2: create containers on worker from job_spec; - move job_spec_to_k8s_pod_spec to batch, only used there now; - simplified volume handling somewhat: Volume now means docker volume, cspec has list of volume mounts. As usual, will probably need a bit of testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7332
https://github.com/hail-is/hail/pull/7333:193,Availability,error,errors,193,- made sure the batch client is closed after a pipeline finishes; - fixed cpu and memory to take ints and floats; - Split AsyncWorkerPool into AsyncWorkerPool and AsyncThrottledGather; - Raise errors in AsyncThrottledGather (errors ignored in AsyncWorkerPool),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7333
https://github.com/hail-is/hail/pull/7333:225,Availability,error,errors,225,- made sure the batch client is closed after a pipeline finishes; - fixed cpu and memory to take ints and floats; - Split AsyncWorkerPool into AsyncWorkerPool and AsyncThrottledGather; - Raise errors in AsyncThrottledGather (errors ignored in AsyncWorkerPool),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7333
https://github.com/hail-is/hail/pull/7333:47,Deployability,pipeline,pipeline,47,- made sure the batch client is closed after a pipeline finishes; - fixed cpu and memory to take ints and floats; - Split AsyncWorkerPool into AsyncWorkerPool and AsyncThrottledGather; - Raise errors in AsyncThrottledGather (errors ignored in AsyncWorkerPool),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7333
https://github.com/hail-is/hail/pull/7335:141,Deployability,update,update-hail-version,141,"* Correct spelling of ""decommissioning"" in help for `--graceful-decommission-timeout`.; * Add space between ""match"" and ""the"" in help for `--update-hail-version`.; * Add punctuation to help for `--update-hail-version` for consistency with other arguments.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7335
https://github.com/hail-is/hail/pull/7335:197,Deployability,update,update-hail-version,197,"* Correct spelling of ""decommissioning"" in help for `--graceful-decommission-timeout`.; * Add space between ""match"" and ""the"" in help for `--update-hail-version`.; * Add punctuation to help for `--update-hail-version` for consistency with other arguments.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7335
https://github.com/hail-is/hail/pull/7335:77,Safety,timeout,timeout,77,"* Correct spelling of ""decommissioning"" in help for `--graceful-decommission-timeout`.; * Add space between ""match"" and ""the"" in help for `--update-hail-version`.; * Add punctuation to help for `--update-hail-version` for consistency with other arguments.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7335
https://github.com/hail-is/hail/pull/7340:241,Availability,error,error,241,I'm not sure why my exception didn't get picked up by the aiohttp.ClientOSError block. I added a plain OSError just in case. Feel free to push back on that. It's possible I didn't have updated is_transient_error code when I got the original error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7340
https://github.com/hail-is/hail/pull/7340:185,Deployability,update,updated,185,I'm not sure why my exception didn't get picked up by the aiohttp.ClientOSError block. I added a plain OSError just in case. Feel free to push back on that. It's possible I didn't have updated is_transient_error code when I got the original error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7340
https://github.com/hail-is/hail/pull/7343:0,Testability,test,testing,0,testing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7343
https://github.com/hail-is/hail/pull/7347:291,Modifiability,config,config,291,"- put_on_ready should never be in an ensure_future, but I had to keep it there when used in Pod.create() because I didn't want to block the create/delete pool waiting for the pods to be on the ready queue; otherwise, it should be fixed everywhere else. - I added `Binds: None` in the Docker config as the default because if we specify a HostConfig, then I believe Docker uses a default of {} which might be allocating a volume unnecessarily. I can't find where I read that before. I can double check the performance of the change if you want. I'm pretty sure it helps.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7347
https://github.com/hail-is/hail/pull/7347:199,Performance,queue,queue,199,"- put_on_ready should never be in an ensure_future, but I had to keep it there when used in Pod.create() because I didn't want to block the create/delete pool waiting for the pods to be on the ready queue; otherwise, it should be fixed everywhere else. - I added `Binds: None` in the Docker config as the default because if we specify a HostConfig, then I believe Docker uses a default of {} which might be allocating a volume unnecessarily. I can't find where I read that before. I can double check the performance of the change if you want. I'm pretty sure it helps.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7347
https://github.com/hail-is/hail/pull/7347:504,Performance,perform,performance,504,"- put_on_ready should never be in an ensure_future, but I had to keep it there when used in Pod.create() because I didn't want to block the create/delete pool waiting for the pods to be on the ready queue; otherwise, it should be fixed everywhere else. - I added `Binds: None` in the Docker config as the default because if we specify a HostConfig, then I believe Docker uses a default of {} which might be allocating a volume unnecessarily. I can't find where I read that before. I can double check the performance of the change if you want. I'm pretty sure it helps.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7347
https://github.com/hail-is/hail/pull/7353:15,Testability,test,tested,15,"FYI @cseed . I tested this locally, but we may have to merge it and see if it works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7353
https://github.com/hail-is/hail/pull/7354:460,Availability,down,down,460,"Apologies for the size, this PR got a bit out of hand. Let me know if you want me to try to break it up. Changes:; - Use custom status for pods, stored in pod and job tables as json. See Pod.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:1176,Availability,echo,echo,1176,"d.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:1638,Availability,error,error,1638,"e creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2019-10-22T09:25:42.477556224Z"",; ""finished_at"": ""2019-10-22T09:25:42.476019599Z"",; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.031185626983642578,; ""creating"":",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:1749,Availability,error,error,1749,"port pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2019-10-22T09:25:42.477556224Z"",; ""finished_at"": ""2019-10-22T09:25:42.476019599Z"",; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.031185626983642578,; ""creating"": 0.09947538375854492,; ""starting"": 4.786264657974243,; ""running"": 0.4418",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:1873,Availability,Error,Error,1873,", not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2019-10-22T09:25:42.477556224Z"",; ""finished_at"": ""2019-10-22T09:25:42.476019599Z"",; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.031185626983642578,; ""creating"": 0.09947538375854492,; ""starting"": 4.786264657974243,; ""running"": 0.44185924530029297,; ""runtime"": 5.228753566741943,; ""uploading_log"": 0.22835826873779297,; ""deleting"":",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:1360,Modifiability,config,config,1360,"a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:754,Testability,log,log,754,"Apologies for the size, this PR got a bit out of hand. Let me know if you want me to try to break it up. Changes:; - Use custom status for pods, stored in pod and job tables as json. See Pod.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:789,Testability,log,logs,789,"Apologies for the size, this PR got a bit out of hand. Let me know if you want me to try to break it up. Changes:; - Use custom status for pods, stored in pod and job tables as json. See Pod.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:877,Testability,log,logs,877,"Apologies for the size, this PR got a bit out of hand. Let me know if you want me to try to break it up. Changes:; - Use custom status for pods, stored in pod and job tables as json. See Pod.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:968,Testability,log,log,968,"Apologies for the size, this PR got a bit out of hand. Let me know if you want me to try to break it up. Changes:; - Use custom status for pods, stored in pod and job tables as json. See Pod.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:1119,Testability,log,log,1119,"r pods, stored in pod and job tables as json. See Pod.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:2022,Testability,test,test,2022,"elf took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2019-10-22T09:25:42.477556224Z"",; ""finished_at"": ""2019-10-22T09:25:42.476019599Z"",; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.031185626983642578,; ""creating"": 0.09947538375854492,; ""starting"": 4.786264657974243,; ""running"": 0.44185924530029297,; ""runtime"": 5.228753566741943,; ""uploading_log"": 0.22835826873779297,; ""deleting"": 0.020263195037841797; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2019-10-22T09:25:48.50560598Z"",; ""finished_at"": ""2019-10-22T09:25:48.645404664Z"",; ""exit_code"": 0; }; },; ""cleanup"": {; ""name"": ""cleanup"",; ""st",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7354:473,Usability,clear,clearly,473,"Apologies for the size, this PR got a bit out of hand. Let me know if you want me to try to break it up. Changes:; - Use custom status for pods, stored in pod and job tables as json. See Pod.status and Container.status in worker.py for the format. Example at the end. Note, ""container_statuses"" items have a field ""container_status"", because container is used in two ways: as a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354
https://github.com/hail-is/hail/pull/7355:35,Performance,Cache,Cache,35,* don't use inefficient unapply; * Cache the returnType of AggSignatures so they persist through copies. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_big_aggregate_compilation 85.6% 4.113 3.519; ----------------------; Geometric mean: 85.6%; Simple mean: 85.6%; Median: 85.6%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7355
https://github.com/hail-is/hail/pull/7355:260,Usability,Simpl,Simple,260,* don't use inefficient unapply; * Cache the returnType of AggSignatures so they persist through copies. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_big_aggregate_compilation 85.6% 4.113 3.519; ----------------------; Geometric mean: 85.6%; Simple mean: 85.6%; Median: 85.6%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7355
https://github.com/hail-is/hail/issues/7358:1504,Testability,test,testing,1504,"I want to finish the Hail Tables cheat sheet. These are things that have come up when discussing this. . These are the things I need to make this cheat sheet:; - [x] `to_matrix_table` needs examples. @tpoterba #7404 ; - [x] `to_matrix_table_row_major` needs examples. @danking (https://github.com/hail-is/hail/pull/7375). These are the things we decided in cheat sheet discussions that we ought to have based on pandas. Feel free to argue we don't want these or something, I'm mostly transcribing the zulip discussion with random assignees: ; - [ ] `count_missing` aggregator. Want to know if there's missing data in a column. @iitalics ; - [ ] `count_present` aggregator. The opposite of `count_missing` (assigned both to same person since it should be basically same, if I'm wrong feel free to reassign). @iitalics ; - [ ] `drop_missing`. The pandas version of this is ""drop any rows in the table with missing values"". Not sure if our version should take a specific column or something. One idea is that with no arguments it will drop any row that has any missing field, but it could also take a list of columns to consider. @catoverdrive ; - [ ] `hl.Table.parallelize` is not a good name. Let's just let there be a constructor for tables that allows you to make tables from local data. If possible, it would also be nice if we could do some better type inference here so that users don't have to pass in a type string the way they do in the current example. @akotlar . These are bugs I ran into while testing the cheat sheet:; - [ ] Calling `show()` on an empty table (I accidentally filtered out all the columns when I messed up a regex) throws some internal java index out of bounds exception. @chrisvittal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7358
https://github.com/hail-is/hail/issues/7364:26,Integrability,rout,router,26,To avoid having to modify router/router.nginx.conf.in for internal.hail.is routes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7364
https://github.com/hail-is/hail/issues/7364:33,Integrability,rout,router,33,To avoid having to modify router/router.nginx.conf.in for internal.hail.is routes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7364
https://github.com/hail-is/hail/issues/7364:75,Integrability,rout,routes,75,To avoid having to modify router/router.nginx.conf.in for internal.hail.is routes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7364
https://github.com/hail-is/hail/issues/7364:3,Safety,avoid,avoid,3,To avoid having to modify router/router.nginx.conf.in for internal.hail.is routes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7364
https://github.com/hail-is/hail/pull/7365:169,Integrability,depend,dependency,169,- Also changed VARCHAR fields back to TEXT where it was VARCHAR(65535).; - Added names to containers for ease in identifying when debugging; - Got rid of the google sdk dependency in the docker worker image (saves 400MB),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7365
https://github.com/hail-is/hail/pull/7377:111,Availability,error,error,111,"Currently, if the type of the `default` argument to `dict.get` does not match the dictionary's value type, the error message contains the dictionary's type and the `default` argument's type. However, the `default` argument's type should be compared to the dictionary's **value** type. This can be particularly confusing when dealing with nested dictionaries. For example:; ```python; d = hl.dict({""foo"": {""foo"": 1}}); d.get(""somekey"", hl.dict({""bar"": {""bar"": 2}})); ```; results in:; ```; TypeError: 'get' expects parameter 'default' to have the same type as the dictionary ; value type, found 'dict<str, dict<str, int32>>' and 'dict<str, dict<str, int32>>'; ```. This change puts the value type instead of the dictionary type in the error message and slightly rewords the message to be clearer about which type is which.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7377
https://github.com/hail-is/hail/pull/7377:734,Availability,error,error,734,"Currently, if the type of the `default` argument to `dict.get` does not match the dictionary's value type, the error message contains the dictionary's type and the `default` argument's type. However, the `default` argument's type should be compared to the dictionary's **value** type. This can be particularly confusing when dealing with nested dictionaries. For example:; ```python; d = hl.dict({""foo"": {""foo"": 1}}); d.get(""somekey"", hl.dict({""bar"": {""bar"": 2}})); ```; results in:; ```; TypeError: 'get' expects parameter 'default' to have the same type as the dictionary ; value type, found 'dict<str, dict<str, int32>>' and 'dict<str, dict<str, int32>>'; ```. This change puts the value type instead of the dictionary type in the error message and slightly rewords the message to be clearer about which type is which.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7377
https://github.com/hail-is/hail/pull/7377:117,Integrability,message,message,117,"Currently, if the type of the `default` argument to `dict.get` does not match the dictionary's value type, the error message contains the dictionary's type and the `default` argument's type. However, the `default` argument's type should be compared to the dictionary's **value** type. This can be particularly confusing when dealing with nested dictionaries. For example:; ```python; d = hl.dict({""foo"": {""foo"": 1}}); d.get(""somekey"", hl.dict({""bar"": {""bar"": 2}})); ```; results in:; ```; TypeError: 'get' expects parameter 'default' to have the same type as the dictionary ; value type, found 'dict<str, dict<str, int32>>' and 'dict<str, dict<str, int32>>'; ```. This change puts the value type instead of the dictionary type in the error message and slightly rewords the message to be clearer about which type is which.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7377
https://github.com/hail-is/hail/pull/7377:740,Integrability,message,message,740,"Currently, if the type of the `default` argument to `dict.get` does not match the dictionary's value type, the error message contains the dictionary's type and the `default` argument's type. However, the `default` argument's type should be compared to the dictionary's **value** type. This can be particularly confusing when dealing with nested dictionaries. For example:; ```python; d = hl.dict({""foo"": {""foo"": 1}}); d.get(""somekey"", hl.dict({""bar"": {""bar"": 2}})); ```; results in:; ```; TypeError: 'get' expects parameter 'default' to have the same type as the dictionary ; value type, found 'dict<str, dict<str, int32>>' and 'dict<str, dict<str, int32>>'; ```. This change puts the value type instead of the dictionary type in the error message and slightly rewords the message to be clearer about which type is which.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7377
https://github.com/hail-is/hail/pull/7377:773,Integrability,message,message,773,"Currently, if the type of the `default` argument to `dict.get` does not match the dictionary's value type, the error message contains the dictionary's type and the `default` argument's type. However, the `default` argument's type should be compared to the dictionary's **value** type. This can be particularly confusing when dealing with nested dictionaries. For example:; ```python; d = hl.dict({""foo"": {""foo"": 1}}); d.get(""somekey"", hl.dict({""bar"": {""bar"": 2}})); ```; results in:; ```; TypeError: 'get' expects parameter 'default' to have the same type as the dictionary ; value type, found 'dict<str, dict<str, int32>>' and 'dict<str, dict<str, int32>>'; ```. This change puts the value type instead of the dictionary type in the error message and slightly rewords the message to be clearer about which type is which.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7377
https://github.com/hail-is/hail/pull/7377:787,Usability,clear,clearer,787,"Currently, if the type of the `default` argument to `dict.get` does not match the dictionary's value type, the error message contains the dictionary's type and the `default` argument's type. However, the `default` argument's type should be compared to the dictionary's **value** type. This can be particularly confusing when dealing with nested dictionaries. For example:; ```python; d = hl.dict({""foo"": {""foo"": 1}}); d.get(""somekey"", hl.dict({""bar"": {""bar"": 2}})); ```; results in:; ```; TypeError: 'get' expects parameter 'default' to have the same type as the dictionary ; value type, found 'dict<str, dict<str, int32>>' and 'dict<str, dict<str, int32>>'; ```. This change puts the value type instead of the dictionary type in the error message and slightly rewords the message to be clearer about which type is which.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7377
https://github.com/hail-is/hail/pull/7378:223,Availability,error,error,223,"This will fail until #7376 lands and allows the test to create an empty matrix table. - document `sep`; - add two tests for importing empty matrix tables, one with a header and one without; - include the offending lines in error messages when files have different numbers of columns; - consistently use `String.split(separator, 0)` instead of using two different approaches which yield inconsistent results.; - simplify `parseHeader` and generalize to empty files; - improve error message when a row field found in the file does not match one of the row fields specified by `row_fields` (a dictionary from row field name to type). NB: A no-header empty file implies no columns in the MT. We print a warning to this effect when we discover an empty file. Resolves #7242",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7378
https://github.com/hail-is/hail/pull/7378:475,Availability,error,error,475,"This will fail until #7376 lands and allows the test to create an empty matrix table. - document `sep`; - add two tests for importing empty matrix tables, one with a header and one without; - include the offending lines in error messages when files have different numbers of columns; - consistently use `String.split(separator, 0)` instead of using two different approaches which yield inconsistent results.; - simplify `parseHeader` and generalize to empty files; - improve error message when a row field found in the file does not match one of the row fields specified by `row_fields` (a dictionary from row field name to type). NB: A no-header empty file implies no columns in the MT. We print a warning to this effect when we discover an empty file. Resolves #7242",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7378
https://github.com/hail-is/hail/pull/7378:229,Integrability,message,messages,229,"This will fail until #7376 lands and allows the test to create an empty matrix table. - document `sep`; - add two tests for importing empty matrix tables, one with a header and one without; - include the offending lines in error messages when files have different numbers of columns; - consistently use `String.split(separator, 0)` instead of using two different approaches which yield inconsistent results.; - simplify `parseHeader` and generalize to empty files; - improve error message when a row field found in the file does not match one of the row fields specified by `row_fields` (a dictionary from row field name to type). NB: A no-header empty file implies no columns in the MT. We print a warning to this effect when we discover an empty file. Resolves #7242",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7378
https://github.com/hail-is/hail/pull/7378:481,Integrability,message,message,481,"This will fail until #7376 lands and allows the test to create an empty matrix table. - document `sep`; - add two tests for importing empty matrix tables, one with a header and one without; - include the offending lines in error messages when files have different numbers of columns; - consistently use `String.split(separator, 0)` instead of using two different approaches which yield inconsistent results.; - simplify `parseHeader` and generalize to empty files; - improve error message when a row field found in the file does not match one of the row fields specified by `row_fields` (a dictionary from row field name to type). NB: A no-header empty file implies no columns in the MT. We print a warning to this effect when we discover an empty file. Resolves #7242",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7378
https://github.com/hail-is/hail/pull/7378:48,Testability,test,test,48,"This will fail until #7376 lands and allows the test to create an empty matrix table. - document `sep`; - add two tests for importing empty matrix tables, one with a header and one without; - include the offending lines in error messages when files have different numbers of columns; - consistently use `String.split(separator, 0)` instead of using two different approaches which yield inconsistent results.; - simplify `parseHeader` and generalize to empty files; - improve error message when a row field found in the file does not match one of the row fields specified by `row_fields` (a dictionary from row field name to type). NB: A no-header empty file implies no columns in the MT. We print a warning to this effect when we discover an empty file. Resolves #7242",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7378
https://github.com/hail-is/hail/pull/7378:114,Testability,test,tests,114,"This will fail until #7376 lands and allows the test to create an empty matrix table. - document `sep`; - add two tests for importing empty matrix tables, one with a header and one without; - include the offending lines in error messages when files have different numbers of columns; - consistently use `String.split(separator, 0)` instead of using two different approaches which yield inconsistent results.; - simplify `parseHeader` and generalize to empty files; - improve error message when a row field found in the file does not match one of the row fields specified by `row_fields` (a dictionary from row field name to type). NB: A no-header empty file implies no columns in the MT. We print a warning to this effect when we discover an empty file. Resolves #7242",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7378
https://github.com/hail-is/hail/pull/7378:411,Usability,simpl,simplify,411,"This will fail until #7376 lands and allows the test to create an empty matrix table. - document `sep`; - add two tests for importing empty matrix tables, one with a header and one without; - include the offending lines in error messages when files have different numbers of columns; - consistently use `String.split(separator, 0)` instead of using two different approaches which yield inconsistent results.; - simplify `parseHeader` and generalize to empty files; - improve error message when a row field found in the file does not match one of the row fields specified by `row_fields` (a dictionary from row field name to type). NB: A no-header empty file implies no columns in the MT. We print a warning to this effect when we discover an empty file. Resolves #7242",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7378
https://github.com/hail-is/hail/pull/7381:5,Deployability,deploy,deploys,5,this deploys a standard ghost installation to the default namespace and routes to it from blog.hail.is; can be customized once deployed. This won't pass tests until #7413 is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381
https://github.com/hail-is/hail/pull/7381:30,Deployability,install,installation,30,this deploys a standard ghost installation to the default namespace and routes to it from blog.hail.is; can be customized once deployed. This won't pass tests until #7413 is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381
https://github.com/hail-is/hail/pull/7381:127,Deployability,deploy,deployed,127,this deploys a standard ghost installation to the default namespace and routes to it from blog.hail.is; can be customized once deployed. This won't pass tests until #7413 is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381
https://github.com/hail-is/hail/pull/7381:72,Integrability,rout,routes,72,this deploys a standard ghost installation to the default namespace and routes to it from blog.hail.is; can be customized once deployed. This won't pass tests until #7413 is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381
https://github.com/hail-is/hail/pull/7381:153,Testability,test,tests,153,this deploys a standard ghost installation to the default namespace and routes to it from blog.hail.is; can be customized once deployed. This won't pass tests until #7413 is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381
https://github.com/hail-is/hail/issues/7384:66,Availability,error,errors,66,"Calling `fb.result()` generates stalled execution, no warnings or errors raised. Should generate some error message, or potentially the result (memory address). cc @catoverdrive . Test case. ```scala; def testString() {; val rt = PString(); val input = ""hello""; val fb = FunctionBuilder.functionBuilder[Region, String, Long]; val srvb = new StagedRegionValueBuilder(fb, rt). fb.emit(; Code(; srvb.start(),; srvb.addString(fb.getArg[String](2)),; srvb.end(); ); ). val region = Region(); val rv = RegionValue(region). val res1 = fb.result()()(region, input); println(""Past res1""). val res2 = fb.result()()(region, input); // never reached; println(""Past res2""); }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384
https://github.com/hail-is/hail/issues/7384:102,Availability,error,error,102,"Calling `fb.result()` generates stalled execution, no warnings or errors raised. Should generate some error message, or potentially the result (memory address). cc @catoverdrive . Test case. ```scala; def testString() {; val rt = PString(); val input = ""hello""; val fb = FunctionBuilder.functionBuilder[Region, String, Long]; val srvb = new StagedRegionValueBuilder(fb, rt). fb.emit(; Code(; srvb.start(),; srvb.addString(fb.getArg[String](2)),; srvb.end(); ); ). val region = Region(); val rv = RegionValue(region). val res1 = fb.result()()(region, input); println(""Past res1""). val res2 = fb.result()()(region, input); // never reached; println(""Past res2""); }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384
https://github.com/hail-is/hail/issues/7384:108,Integrability,message,message,108,"Calling `fb.result()` generates stalled execution, no warnings or errors raised. Should generate some error message, or potentially the result (memory address). cc @catoverdrive . Test case. ```scala; def testString() {; val rt = PString(); val input = ""hello""; val fb = FunctionBuilder.functionBuilder[Region, String, Long]; val srvb = new StagedRegionValueBuilder(fb, rt). fb.emit(; Code(; srvb.start(),; srvb.addString(fb.getArg[String](2)),; srvb.end(); ); ). val region = Region(); val rv = RegionValue(region). val res1 = fb.result()()(region, input); println(""Past res1""). val res2 = fb.result()()(region, input); // never reached; println(""Past res2""); }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384
https://github.com/hail-is/hail/issues/7384:180,Testability,Test,Test,180,"Calling `fb.result()` generates stalled execution, no warnings or errors raised. Should generate some error message, or potentially the result (memory address). cc @catoverdrive . Test case. ```scala; def testString() {; val rt = PString(); val input = ""hello""; val fb = FunctionBuilder.functionBuilder[Region, String, Long]; val srvb = new StagedRegionValueBuilder(fb, rt). fb.emit(; Code(; srvb.start(),; srvb.addString(fb.getArg[String](2)),; srvb.end(); ); ). val region = Region(); val rv = RegionValue(region). val res1 = fb.result()()(region, input); println(""Past res1""). val res2 = fb.result()()(region, input); // never reached; println(""Past res2""); }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384
https://github.com/hail-is/hail/issues/7384:205,Testability,test,testString,205,"Calling `fb.result()` generates stalled execution, no warnings or errors raised. Should generate some error message, or potentially the result (memory address). cc @catoverdrive . Test case. ```scala; def testString() {; val rt = PString(); val input = ""hello""; val fb = FunctionBuilder.functionBuilder[Region, String, Long]; val srvb = new StagedRegionValueBuilder(fb, rt). fb.emit(; Code(; srvb.start(),; srvb.addString(fb.getArg[String](2)),; srvb.end(); ); ). val region = Region(); val rv = RegionValue(region). val res1 = fb.result()()(region, input); println(""Past res1""). val res2 = fb.result()()(region, input); // never reached; println(""Past res2""); }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384
https://github.com/hail-is/hail/pull/7385:622,Safety,timeout,timeout,622,"The actual necessary fix is `history.pushState("""", document.title, loc.pathname + loc.search);` instead of `history.pushState("""", document.title, loc.pathname);`. Given the nervousness around JS and its use in docs (many imperative scripts interacting), I've decided to take a simpler approach that guarantees that Firefox will have behavior inconsistent with Chrome/Safari. In practice it isn't so bad (initially scrolls too far instead of starting at top of page). There may also be a chance that the browser's built-in scroll will act after the JS command, but there simply isn't a better way (besides setting a longer timeout), or the version that alters the url.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7385
https://github.com/hail-is/hail/pull/7385:277,Usability,simpl,simpler,277,"The actual necessary fix is `history.pushState("""", document.title, loc.pathname + loc.search);` instead of `history.pushState("""", document.title, loc.pathname);`. Given the nervousness around JS and its use in docs (many imperative scripts interacting), I've decided to take a simpler approach that guarantees that Firefox will have behavior inconsistent with Chrome/Safari. In practice it isn't so bad (initially scrolls too far instead of starting at top of page). There may also be a chance that the browser's built-in scroll will act after the JS command, but there simply isn't a better way (besides setting a longer timeout), or the version that alters the url.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7385
https://github.com/hail-is/hail/pull/7385:570,Usability,simpl,simply,570,"The actual necessary fix is `history.pushState("""", document.title, loc.pathname + loc.search);` instead of `history.pushState("""", document.title, loc.pathname);`. Given the nervousness around JS and its use in docs (many imperative scripts interacting), I've decided to take a simpler approach that guarantees that Firefox will have behavior inconsistent with Chrome/Safari. In practice it isn't so bad (initially scrolls too far instead of starting at top of page). There may also be a chance that the browser's built-in scroll will act after the JS command, but there simply isn't a better way (besides setting a longer timeout), or the version that alters the url.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7385
https://github.com/hail-is/hail/pull/7386:554,Modifiability,refactor,refactored,554,"closes #7357. * implements `.tail` function on tables:. ```python; def tail(self, n) -> 'Table':; """"""Subset table to last `n` rows. Examples; --------; Subset to the last three rows:. >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; -----. The number of partitions in the new table is equal to the number of; partitions containing the last `n` rows. Parameters; ----------; n : int; Number of rows to include. Returns; -------; :class:`.Table`; Table including the last `n` rows.; """""". return Table(TableTail(self._tir, n)); ```. * refactored some of the logic in `TableHead`, because a lot of the behavior is the same. * specifically, moved some partition-counts calculations to `is.hail.utils.PartitionCounts`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7386
https://github.com/hail-is/hail/pull/7386:577,Testability,log,logic,577,"closes #7357. * implements `.tail` function on tables:. ```python; def tail(self, n) -> 'Table':; """"""Subset table to last `n` rows. Examples; --------; Subset to the last three rows:. >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; -----. The number of partitions in the new table is equal to the number of; partitions containing the last `n` rows. Parameters; ----------; n : int; Number of rows to include. Returns; -------; :class:`.Table`; Table including the last `n` rows.; """""". return Table(TableTail(self._tir, n)); ```. * refactored some of the logic in `TableHead`, because a lot of the behavior is the same. * specifically, moved some partition-counts calculations to `is.hail.utils.PartitionCounts`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7386
https://github.com/hail-is/hail/pull/7391:229,Testability,test,tests,229,"I created the `hl._nd` module, and gave it 4 functions:. 1. `hl._nd.array`: This is an alias for the current `hl._ndarray`. Once this PR goes in, there will be a follow up PR that pulls out this old function and fixes up all the tests. . 2. `hl._nd.full`: Same as `np.full`. Takes two arguments. First argument is the shape, second argument is the argument to fill the ndarray with. . 3. `hl._nd.zeros`: Same as `np.zeros`. Make array full of 0s. . 4. `hl._nd.ones`: Same as `np.ones`. Make array full of 1s. . One weird thing is it seems like doing `hl.nd` also works, in addition to `hl._nd`. Not sure why that is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7391
https://github.com/hail-is/hail/issues/7392:169,Availability,error,errors,169,"cc @cseed . Is this something you would be interested in having back in the codebase? Konrad and Alicia would like it, and it could be helpful for debugging, since Hail errors (say stuck jobs) often require the log, and users have to remember to persist those logs and send them to us.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7392
https://github.com/hail-is/hail/issues/7392:211,Testability,log,log,211,"cc @cseed . Is this something you would be interested in having back in the codebase? Konrad and Alicia would like it, and it could be helpful for debugging, since Hail errors (say stuck jobs) often require the log, and users have to remember to persist those logs and send them to us.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7392
https://github.com/hail-is/hail/issues/7392:260,Testability,log,logs,260,"cc @cseed . Is this something you would be interested in having back in the codebase? Konrad and Alicia would like it, and it could be helpful for debugging, since Hail errors (say stuck jobs) often require the log, and users have to remember to persist those logs and send them to us.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7392
https://github.com/hail-is/hail/issues/7393:64,Deployability,release,release,64,easy to replicate:; ```; ht = hl.read_table('gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht'); ht.explode(ht.vep.transcript_consequences).show(); ```; [hail-20191028-1120.log](https://github.com/hail-is/hail/files/3779402/hail-20191028-1120.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7393
https://github.com/hail-is/hail/issues/7393:195,Testability,log,log,195,easy to replicate:; ```; ht = hl.read_table('gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht'); ht.explode(ht.vep.transcript_consequences).show(); ```; [hail-20191028-1120.log](https://github.com/hail-is/hail/files/3779402/hail-20191028-1120.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7393
https://github.com/hail-is/hail/issues/7393:265,Testability,log,log,265,easy to replicate:; ```; ht = hl.read_table('gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht'); ht.explode(ht.vep.transcript_consequences).show(); ```; [hail-20191028-1120.log](https://github.com/hail-is/hail/files/3779402/hail-20191028-1120.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7393
https://github.com/hail-is/hail/pull/7394:524,Integrability,interface,interface,524,"~~stacked on #7386~~. * implements `.tail` method on `MatrixTable`. ```python; def tail(self, n: Optional[int], n_cols: Optional[int] = None) -> 'MatrixTable':; """"""Subset matrix to last `n` rows. .... Parameters; ----------; n : :obj:`int`; Number of rows to include (all rows included if ``None``).; n_cols : :obj:`int`, optional; Number of cols to include (all cols included if ``None``).; Returns; -------; :class:`.MatrixTable`; Matrix including the last `n` rows and last `n_cols` cols.; """"""; ```. question: should the interface have the same backwards compatibility naming issue as `.head`, to preserve consistency? or should the keyword arguments be `n_cols` and `n_rows`?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7394
https://github.com/hail-is/hail/issues/7396:205,Availability,error,error,205,to reproduce:. ```; import hail as hl; t = hl.utils.range_table(10); t = t.annotate(**{f'f{i}': 0 for i in range(1300)}); t.write('foo.ht'); hl.read_table('foo.ht')._force_count(); ```. MethodCodeTooLarge error is thrown inside the decoder.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7396
https://github.com/hail-is/hail/pull/7397:90,Deployability,update,update,90,This is necessary for the user account to execute (call) stored procedures. We'll have to update batch2/deployed users by hand.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7397
https://github.com/hail-is/hail/pull/7397:104,Deployability,deploy,deployed,104,This is necessary for the user account to execute (call) stored procedures. We'll have to update batch2/deployed users by hand.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7397
https://github.com/hail-is/hail/pull/7399:11,Availability,error,error,11,"Fixes this error message in logs:. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 302, in batch_callback; await asyncio.shield(batch_callback_handler(request)); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 276, in batch_callback_handler; await wb.notify_batch_changed(); TypeError: notify_batch_changed() missing 1 required positional argument: 'app'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7399
https://github.com/hail-is/hail/pull/7399:17,Integrability,message,message,17,"Fixes this error message in logs:. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 302, in batch_callback; await asyncio.shield(batch_callback_handler(request)); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 276, in batch_callback_handler; await wb.notify_batch_changed(); TypeError: notify_batch_changed() missing 1 required positional argument: 'app'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7399
https://github.com/hail-is/hail/pull/7399:28,Testability,log,logs,28,"Fixes this error message in logs:. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 302, in batch_callback; await asyncio.shield(batch_callback_handler(request)); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 276, in batch_callback_handler; await wb.notify_batch_changed(); TypeError: notify_batch_changed() missing 1 required positional argument: 'app'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7399
https://github.com/hail-is/hail/issues/7400:38,Testability,log,logs,38,"Should be simple, run this library on logs before serving them:. https://github.com/ralphbean/ansi2html",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7400
https://github.com/hail-is/hail/issues/7400:10,Usability,simpl,simple,10,"Should be simple, run this library on logs before serving them:. https://github.com/ralphbean/ansi2html",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7400
https://github.com/hail-is/hail/pull/7407:0,Testability,test,testing,0,testing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7407
https://github.com/hail-is/hail/issues/7408:27,Testability,test,test,27,some version of `hl.float('test')` that returns `None` if it can't be coerced (rather than throw `NumberFormatException`) would be super useful. or a regex check could also accomplish this (with a `case`).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7408
https://github.com/hail-is/hail/issues/7410:424,Availability,ERROR,ERROR,424,"From Akhil:. ```; Traceback (most recent call last):; File ""/tmp/e9b3538af9b14d6ba440edbdf89e1ef1/phewas_rvas_jhs_unrel.py"", line 55, in <module>; print(mt.describe()); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 85, in describe; rowstr = ""\nRows: \n"" + ""\n "".join([""{}: {}"".format(k, v._type) for k, v in self._row_keys.items()]); AttributeError: 'list' object has no attribute 'items'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [e9b3538af9b14d6ba440edbdf89e1ef1] failed with error:; Job failed with message [AttributeError: 'list' object has no attribute 'items']. Additional details can be found in 'gs://dataproc-37b10a9c-3ebd-47a4-9228-bf38bdfba439-us/google-cloud-dataproc-metainfo/e432509c-1bea-4839-9cb8-4d404d195a35/jobs/e9b3538af9b14d6ba440edbdf89e1ef1/driveroutput'.; ```; ```; mt = hl.read_matrix_table('gs://jhs_data_topmed/HCLOF_JHS_AF_01_unrel.mt'); mt.describe(); print(mt.count()); mt = mt.annotate_rows(genes = mt.vep.transcript_consequences.gene_symbol); mt = mt.group_rows_by(mt.genes); print(mt.describe()); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7410
https://github.com/hail-is/hail/issues/7410:520,Availability,error,error,520,"From Akhil:. ```; Traceback (most recent call last):; File ""/tmp/e9b3538af9b14d6ba440edbdf89e1ef1/phewas_rvas_jhs_unrel.py"", line 55, in <module>; print(mt.describe()); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 85, in describe; rowstr = ""\nRows: \n"" + ""\n "".join([""{}: {}"".format(k, v._type) for k, v in self._row_keys.items()]); AttributeError: 'list' object has no attribute 'items'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [e9b3538af9b14d6ba440edbdf89e1ef1] failed with error:; Job failed with message [AttributeError: 'list' object has no attribute 'items']. Additional details can be found in 'gs://dataproc-37b10a9c-3ebd-47a4-9228-bf38bdfba439-us/google-cloud-dataproc-metainfo/e432509c-1bea-4839-9cb8-4d404d195a35/jobs/e9b3538af9b14d6ba440edbdf89e1ef1/driveroutput'.; ```; ```; mt = hl.read_matrix_table('gs://jhs_data_topmed/HCLOF_JHS_AF_01_unrel.mt'); mt.describe(); print(mt.count()); mt = mt.annotate_rows(genes = mt.vep.transcript_consequences.gene_symbol); mt = mt.group_rows_by(mt.genes); print(mt.describe()); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7410
https://github.com/hail-is/hail/issues/7410:544,Integrability,message,message,544,"From Akhil:. ```; Traceback (most recent call last):; File ""/tmp/e9b3538af9b14d6ba440edbdf89e1ef1/phewas_rvas_jhs_unrel.py"", line 55, in <module>; print(mt.describe()); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 85, in describe; rowstr = ""\nRows: \n"" + ""\n "".join([""{}: {}"".format(k, v._type) for k, v in self._row_keys.items()]); AttributeError: 'list' object has no attribute 'items'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [e9b3538af9b14d6ba440edbdf89e1ef1] failed with error:; Job failed with message [AttributeError: 'list' object has no attribute 'items']. Additional details can be found in 'gs://dataproc-37b10a9c-3ebd-47a4-9228-bf38bdfba439-us/google-cloud-dataproc-metainfo/e432509c-1bea-4839-9cb8-4d404d195a35/jobs/e9b3538af9b14d6ba440edbdf89e1ef1/driveroutput'.; ```; ```; mt = hl.read_matrix_table('gs://jhs_data_topmed/HCLOF_JHS_AF_01_unrel.mt'); mt.describe(); print(mt.count()); mt = mt.annotate_rows(genes = mt.vep.transcript_consequences.gene_symbol); mt = mt.group_rows_by(mt.genes); print(mt.describe()); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7410
https://github.com/hail-is/hail/pull/7412:98,Availability,error,errors,98,"@tpoterba tried to run some jobs on the batch2 instance in the default namespace. He ran into two errors when trying to get log files (one while the job was running and the other when it terminated):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File ""/usr/local/lib/python3.6/dist-packages/gear/auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 49, in _read_logs; return await self.app['driver'].read_pod_logs(self._pod_name); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 160, in __getitem__; return self._state[key]; KeyError: 'driver'; ```. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 636, in download_to_file; self._do_download(transport, file_obj, download_url, headers, start, end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 574, in _do_download; download.consume(transport); File ""/usr/local/lib/python3.6/dist-pac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:1933,Availability,down,download,1933,"auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 49, in _read_logs; return await self.app['driver'].read_pod_logs(self._pod_name); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 160, in __getitem__; return self._state[key]; KeyError: 'driver'; ```. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 636, in download_to_file; self._do_download(transport, file_obj, download_url, headers, start, end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 574, in _do_download; download.consume(transport); File ""/usr/local/lib/python3.6/dist-packages/google/resumable_media/requests/download.py"", line 171, in consume; self._process_response(result); File ""/usr/local/lib/python3.6/dist-packages/google/resumable_media/_download.py"", line 171, in _process_response; response, _ACCEPTABLE_STATUS_CODES, self._get_status_code; File ""/usr/local/lib/python3.6/dist-packages/google/resumable_media/_helpers.py"", line 96, in require_status_code; *status_codes; google.resumable_media.common.InvalidResponse: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:2039,Availability,down,download,2039,"cal/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 49, in _read_logs; return await self.app['driver'].read_pod_logs(self._pod_name); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 160, in __getitem__; return self._state[key]; KeyError: 'driver'; ```. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 636, in download_to_file; self._do_download(transport, file_obj, download_url, headers, start, end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 574, in _do_download; download.consume(transport); File ""/usr/local/lib/python3.6/dist-packages/google/resumable_media/requests/download.py"", line 171, in consume; self._process_response(result); File ""/usr/local/lib/python3.6/dist-packages/google/resumable_media/_download.py"", line 171, in _process_response; response, _ACCEPTABLE_STATUS_CODES, self._get_status_code; File ""/usr/local/lib/python3.6/dist-packages/google/resumable_media/_helpers.py"", line 96, in require_status_code; *status_codes; google.resumable_media.common.InvalidResponse: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await hand",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:5608,Availability,down,download,5608,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:5855,Availability,error,error,5855,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:5900,Availability,avail,available,5900,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:849,Integrability,wrap,wrapped,849,"@tpoterba tried to run some jobs on the batch2 instance in the default namespace. He ran into two errors when trying to get log files (one while the job was running and the other when it terminated):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File ""/usr/local/lib/python3.6/dist-packages/gear/auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 49, in _read_logs; return await self.app['driver'].read_pod_logs(self._pod_name); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 160, in __getitem__; return self._state[key]; KeyError: 'driver'; ```. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 636, in download_to_file; self._do_download(transport, file_obj, download_url, headers, start, end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 574, in _do_download; download.consume(transport); File ""/usr/local/lib/python3.6/dist-pac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:943,Integrability,wrap,wrapped,943,"@tpoterba tried to run some jobs on the batch2 instance in the default namespace. He ran into two errors when trying to get log files (one while the job was running and the other when it terminated):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File ""/usr/local/lib/python3.6/dist-packages/gear/auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 49, in _read_logs; return await self.app['driver'].read_pod_logs(self._pod_name); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 160, in __getitem__; return self._state[key]; KeyError: 'driver'; ```. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 636, in download_to_file; self._do_download(transport, file_obj, download_url, headers, start, end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 574, in _do_download; download.consume(transport); File ""/usr/local/lib/python3.6/dist-pac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:3288,Integrability,wrap,wrapped,3288,"/local/lib/python3.6/dist-packages/google/resumable_media/_helpers.py"", line 96, in require_status_code; *status_codes; google.resumable_media.common.InvalidResponse: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File ""/usr/local/lib/python3.6/dist-packages/gear/auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 57, in _read_logs; return {k: v for k, v in await future_logs}; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 52, in _read_log_from_gcs; pod_log = await self.app['log_store'].read_gs_file(LogStore.container_log_path(self.directory, task_name)); File ""/usr/local/lib/python3.6/dist-packages/batch/log_store.py"", line 40, in read_gs_file; return await self.gcs.read_gs_file(uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:3382,Integrability,wrap,wrapped,3382,"esumable_media.common.InvalidResponse: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File ""/usr/local/lib/python3.6/dist-packages/gear/auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 57, in _read_logs; return {k: v for k, v in await future_logs}; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 52, in _read_log_from_gcs; pod_log = await self.app['log_store'].read_gs_file(LogStore.container_log_path(self.directory, task_name)); File ""/usr/local/lib/python3.6/dist-packages/batch/log_store.py"", line 40, in read_gs_file; return await self.gcs.read_gs_file(uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 27, in read_gs_file; return await self._wrapped_read_gs_file(self, uri); File ""/usr/local/lib/python3.6/dist-packages/batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:4453,Integrability,wrap,wrapped,4453,"hon3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 57, in _read_logs; return {k: v for k, v in await future_logs}; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 52, in _read_log_from_gcs; pod_log = await self.app['log_store'].read_gs_file(LogStore.container_log_path(self.directory, task_name)); File ""/usr/local/lib/python3.6/dist-packages/batch/log_store.py"", line 40, in read_gs_file; return await self.gcs.read_gs_file(uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 27, in read_gs_file; return await self._wrapped_read_gs_file(self, uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 37, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.fr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:5506,Integrability,message,message,5506,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:4643,Performance,concurren,concurrent,4643,"ackages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 57, in _read_logs; return {k: v for k, v in await future_logs}; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 52, in _read_log_from_gcs; pod_log = await self.app['log_store'].read_gs_file(LogStore.container_log_path(self.directory, task_name)); File ""/usr/local/lib/python3.6/dist-packages/batch/log_store.py"", line 40, in read_gs_file; return await self.gcs.read_gs_file(uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 27, in read_gs_file; return await self._wrapped_read_gs_file(self, uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 37, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:6420,Performance,load,loading,6420,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:124,Testability,log,log,124,"@tpoterba tried to run some jobs on the batch2 instance in the default namespace. He ran into two errors when trying to get log files (one while the job was running and the other when it terminated):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File ""/usr/local/lib/python3.6/dist-packages/gear/auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 49, in _read_logs; return await self.app['driver'].read_pod_logs(self._pod_name); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 160, in __getitem__; return self._state[key]; KeyError: 'driver'; ```. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 636, in download_to_file; self._do_download(transport, file_obj, download_url, headers, start, end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 574, in _do_download; download.consume(transport); File ""/usr/local/lib/python3.6/dist-pac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:4031,Testability,Log,LogStore,4031,"st); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/prometheus_async/aio/_decorators.py"", line 42, in time_decorator; rv = await wrapped(*args, **kw); File ""/usr/local/lib/python3.6/dist-packages/gear/auth.py"", line 86, in wrapped; return await fun(request, userdata, *args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 383, in ui_get_job_log; 'job_log': await _get_job_log(request.app, batch_id, job_id, user); File ""/usr/local/lib/python3.6/dist-packages/batch/front_end/front_end.py"", line 112, in _get_job_log; job_log = await job._read_logs(); File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 57, in _read_logs; return {k: v for k, v in await future_logs}; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 52, in _read_log_from_gcs; pod_log = await self.app['log_store'].read_gs_file(LogStore.container_log_path(self.directory, task_name)); File ""/usr/local/lib/python3.6/dist-packages/batch/log_store.py"", line 40, in read_gs_file; return await self.gcs.read_gs_file(uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 27, in read_gs_file; return await self._wrapped_read_gs_file(self, uri); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 37, in wrapped; **kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/pyth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:5705,Testability,log,log,5705,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/pull/7412:6000,Testability,log,logs,6000,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7412
https://github.com/hail-is/hail/issues/7417:78,Integrability,depend,dependency,78,A stacked label appears to prevent a child from merging even after its parent/dependency is merged. This shouldn't happen: once the dependency is merged the stacked label effectively becomes a noop.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417
https://github.com/hail-is/hail/issues/7417:132,Integrability,depend,dependency,132,A stacked label appears to prevent a child from merging even after its parent/dependency is merged. This shouldn't happen: once the dependency is merged the stacked label effectively becomes a noop.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417
https://github.com/hail-is/hail/issues/7419:14,Usability,clear,clear,14,"We don't have clear issues now, but an open question about what these differences are, and whether they're user-facing. cc @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7419
https://github.com/hail-is/hail/pull/7420:489,Availability,failure,failure,489,"> +1,738 -2,452. Sorry :-(. High level summary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirror",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:812,Availability,down,down,812,"> +1,738 -2,452. Sorry :-(. High level summary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirror",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1598,Availability,failure,failure,1598,"case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:2137,Deployability,update,updated,2137,"orker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me know if you have any questions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1628,Energy Efficiency,Schedul,Scheduler,1628,"orker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me know if you have any questions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1639,Energy Efficiency,Schedul,Scheduler,1639,"orker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me know if you have any questions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1675,Energy Efficiency,schedul,schedules,1675,"orker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me know if you have any questions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1866,Energy Efficiency,schedul,scheduled,1866,"orker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me know if you have any questions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1925,Energy Efficiency,schedul,scheduled,1925,"orker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me know if you have any questions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:2182,Energy Efficiency,schedul,scheduled,2182,"orker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me know if you have any questions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:791,Integrability,interface,interface,791,"> +1,738 -2,452. Sorry :-(. High level summary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirror",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1266,Security,secur,securing,1266,"ds. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:428,Testability,log,logging,428,"> +1,738 -2,452. Sorry :-(. High level summary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirror",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1040,Testability,log,log,1040,"ummary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Ad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:1044,Testability,test,test,1044,"ummary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Ad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:2546,Testability,test,tests,2546,"orker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirrored in memory as Instance objects.; - Added a ready_cores table with a single row that has the total core count of the ready jobs. It is updated by the stored procedures as jobs are scheduled/unscheduled/marked complete. It is used by the instance pool control loop. This is great, and something we couldn't easily see before. Things that got removed that I will add back in the next PRs:; - Database retries; - Instance pool heal loop; - Instance health; - attempt tokens (from the Google backend). These are all pretty easy. Then back to scale tests. Let me know if you have any questions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:387,Usability,Simpl,Simplified,387,"> +1,738 -2,452. Sorry :-(. High level summary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirror",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7420:770,Usability,simpl,simple,770,"> +1,738 -2,452. Sorry :-(. High level summary changes:; - batch and batch2: Removed /batch endpoint limit and offset options, added include_jobs. Limit was ever only 0 or None. Going forward, /batch should never return jobs, and /jobs endpoint should always paginate.; - Got rid of pods. There is no pod_status options on the client, but they are left in for legacy reasons (batch).; - Simplified worker startup and cleaned up logging paths. The old code could restart the worker after a failure after it had activated. I don't think we're equipped for that case. I do explicitly pull the worker image (with one retry) before trying to run it.; - Batch and Job are gone. database.py is effectively gone. Almost everywhere interacts directly with the database using the simple gear.Database interface, and drops down to aiomysql directly when that is insufficient (e.g. transaction with multiple executemany for /jobs/create). When we pass around data representing a job or batch, it's normally a data record (a dict).; - Added the running log test from your PR.; - The job status is no longer written to a file, just in the database jobs.status.; - I moved the INSTANCE_ID to the database. There is now a table called tokens. It has the instance id and a token for securing communication between the front end and the driver (currently unused).; - Operations that need to be atomic in the database are now implemented as stored procedures which can be called with the check_call_procedure helper in database.py. They return a row with a field rc (return code) that is 0 on success and non-zero on failure.; - Renamed Driver => Scheduler. Scheduler has two threads, one that schedules jobs that are in the Ready state, and one that cancels cancelled jobs in the Running state. There is a new job state Ready. A job is Ready if its parents are complete and it is not scheduled (instance_id is null). A job is Running if it is scheduled (instance_id is not null).; - The full set of instances are mirror",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7420
https://github.com/hail-is/hail/pull/7425:109,Availability,toler,tolerate,109,- add priority class infrastructure throughout; - all pod specs have resource requests and limits; - make es tolerate preemptibles; - make monitoring router tolerate preemptibles. Deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7425
https://github.com/hail-is/hail/pull/7425:157,Availability,toler,tolerate,157,- add priority class infrastructure throughout; - all pod specs have resource requests and limits; - make es tolerate preemptibles; - make monitoring router tolerate preemptibles. Deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7425
https://github.com/hail-is/hail/pull/7425:180,Deployability,Deploy,Deployed,180,- add priority class infrastructure throughout; - all pod specs have resource requests and limits; - make es tolerate preemptibles; - make monitoring router tolerate preemptibles. Deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7425
https://github.com/hail-is/hail/pull/7425:139,Energy Efficiency,monitor,monitoring,139,- add priority class infrastructure throughout; - all pod specs have resource requests and limits; - make es tolerate preemptibles; - make monitoring router tolerate preemptibles. Deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7425
https://github.com/hail-is/hail/pull/7425:150,Integrability,rout,router,150,- add priority class infrastructure throughout; - all pod specs have resource requests and limits; - make es tolerate preemptibles; - make monitoring router tolerate preemptibles. Deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7425
https://github.com/hail-is/hail/pull/7427:232,Testability,Test,Tested,232,"Implements Katex. Changes to goto.js (and its position on the page) has to do with Firefox's apparently broken history.scrollRestoration = 'manual' handling on page refresh (in that it still automatically restores scroll position). Tested in Chrome, Firefox (latest), Safari 13.0.1, Microsoft Edge (Mac Beta 78.0.276.20). Edge should mostly behave like Chrome, runs now on Chromium.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7427
https://github.com/hail-is/hail/pull/7431:80,Energy Efficiency,reduce,reduce,80,"Straightforward as notebook is stateless. Also, add resource limits on blog and reduce memory (it is currently using 87Mi).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7431
https://github.com/hail-is/hail/pull/7432:99,Availability,down,down,99,"Also, make batch pods eviction-safe. This should allow the cluster autoscaler to scale the cluster down, according to: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7432
https://github.com/hail-is/hail/pull/7432:31,Safety,safe,safe,31,"Also, make batch pods eviction-safe. This should allow the cluster autoscaler to scale the cluster down, according to: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7432
https://github.com/hail-is/hail/pull/7434:9,Testability,test,test,9,The last test for making sure PRs get merged is still a work in progress.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7434
https://github.com/hail-is/hail/pull/7435:53,Testability,test,test,53,"fix /batches endpoint for unconstrained query, added test: there was a trailing AND; make worker create pod endpoint idempotent",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7435
https://github.com/hail-is/hail/pull/7436:28,Testability,log,log,28,"Changes:; - When looking up log in gs, return None if we get 404/NotFound.; - Make sure the number of created jobs matches expected. This handles the case where job creation fails but /close still gets called.; - make bounded gather take awaitables, raise exceptions by default",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7436
https://github.com/hail-is/hail/pull/7438:32,Energy Efficiency,monitor,monitoring,32,Changes:; - Added back instance monitoring loop. Don't track instance health yet.; - Added time_created and last_updated to instances table in UNIX time. Add to last of things to rename: Instance => Worker. Instance is a GCE instance. Worker is a process running on a instance that executes jobs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7438
https://github.com/hail-is/hail/pull/7440:5,Security,secur,secures,5,"This secures the batch2 driver endpoints. The driver is called by workers on activate, deactivate and when a job is complete. It is also called by the front end to notify it when a batch is closed, cancelled or deleted. Workers have a random token stored in the database that is sent along with requests. The front end and driver share another random, internal token which is also placed in the authorization header. The driver has to decorators to protect request handlers: batch_only and instances_only.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7440
https://github.com/hail-is/hail/pull/7440:395,Security,authoriz,authorization,395,"This secures the batch2 driver endpoints. The driver is called by workers on activate, deactivate and when a job is complete. It is also called by the front end to notify it when a batch is closed, cancelled or deleted. Workers have a random token stored in the database that is sent along with requests. The front end and driver share another random, internal token which is also placed in the authorization header. The driver has to decorators to protect request handlers: batch_only and instances_only.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7440
https://github.com/hail-is/hail/pull/7441:700,Availability,error,errors,700,"One more if/when you want to take a look. Changes:; - Keep track of failed requests to workers, failed_request_count. Reset on a successful communication in Instance.mark_healthy.; - Don't retry the /job/create request. The scheduler loop will just retry.; - Don't schedule on nodes with failed count > 1. This basically ignores 1-off hiccups.; - The /jobs/delete (unscheduled) call is an interested situation. I decided to retry with back off and stop if the instance gets deactivated. Retry abstractions seem hard, I'm not quite sure how to share this code with request_retry_transient_error, for example, given different exit conditions. Sometime to think about as we see more examples.; - Ignore errors for each schedule attempt, so if there is a failure, count the request as failed and keep scheduling the current block of jobs before getting another block.; - Don't kill unhealthy instances. You might object to this, but I'm worried about when a job has been running for 3hrs (or 5 weeks, once we support scheduling on non-premptibles) and we delete an instance after a 5m network disconnect. I appreciate the need to clean up resources, I have more thoughts about that that I'll write elsewhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7441
https://github.com/hail-is/hail/pull/7441:751,Availability,failure,failure,751,"One more if/when you want to take a look. Changes:; - Keep track of failed requests to workers, failed_request_count. Reset on a successful communication in Instance.mark_healthy.; - Don't retry the /job/create request. The scheduler loop will just retry.; - Don't schedule on nodes with failed count > 1. This basically ignores 1-off hiccups.; - The /jobs/delete (unscheduled) call is an interested situation. I decided to retry with back off and stop if the instance gets deactivated. Retry abstractions seem hard, I'm not quite sure how to share this code with request_retry_transient_error, for example, given different exit conditions. Sometime to think about as we see more examples.; - Ignore errors for each schedule attempt, so if there is a failure, count the request as failed and keep scheduling the current block of jobs before getting another block.; - Don't kill unhealthy instances. You might object to this, but I'm worried about when a job has been running for 3hrs (or 5 weeks, once we support scheduling on non-premptibles) and we delete an instance after a 5m network disconnect. I appreciate the need to clean up resources, I have more thoughts about that that I'll write elsewhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7441
https://github.com/hail-is/hail/pull/7441:224,Energy Efficiency,schedul,scheduler,224,"One more if/when you want to take a look. Changes:; - Keep track of failed requests to workers, failed_request_count. Reset on a successful communication in Instance.mark_healthy.; - Don't retry the /job/create request. The scheduler loop will just retry.; - Don't schedule on nodes with failed count > 1. This basically ignores 1-off hiccups.; - The /jobs/delete (unscheduled) call is an interested situation. I decided to retry with back off and stop if the instance gets deactivated. Retry abstractions seem hard, I'm not quite sure how to share this code with request_retry_transient_error, for example, given different exit conditions. Sometime to think about as we see more examples.; - Ignore errors for each schedule attempt, so if there is a failure, count the request as failed and keep scheduling the current block of jobs before getting another block.; - Don't kill unhealthy instances. You might object to this, but I'm worried about when a job has been running for 3hrs (or 5 weeks, once we support scheduling on non-premptibles) and we delete an instance after a 5m network disconnect. I appreciate the need to clean up resources, I have more thoughts about that that I'll write elsewhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7441
https://github.com/hail-is/hail/pull/7441:265,Energy Efficiency,schedul,schedule,265,"One more if/when you want to take a look. Changes:; - Keep track of failed requests to workers, failed_request_count. Reset on a successful communication in Instance.mark_healthy.; - Don't retry the /job/create request. The scheduler loop will just retry.; - Don't schedule on nodes with failed count > 1. This basically ignores 1-off hiccups.; - The /jobs/delete (unscheduled) call is an interested situation. I decided to retry with back off and stop if the instance gets deactivated. Retry abstractions seem hard, I'm not quite sure how to share this code with request_retry_transient_error, for example, given different exit conditions. Sometime to think about as we see more examples.; - Ignore errors for each schedule attempt, so if there is a failure, count the request as failed and keep scheduling the current block of jobs before getting another block.; - Don't kill unhealthy instances. You might object to this, but I'm worried about when a job has been running for 3hrs (or 5 weeks, once we support scheduling on non-premptibles) and we delete an instance after a 5m network disconnect. I appreciate the need to clean up resources, I have more thoughts about that that I'll write elsewhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7441
https://github.com/hail-is/hail/pull/7441:716,Energy Efficiency,schedul,schedule,716,"One more if/when you want to take a look. Changes:; - Keep track of failed requests to workers, failed_request_count. Reset on a successful communication in Instance.mark_healthy.; - Don't retry the /job/create request. The scheduler loop will just retry.; - Don't schedule on nodes with failed count > 1. This basically ignores 1-off hiccups.; - The /jobs/delete (unscheduled) call is an interested situation. I decided to retry with back off and stop if the instance gets deactivated. Retry abstractions seem hard, I'm not quite sure how to share this code with request_retry_transient_error, for example, given different exit conditions. Sometime to think about as we see more examples.; - Ignore errors for each schedule attempt, so if there is a failure, count the request as failed and keep scheduling the current block of jobs before getting another block.; - Don't kill unhealthy instances. You might object to this, but I'm worried about when a job has been running for 3hrs (or 5 weeks, once we support scheduling on non-premptibles) and we delete an instance after a 5m network disconnect. I appreciate the need to clean up resources, I have more thoughts about that that I'll write elsewhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7441
https://github.com/hail-is/hail/pull/7441:797,Energy Efficiency,schedul,scheduling,797,"One more if/when you want to take a look. Changes:; - Keep track of failed requests to workers, failed_request_count. Reset on a successful communication in Instance.mark_healthy.; - Don't retry the /job/create request. The scheduler loop will just retry.; - Don't schedule on nodes with failed count > 1. This basically ignores 1-off hiccups.; - The /jobs/delete (unscheduled) call is an interested situation. I decided to retry with back off and stop if the instance gets deactivated. Retry abstractions seem hard, I'm not quite sure how to share this code with request_retry_transient_error, for example, given different exit conditions. Sometime to think about as we see more examples.; - Ignore errors for each schedule attempt, so if there is a failure, count the request as failed and keep scheduling the current block of jobs before getting another block.; - Don't kill unhealthy instances. You might object to this, but I'm worried about when a job has been running for 3hrs (or 5 weeks, once we support scheduling on non-premptibles) and we delete an instance after a 5m network disconnect. I appreciate the need to clean up resources, I have more thoughts about that that I'll write elsewhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7441
https://github.com/hail-is/hail/pull/7441:1013,Energy Efficiency,schedul,scheduling,1013,"One more if/when you want to take a look. Changes:; - Keep track of failed requests to workers, failed_request_count. Reset on a successful communication in Instance.mark_healthy.; - Don't retry the /job/create request. The scheduler loop will just retry.; - Don't schedule on nodes with failed count > 1. This basically ignores 1-off hiccups.; - The /jobs/delete (unscheduled) call is an interested situation. I decided to retry with back off and stop if the instance gets deactivated. Retry abstractions seem hard, I'm not quite sure how to share this code with request_retry_transient_error, for example, given different exit conditions. Sometime to think about as we see more examples.; - Ignore errors for each schedule attempt, so if there is a failure, count the request as failed and keep scheduling the current block of jobs before getting another block.; - Don't kill unhealthy instances. You might object to this, but I'm worried about when a job has been running for 3hrs (or 5 weeks, once we support scheduling on non-premptibles) and we delete an instance after a 5m network disconnect. I appreciate the need to clean up resources, I have more thoughts about that that I'll write elsewhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7441
https://github.com/hail-is/hail/pull/7442:203,Deployability,pipeline,pipeline,203,"If there are live instances, batch2 won't startup. Instances get the instance pool via the app, so it needs to be set before calling InstancePool.async_init. Also, convert cores and memory to strings in pipeline. The batch2 API requires a string for cpu.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7442
https://github.com/hail-is/hail/pull/7443:43,Security,integrity,integrity,43,"A little chaos testing revealed a database integrity issue. If jobs.state = Running, instance_id must be non-null. I incorrectly had `ON DELETE SET NULL`. Instead, make sure that the instance has been deactivated (which reschedules all jobs, setting state = Ready) before deleting the instance entry. Also, feedback on cancellation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7443
https://github.com/hail-is/hail/pull/7443:15,Testability,test,testing,15,"A little chaos testing revealed a database integrity issue. If jobs.state = Running, instance_id must be non-null. I incorrectly had `ON DELETE SET NULL`. Instead, make sure that the instance has been deactivated (which reschedules all jobs, setting state = Ready) before deleting the instance entry. Also, feedback on cancellation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7443
https://github.com/hail-is/hail/pull/7443:307,Usability,feedback,feedback,307,"A little chaos testing revealed a database integrity issue. If jobs.state = Running, instance_id must be non-null. I incorrectly had `ON DELETE SET NULL`. Instead, make sure that the instance has been deactivated (which reschedules all jobs, setting state = Ready) before deleting the instance entry. Also, feedback on cancellation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7443
https://github.com/hail-is/hail/pull/7445:108,Availability,error,errors,108,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:1124,Availability,error,errors,1124,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:330,Deployability,upgrade,upgrade,330,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:1222,Deployability,deploy,deploy,1222,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:1326,Performance,race condition,race condition,1326,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:623,Security,access,access,623,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:722,Security,access,access,722,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:561,Testability,log,logs,561,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:912,Testability,log,logs,912,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7445:1307,Testability,log,logic,1307,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445
https://github.com/hail-is/hail/pull/7447:358,Availability,avail,available,358,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:516,Availability,down,down,516,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:1335,Availability,down,down,1335,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:237,Deployability,upgrade,upgrade,237,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:411,Deployability,upgrade,upgrade,411,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:1213,Integrability,rout,route,1213,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:829,Security,access,access,829,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:979,Security,access,access-to-specific-ip,979,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:590,Testability,log,logs,590,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:714,Testability,log,log,714,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:804,Testability,log,logs,804,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:1495,Testability,test,tests,1495,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7447:155,Usability,clear,clear,155,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447
https://github.com/hail-is/hail/pull/7448:81,Integrability,wrap,wrap,81,"`ArrayIterator.toEmitTriplet` generates a lot of bytecode, so its a good idea to wrap it in a separate method.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7448
https://github.com/hail-is/hail/pull/7454:110,Testability,test,tested,110,I didn't know if we should have the k8s server URL in the repo. I left it out for now just in case. I haven't tested the service account changes yet. Stacked on #7434,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7454
https://github.com/hail-is/hail/pull/7455:122,Deployability,pipeline,pipeline,122,"I believe this will solve a bug Alicia is hitting, but I am having; trouble replicating it without the ability to run her pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7455
https://github.com/hail-is/hail/pull/7456:30,Performance,cache,cache,30,"Images are stored in a global cache, so we have to be careful that a private image pulled by one user isn't executed by another user. Therefore, pull each time for private images. We could speed this up by having a per-user private image cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7456
https://github.com/hail-is/hail/pull/7456:238,Performance,cache,cache,238,"Images are stored in a global cache, so we have to be careful that a private image pulled by one user isn't executed by another user. Therefore, pull each time for private images. We could speed this up by having a per-user private image cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7456
https://github.com/hail-is/hail/pull/7458:231,Deployability,update,updated,231,"This adds a bare-bones status page for the batch2-driver. It includes:; - a Driver dropdown from the Batch2 header item,; - the instance ID and ready cores,; - list of instances with their name, state, free cores, created and last updated time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7458
https://github.com/hail-is/hail/pull/7463:19,Integrability,message,message,19,Fixes this warning message I got when activating the instance:; ```; /usr/local/lib/python3.6/dist-packages/aiomysql/cursors.py:192: Warning: Column 'activation_token' cannot be null; await self._do_get_result(); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7463
https://github.com/hail-is/hail/issues/7464:333,Availability,Error,Error,333,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. hail v 0.2. I can't build from sources. Error below. $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SPARK_VERSION=2.3.2; make -C src/main/c prebuilt; make[1]: Entering directory `/share/apps/luffy/hail/hail/src/main/c'; make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.; make[1]: Leaving directory `/share/apps/luffy/hail/hail/src/main/c'; make: *** [native-lib-prebuilt] Error 2. How can I fix this?. thank you",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7464
https://github.com/hail-is/hail/issues/7464:697,Availability,Error,Error,697,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. hail v 0.2. I can't build from sources. Error below. $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SPARK_VERSION=2.3.2; make -C src/main/c prebuilt; make[1]: Entering directory `/share/apps/luffy/hail/hail/src/main/c'; make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.; make[1]: Leaving directory `/share/apps/luffy/hail/hail/src/main/c'; make: *** [native-lib-prebuilt] Error 2. How can I fix this?. thank you",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7464
https://github.com/hail-is/hail/issues/7464:353,Deployability,install,install-on-cluster,353,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. hail v 0.2. I can't build from sources. Error below. $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SPARK_VERSION=2.3.2; make -C src/main/c prebuilt; make[1]: Entering directory `/share/apps/luffy/hail/hail/src/main/c'; make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.; make[1]: Leaving directory `/share/apps/luffy/hail/hail/src/main/c'; make: *** [native-lib-prebuilt] Error 2. How can I fix this?. thank you",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7464
https://github.com/hail-is/hail/pull/7470:128,Security,validat,validation,128,"First of 3 changes, I think:; - batch2 secret field now requires namespace and it is used for lookup (it was required before in validation, but ignored); - CI now sets namespace consistently. Since batch1 pods can only mount secrets from batch-pods, it only sets this to batch-pods. The one exception is when the secretes come from a runImage step. Then the namespace is taken from the runImage step if it is present, and otherwise defaults to batch-pods. Two more PRs:; 1. Set the namespace in all runImage step secrets; 2. Make the namespace required in runImage steps. Then, when we switch CI from batch1 => batch2, we can create runImage steps that use secrets from namespaces other than batch-pods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7470
https://github.com/hail-is/hail/pull/7476:54,Deployability,integrat,integration,54,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:54,Integrability,integrat,integration,54,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:259,Performance,Optimiz,Optimize,259,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:402,Performance,Optimiz,Optimize,402,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:555,Performance,Optimiz,Optimize,555,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:694,Performance,Optimiz,Optimize,694,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:837,Performance,Optimiz,Optimize,837,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:989,Performance,Optimiz,Optimize,989,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:1136,Performance,Optimiz,Optimize,1136,"us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:1280,Performance,Optimiz,Optimize,1280,"mize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:1433,Performance,Optimiz,Optimize,1433,"e -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: IN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:1572,Performance,Optimiz,Optimize,1572,"e -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:1715,Performance,Optimiz,Optimize,1715,"mize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:1867,Performance,Optimiz,Optimize,1867,"- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:2014,Performance,Optimiz,Optimize,2014,"mize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:2158,Performance,Optimiz,Optimize,2158,"ze -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEval",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:2312,Performance,Optimiz,Optimize,2312," -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:2451,Performance,Optimiz,Optimize,2451," -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 roo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:2594,Performance,Optimiz,Optimize,2594,"ize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:2746,Performance,Optimiz,Optimize,2746," ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ConvertToSafeValue' (2): 12.828ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'InitializeCompiledFunction' (2): 14.536ms; 2019-11-06 18",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:2893,Performance,Optimiz,Optimize,2893,"-- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ConvertToSafeValue' (2): 12.828ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'InitializeCompiledFunction' (2): 14.536ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ForwardRelationalLets' (7): 20.388ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'FoldConst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:567,Usability,Simpl,Simplify,567,"stacked on #7446. This is the first step toward broad integration of the timer throughout the compiler. This gives us output like:; ```; 2019-11-06 18:44:11 root: INFO: Timer: all timings:; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 5.811ms, total 29.474ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:1445,Usability,Simpl,Simplify,1445,"e -- ExtractIntervalFilters : 21.579ms, total 51.305ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 38.711ms, total 90.246ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 58.138ms, total 148.873ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: IN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:2324,Usability,Simpl,Simplify,2324," -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7476:4208,Usability,Simpl,Simplify,4208,"aken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ConvertToSafeValue' (2): 12.828ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'InitializeCompiledFunction' (2): 14.536ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ForwardRelationalLets' (7): 20.388ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'FoldConstants' (7): 21.650ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'RunCompiledFunction' (2): 44.571ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ExtractIntervalFilters' (7): 70.933ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'LoweringTransformation' (3): 79.703ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Simplify' (7): 135.960ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ForwardLets' (7): 200.163ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Compile' (2): 269.978ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'PruneDeadFields' (7): 274.850ms; 2019-11-06 18:44:11 root: INFO: Fraction covered by a tagged leaf: 13.383s (7.9%); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7476
https://github.com/hail-is/hail/pull/7479:203,Modifiability,variab,variable,203,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:531,Modifiability,variab,variables,531,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:554,Modifiability,variab,variables,554,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:710,Modifiability,variab,variable,710,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:875,Modifiability,variab,variables,875,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:994,Modifiability,variab,variables,994,"rm aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overridden. For all IR nodes, there is a map from IR child index to Renderable chil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:1165,Modifiability,variab,variable,1165,"rm aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overridden. For all IR nodes, there is a map from IR child index to Renderable chil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:72,Performance,perform,perform,72,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:433,Performance,perform,performs,433,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7479:1803,Safety,avoid,avoid,1803,"es the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overridden. For all IR nodes, there is a map from IR child index to Renderable child index, defined in `renderable_idx_of_child`, which is used to define the `bindings` and similar methods in terms of the `renderable_bindings` and similar. This is messier than I would have liked, but I couldn't think of a better way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479
https://github.com/hail-is/hail/pull/7485:18,Testability,assert,asserts,18,"Also add explicit asserts at the top of adjust_for_{add, remove}. We got it for free for remove (since .remove throws if missing) but for add.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7485
https://github.com/hail-is/hail/pull/7492:36,Modifiability,config,config,36,delint batch2; fix worker boot disk config; internal-gateway: propagate X-Forwarded-Proto for blog,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7492
https://github.com/hail-is/hail/pull/7495:18,Integrability,rout,router,18,add hello rule to router; parameterize wait-for by location; use in test-ci,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7495
https://github.com/hail-is/hail/pull/7495:26,Modifiability,parameteriz,parameterize,26,add hello rule to router; parameterize wait-for by location; use in test-ci,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7495
https://github.com/hail-is/hail/pull/7495:68,Testability,test,test-ci,68,add hello rule to router; parameterize wait-for by location; use in test-ci,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7495
https://github.com/hail-is/hail/pull/7499:41,Integrability,depend,depend,41,I'm posting to test that some tests that depend on lapack libraries still pass in CI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7499
https://github.com/hail-is/hail/pull/7499:15,Testability,test,test,15,I'm posting to test that some tests that depend on lapack libraries still pass in CI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7499
https://github.com/hail-is/hail/pull/7499:30,Testability,test,tests,30,I'm posting to test that some tests that depend on lapack libraries still pass in CI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7499
https://github.com/hail-is/hail/pull/7507:17,Deployability,pipeline,pipeline,17,"Directly run the pipeline tests, instead of launching a pod to run them. This is the first of a bunch of these (create accounts, various create tables, various tests). The only hard one is test-batch, since it needs to handle a callback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7507
https://github.com/hail-is/hail/pull/7507:26,Testability,test,tests,26,"Directly run the pipeline tests, instead of launching a pod to run them. This is the first of a bunch of these (create accounts, various create tables, various tests). The only hard one is test-batch, since it needs to handle a callback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7507
https://github.com/hail-is/hail/pull/7507:160,Testability,test,tests,160,"Directly run the pipeline tests, instead of launching a pod to run them. This is the first of a bunch of these (create accounts, various create tables, various tests). The only hard one is test-batch, since it needs to handle a callback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7507
https://github.com/hail-is/hail/pull/7507:189,Testability,test,test-batch,189,"Directly run the pipeline tests, instead of launching a pod to run them. This is the first of a bunch of these (create accounts, various create tables, various tests). The only hard one is test-batch, since it needs to handle a callback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7507
https://github.com/hail-is/hail/issues/7513:303,Deployability,release,released,303,PySpark is broken on Python 3.8:. - pyspark issue http://mail-archives.apache.org/mod_mbox/spark-issues/201910.mbox/%3CJIRA.13263529.1571661486000.10415.1571661540017@Atlassian.JIRA%3E; - pyspark fix: https://github.com/apache/spark/commit/811d563fbf60203377e8462e4fad271c1140b4fa. The fix has not been released yet. It's in 3.0.0-preview.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7513
https://github.com/hail-is/hail/pull/7516:130,Testability,test,tests,130,"I wasn't sure what this should look like. For now it does this, would be open to suggestions on how it should look. Currently the tests just test that it doesn't crash, if you're good with how this output looks I'll figure out how to test the output. Some examples:. Boring case with 0 dimensional ndarrays; ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.array(t.idx)); t.show(); +-------+---------------------------+; | idx | a |; +-------+---------------------------+; | int32 | ndarray<int32, 0> |; +-------+---------------------------+; | 0 | ndarray{shape=(), data=0} |; | 1 | ndarray{shape=(), data=1} |; | 2 | ndarray{shape=(), data=2} |; | 3 | ndarray{shape=(), data=3} |; | 4 | ndarray{shape=(), data=4} |; | 5 | ndarray{shape=(), data=5} |; +-------+---------------------------+; ```. Less boring case with 1d ndarrays of length `idx`. ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.arange(t.idx)); t.show(); +-------+------------------------------------------+; | idx | a |; +-------+------------------------------------------+; | int32 | ndarray<int32, 1> |; +-------+------------------------------------------+; | 0 | ndarray{shape=(0), data=[]} |; | 1 | ndarray{shape=(1), data=[0]} |; | 2 | ndarray{shape=(2), data=[0, 1]} |; | 3 | ndarray{shape=(3), data=[0, 1, 2]} |; | 4 | ndarray{shape=(4), data=[0, 1, 2, 3]} |; | 5 | ndarray{shape=(5), data=[0, 1, 2, 3, 4]} |; +-------+------------------------------------------+; ```. Now, 2 dimensional:. ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.arange(t.idx).reshape((3, 2))); t.show(); +-------+------------------------------------------------------+; | idx | a |; +-------+------------------------------------------------------+; | int32 | ndarray<int32, 2> |; +-------+------------------------------------------------------+; | 0 | ndarray{shape=(3, 2), data=[[0, 1], [2, 3], [4, 5]]} |; | 1 | ndarray{shape=(3, 2), data=[[0, 1], [2, 3], [4, 5]]} |; | 2 | ndarray{shape=(3, 2), data=[[0, 1], [2,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7516
https://github.com/hail-is/hail/pull/7516:141,Testability,test,test,141,"I wasn't sure what this should look like. For now it does this, would be open to suggestions on how it should look. Currently the tests just test that it doesn't crash, if you're good with how this output looks I'll figure out how to test the output. Some examples:. Boring case with 0 dimensional ndarrays; ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.array(t.idx)); t.show(); +-------+---------------------------+; | idx | a |; +-------+---------------------------+; | int32 | ndarray<int32, 0> |; +-------+---------------------------+; | 0 | ndarray{shape=(), data=0} |; | 1 | ndarray{shape=(), data=1} |; | 2 | ndarray{shape=(), data=2} |; | 3 | ndarray{shape=(), data=3} |; | 4 | ndarray{shape=(), data=4} |; | 5 | ndarray{shape=(), data=5} |; +-------+---------------------------+; ```. Less boring case with 1d ndarrays of length `idx`. ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.arange(t.idx)); t.show(); +-------+------------------------------------------+; | idx | a |; +-------+------------------------------------------+; | int32 | ndarray<int32, 1> |; +-------+------------------------------------------+; | 0 | ndarray{shape=(0), data=[]} |; | 1 | ndarray{shape=(1), data=[0]} |; | 2 | ndarray{shape=(2), data=[0, 1]} |; | 3 | ndarray{shape=(3), data=[0, 1, 2]} |; | 4 | ndarray{shape=(4), data=[0, 1, 2, 3]} |; | 5 | ndarray{shape=(5), data=[0, 1, 2, 3, 4]} |; +-------+------------------------------------------+; ```. Now, 2 dimensional:. ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.arange(t.idx).reshape((3, 2))); t.show(); +-------+------------------------------------------------------+; | idx | a |; +-------+------------------------------------------------------+; | int32 | ndarray<int32, 2> |; +-------+------------------------------------------------------+; | 0 | ndarray{shape=(3, 2), data=[[0, 1], [2, 3], [4, 5]]} |; | 1 | ndarray{shape=(3, 2), data=[[0, 1], [2, 3], [4, 5]]} |; | 2 | ndarray{shape=(3, 2), data=[[0, 1], [2,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7516
https://github.com/hail-is/hail/pull/7516:234,Testability,test,test,234,"I wasn't sure what this should look like. For now it does this, would be open to suggestions on how it should look. Currently the tests just test that it doesn't crash, if you're good with how this output looks I'll figure out how to test the output. Some examples:. Boring case with 0 dimensional ndarrays; ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.array(t.idx)); t.show(); +-------+---------------------------+; | idx | a |; +-------+---------------------------+; | int32 | ndarray<int32, 0> |; +-------+---------------------------+; | 0 | ndarray{shape=(), data=0} |; | 1 | ndarray{shape=(), data=1} |; | 2 | ndarray{shape=(), data=2} |; | 3 | ndarray{shape=(), data=3} |; | 4 | ndarray{shape=(), data=4} |; | 5 | ndarray{shape=(), data=5} |; +-------+---------------------------+; ```. Less boring case with 1d ndarrays of length `idx`. ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.arange(t.idx)); t.show(); +-------+------------------------------------------+; | idx | a |; +-------+------------------------------------------+; | int32 | ndarray<int32, 1> |; +-------+------------------------------------------+; | 0 | ndarray{shape=(0), data=[]} |; | 1 | ndarray{shape=(1), data=[0]} |; | 2 | ndarray{shape=(2), data=[0, 1]} |; | 3 | ndarray{shape=(3), data=[0, 1, 2]} |; | 4 | ndarray{shape=(4), data=[0, 1, 2, 3]} |; | 5 | ndarray{shape=(5), data=[0, 1, 2, 3, 4]} |; +-------+------------------------------------------+; ```. Now, 2 dimensional:. ```; t = hl.utils.range_table(6); t = t.annotate(a = hl._nd.arange(t.idx).reshape((3, 2))); t.show(); +-------+------------------------------------------------------+; | idx | a |; +-------+------------------------------------------------------+; | int32 | ndarray<int32, 2> |; +-------+------------------------------------------------------+; | 0 | ndarray{shape=(3, 2), data=[[0, 1], [2, 3], [4, 5]]} |; | 1 | ndarray{shape=(3, 2), data=[[0, 1], [2, 3], [4, 5]]} |; | 2 | ndarray{shape=(3, 2), data=[[0, 1], [2,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7516
https://github.com/hail-is/hail/pull/7518:94,Deployability,pipeline,pipeline,94,This includes various changes that have been made in the course of working with DSP ; on this pipeline.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7518
https://github.com/hail-is/hail/pull/7520:80,Availability,ERROR,ERROR,80,Fixes this problem:. ```; + xargs -r gcloud -q compute instances delete --zone; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; You may set it for your current workspace by running:. $ gcloud config set project VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7520
https://github.com/hail-is/hail/pull/7520:238,Modifiability,config,config,238,Fixes this problem:. ```; + xargs -r gcloud -q compute instances delete --zone; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; You may set it for your current workspace by running:. $ gcloud config set project VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7520
https://github.com/hail-is/hail/pull/7520:312,Modifiability,variab,variable,312,Fixes this problem:. ```; + xargs -r gcloud -q compute instances delete --zone; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; You may set it for your current workspace by running:. $ gcloud config set project VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7520
https://github.com/hail-is/hail/pull/7522:126,Usability,simpl,simpler,126,"The other option to fix this was to make `job_record_to_dict` have an extra layer of getopt everywhere, so I thought this was simpler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7522
https://github.com/hail-is/hail/pull/7523:97,Availability,resilien,resilient,97,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:110,Availability,failure,failure,110,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:23,Deployability,integrat,integration,23,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:969,Deployability,Update,Update,969,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:1014,Energy Efficiency,reduce,reduce,1014,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:1296,Energy Efficiency,reduce,reduced,1296,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:23,Integrability,integrat,integration,23,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:343,Integrability,rout,routines,343,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:369,Integrability,message,messages,369,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:435,Integrability,rout,routine,435,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:306,Performance,perform,performance,306,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:559,Performance,throughput,throughput,559,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:599,Performance,throughput,throughput,599,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:864,Performance,load,load,864,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:1094,Performance,throughput,throughput,1094,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:1134,Performance,throughput,throughput,1134,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:1304,Performance,throughput,throughput,1304,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:824,Testability,test,test,824,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:991,Testability,benchmark,benchmark,991,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7523:1188,Testability,test,test,1188,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523
https://github.com/hail-is/hail/pull/7530:53,Testability,log,log,53,status is just status returned by worker; unify job /log and /status into single job page (batch2 and ci); include spec in GET job (api and ui) calls,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7530
https://github.com/hail-is/hail/pull/7534:100,Availability,down,download,100,Is there a way for me to test this further? My experiments show that clone+merge is ~20 seconds but download from GCS is ~3s. This should seed up the feedback substantially for anyone working on an image that transitively depends on other images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534
https://github.com/hail-is/hail/pull/7534:222,Integrability,depend,depends,222,Is there a way for me to test this further? My experiments show that clone+merge is ~20 seconds but download from GCS is ~3s. This should seed up the feedback substantially for anyone working on an image that transitively depends on other images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534
https://github.com/hail-is/hail/pull/7534:25,Testability,test,test,25,Is there a way for me to test this further? My experiments show that clone+merge is ~20 seconds but download from GCS is ~3s. This should seed up the feedback substantially for anyone working on an image that transitively depends on other images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534
https://github.com/hail-is/hail/pull/7534:150,Usability,feedback,feedback,150,Is there a way for me to test this further? My experiments show that clone+merge is ~20 seconds but download from GCS is ~3s. This should seed up the feedback substantially for anyone working on an image that transitively depends on other images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534
https://github.com/hail-is/hail/pull/7542:31,Availability,error,errors,31,This is causing CI to have 500 errors.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7542
https://github.com/hail-is/hail/pull/7544:69,Energy Efficiency,power,powerpoint,69,"This PR includes the PDF of the Hail Tables cheatsheet. I'll add the powerpoint file as well once someone has approved the pdf so that others can change in the future. If github is just complaining that it's a binary file and doesn't show the preview, click the ""..."" on the right hand side of the filename bar thing and click ""view file"". Any feedback is welcome. . Fixes #5388 (though we should make a new issue for adding a matrix table cheat sheet). Assigned to Tim, but also put Gopal on it since he's been thinking about tutorials",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7544
https://github.com/hail-is/hail/pull/7544:344,Usability,feedback,feedback,344,"This PR includes the PDF of the Hail Tables cheatsheet. I'll add the powerpoint file as well once someone has approved the pdf so that others can change in the future. If github is just complaining that it's a binary file and doesn't show the preview, click the ""..."" on the right hand side of the filename bar thing and click ""view file"". Any feedback is welcome. . Fixes #5388 (though we should make a new issue for adding a matrix table cheat sheet). Assigned to Tim, but also put Gopal on it since he's been thinking about tutorials",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7544
https://github.com/hail-is/hail/issues/7545:106,Availability,error,error,106,"I am not sure what it should look like, but right now trying to show a table with no row fields causes an error. To replicate:. ```; hl.utils.range_table(10).key_by().select().show(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7545
https://github.com/hail-is/hail/issues/7548:273,Testability,log,log,273,"To replicate:. - create a PR; - open some started by incomplete job from the PR build; - refresh repeatedly; - just as the job is finished, you'll receive a 500 ISE; - your next refresh will probably hang; - your next refresh will probably show you a page with None as the log for Input and Main (instead of the expected empty string and full log) and None for Output; - finally, your next refresh will contain the complete job log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7548
https://github.com/hail-is/hail/issues/7548:343,Testability,log,log,343,"To replicate:. - create a PR; - open some started by incomplete job from the PR build; - refresh repeatedly; - just as the job is finished, you'll receive a 500 ISE; - your next refresh will probably hang; - your next refresh will probably show you a page with None as the log for Input and Main (instead of the expected empty string and full log) and None for Output; - finally, your next refresh will contain the complete job log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7548
https://github.com/hail-is/hail/issues/7548:428,Testability,log,log,428,"To replicate:. - create a PR; - open some started by incomplete job from the PR build; - refresh repeatedly; - just as the job is finished, you'll receive a 500 ISE; - your next refresh will probably hang; - your next refresh will probably show you a page with None as the log for Input and Main (instead of the expected empty string and full log) and None for Output; - finally, your next refresh will contain the complete job log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7548
https://github.com/hail-is/hail/pull/7552:34,Testability,test,testing,34,Add 10 randomly generated VCFs as testing resource,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7552
https://github.com/hail-is/hail/pull/7556:16,Testability,test,test,16,"Also fixed some test bugs (the pattern matching tests didn't actually test that the pattern was found...just that if it is found, it has the right value).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7556
https://github.com/hail-is/hail/pull/7556:48,Testability,test,tests,48,"Also fixed some test bugs (the pattern matching tests didn't actually test that the pattern was found...just that if it is found, it has the right value).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7556
https://github.com/hail-is/hail/pull/7556:70,Testability,test,test,70,"Also fixed some test bugs (the pattern matching tests didn't actually test that the pattern was found...just that if it is found, it has the right value).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7556
https://github.com/hail-is/hail/issues/7558:49,Deployability,pipeline,pipeline,49,The correct thing is to expose the entrypoint in pipeline/batch_client and add it to the config for a job.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7558
https://github.com/hail-is/hail/issues/7558:89,Modifiability,config,config,89,The correct thing is to expose the entrypoint in pipeline/batch_client and add it to the config for a job.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7558
https://github.com/hail-is/hail/issues/7558:24,Security,expose,expose,24,The correct thing is to expose the entrypoint in pipeline/batch_client and add it to the config for a job.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7558
https://github.com/hail-is/hail/pull/7566:484,Modifiability,variab,variable,484,"The main goal of this PR was to remove some of the vector/scalar logic from the BlockMatrixMap2 node, and to support the scalar operations on BlockMatrixMap. I basically accomplished this by taking the cases that are matched on in BlockMatrixMap2 and lifting them into the Simplify rules. The only endpoint that I believe I needed to cover was the BlockMatrix.pyExecute() one; all the others will go through the usual CompileAndEvaluate. There's another part of the PR that fixes the variable bindings, which are currently hard-coded and unchecked. I needed this to construct the right expressions for the IR expressions, so I changed it to handle variable bindings with the rest of our IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7566
https://github.com/hail-is/hail/pull/7566:648,Modifiability,variab,variable,648,"The main goal of this PR was to remove some of the vector/scalar logic from the BlockMatrixMap2 node, and to support the scalar operations on BlockMatrixMap. I basically accomplished this by taking the cases that are matched on in BlockMatrixMap2 and lifting them into the Simplify rules. The only endpoint that I believe I needed to cover was the BlockMatrix.pyExecute() one; all the others will go through the usual CompileAndEvaluate. There's another part of the PR that fixes the variable bindings, which are currently hard-coded and unchecked. I needed this to construct the right expressions for the IR expressions, so I changed it to handle variable bindings with the rest of our IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7566
https://github.com/hail-is/hail/pull/7566:65,Testability,log,logic,65,"The main goal of this PR was to remove some of the vector/scalar logic from the BlockMatrixMap2 node, and to support the scalar operations on BlockMatrixMap. I basically accomplished this by taking the cases that are matched on in BlockMatrixMap2 and lifting them into the Simplify rules. The only endpoint that I believe I needed to cover was the BlockMatrix.pyExecute() one; all the others will go through the usual CompileAndEvaluate. There's another part of the PR that fixes the variable bindings, which are currently hard-coded and unchecked. I needed this to construct the right expressions for the IR expressions, so I changed it to handle variable bindings with the rest of our IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7566
https://github.com/hail-is/hail/pull/7566:273,Usability,Simpl,Simplify,273,"The main goal of this PR was to remove some of the vector/scalar logic from the BlockMatrixMap2 node, and to support the scalar operations on BlockMatrixMap. I basically accomplished this by taking the cases that are matched on in BlockMatrixMap2 and lifting them into the Simplify rules. The only endpoint that I believe I needed to cover was the BlockMatrix.pyExecute() one; all the others will go through the usual CompileAndEvaluate. There's another part of the PR that fixes the variable bindings, which are currently hard-coded and unchecked. I needed this to construct the right expressions for the IR expressions, so I changed it to handle variable bindings with the rest of our IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7566
https://github.com/hail-is/hail/pull/7570:150,Availability,down,download,150,"I added a cheatsheets page to the docs. It has a link to the single current cheat sheet. I just used the fact that it's already on github to create a download link, if that's bad practice I'm happy to do something else.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7570
https://github.com/hail-is/hail/issues/7572:287,Modifiability,enhance,enhanced,287,"shoudn't be terribly surprising that `rand_unif` has weird behavior, but here's a case that is definitely The Wrong Thing:. ```; Python 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 13:19:00); Type 'copyright', 'credits' or 'license' for more information; IPython 7.6.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl. In [2]: r = hl.rand_unif(0, 1). In [3]: hl.eval(r); Out[3]: 0.5387579341676381. In [4]: hl.eval(hl.tuple([r, r])); Out[4]: (0.5387579341676381, 0.5387579341676381); ```. okay, this makes sense becuase they have the same seed:; ```; In [5]: print(hl.tuple([r, r])._ir); (MakeTuple (0 1) (ApplySeeded rand_unif 806694938962853089 Float64 (Apply toFloat64 Float64 (I32 0)) (Apply toFloat64 Float64 (I32 1))) (ApplySeeded rand_unif 806694938962853089 Float64 (Apply toFloat64 Float64 (I32 0)) (Apply toFloat64 Float64 (I32 1)))); ```. how about this:; ```; In [6]: hl.eval(hl.range(2).map(lambda x: r)); Out[6]: [0.5387579341676381, 0.9394799645512691]; ```. odd. but maybe rand_unif inside an iteration has some semantics for advancing the RNG (like an aggregation). ```; In [7]: p = 1 - r. In [8]: hl.eval(hl.range(2).map(lambda x: p)); Out[8]: [0.46124206583236194, 0.06052003544873086]; ```; ok... ```; In [9]: hl.eval((p, hl.range(2).map(lambda x: p))); Out[9]: (0.46124206583236194, [0.46124206583236194, 0.46124206583236194]); ```; wtf? . if you look in the logs, its explained by the fact that only the final IR triggers CSE:; ```; (Let __cse_1; (ApplyBinaryPrimOp Subtract; (ApplyIR toFloat64 Float64; (I32 1)); (ApplySeeded rand_unif 806694938962853089 Float64; (ApplyIR toFloat64 Float64; (I32 0)); (ApplyIR toFloat64 Float64; (I32 1)))); (MakeTuple (0 1); (Ref __cse_1); (ArrayMap __uid_5; (ArrayRange; (I32 0); (I32 2); (I32 1)); (Ref __cse_1)))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7572
https://github.com/hail-is/hail/issues/7572:1423,Testability,log,logs,1423,"shoudn't be terribly surprising that `rand_unif` has weird behavior, but here's a case that is definitely The Wrong Thing:. ```; Python 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 13:19:00); Type 'copyright', 'credits' or 'license' for more information; IPython 7.6.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl. In [2]: r = hl.rand_unif(0, 1). In [3]: hl.eval(r); Out[3]: 0.5387579341676381. In [4]: hl.eval(hl.tuple([r, r])); Out[4]: (0.5387579341676381, 0.5387579341676381); ```. okay, this makes sense becuase they have the same seed:; ```; In [5]: print(hl.tuple([r, r])._ir); (MakeTuple (0 1) (ApplySeeded rand_unif 806694938962853089 Float64 (Apply toFloat64 Float64 (I32 0)) (Apply toFloat64 Float64 (I32 1))) (ApplySeeded rand_unif 806694938962853089 Float64 (Apply toFloat64 Float64 (I32 0)) (Apply toFloat64 Float64 (I32 1)))); ```. how about this:; ```; In [6]: hl.eval(hl.range(2).map(lambda x: r)); Out[6]: [0.5387579341676381, 0.9394799645512691]; ```. odd. but maybe rand_unif inside an iteration has some semantics for advancing the RNG (like an aggregation). ```; In [7]: p = 1 - r. In [8]: hl.eval(hl.range(2).map(lambda x: p)); Out[8]: [0.46124206583236194, 0.06052003544873086]; ```; ok... ```; In [9]: hl.eval((p, hl.range(2).map(lambda x: p))); Out[9]: (0.46124206583236194, [0.46124206583236194, 0.46124206583236194]); ```; wtf? . if you look in the logs, its explained by the fact that only the final IR triggers CSE:; ```; (Let __cse_1; (ApplyBinaryPrimOp Subtract; (ApplyIR toFloat64 Float64; (I32 1)); (ApplySeeded rand_unif 806694938962853089 Float64; (ApplyIR toFloat64 Float64; (I32 0)); (ApplyIR toFloat64 Float64; (I32 1)))); (MakeTuple (0 1); (Ref __cse_1); (ArrayMap __uid_5; (ArrayRange; (I32 0); (I32 2); (I32 1)); (Ref __cse_1)))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7572
https://github.com/hail-is/hail/pull/7573:130,Security,expose,exposed,130,"This allows us to not forcibly densify when doing map operations that would otherwise densify, e.g. bm + 1. . This behavior isn't exposed and I don't believe we want to expose it, since preserving the sparsity for functions where f(0) != 0 means we don't uniformly map all the elements of the matrix, but it can be useful in some cases. One thing to be careful about is that this only preserves sparsity at the block level---if any elements have been zeroed out in blocks during a sparsify operation, those will be mapped as usual. (This is because we don't currently preserve information about which elements have been zeroed out due to a sparsify operation.) As an illustration, we'd see:. ```; >>> bm = hl.linalg.BlockMatrix.fill(12, 12, 7, 3); >>> bm = bm.sparsify_band(-1, 1); >>> bm.to_numpy(); array([[7., 7., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],; [7., 7., 7., 0., 0., 0., 0., 0., 0., 0., 0., 0.],; [0., 7., 7., 7., 0., 0., 0., 0., 0., 0., 0., 0.],; [0., 0., 7., 7., 7., 0., 0., 0., 0., 0., 0., 0.],; [0., 0., 0., 7., 7., 7., 0., 0., 0., 0., 0., 0.],; [0., 0., 0., 0., 7., 7., 7., 0., 0., 0., 0., 0.],; [0., 0., 0., 0., 0., 7., 7., 7., 0., 0., 0., 0.],; [0., 0., 0., 0., 0., 0., 7., 7., 7., 0., 0., 0.],; [0., 0., 0., 0., 0., 0., 0., 7., 7., 7., 0., 0.],; [0., 0., 0., 0., 0., 0., 0., 0., 7., 7., 7., 0.],; [0., 0., 0., 0., 0., 0., 0., 0., 0., 7., 7., 7.],; [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 7., 7.]]); >>> bm._apply_map(lambda i: i + 1).to_numpy(); array([[8., 8., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],; [8., 8., 8., 1., 1., 1., 1., 1., 1., 1., 1., 1.],; [1., 8., 8., 8., 1., 1., 1., 1., 1., 1., 1., 1.],; [1., 1., 8., 8., 8., 1., 1., 1., 1., 1., 1., 1.],; [1., 1., 1., 8., 8., 8., 1., 1., 1., 1., 1., 1.],; [1., 1., 1., 1., 8., 8., 8., 1., 1., 1., 1., 1.],; [1., 1., 1., 1., 1., 8., 8., 8., 1., 1., 1., 1.],; [1., 1., 1., 1., 1., 1., 8., 8., 8., 1., 1., 1.],; [1., 1., 1., 1., 1., 1., 1., 8., 8., 8., 1., 1.],; [1., 1., 1., 1., 1., 1., 1., 1., 8., 8., 8., 1.],; [1., 1., 1., 1.,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7573
https://github.com/hail-is/hail/pull/7573:169,Security,expose,expose,169,"This allows us to not forcibly densify when doing map operations that would otherwise densify, e.g. bm + 1. . This behavior isn't exposed and I don't believe we want to expose it, since preserving the sparsity for functions where f(0) != 0 means we don't uniformly map all the elements of the matrix, but it can be useful in some cases. One thing to be careful about is that this only preserves sparsity at the block level---if any elements have been zeroed out in blocks during a sparsify operation, those will be mapped as usual. (This is because we don't currently preserve information about which elements have been zeroed out due to a sparsify operation.) As an illustration, we'd see:. ```; >>> bm = hl.linalg.BlockMatrix.fill(12, 12, 7, 3); >>> bm = bm.sparsify_band(-1, 1); >>> bm.to_numpy(); array([[7., 7., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],; [7., 7., 7., 0., 0., 0., 0., 0., 0., 0., 0., 0.],; [0., 7., 7., 7., 0., 0., 0., 0., 0., 0., 0., 0.],; [0., 0., 7., 7., 7., 0., 0., 0., 0., 0., 0., 0.],; [0., 0., 0., 7., 7., 7., 0., 0., 0., 0., 0., 0.],; [0., 0., 0., 0., 7., 7., 7., 0., 0., 0., 0., 0.],; [0., 0., 0., 0., 0., 7., 7., 7., 0., 0., 0., 0.],; [0., 0., 0., 0., 0., 0., 7., 7., 7., 0., 0., 0.],; [0., 0., 0., 0., 0., 0., 0., 7., 7., 7., 0., 0.],; [0., 0., 0., 0., 0., 0., 0., 0., 7., 7., 7., 0.],; [0., 0., 0., 0., 0., 0., 0., 0., 0., 7., 7., 7.],; [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 7., 7.]]); >>> bm._apply_map(lambda i: i + 1).to_numpy(); array([[8., 8., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],; [8., 8., 8., 1., 1., 1., 1., 1., 1., 1., 1., 1.],; [1., 8., 8., 8., 1., 1., 1., 1., 1., 1., 1., 1.],; [1., 1., 8., 8., 8., 1., 1., 1., 1., 1., 1., 1.],; [1., 1., 1., 8., 8., 8., 1., 1., 1., 1., 1., 1.],; [1., 1., 1., 1., 8., 8., 8., 1., 1., 1., 1., 1.],; [1., 1., 1., 1., 1., 8., 8., 8., 1., 1., 1., 1.],; [1., 1., 1., 1., 1., 1., 8., 8., 8., 1., 1., 1.],; [1., 1., 1., 1., 1., 1., 1., 8., 8., 8., 1., 1.],; [1., 1., 1., 1., 1., 1., 1., 1., 8., 8., 8., 1.],; [1., 1., 1., 1.,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7573
https://github.com/hail-is/hail/pull/7578:26,Testability,test,tests,26,* Move some MatrixIRSuite tests to Python; * delete covered MatrixIRSuite test. Stacked on #7562,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7578
https://github.com/hail-is/hail/pull/7578:74,Testability,test,test,74,* Move some MatrixIRSuite tests to Python; * delete covered MatrixIRSuite test. Stacked on #7562,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7578
https://github.com/hail-is/hail/pull/7583:11,Modifiability,enhance,enhancement,11,This is an enhancement of #7498. We adjust the number of cores in a user's request to make sure they have enough memory based on the memory per core rather than giving them the exact resources they asked for.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7583
https://github.com/hail-is/hail/issues/7584:40,Availability,error,error,40,"This should give a helpful python-level error message bout balding Nichols having an integer column key, but instead I get a Scala match error.; ```; In [9]: hl.export_vcf(hl.balding_nichols_model(3,10,10, n_partitions=3), '/tmp/foo.vcf') ; ```; randomly assigned",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7584
https://github.com/hail-is/hail/issues/7584:137,Availability,error,error,137,"This should give a helpful python-level error message bout balding Nichols having an integer column key, but instead I get a Scala match error.; ```; In [9]: hl.export_vcf(hl.balding_nichols_model(3,10,10, n_partitions=3), '/tmp/foo.vcf') ; ```; randomly assigned",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7584
https://github.com/hail-is/hail/issues/7584:46,Integrability,message,message,46,"This should give a helpful python-level error message bout balding Nichols having an integer column key, but instead I get a Scala match error.; ```; In [9]: hl.export_vcf(hl.balding_nichols_model(3,10,10, n_partitions=3), '/tmp/foo.vcf') ; ```; randomly assigned",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7584
https://github.com/hail-is/hail/pull/7587:360,Modifiability,variab,variables,360,"Happy to hear suggestions as to better ways to do this, but I've made too many emitter typos with while loops. I keep either doing something like. ```; i := 0,; j := 0,; Code.whileLoop(i < M,; Code.whileLoop(j < N, ; ???,; j := j + 1; ); i := i + 1; ); ```; (j should be inside the outer whileLoop),; forgetting one of the increments, or forgetting to set the variables to 0 at the beginning. `Code.forLoop` is to force me to include all parts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7587
https://github.com/hail-is/hail/pull/7590:9,Testability,test,test,9,Directly test CI instead of going through pod.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7590
https://github.com/hail-is/hail/pull/7591:119,Deployability,pipeline,pipeline,119,Also allow incremental file creation. Necessary for forthcoming work that reconfigures the dependency structure of the pipeline benchmarks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7591
https://github.com/hail-is/hail/pull/7591:91,Integrability,depend,dependency,91,Also allow incremental file creation. Necessary for forthcoming work that reconfigures the dependency structure of the pipeline benchmarks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7591
https://github.com/hail-is/hail/pull/7591:128,Testability,benchmark,benchmarks,128,Also allow incremental file creation. Necessary for forthcoming work that reconfigures the dependency structure of the pipeline benchmarks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7591
https://github.com/hail-is/hail/pull/7592:151,Deployability,update,update,151,Permit the Dockerfile to be specified inline:; ```yaml; - kind: buildImage; name: inline_image; dockerFile:; inline: |; FROM ubuntu:18.04; RUN apt-get update && apt-get install git; contextPath: .; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7592
https://github.com/hail-is/hail/pull/7592:169,Deployability,install,install,169,Permit the Dockerfile to be specified inline:; ```yaml; - kind: buildImage; name: inline_image; dockerFile:; inline: |; FROM ubuntu:18.04; RUN apt-get update && apt-get install git; contextPath: .; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7592
https://github.com/hail-is/hail/pull/7593:97,Availability,resilien,resilient,97,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:110,Availability,failure,failure,110,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:23,Deployability,integrat,integration,23,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:969,Deployability,Update,Update,969,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:1014,Energy Efficiency,reduce,reduce,1014,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:1296,Energy Efficiency,reduce,reduced,1296,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:23,Integrability,integrat,integration,23,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:343,Integrability,rout,routines,343,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:369,Integrability,message,messages,369,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:435,Integrability,rout,routine,435,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:306,Performance,perform,performance,306,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:559,Performance,throughput,throughput,559,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:599,Performance,throughput,throughput,599,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:864,Performance,load,load,864,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:1094,Performance,throughput,throughput,1094,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:1134,Performance,throughput,throughput,1134,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:1304,Performance,throughput,throughput,1304,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:824,Testability,test,test,824,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:991,Testability,benchmark,benchmark,991,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7593:1188,Testability,test,test,1188,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593
https://github.com/hail-is/hail/pull/7596:43,Security,validat,validated,43,Add billing projects. Billing projects are validated once in /batches/create.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7596
https://github.com/hail-is/hail/pull/7597:17,Deployability,patch,patch,17,Forgot to change patch number,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7597
https://github.com/hail-is/hail/pull/7598:396,Deployability,deploy,deployed,396,@cseed High priority. CI is leaking batch workers because ZONE is the empty string which fucks up the delete instances step in every PR's build.yaml. ---. copied from Zulip:. FYI:. CI is leaking workers because ZONE is not set. This was caused by a broken Makefile see https://github.com/hail-is/hail/pull/7598 which fixes the makefile. CI keeps passing on the unset-ness of ZONE each time it is deployed. Without ZONE set the delete batch2 instances PR build step fails because the command line invocation is broken (the zone is set to--project and the project is unset) https://ci.hail.is/batches/779/jobs/73. I'm going to redeploy the exact CI YAML that is current deployed (I got it from k get deployments ci -o yaml and removed the stuff that cannot be applied) but with the ZONE set.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7598
https://github.com/hail-is/hail/pull/7598:668,Deployability,deploy,deployed,668,@cseed High priority. CI is leaking batch workers because ZONE is the empty string which fucks up the delete instances step in every PR's build.yaml. ---. copied from Zulip:. FYI:. CI is leaking workers because ZONE is not set. This was caused by a broken Makefile see https://github.com/hail-is/hail/pull/7598 which fixes the makefile. CI keeps passing on the unset-ness of ZONE each time it is deployed. Without ZONE set the delete batch2 instances PR build step fails because the command line invocation is broken (the zone is set to--project and the project is unset) https://ci.hail.is/batches/779/jobs/73. I'm going to redeploy the exact CI YAML that is current deployed (I got it from k get deployments ci -o yaml and removed the stuff that cannot be applied) but with the ZONE set.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7598
https://github.com/hail-is/hail/pull/7598:698,Deployability,deploy,deployments,698,@cseed High priority. CI is leaking batch workers because ZONE is the empty string which fucks up the delete instances step in every PR's build.yaml. ---. copied from Zulip:. FYI:. CI is leaking workers because ZONE is not set. This was caused by a broken Makefile see https://github.com/hail-is/hail/pull/7598 which fixes the makefile. CI keeps passing on the unset-ness of ZONE each time it is deployed. Without ZONE set the delete batch2 instances PR build step fails because the command line invocation is broken (the zone is set to--project and the project is unset) https://ci.hail.is/batches/779/jobs/73. I'm going to redeploy the exact CI YAML that is current deployed (I got it from k get deployments ci -o yaml and removed the stuff that cannot be applied) but with the ZONE set.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7598
https://github.com/hail-is/hail/pull/7599:146,Modifiability,refactor,refactor,146,(since doesn't use any constructor arguments or other instance state). Should cost a bit less memory per region instance. Part of upcoming larger refactor that removes region instances where possible,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7599
https://github.com/hail-is/hail/pull/7600:104,Safety,avoid,avoid,104,"I added the columns to the batches table for the real-time cost estimate even though that will be v2 to avoid having to reset the database twice. Although, we may need to do that anyways so the fields are properly interpreted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600
https://github.com/hail-is/hail/pull/7601:5,Testability,test,test,5,"make test doesn't work on local, so putting up for CI to run it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7601
https://github.com/hail-is/hail/pull/7604:35,Performance,perform,perform,35,"I think the database insert didn't perform as well with > 1000 jobs per insert in the front_end create_jobs, but I can't figure out where I got that. Feel free to reject this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7604
https://github.com/hail-is/hail/pull/7614:809,Deployability,update,update,809,"I don't think I'm quite satisfied with this implementation, although I think that it will do the things we need it to do, for the most part. It mostly adheres to the structure that @chrisvittal was implementing in #5228, although I had some questions/notes about implementation/interface and would love some input:. * I'd like to implement (in python) some sort of while loop construct, but I'm not sure what it looks like. I think I'd like to think of that as our primary loop construct since the general recursive loop function can be confusing to start with in terms of what's allowed (what is tail-recursive? what is non-tail-recursive?) if you're just trying to implement some convergence criteria. Some initial thoughts on interface:; ```; 1:; loop = hl.WhileBuilder(i=0, x=0); loop.cond(loop.i < 10); .update(x = loop.x + i, ; i = loop.i + 1); .result(loop.x). 2: ; loop = hl.while_loop(; lambda i, x: i < 10, ; lambda i, x: (x + i, i + 1), ; lambda i, x: x, ; 0, 0). 3:; loop = hl.while_loop(; lambda i, x: ; hl.loop.cond(i < 10); .update(i + 1, x + i); .result(x).; 0, 0); ```; but I'm not sure I really like any of them.; * the scoping for `Recur` is difficult to check and enforce.; * It's difficult to check if a TailLoop is invalidly attempting to recur a function from a different loop (in a nested environment), except by the type signature. I think I could give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:1040,Deployability,update,update,1040,"ntation, although I think that it will do the things we need it to do, for the most part. It mostly adheres to the structure that @chrisvittal was implementing in #5228, although I had some questions/notes about implementation/interface and would love some input:. * I'd like to implement (in python) some sort of while loop construct, but I'm not sure what it looks like. I think I'd like to think of that as our primary loop construct since the general recursive loop function can be confusing to start with in terms of what's allowed (what is tail-recursive? what is non-tail-recursive?) if you're just trying to implement some convergence criteria. Some initial thoughts on interface:; ```; 1:; loop = hl.WhileBuilder(i=0, x=0); loop.cond(loop.i < 10); .update(x = loop.x + i, ; i = loop.i + 1); .result(loop.x). 2: ; loop = hl.while_loop(; lambda i, x: i < 10, ; lambda i, x: (x + i, i + 1), ; lambda i, x: x, ; 0, 0). 3:; loop = hl.while_loop(; lambda i, x: ; hl.loop.cond(i < 10); .update(i + 1, x + i); .result(x).; 0, 0); ```; but I'm not sure I really like any of them.; * the scoping for `Recur` is difficult to check and enforce.; * It's difficult to check if a TailLoop is invalidly attempting to recur a function from a different loop (in a nested environment), except by the type signature. I think I could give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I have not yet implemented it because I think it migh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:2987,Energy Efficiency,allocate,allocate,2987,"give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I have not yet implemented it because I think it might make this python interface more difficult to support, and I rather like its simplicity.; * @patrick-schultz and I were talking about rewriting loops in the stream interface.; * We can only emit calls to recur in the same method that the original loop is defined in, because we are using jumps to implement them. This means we need to know that we are not going to wrap any calls that contain `Recur` in a method. I don't believe there are any cases where we wrap `If` conditions or `Let` bodies in methods, so this is fine for now, but we're not enforcing it in any way. I believe that the iteration for the stream codegen stuff will always be in the same method, so we wouldn't have to deal with this specially there.; * I've implemented a pretty simple version of here, as part of the `Streamify` pass. I believe it should handle all the valid tail-recursive cases, but I don't think we want to use it right now since it'll always allocate (as opposed to not allocating if all state is primitive). We could potentially revisit this once we can allow stream elements like this to basically store their elements in primitive fields, if that prevents allocation. You can take a look here: https://github.com/catoverdrive/hail/compare/loops...catoverdrive:loops-as-stream?expand=1. cc @cseed @patrick-schultz @chrisvittal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:278,Integrability,interface,interface,278,"I don't think I'm quite satisfied with this implementation, although I think that it will do the things we need it to do, for the most part. It mostly adheres to the structure that @chrisvittal was implementing in #5228, although I had some questions/notes about implementation/interface and would love some input:. * I'd like to implement (in python) some sort of while loop construct, but I'm not sure what it looks like. I think I'd like to think of that as our primary loop construct since the general recursive loop function can be confusing to start with in terms of what's allowed (what is tail-recursive? what is non-tail-recursive?) if you're just trying to implement some convergence criteria. Some initial thoughts on interface:; ```; 1:; loop = hl.WhileBuilder(i=0, x=0); loop.cond(loop.i < 10); .update(x = loop.x + i, ; i = loop.i + 1); .result(loop.x). 2: ; loop = hl.while_loop(; lambda i, x: i < 10, ; lambda i, x: (x + i, i + 1), ; lambda i, x: x, ; 0, 0). 3:; loop = hl.while_loop(; lambda i, x: ; hl.loop.cond(i < 10); .update(i + 1, x + i); .result(x).; 0, 0); ```; but I'm not sure I really like any of them.; * the scoping for `Recur` is difficult to check and enforce.; * It's difficult to check if a TailLoop is invalidly attempting to recur a function from a different loop (in a nested environment), except by the type signature. I think I could give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:729,Integrability,interface,interface,729,"I don't think I'm quite satisfied with this implementation, although I think that it will do the things we need it to do, for the most part. It mostly adheres to the structure that @chrisvittal was implementing in #5228, although I had some questions/notes about implementation/interface and would love some input:. * I'd like to implement (in python) some sort of while loop construct, but I'm not sure what it looks like. I think I'd like to think of that as our primary loop construct since the general recursive loop function can be confusing to start with in terms of what's allowed (what is tail-recursive? what is non-tail-recursive?) if you're just trying to implement some convergence criteria. Some initial thoughts on interface:; ```; 1:; loop = hl.WhileBuilder(i=0, x=0); loop.cond(loop.i < 10); .update(x = loop.x + i, ; i = loop.i + 1); .result(loop.x). 2: ; loop = hl.while_loop(; lambda i, x: i < 10, ; lambda i, x: (x + i, i + 1), ; lambda i, x: x, ; 0, 0). 3:; loop = hl.while_loop(; lambda i, x: ; hl.loop.cond(i < 10); .update(i + 1, x + i); .result(x).; 0, 0); ```; but I'm not sure I really like any of them.; * the scoping for `Recur` is difficult to check and enforce.; * It's difficult to check if a TailLoop is invalidly attempting to recur a function from a different loop (in a nested environment), except by the type signature. I think I could give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:2071,Integrability,interface,interface,2071,"(x).; 0, 0); ```; but I'm not sure I really like any of them.; * the scoping for `Recur` is difficult to check and enforce.; * It's difficult to check if a TailLoop is invalidly attempting to recur a function from a different loop (in a nested environment), except by the type signature. I think I could give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I have not yet implemented it because I think it might make this python interface more difficult to support, and I rather like its simplicity.; * @patrick-schultz and I were talking about rewriting loops in the stream interface.; * We can only emit calls to recur in the same method that the original loop is defined in, because we are using jumps to implement them. This means we need to know that we are not going to wrap any calls that contain `Recur` in a method. I don't believe there are any cases where we wrap `If` conditions or `Let` bodies in methods, so this is fine for now, but we're not enforcing it in any way. I believe that the iteration for the stream codegen stuff will always be in the same method, so we wouldn't have to deal with this specially there.; * I've implemented a pretty simple version of here, as part of the `Streamify` pass. I believe it should handle all the valid tail-recursive cases, but I don't think we want to use it right now since it'll always allocate (as opposed to not allocating if all state is primitive). We could potenti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:2217,Integrability,interface,interface,2217,"enforce.; * It's difficult to check if a TailLoop is invalidly attempting to recur a function from a different loop (in a nested environment), except by the type signature. I think I could give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I have not yet implemented it because I think it might make this python interface more difficult to support, and I rather like its simplicity.; * @patrick-schultz and I were talking about rewriting loops in the stream interface.; * We can only emit calls to recur in the same method that the original loop is defined in, because we are using jumps to implement them. This means we need to know that we are not going to wrap any calls that contain `Recur` in a method. I don't believe there are any cases where we wrap `If` conditions or `Let` bodies in methods, so this is fine for now, but we're not enforcing it in any way. I believe that the iteration for the stream codegen stuff will always be in the same method, so we wouldn't have to deal with this specially there.; * I've implemented a pretty simple version of here, as part of the `Streamify` pass. I believe it should handle all the valid tail-recursive cases, but I don't think we want to use it right now since it'll always allocate (as opposed to not allocating if all state is primitive). We could potentially revisit this once we can allow stream elements like this to basically store their elements in primitive field",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:2418,Integrability,wrap,wrap,2418,"give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I have not yet implemented it because I think it might make this python interface more difficult to support, and I rather like its simplicity.; * @patrick-schultz and I were talking about rewriting loops in the stream interface.; * We can only emit calls to recur in the same method that the original loop is defined in, because we are using jumps to implement them. This means we need to know that we are not going to wrap any calls that contain `Recur` in a method. I don't believe there are any cases where we wrap `If` conditions or `Let` bodies in methods, so this is fine for now, but we're not enforcing it in any way. I believe that the iteration for the stream codegen stuff will always be in the same method, so we wouldn't have to deal with this specially there.; * I've implemented a pretty simple version of here, as part of the `Streamify` pass. I believe it should handle all the valid tail-recursive cases, but I don't think we want to use it right now since it'll always allocate (as opposed to not allocating if all state is primitive). We could potentially revisit this once we can allow stream elements like this to basically store their elements in primitive fields, if that prevents allocation. You can take a look here: https://github.com/catoverdrive/hail/compare/loops...catoverdrive:loops-as-stream?expand=1. cc @cseed @patrick-schultz @chrisvittal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:2512,Integrability,wrap,wrap,2512,"give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I have not yet implemented it because I think it might make this python interface more difficult to support, and I rather like its simplicity.; * @patrick-schultz and I were talking about rewriting loops in the stream interface.; * We can only emit calls to recur in the same method that the original loop is defined in, because we are using jumps to implement them. This means we need to know that we are not going to wrap any calls that contain `Recur` in a method. I don't believe there are any cases where we wrap `If` conditions or `Let` bodies in methods, so this is fine for now, but we're not enforcing it in any way. I believe that the iteration for the stream codegen stuff will always be in the same method, so we wouldn't have to deal with this specially there.; * I've implemented a pretty simple version of here, as part of the `Streamify` pass. I believe it should handle all the valid tail-recursive cases, but I don't think we want to use it right now since it'll always allocate (as opposed to not allocating if all state is primitive). We could potentially revisit this once we can allow stream elements like this to basically store their elements in primitive fields, if that prevents allocation. You can take a look here: https://github.com/catoverdrive/hail/compare/loops...catoverdrive:loops-as-stream?expand=1. cc @cseed @patrick-schultz @chrisvittal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:2130,Usability,simpl,simplicity,2130,"(x).; 0, 0); ```; but I'm not sure I really like any of them.; * the scoping for `Recur` is difficult to check and enforce.; * It's difficult to check if a TailLoop is invalidly attempting to recur a function from a different loop (in a nested environment), except by the type signature. I think I could give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I have not yet implemented it because I think it might make this python interface more difficult to support, and I rather like its simplicity.; * @patrick-schultz and I were talking about rewriting loops in the stream interface.; * We can only emit calls to recur in the same method that the original loop is defined in, because we are using jumps to implement them. This means we need to know that we are not going to wrap any calls that contain `Recur` in a method. I don't believe there are any cases where we wrap `If` conditions or `Let` bodies in methods, so this is fine for now, but we're not enforcing it in any way. I believe that the iteration for the stream codegen stuff will always be in the same method, so we wouldn't have to deal with this specially there.; * I've implemented a pretty simple version of here, as part of the `Streamify` pass. I believe it should handle all the valid tail-recursive cases, but I don't think we want to use it right now since it'll always allocate (as opposed to not allocating if all state is primitive). We could potenti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7614:2802,Usability,simpl,simple,2802,"give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I have not yet implemented it because I think it might make this python interface more difficult to support, and I rather like its simplicity.; * @patrick-schultz and I were talking about rewriting loops in the stream interface.; * We can only emit calls to recur in the same method that the original loop is defined in, because we are using jumps to implement them. This means we need to know that we are not going to wrap any calls that contain `Recur` in a method. I don't believe there are any cases where we wrap `If` conditions or `Let` bodies in methods, so this is fine for now, but we're not enforcing it in any way. I believe that the iteration for the stream codegen stuff will always be in the same method, so we wouldn't have to deal with this specially there.; * I've implemented a pretty simple version of here, as part of the `Streamify` pass. I believe it should handle all the valid tail-recursive cases, but I don't think we want to use it right now since it'll always allocate (as opposed to not allocating if all state is primitive). We could potentially revisit this once we can allow stream elements like this to basically store their elements in primitive fields, if that prevents allocation. You can take a look here: https://github.com/catoverdrive/hail/compare/loops...catoverdrive:loops-as-stream?expand=1. cc @cseed @patrick-schultz @chrisvittal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614
https://github.com/hail-is/hail/pull/7615:40,Deployability,patch,patch,40,"It seems not right to pin to a specific patch version of python 3.7. This is unnecessarily more generous, but seems fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7615
https://github.com/hail-is/hail/pull/7617:116,Energy Efficiency,reduce,reduce,116,"Makes elementsOffsetTable private, it's only used internally and by defensively making class members private we can reduce the cognitive complexity of our codebase. More importantly, avoid unnecessarily initializing the elementsOffsetTable array.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7617
https://github.com/hail-is/hail/pull/7617:183,Safety,avoid,avoid,183,"Makes elementsOffsetTable private, it's only used internally and by defensively making class members private we can reduce the cognitive complexity of our codebase. More importantly, avoid unnecessarily initializing the elementsOffsetTable array.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7617
https://github.com/hail-is/hail/issues/7619:54,Testability,log,logs,54,"When the Output step of a CI job is running, the Main logs (which were previously visible while Main was running) are set to `None` in the job page. After the Output step completes, the Main logs become visible again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7619
https://github.com/hail-is/hail/issues/7619:191,Testability,log,logs,191,"When the Output step of a CI job is running, the Main logs (which were previously visible while Main was running) are set to `None` in the job page. After the Output step completes, the Main logs become visible again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7619
https://github.com/hail-is/hail/pull/7620:106,Energy Efficiency,efficient,efficient,106,"Clarify the meaning of the 4 and 4L, don't repeat (length + 7) >>> 3, and get rid of the potentially less efficient version (length + 7) / 8.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7620
https://github.com/hail-is/hail/issues/7623:254,Safety,risk,risk,254,"Batch lives in a different namespace than it's client and has substantially more credentials. The `batch_callback` URL can be used to make batch POST (without its credentials) to an arbitrary URL. Are there services in the internal namespace that are at risk from malicious, uncredentialed POSTs?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7623
https://github.com/hail-is/hail/issues/7624:188,Availability,error,error,188,"```; (1146, \""Table 'pr-7592-notebook-yi0bdn43fu8g.workshops' doesn't exist\"")""; ```. I'm not sure what's going on, but I noticed the notebook server cannot respond to /images due to this error. We should be scanning the logs of every service for ERROR log messages on each PR. I saw this because the image-fetchers are failing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7624
https://github.com/hail-is/hail/issues/7624:247,Availability,ERROR,ERROR,247,"```; (1146, \""Table 'pr-7592-notebook-yi0bdn43fu8g.workshops' doesn't exist\"")""; ```. I'm not sure what's going on, but I noticed the notebook server cannot respond to /images due to this error. We should be scanning the logs of every service for ERROR log messages on each PR. I saw this because the image-fetchers are failing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7624
https://github.com/hail-is/hail/issues/7624:257,Integrability,message,messages,257,"```; (1146, \""Table 'pr-7592-notebook-yi0bdn43fu8g.workshops' doesn't exist\"")""; ```. I'm not sure what's going on, but I noticed the notebook server cannot respond to /images due to this error. We should be scanning the logs of every service for ERROR log messages on each PR. I saw this because the image-fetchers are failing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7624
https://github.com/hail-is/hail/issues/7624:221,Testability,log,logs,221,"```; (1146, \""Table 'pr-7592-notebook-yi0bdn43fu8g.workshops' doesn't exist\"")""; ```. I'm not sure what's going on, but I noticed the notebook server cannot respond to /images due to this error. We should be scanning the logs of every service for ERROR log messages on each PR. I saw this because the image-fetchers are failing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7624
https://github.com/hail-is/hail/issues/7624:253,Testability,log,log,253,"```; (1146, \""Table 'pr-7592-notebook-yi0bdn43fu8g.workshops' doesn't exist\"")""; ```. I'm not sure what's going on, but I noticed the notebook server cannot respond to /images due to this error. We should be scanning the logs of every service for ERROR log messages on each PR. I saw this because the image-fetchers are failing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7624
https://github.com/hail-is/hail/pull/7626:356,Integrability,depend,dependsOn,356,I want to use this in the PR that uses one git repo checkout step for the whole build. There I'll have one step to clone the repo:; ```; - kind: runImage; name: clone; image:; valueFrom: git_bash_image.image; script: |; mkdir /io/repo; cd /io/repo; {{ code.checkout_script }}; cd ..; tar -cvzf repo.tgz repo; outputs:; - from: /io/repo.tgz; to: /repo.tgz; dependsOn:; - git_bash_image; ```; and every buildImage step will be able to extract only the relevant files from that large tar archive:; ```; - kind: buildImage; name: base_image; dockerFile: docker/Dockerfile.base; contextPath: /; publishAs: base; inputs:; - from: /repo.tgz; to: /; extract:; - docker; - pylintrc; dependsOn:; - clone; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626
https://github.com/hail-is/hail/pull/7626:674,Integrability,depend,dependsOn,674,I want to use this in the PR that uses one git repo checkout step for the whole build. There I'll have one step to clone the repo:; ```; - kind: runImage; name: clone; image:; valueFrom: git_bash_image.image; script: |; mkdir /io/repo; cd /io/repo; {{ code.checkout_script }}; cd ..; tar -cvzf repo.tgz repo; outputs:; - from: /io/repo.tgz; to: /repo.tgz; dependsOn:; - git_bash_image; ```; and every buildImage step will be able to extract only the relevant files from that large tar archive:; ```; - kind: buildImage; name: base_image; dockerFile: docker/Dockerfile.base; contextPath: /; publishAs: base; inputs:; - from: /repo.tgz; to: /; extract:; - docker; - pylintrc; dependsOn:; - clone; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626
https://github.com/hail-is/hail/pull/7628:650,Availability,error,errors,650,"Three changes:. 1. I noticed the costs were all 0. I think this is due to `cores_mcpu INTO cores_mcpu` scoping issues in the attempt triggers, where the local variable was always zero. At least changing the local variable name immediately fixed the problem. 2. I wrote a test to verify the costs were non-zero and consistent with the reported timing for the succeeding job. It is hard to do on the cost string, so I included msec_mcpu in the batch/job status response to verify it. In trying to verify it, I noticed that timestamps in the attempts table were slightly truncated compared to start/end times in the status (JSON). This lead to rounding errors and slight disagreement. 3. Rather descend into the floating point rabbit hole of madness, I changed times everywhere to be stored as integers in milliseconds (like unix time, since the epoch). In the database, they are not BIGINT. Millisecond resolution seems fine for everything we're building. 4. (Bonus change!) Don't let timing for jobs be negative. This will require another reset.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7628
https://github.com/hail-is/hail/pull/7628:159,Modifiability,variab,variable,159,"Three changes:. 1. I noticed the costs were all 0. I think this is due to `cores_mcpu INTO cores_mcpu` scoping issues in the attempt triggers, where the local variable was always zero. At least changing the local variable name immediately fixed the problem. 2. I wrote a test to verify the costs were non-zero and consistent with the reported timing for the succeeding job. It is hard to do on the cost string, so I included msec_mcpu in the batch/job status response to verify it. In trying to verify it, I noticed that timestamps in the attempts table were slightly truncated compared to start/end times in the status (JSON). This lead to rounding errors and slight disagreement. 3. Rather descend into the floating point rabbit hole of madness, I changed times everywhere to be stored as integers in milliseconds (like unix time, since the epoch). In the database, they are not BIGINT. Millisecond resolution seems fine for everything we're building. 4. (Bonus change!) Don't let timing for jobs be negative. This will require another reset.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7628
https://github.com/hail-is/hail/pull/7628:213,Modifiability,variab,variable,213,"Three changes:. 1. I noticed the costs were all 0. I think this is due to `cores_mcpu INTO cores_mcpu` scoping issues in the attempt triggers, where the local variable was always zero. At least changing the local variable name immediately fixed the problem. 2. I wrote a test to verify the costs were non-zero and consistent with the reported timing for the succeeding job. It is hard to do on the cost string, so I included msec_mcpu in the batch/job status response to verify it. In trying to verify it, I noticed that timestamps in the attempts table were slightly truncated compared to start/end times in the status (JSON). This lead to rounding errors and slight disagreement. 3. Rather descend into the floating point rabbit hole of madness, I changed times everywhere to be stored as integers in milliseconds (like unix time, since the epoch). In the database, they are not BIGINT. Millisecond resolution seems fine for everything we're building. 4. (Bonus change!) Don't let timing for jobs be negative. This will require another reset.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7628
https://github.com/hail-is/hail/pull/7628:271,Testability,test,test,271,"Three changes:. 1. I noticed the costs were all 0. I think this is due to `cores_mcpu INTO cores_mcpu` scoping issues in the attempt triggers, where the local variable was always zero. At least changing the local variable name immediately fixed the problem. 2. I wrote a test to verify the costs were non-zero and consistent with the reported timing for the succeeding job. It is hard to do on the cost string, so I included msec_mcpu in the batch/job status response to verify it. In trying to verify it, I noticed that timestamps in the attempts table were slightly truncated compared to start/end times in the status (JSON). This lead to rounding errors and slight disagreement. 3. Rather descend into the floating point rabbit hole of madness, I changed times everywhere to be stored as integers in milliseconds (like unix time, since the epoch). In the database, they are not BIGINT. Millisecond resolution seems fine for everything we're building. 4. (Bonus change!) Don't let timing for jobs be negative. This will require another reset.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7628
https://github.com/hail-is/hail/issues/7632:1137,Availability,avail,available,1137,"Hi, . I'm using the concordance function to compare two sets of data, and I feel the n_discordant (last column) is not correct. . For example: ; ```; chr1:930314 [""C"",""T""] {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} [[0,0,0,0,0],[0,0,21,1,0],[0,0,2057,0,0],[0,0,0,91,0],[0,0,0,0,3]] 2172; chr1:946538 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,5,3,0],[0,0,1868,1,1],[0,0,0,279,0],[0,0,0,0,16]] 2170; chr1:946653 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,856,275,66],[0,0,386,74,7],[0,0,16,415,33],[0,0,0,3,42]] 1898; ```. In the first example, I thought the n_discordant should be 0 if the `concordance` field is correct, isn't it?. The code I was using: ; `global_GA_both, samples_GA_both, SNPs_GA_both = hl.concordance(mt_exome, mt_GAsP_ft)`. The Hail version:; ```Running on Apache Spark version 2.4.3; SparkUI available at http://spark-master:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.26-2dcc3d963867; LOGGING: writing to Concordance_2019_11_28_hail.log; ```. When I was using google Terra Hail 0.2.11-daed180b84d8, I didn't have this issue. The output didn't have `left_row` or `right_row`. Cheers,; Qinqin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7632
https://github.com/hail-is/hail/issues/7632:1280,Testability,LOG,LOGGING,1280,"Hi, . I'm using the concordance function to compare two sets of data, and I feel the n_discordant (last column) is not correct. . For example: ; ```; chr1:930314 [""C"",""T""] {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} [[0,0,0,0,0],[0,0,21,1,0],[0,0,2057,0,0],[0,0,0,91,0],[0,0,0,0,3]] 2172; chr1:946538 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,5,3,0],[0,0,1868,1,1],[0,0,0,279,0],[0,0,0,0,16]] 2170; chr1:946653 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,856,275,66],[0,0,386,74,7],[0,0,16,415,33],[0,0,0,3,42]] 1898; ```. In the first example, I thought the n_discordant should be 0 if the `concordance` field is correct, isn't it?. The code I was using: ; `global_GA_both, samples_GA_both, SNPs_GA_both = hl.concordance(mt_exome, mt_GAsP_ft)`. The Hail version:; ```Running on Apache Spark version 2.4.3; SparkUI available at http://spark-master:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.26-2dcc3d963867; LOGGING: writing to Concordance_2019_11_28_hail.log; ```. When I was using google Terra Hail 0.2.11-daed180b84d8, I didn't have this issue. The output didn't have `left_row` or `right_row`. Cheers,; Qinqin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7632
https://github.com/hail-is/hail/issues/7632:1328,Testability,log,log,1328,"Hi, . I'm using the concordance function to compare two sets of data, and I feel the n_discordant (last column) is not correct. . For example: ; ```; chr1:930314 [""C"",""T""] {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} [[0,0,0,0,0],[0,0,21,1,0],[0,0,2057,0,0],[0,0,0,91,0],[0,0,0,0,3]] 2172; chr1:946538 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,5,3,0],[0,0,1868,1,1],[0,0,0,279,0],[0,0,0,0,16]] 2170; chr1:946653 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,856,275,66],[0,0,386,74,7],[0,0,16,415,33],[0,0,0,3,42]] 1898; ```. In the first example, I thought the n_discordant should be 0 if the `concordance` field is correct, isn't it?. The code I was using: ; `global_GA_both, samples_GA_both, SNPs_GA_both = hl.concordance(mt_exome, mt_GAsP_ft)`. The Hail version:; ```Running on Apache Spark version 2.4.3; SparkUI available at http://spark-master:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.26-2dcc3d963867; LOGGING: writing to Concordance_2019_11_28_hail.log; ```. When I was using google Terra Hail 0.2.11-daed180b84d8, I didn't have this issue. The output didn't have `left_row` or `right_row`. Cheers,; Qinqin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7632
https://github.com/hail-is/hail/pull/7633:172,Energy Efficiency,reduce,reduce,172,"Ready to look at. Assigning John because this code was related to NDArray. Purpose of this was to clean up the function a bit (some unnecessary assignments, code clarity), reduce the number of loops needed to detect a missing value. We don't need to check every bit O(N) to determine whether a value is missing. Instead, it is sufficient to check groups of at least 1 byte, and in the case that there are more than 64 values, groups of 8 bytes, or 1 byte. We could further improve this by removing the condition in the loop in favor of 2 loops (one over floor(nMissingBytes / 8), one over the remainder), but I think this gets us most of the benefit. We could also check for groups of 4 bytes (int) when groups of 8 bytes (long) are exhausted, but that doesn't really bring us much, since the major benefit comes from the largest batch group. I also factored out the checking function, because I will use this in upcasting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7633
https://github.com/hail-is/hail/pull/7633:209,Safety,detect,detect,209,"Ready to look at. Assigning John because this code was related to NDArray. Purpose of this was to clean up the function a bit (some unnecessary assignments, code clarity), reduce the number of loops needed to detect a missing value. We don't need to check every bit O(N) to determine whether a value is missing. Instead, it is sufficient to check groups of at least 1 byte, and in the case that there are more than 64 values, groups of 8 bytes, or 1 byte. We could further improve this by removing the condition in the loop in favor of 2 loops (one over floor(nMissingBytes / 8), one over the remainder), but I think this gets us most of the benefit. We could also check for groups of 4 bytes (int) when groups of 8 bytes (long) are exhausted, but that doesn't really bring us much, since the major benefit comes from the largest batch group. I also factored out the checking function, because I will use this in upcasting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7633
https://github.com/hail-is/hail/pull/7634:17,Energy Efficiency,schedul,scheduler,17,"The query in the scheduler was running incredibly slowly. So slowly, I didn't have the patience to let it finish. . ```; mysql> EXPLAIN; -> SELECT job_id, batch_id, spec, cores_mcpu,; -> ((jobs.cancelled OR batches.cancelled) AND NOT always_run) AS cancel,; -> userdata, user; -> FROM jobs; -> INNER JOIN batches ON batches.id = jobs.batch_id; -> WHERE jobs.state = 'Ready' AND batches.closed; -> LIMIT 50;; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; | 1 | SIMPLE | batches | ALL | PRIMARY | NULL | NULL | NULL | 31 | Using where |; | 1 | SIMPLE | jobs | ref | PRIMARY,jobs_state | PRIMARY | 8 | batch2.batches.id | 84 | Using where |; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; 2 rows in set (0.01 sec); ```. This is because the query was querying closed batches, and then joining against jobs and using a where condition to find ready jobs. This is an insane execution plan and I still can't believe MySQL is choosing it by default. To fix this, I changed the inner join to a straight join: https://dev.mysql.com/doc/refman/5.6/en/join.html. Straight join is a MySQL extension that always scans the left table first. This leads to the correct execution plan which runs instantly:. ```; mysql> EXPLAIN; -> SELECT job_id, batch_id, spec, cores_mcpu,; -> ((jobs.cancelled OR batches.cancelled) AND NOT always_run) AS cancel,; -> userdata, user; -> FROM jobs; -> STRAIGHT_JOIN batches ON batches.id = jobs.batch_id; -> WHERE jobs.state = 'Ready' AND batches.closed; -> LIMIT 50;; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; | id | select_ty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7634
https://github.com/hail-is/hail/pull/7634:746,Usability,SIMPL,SIMPLE,746,"The query in the scheduler was running incredibly slowly. So slowly, I didn't have the patience to let it finish. . ```; mysql> EXPLAIN; -> SELECT job_id, batch_id, spec, cores_mcpu,; -> ((jobs.cancelled OR batches.cancelled) AND NOT always_run) AS cancel,; -> userdata, user; -> FROM jobs; -> INNER JOIN batches ON batches.id = jobs.batch_id; -> WHERE jobs.state = 'Ready' AND batches.closed; -> LIMIT 50;; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; | 1 | SIMPLE | batches | ALL | PRIMARY | NULL | NULL | NULL | 31 | Using where |; | 1 | SIMPLE | jobs | ref | PRIMARY,jobs_state | PRIMARY | 8 | batch2.batches.id | 84 | Using where |; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; 2 rows in set (0.01 sec); ```. This is because the query was querying closed batches, and then joining against jobs and using a where condition to find ready jobs. This is an insane execution plan and I still can't believe MySQL is choosing it by default. To fix this, I changed the inner join to a straight join: https://dev.mysql.com/doc/refman/5.6/en/join.html. Straight join is a MySQL extension that always scans the left table first. This leads to the correct execution plan which runs instantly:. ```; mysql> EXPLAIN; -> SELECT job_id, batch_id, spec, cores_mcpu,; -> ((jobs.cancelled OR batches.cancelled) AND NOT always_run) AS cancel,; -> userdata, user; -> FROM jobs; -> STRAIGHT_JOIN batches ON batches.id = jobs.batch_id; -> WHERE jobs.state = 'Ready' AND batches.closed; -> LIMIT 50;; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; | id | select_ty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7634
https://github.com/hail-is/hail/pull/7634:828,Usability,SIMPL,SIMPLE,828,"The query in the scheduler was running incredibly slowly. So slowly, I didn't have the patience to let it finish. . ```; mysql> EXPLAIN; -> SELECT job_id, batch_id, spec, cores_mcpu,; -> ((jobs.cancelled OR batches.cancelled) AND NOT always_run) AS cancel,; -> userdata, user; -> FROM jobs; -> INNER JOIN batches ON batches.id = jobs.batch_id; -> WHERE jobs.state = 'Ready' AND batches.closed; -> LIMIT 50;; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; | 1 | SIMPLE | batches | ALL | PRIMARY | NULL | NULL | NULL | 31 | Using where |; | 1 | SIMPLE | jobs | ref | PRIMARY,jobs_state | PRIMARY | 8 | batch2.batches.id | 84 | Using where |; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; 2 rows in set (0.01 sec); ```. This is because the query was querying closed batches, and then joining against jobs and using a where condition to find ready jobs. This is an insane execution plan and I still can't believe MySQL is choosing it by default. To fix this, I changed the inner join to a straight join: https://dev.mysql.com/doc/refman/5.6/en/join.html. Straight join is a MySQL extension that always scans the left table first. This leads to the correct execution plan which runs instantly:. ```; mysql> EXPLAIN; -> SELECT job_id, batch_id, spec, cores_mcpu,; -> ((jobs.cancelled OR batches.cancelled) AND NOT always_run) AS cancel,; -> userdata, user; -> FROM jobs; -> STRAIGHT_JOIN batches ON batches.id = jobs.batch_id; -> WHERE jobs.state = 'Ready' AND batches.closed; -> LIMIT 50;; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; | id | select_ty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7634
https://github.com/hail-is/hail/pull/7634:2221,Usability,SIMPL,SIMPLE,2221,"----------+; | 1 | SIMPLE | batches | ALL | PRIMARY | NULL | NULL | NULL | 31 | Using where |; | 1 | SIMPLE | jobs | ref | PRIMARY,jobs_state | PRIMARY | 8 | batch2.batches.id | 84 | Using where |; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; 2 rows in set (0.01 sec); ```. This is because the query was querying closed batches, and then joining against jobs and using a where condition to find ready jobs. This is an insane execution plan and I still can't believe MySQL is choosing it by default. To fix this, I changed the inner join to a straight join: https://dev.mysql.com/doc/refman/5.6/en/join.html. Straight join is a MySQL extension that always scans the left table first. This leads to the correct execution plan which runs instantly:. ```; mysql> EXPLAIN; -> SELECT job_id, batch_id, spec, cores_mcpu,; -> ((jobs.cancelled OR batches.cancelled) AND NOT always_run) AS cancel,; -> userdata, user; -> FROM jobs; -> STRAIGHT_JOIN batches ON batches.id = jobs.batch_id; -> WHERE jobs.state = 'Ready' AND batches.closed; -> LIMIT 50;; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; | 1 | SIMPLE | jobs | ref | PRIMARY,jobs_state | jobs_state | 122 | const | 14264 | Using index condition |; | 1 | SIMPLE | batches | eq_ref | PRIMARY | PRIMARY | 8 | batch2.jobs.batch_id | 1 | Using where |; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; 2 rows in set (0.00 sec); ```. Now it scans the jobs table first using the job_state index as desired. Once this goes in I'll turn up the instance pool size again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7634
https://github.com/hail-is/hail/pull/7634:2330,Usability,SIMPL,SIMPLE,2330,"----------+; | 1 | SIMPLE | batches | ALL | PRIMARY | NULL | NULL | NULL | 31 | Using where |; | 1 | SIMPLE | jobs | ref | PRIMARY,jobs_state | PRIMARY | 8 | batch2.batches.id | 84 | Using where |; +----+-------------+---------+------+--------------------+---------+---------+-------------------+------+-------------+; 2 rows in set (0.01 sec); ```. This is because the query was querying closed batches, and then joining against jobs and using a where condition to find ready jobs. This is an insane execution plan and I still can't believe MySQL is choosing it by default. To fix this, I changed the inner join to a straight join: https://dev.mysql.com/doc/refman/5.6/en/join.html. Straight join is a MySQL extension that always scans the left table first. This leads to the correct execution plan which runs instantly:. ```; mysql> EXPLAIN; -> SELECT job_id, batch_id, spec, cores_mcpu,; -> ((jobs.cancelled OR batches.cancelled) AND NOT always_run) AS cancel,; -> userdata, user; -> FROM jobs; -> STRAIGHT_JOIN batches ON batches.id = jobs.batch_id; -> WHERE jobs.state = 'Ready' AND batches.closed; -> LIMIT 50;; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; | 1 | SIMPLE | jobs | ref | PRIMARY,jobs_state | jobs_state | 122 | const | 14264 | Using index condition |; | 1 | SIMPLE | batches | eq_ref | PRIMARY | PRIMARY | 8 | batch2.jobs.batch_id | 1 | Using where |; +----+-------------+---------+--------+--------------------+------------+---------+----------------------+-------+-----------------------+; 2 rows in set (0.00 sec); ```. Now it scans the jobs table first using the job_state index as desired. Once this goes in I'll turn up the instance pool size again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7634
https://github.com/hail-is/hail/pull/7635:562,Availability,error,error,562,"Batch page shows max 50 jobs paginated, with next page button. Search box supports search with a simple language of terms:; - k=v - match jobs with attribute key k and value v; - has:k - match jobs with attribute k, any value; - state - match jobs with the corresponding states. Search state terms are lower case (unlike actual job states, which I plan to change) and the recognized state terms are:. ```; state_query_values = {; 'pending': ['Pending'],; 'ready': ['Ready'],; 'running': ['Running'],; 'live': ['Ready', 'Running'],; 'cancelled': ['Cancelled'],; 'error': ['Error'],; 'failed': ['Failed'],; 'bad': ['Error', 'Failed'],; 'success': ['success'],; 'done': ['Cancelled', 'Error', 'Failed', 'Success']; }; ```; as you can see, some state search terms, like done, match multiple job states. ; - !term - match jobs that don't match term. To select specific names, you can do `name=foo`. Next steps:; - Make the corresponding changes to the API so you can iterate paginated through all jobs in a batch.; - Make batches page paginated, too.; - Help information about the search syntax.; - We'll probably want to order by fields other than just id.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7635
https://github.com/hail-is/hail/pull/7635:572,Availability,Error,Error,572,"Batch page shows max 50 jobs paginated, with next page button. Search box supports search with a simple language of terms:; - k=v - match jobs with attribute key k and value v; - has:k - match jobs with attribute k, any value; - state - match jobs with the corresponding states. Search state terms are lower case (unlike actual job states, which I plan to change) and the recognized state terms are:. ```; state_query_values = {; 'pending': ['Pending'],; 'ready': ['Ready'],; 'running': ['Running'],; 'live': ['Ready', 'Running'],; 'cancelled': ['Cancelled'],; 'error': ['Error'],; 'failed': ['Failed'],; 'bad': ['Error', 'Failed'],; 'success': ['success'],; 'done': ['Cancelled', 'Error', 'Failed', 'Success']; }; ```; as you can see, some state search terms, like done, match multiple job states. ; - !term - match jobs that don't match term. To select specific names, you can do `name=foo`. Next steps:; - Make the corresponding changes to the API so you can iterate paginated through all jobs in a batch.; - Make batches page paginated, too.; - Help information about the search syntax.; - We'll probably want to order by fields other than just id.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7635
https://github.com/hail-is/hail/pull/7635:614,Availability,Error,Error,614,"Batch page shows max 50 jobs paginated, with next page button. Search box supports search with a simple language of terms:; - k=v - match jobs with attribute key k and value v; - has:k - match jobs with attribute k, any value; - state - match jobs with the corresponding states. Search state terms are lower case (unlike actual job states, which I plan to change) and the recognized state terms are:. ```; state_query_values = {; 'pending': ['Pending'],; 'ready': ['Ready'],; 'running': ['Running'],; 'live': ['Ready', 'Running'],; 'cancelled': ['Cancelled'],; 'error': ['Error'],; 'failed': ['Failed'],; 'bad': ['Error', 'Failed'],; 'success': ['success'],; 'done': ['Cancelled', 'Error', 'Failed', 'Success']; }; ```; as you can see, some state search terms, like done, match multiple job states. ; - !term - match jobs that don't match term. To select specific names, you can do `name=foo`. Next steps:; - Make the corresponding changes to the API so you can iterate paginated through all jobs in a batch.; - Make batches page paginated, too.; - Help information about the search syntax.; - We'll probably want to order by fields other than just id.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7635
https://github.com/hail-is/hail/pull/7635:682,Availability,Error,Error,682,"Batch page shows max 50 jobs paginated, with next page button. Search box supports search with a simple language of terms:; - k=v - match jobs with attribute key k and value v; - has:k - match jobs with attribute k, any value; - state - match jobs with the corresponding states. Search state terms are lower case (unlike actual job states, which I plan to change) and the recognized state terms are:. ```; state_query_values = {; 'pending': ['Pending'],; 'ready': ['Ready'],; 'running': ['Running'],; 'live': ['Ready', 'Running'],; 'cancelled': ['Cancelled'],; 'error': ['Error'],; 'failed': ['Failed'],; 'bad': ['Error', 'Failed'],; 'success': ['success'],; 'done': ['Cancelled', 'Error', 'Failed', 'Success']; }; ```; as you can see, some state search terms, like done, match multiple job states. ; - !term - match jobs that don't match term. To select specific names, you can do `name=foo`. Next steps:; - Make the corresponding changes to the API so you can iterate paginated through all jobs in a batch.; - Make batches page paginated, too.; - Help information about the search syntax.; - We'll probably want to order by fields other than just id.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7635
https://github.com/hail-is/hail/pull/7635:97,Usability,simpl,simple,97,"Batch page shows max 50 jobs paginated, with next page button. Search box supports search with a simple language of terms:; - k=v - match jobs with attribute key k and value v; - has:k - match jobs with attribute k, any value; - state - match jobs with the corresponding states. Search state terms are lower case (unlike actual job states, which I plan to change) and the recognized state terms are:. ```; state_query_values = {; 'pending': ['Pending'],; 'ready': ['Ready'],; 'running': ['Running'],; 'live': ['Ready', 'Running'],; 'cancelled': ['Cancelled'],; 'error': ['Error'],; 'failed': ['Failed'],; 'bad': ['Error', 'Failed'],; 'success': ['success'],; 'done': ['Cancelled', 'Error', 'Failed', 'Success']; }; ```; as you can see, some state search terms, like done, match multiple job states. ; - !term - match jobs that don't match term. To select specific names, you can do `name=foo`. Next steps:; - Make the corresponding changes to the API so you can iterate paginated through all jobs in a batch.; - Make batches page paginated, too.; - Help information about the search syntax.; - We'll probably want to order by fields other than just id.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7635
https://github.com/hail-is/hail/pull/7636:259,Availability,toler,toleration,259,"And separate preemptible and non-preemptible workloads to run in their own pools. This should fix the problem where the non-preemptible pool fills up with preemptible things but k8s can't evict. When this is ready to go in, I will remove the preemptible pool toleration and redeploy the infrastructure components by hand.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636
https://github.com/hail-is/hail/pull/7637:57,Testability,log,logs,57,I saw a case where this loop was spinning generating $$$ logs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7637
