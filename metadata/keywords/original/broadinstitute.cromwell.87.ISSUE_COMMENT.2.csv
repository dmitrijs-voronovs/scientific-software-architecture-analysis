id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:890,Modifiability,config,config,890,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:982,Modifiability,config,config,982,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:1036,Modifiability,config,config,1036,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:484,Performance,load,loaded,484,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:731,Usability,simpl,simplest,731,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:248,Availability,resilien,resilient,248,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:463,Integrability,depend,dependencies,463,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:1061,Modifiability,maintainab,maintainability,1061,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:268,Testability,test,testing,268,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:288,Testability,test,testing,288,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:966,Testability,test,testing,966,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:1356,Testability,test,test,1356,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469427900:87,Deployability,install,installing,87,"Another point that Eric and I came across is that a ""Docker in Docker"" solution - i.e. installing Docker inside the Docker container where he's running Cromwell - is not good either because it necessitates pushing and re-pulling the Docker image he's iterating on, which makes for annoyingly long cycle times and can't work with bad or no Internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469427900
https://github.com/broadinstitute/cromwell/pull/1836#issuecomment-273212258:18,Availability,failure,failures,18,"Some genuine test failures and a lot of ToLs. Once you sort that all out, üëç from me. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1836/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1836#issuecomment-273212258
https://github.com/broadinstitute/cromwell/pull/1836#issuecomment-273212258:13,Testability,test,test,13,"Some genuine test failures and a lot of ToLs. Once you sort that all out, üëç from me. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1836/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1836#issuecomment-273212258
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261:229,Deployability,update,update,229,It's expected that `wdltool 0.4` will not validate this as the `String main_output = hello_and_goodbye.hello_output` syntax in workflow outputs was introduced specifically for sub workflows which `wdltool 0.4` pre-dates.; Try to update to the latest version of wdltool and it should validate.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261:42,Security,validat,validate,42,It's expected that `wdltool 0.4` will not validate this as the `String main_output = hello_and_goodbye.hello_output` syntax in workflow outputs was introduced specifically for sub workflows which `wdltool 0.4` pre-dates.; Try to update to the latest version of wdltool and it should validate.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261:283,Security,validat,validate,283,It's expected that `wdltool 0.4` will not validate this as the `String main_output = hello_and_goodbye.hello_output` syntax in workflow outputs was introduced specifically for sub workflows which `wdltool 0.4` pre-dates.; Try to update to the latest version of wdltool and it should validate.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:18,Deployability,release,release,18,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:458,Deployability,update,update,458,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:138,Safety,avoid,avoid,138,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:267,Security,validat,validate,267,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:512,Security,validat,validate,512,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271878243:34,Deployability,release,release,34,"Are you guys going to do a proper release? Or label an existing tag as; newest release?. On Tue, Jan 10, 2017 at 4:35 PM, Jeff Gentry <notifications@github.com>; wrote:. > @Horneth <https://github.com/Horneth> @LeeTL1220; > <https://github.com/LeeTL1220> That's a good point. The latest version of; > wdltool should always work against the latest wdl4s/Cromwell; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271705111>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk07G1Ukk27IO1RNKJa46Dg9Vs14uks5rQ_mWgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271878243
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271878243:79,Deployability,release,release,79,"Are you guys going to do a proper release? Or label an existing tag as; newest release?. On Tue, Jan 10, 2017 at 4:35 PM, Jeff Gentry <notifications@github.com>; wrote:. > @Horneth <https://github.com/Horneth> @LeeTL1220; > <https://github.com/LeeTL1220> That's a good point. The latest version of; > wdltool should always work against the latest wdl4s/Cromwell; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271705111>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk07G1Ukk27IO1RNKJa46Dg9Vs14uks5rQ_mWgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271878243
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271884647:15,Deployability,release,release,15,Just did a 0.8 release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271884647
https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271909372:52,Deployability,release,releases,52,Check out https://github.com/broadinstitute/wdltool/releases/tag/0.8,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271909372
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-328092858:90,Integrability,depend,depending,90,"Ive also come across a few scenarios where a task was run that produced different output, depending on whether or not some flag was set in an input file. In most cases I just name the files the same, or use a glob, but there have been some scenarios where the resulting names were totally different with different extensions and I had to create seperate tasks for them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-328092858
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-338062727:69,Testability,log,logic,69,I want to +1 on this issue. . This is useful in a WDL as you can add logic to make the WDL easier/cleaner to read and maintain:. > if (defined(upstream_task.optional_output)) {...},MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-338062727
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-339786407:507,Integrability,protocol,protocol,507,Hi @dheiman - I know you're already aware as you're on the openwdl mailing list but just wanted to point out that w/ the move to the OpenWDL governance Cromwell issues aren't the path to effect change in WDL. . I've been looking at how to transfer WDL related issues from this repo to that one but in the meantime note that the way to have changes find their way into the spec would be to discuss the topic on the mail list/gitter and ultimately to open a PR with proposed changes to the spec. See the [rfc protocol](https://github.com/openwdl/wdl/blob/master/GOVERNANCE.md#rfc-process) for more details.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-339786407
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503:1085,Integrability,protocol,protocol,1085,"@geoffjentry This issue has been in the WDL repo ([#20](https://github.com/openwdl/wdl/issues/20)) for nearly 2 years, and has the blessing of the originator of WDL (@scottfrazer). And we can see here that at least one of the current core team members (@patmagee) has voiced support. I think a good initial pass for OpenWDL should be to go through the outstanding issue list rather than start from scratch. Having to resubmit via the mailing list and go through the entire RFC procedure seems extremely heavy-handed for someone who's an end-user of WDL via either FireCloud or Cromwell. This is a capability I, and several others, have a strong use-case for. While I might have suggestions on how its implementation should look like (same as for inputs, just in the output section), in the grand scheme of things, all I want is a capability that will make FireCloud/Cromwell easier for me to use; I care about the ""what"", not the ""how"". There needs to continue to be a path for end-user-requested enhancements, rather than just developer-requested enhancements, which is what the [rfc protocol](https://github.com/openwdl/wdl/blob/master/GOVERNANCE.md#rfc-process), as outlined, really seems geared towards.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503:997,Modifiability,enhance,enhancements,997,"@geoffjentry This issue has been in the WDL repo ([#20](https://github.com/openwdl/wdl/issues/20)) for nearly 2 years, and has the blessing of the originator of WDL (@scottfrazer). And we can see here that at least one of the current core team members (@patmagee) has voiced support. I think a good initial pass for OpenWDL should be to go through the outstanding issue list rather than start from scratch. Having to resubmit via the mailing list and go through the entire RFC procedure seems extremely heavy-handed for someone who's an end-user of WDL via either FireCloud or Cromwell. This is a capability I, and several others, have a strong use-case for. While I might have suggestions on how its implementation should look like (same as for inputs, just in the output section), in the grand scheme of things, all I want is a capability that will make FireCloud/Cromwell easier for me to use; I care about the ""what"", not the ""how"". There needs to continue to be a path for end-user-requested enhancements, rather than just developer-requested enhancements, which is what the [rfc protocol](https://github.com/openwdl/wdl/blob/master/GOVERNANCE.md#rfc-process), as outlined, really seems geared towards.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503:1048,Modifiability,enhance,enhancements,1048,"@geoffjentry This issue has been in the WDL repo ([#20](https://github.com/openwdl/wdl/issues/20)) for nearly 2 years, and has the blessing of the originator of WDL (@scottfrazer). And we can see here that at least one of the current core team members (@patmagee) has voiced support. I think a good initial pass for OpenWDL should be to go through the outstanding issue list rather than start from scratch. Having to resubmit via the mailing list and go through the entire RFC procedure seems extremely heavy-handed for someone who's an end-user of WDL via either FireCloud or Cromwell. This is a capability I, and several others, have a strong use-case for. While I might have suggestions on how its implementation should look like (same as for inputs, just in the output section), in the grand scheme of things, all I want is a capability that will make FireCloud/Cromwell easier for me to use; I care about the ""what"", not the ""how"". There needs to continue to be a path for end-user-requested enhancements, rather than just developer-requested enhancements, which is what the [rfc protocol](https://github.com/openwdl/wdl/blob/master/GOVERNANCE.md#rfc-process), as outlined, really seems geared towards.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342191752:56,Usability,clear,clear,56,"Thanks for that additional info, Jeff. Just so I can be clear: other than ""age"" and ""more detail,"" is there an essential difference between the aims of what David linked to this thread and the aims of the OP? The goal of the OP is to have the WDL spec change to support optional outputs, correct? And David is essentially adding a +1 [or +10, if that's legal :) ] to this. Regardless, we're happy to put forth effort towards the OpenWDL forums as you suggest, and possibly even contribute code when possible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342191752
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342223670:529,Deployability,update,update,529,"@noblem The primary difference is that the path forward to have the spec change is to present a PR and not to have an issue open. My point in my last reply is that someone might see it at some point and make a PR for you but there's no guarantee of that happening in a timely fashion whereas opening a PR against the spec yourself does guarantee that happening :). FWIW I realize that the switcharoo in process can be frustrating. As an example of how we feel the pain as well, there are cases where we (Cromwell team) forgot to update the spec document when we made changes which now means that those changes aren't actually part of the spec. So for instance [this PR](https://github.com/openwdl/wdl/pull/148) is just trying to specify behavior Cromwell already does but is likely to wind up different than what we did & thus making Cromwell non-conforming. That's what we get for being lazy 6 months ago :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342223670
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-520618084:6,Usability,clear,clear,6,"To be clear, this is actually the current cromwell behavior, even though it's not in the spec, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-520618084
https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-274351812:54,Performance,concurren,concurrent-job-limit,54,Despite the text in the issue I'm assuming you meant `concurrent-job-limit`?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-274351812
https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:85,Deployability,Configurat,Configuration,85,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191
https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:85,Modifiability,Config,Configuration,85,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191
https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:190,Modifiability,config,configure-cromwell,190,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191
https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:13,Performance,concurren,concurrent-job-limit,13,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191
https://github.com/broadinstitute/cromwell/issues/1843#issuecomment-330608524:82,Security,hash,hash,82,@geoffjentry how much effort would it be to include the last modified date in the hash? ; @meganshand do you still want this feature? Or have you found a workaround?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1843#issuecomment-330608524
https://github.com/broadinstitute/cromwell/issues/1843#issuecomment-519641622:147,Modifiability,Config,Configuring,147,This has been added recently for shared file system: https://github.com/broadinstitute/cromwell/blob/90154ed22b2a78dfbb1c5342a8f0d39164aaeac8/docs/Configuring.md#local-filesystem-options,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1843#issuecomment-519641622
https://github.com/broadinstitute/cromwell/pull/1845#issuecomment-272188593:37,Testability,test,testing,37,"In a dream world, I'd love to see us testing that these attributes are not logged, as expected. But IRL üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1845/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1845#issuecomment-272188593
https://github.com/broadinstitute/cromwell/pull/1845#issuecomment-272188593:75,Testability,log,logged,75,"In a dream world, I'd love to see us testing that these attributes are not logged, as expected. But IRL üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1845/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1845#issuecomment-272188593
https://github.com/broadinstitute/cromwell/issues/1846#issuecomment-274598829:33,Availability,error,error,33,"It appears that this is the same error being referenced in #1782, closing that one but leaving the breadcrumb in case someone wants a stacktrace",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1846#issuecomment-274598829
https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292:39,Availability,failure,failure,39,And/or include a link to stderr in the failure message?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292
https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292:47,Integrability,message,message,47,And/or include a link to stderr in the failure message?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292
https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-272218648:73,Availability,error,error,73,"Might this eventually expand to be a more general purpose ""allow certain error codes to be retried""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-272218648
https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-274350954:218,Security,expose,expose,218,Oops I just opened #1888 which is effectively a dup of this. I'll just add some notes here instead. We discussed in person that the key stakeholders wanted this to always happen a couple of times and that we would not expose this as an option for now. . IMO I would prefer to see this sort of JES-backend-specific retrying being handled as part of the backend's responsibility. I haven't looked closely at the code to assess how much that desire makes sense,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-274350954
https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179:173,Deployability,rolling,rolling,173,"@geoffjentry @cjllanwarne do you think your swagger is in good enough shape now for codegen to work well? Green was hoping to use your client in our next project instead of rolling yet-another-of-our-own, but some of the endpoints we need (top-level query, and labels patch) haven't been implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179
https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179:268,Deployability,patch,patch,268,"@geoffjentry @cjllanwarne do you think your swagger is in good enough shape now for codegen to work well? Green was hoping to use your client in our next project instead of rolling yet-another-of-our-own, but some of the endpoints we need (top-level query, and labels patch) haven't been implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179
https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179:106,Energy Efficiency,Green,Green,106,"@geoffjentry @cjllanwarne do you think your swagger is in good enough shape now for codegen to work well? Green was hoping to use your client in our next project instead of rolling yet-another-of-our-own, but some of the endpoints we need (top-level query, and labels patch) haven't been implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179
https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272593421:29,Testability,benchmark,benchmark,29,"Would be good to run a quick benchmark to see if this makes any actual; difference -- not because it would take a lot of time to implement but; because it would be good to know if this is real or fiction (and if it's; the former we can spread the knowledge). On Friday, January 13, 2017, Jeff Gentry <notifications@github.com> wrote:. > I never thought of this before but when a web connection is made, the; > system will linearly scan through the spray/akka-http directives until it; > finds a match. While it's going to be a tiny overhead in the aggregate it; > could add up and it'd be a trivial change to make sure the endpoints we; > view as more common are at the front; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1858>, or mute the; > thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g7tCLLKPENxDzCVvxaaFel92k1Jxks5rR8Z6gaJpZM4LjLud>; > .; >. -- ; -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272593421
https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272594504:0,Integrability,Depend,Depending,0,"Depending on your definition of `actual` it is actual, the devs themselves confirmed this :) Whether or not it can ever add up to be meaningful for us, who knows. . To be clear I think at most this would be a tiny effect, it just seemed like something which could take 5 mins to do as opposed to some of our real problems ;) As PO I wouldn't worry too much about this ticket :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272594504
https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272594504:171,Usability,clear,clear,171,"Depending on your definition of `actual` it is actual, the devs themselves confirmed this :) Whether or not it can ever add up to be meaningful for us, who knows. . To be clear I think at most this would be a tiny effect, it just seemed like something which could take 5 mins to do as opposed to some of our real problems ;) As PO I wouldn't worry too much about this ticket :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272594504
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3688,Deployability,pipeline,pipelineArgs,3688,9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a8888400,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:2823,Security,access,access,2823,"-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3763,Security,access,access,3763,9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a8888400,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:1889,Testability,log,log,1889,"28Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt""; startTime: '2017-01-13T23:04:22.693611455Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html""; startTime: '2017-01-13T23:08:18.577755879Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt""; startTime: '2017-01-13T23:12:21.761179493Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localC",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:2138,Testability,log,log,2138,"3:04:22.693611455Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html""; startTime: '2017-01-13T23:08:18.577755879Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt""; startTime: '2017-01-13T23:12:21.761179493Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:2232,Testability,benchmark,benchmark,2232,"-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html""; startTime: '2017-01-13T23:08:18.577755879Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt""; startTime: '2017-01-13T23:12:21.761179493Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3112,Testability,log,log,3112,"a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3133,Testability,log,log,3133,"df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3183,Testability,log,log,3183,7191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3201,Testability,log,log,3201,; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/Callin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3459,Testability,benchmark,benchmark,3459,9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a8888400,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3865,Testability,benchmark,benchmark,3865,p.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:4320,Testability,log,logging,4320,ect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/C,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:4503,Testability,log,log,4503,tPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; pr,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:4710,Testability,log,log,4710, ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:4877,Testability,log,log,4877,f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alph,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:4892,Testability,log,log,4892,972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetad,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:5062,Testability,log,log,5062,56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-16952835813372226956; instanceName: ggp-16952835813372226956; machineType: us-central1-b/n1-standard-2; zone: us-central1-b;,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:5542,Testability,benchmark,benchmark,5542,a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-16952835813372226956; instanceName: ggp-16952835813372226956; machineType: us-central1-b/n1-standard-2; zone: us-central1-b; startTime: '2017-01-13T22:46:12Z'; name: operations/ELDjidCZKxiM28n8osukousBIN6K28SDHCoPcHJvZHVjdGlvblF1ZXVl; wm8b6-23c:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145
https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-274493127:550,Security,access,access,550,"I got some clarification. . @ruchim showed me how to use the API to get preemption data. However, using that API I looked for evidence of preemption in four submissions, each launching 1000 workflows, and each workflow conducting a 25-way scatter. All of these scatter jobs are set to run on preemptiible VMs. So that is 4 X 1000 X 25 = 100K jobs run on preemptible machines. Each job takes between 30 minutes and an hour to run. I saw no reported incidents of preemption. I'm not sure that I believe this is the case, and am wondering if either I'm access the preemption data incorrectly, or if cromwell is not reporting is correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-274493127
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-290740467:45,Deployability,update,update,45,"@abaumann is the issue that Cromwell doesn't update the state of the ""Running"" task, which has failed (or stopped), or that it keeps running that task, which should have failed (or stopped)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-290740467
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-308771792:165,Deployability,update,updated,165,"Sorry @katevoss I didn't answer this before - it's the issue that the workflow fails because we are in fail fast state, but the calls inside that workflow don't get updated - so you can get a workflow that says ""Failed"" with a bunch of tasks that say ""Running"", which is confusing to users and they often ask if they are still running or not (the answer is that they are until they get to a final state, but no subsequent parts of the workflow continue after that point)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-308771792
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-308804360:48,Performance,cache,cache,48,"This bug is blocking Cromwell's ability to call cache, due to the fact that Cromwell won't pull ""Running"" calls as ""Succeeded"" ones. This would be very helpful to fix soon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-308804360
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-327935802:78,Deployability,update,updated,78,@knoblett is this still blocking Cromwell's call caching because calls aren't updated as succeeded? I haven't heard this come up in a while.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-327935802
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731:251,Safety,abort,aborts,251,"just want to also be sure to make it clear there are more issues related to that forum post than this specific issue - this one is on making sure the task statuses reach a final state when the workflow ends in a terminal state - that's different than aborts not working - both are an issue, but different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731:37,Usability,clear,clear,37,"just want to also be sure to make it clear there are more issues related to that forum post than this specific issue - this one is on making sure the task statuses reach a final state when the workflow ends in a terminal state - that's different than aborts not working - both are an issue, but different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164:113,Availability,reliab,reliability,113,I don't think these issues block anybody - they just lead to constant questions and give a bad impression of our reliability. People are often worried they are still spending money because it looks that way. I could do a query to probably find how often aborts don't work if that helps. . There isn't a workaround to either issue - only that we tell users it's ok after we dig in to find out that it is and they just deal with the inconsistency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164:254,Safety,abort,aborts,254,I don't think these issues block anybody - they just lead to constant questions and give a bad impression of our reliability. People are often worried they are still spending money because it looks that way. I could do a query to probably find how often aborts don't work if that helps. . There isn't a workaround to either issue - only that we tell users it's ok after we dig in to find out that it is and they just deal with the inconsistency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869:199,Availability,reliab,reliable,199,"right now that's not the default in FC, nor do we expose it in the UI - people have used it and it does help for some circumstances where you need it, but it seems like overkill when all you want is reliable statuses. it also won't help with the aborting issue which is what the gatk post was",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869:246,Safety,abort,aborting,246,"right now that's not the default in FC, nor do we expose it in the UI - people have used it and it does help for some circumstances where you need it, but it seems like overkill when all you want is reliable statuses. it also won't help with the aborting issue which is what the gatk post was",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869:50,Security,expose,expose,50,"right now that's not the default in FC, nor do we expose it in the UI - people have used it and it does help for some circumstances where you need it, but it seems like overkill when all you want is reliable statuses. it also won't help with the aborting issue which is what the gatk post was",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334499574:70,Safety,abort,aborting,70,@bradtaylor can you clarify which issue is more concerning? Is it the aborting issue or the status updating?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334499574
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788:272,Safety,abort,aborted,272,"There are two tables in the cromwell database that are out of sync. Unofficially, if one knows what they're looking for, one can edit the values directly in the database. Often the case is the `WorkflowStoreEntry` doesn't have a record for the workflow, thus it cannot be aborted or resumed-on-restart, yet the last row in `MetadataEntry` says the workflow is still `Running`. A ""reconciler"" could write a final row into `MetadataEntry` with Aborted/Success/Fail. This could be called:; - Manually by a user/service; - Automatically whenever a user requests status of a workflow; - Automatically by a MetadataEntry sweeper looking for workflows w/o a finalization and no row in WorkflowStoreEntry; - Other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788:442,Safety,Abort,Aborted,442,"There are two tables in the cromwell database that are out of sync. Unofficially, if one knows what they're looking for, one can edit the values directly in the database. Often the case is the `WorkflowStoreEntry` doesn't have a record for the workflow, thus it cannot be aborted or resumed-on-restart, yet the last row in `MetadataEntry` says the workflow is still `Running`. A ""reconciler"" could write a final row into `MetadataEntry` with Aborted/Success/Fail. This could be called:; - Manually by a user/service; - Automatically whenever a user requests status of a workflow; - Automatically by a MetadataEntry sweeper looking for workflows w/o a finalization and no row in WorkflowStoreEntry; - Other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788:283,Usability,resume,resumed-on-restart,283,"There are two tables in the cromwell database that are out of sync. Unofficially, if one knows what they're looking for, one can edit the values directly in the database. Often the case is the `WorkflowStoreEntry` doesn't have a record for the workflow, thus it cannot be aborted or resumed-on-restart, yet the last row in `MetadataEntry` says the workflow is still `Running`. A ""reconciler"" could write a final row into `MetadataEntry` with Aborted/Success/Fail. This could be called:; - Manually by a user/service; - Automatically whenever a user requests status of a workflow; - Automatically by a MetadataEntry sweeper looking for workflows w/o a finalization and no row in WorkflowStoreEntry; - Other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334862844:156,Safety,abort,aborting-state,156,"This was reported by another [FireCloud user](https://gatkforums.broadinstitute.org/firecloud/discussion/10352/removing-workflows-stuck-in-the-submitted-or-aborting-state#latest); Is there a way that we can force these jobs into Aborted so that the workspace stops appearing as ""Running""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334862844
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334862844:229,Safety,Abort,Aborted,229,"This was reported by another [FireCloud user](https://gatkforums.broadinstitute.org/firecloud/discussion/10352/removing-workflows-stuck-in-the-submitted-or-aborting-state#latest); Is there a way that we can force these jobs into Aborted so that the workspace stops appearing as ""Running""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334862844
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019:11,Deployability,update,update,11,"Yes we can update the database manually but I hesitate to do that unless it's really serious. Since this specific ticket is not about aborts, should this be moved to a ticket about aborts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019:134,Safety,abort,aborts,134,"Yes we can update the database manually but I hesitate to do that unless it's really serious. Since this specific ticket is not about aborts, should this be moved to a ticket about aborts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019:181,Safety,abort,aborts,181,"Yes we can update the database manually but I hesitate to do that unless it's really serious. Since this specific ticket is not about aborts, should this be moved to a ticket about aborts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:289,Energy Efficiency,charge,charged,289,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:97,Safety,abort,aborted,97,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:149,Safety,abort,abort,149,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:314,Safety,abort,aborting,314,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:351,Safety,abort,abort,351,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335551347:26,Safety,abort,aborts,26,"Don't know the ticket for aborts - but yes we can confirm individual submissions/workflows, however it's tedious process and you need an admin to do it. Almost every time I've checked it's just a matter of statuses being incorrect and not that the machine is still running",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335551347
https://github.com/broadinstitute/cromwell/issues/1868#issuecomment-273642329:186,Performance,race condition,race condition,186,"This sounds very familiar... and in that foggy memory it wasn't what we; thought it was (ie stdout still being flushed by the time we were reading; it). Are we sure it's not a different race condition, with the same; effect? For example, we aren't really waiting until the task is finished; before checking?. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Jan 17, 2017 at 8:44 PM, Paul Grosu <notifications@github.com>; wrote:. > @geoffjentry <https://github.com/geoffjentry> Maybe you write first write; > a file called lock or lock_specific_filename, which denotes that things are; > still in the process of being written. Once all the files have been written; > then you just remove the lock files. Any system trying to read will first; > look for the lock files. If they are found it will wait with an exponential; > or constant periodic backoff, otherwise it will start reading the files.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1868#issuecomment-273358074>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g0OI5VfW6ajtvkrwkyPWzbtmzljWks5rTW6HgaJpZM4LmWsT>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1868#issuecomment-273642329
https://github.com/broadinstitute/cromwell/issues/1872#issuecomment-273598671:45,Availability,error,errors,45,also note that this function retries all 404 errors. But if we know that is an expected outcome we handle it before we get to this function.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1872#issuecomment-273598671
https://github.com/broadinstitute/cromwell/issues/1874#issuecomment-273625290:291,Performance,concurren,concurrent-workflows-limit,291,"One other possible explanation could be a bug we found in the post processing of a successful job when cromwell tries to read the value of the RC file. If for some reason cromwell failed to read it, it would get stuck in a stale running state. If that happens to enough workflows and then a concurrent-workflows-limit set, then it would produce the observed behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1874#issuecomment-273625290
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-273824799:161,Deployability,release,release,161,"Thanks for the report! This should actually be resolved already by https://github.com/broadinstitute/cromwell/pull/1857, so should be fixed in the next Cromwell release. In the meantime (if you're feeling brave), you could try running directly from develop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-273824799
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4751,Availability,error,error,4751,"e_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4632,Deployability,configurat,configuration,4632,"79] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$po",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4219,Integrability,Message,Message,4219,"ConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: command: ""/bin/bash"" ""/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/execution/script.submit""; [2017-01-20 09:31:16,78] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: job id: 2329; [2017-01-20 09:31:16,79] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4632,Modifiability,config,configuration,4632,"79] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$po",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5187,Modifiability,config,config,5187,"17-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.pr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5255,Modifiability,Config,ConfigAsyncJobExecutionActor,5255,"orkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5448,Modifiability,config,config,5448,"/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5512,Modifiability,Config,ConfigAsyncJobExecutionActor,5512,"002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:5948,Performance,concurren,concurrent,5948,ception: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6016,Performance,concurren,concurrent,6016,de was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6084,Performance,concurren,concurrent,6084,class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6544,Performance,concurren,concurrent,6544,"3); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6869,Performance,concurren,concurrent,6869,"pply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:6943,Performance,concurren,concurrent,6943,"concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J1-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:7029,Performance,concurren,concurrent,7029,"n$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J1-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:7107,Performance,concurren,concurrent,7107,"n$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J1-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:428,Testability,test,tests,428,"@cjllanwarne so the secondary issue of the file paths listed in the file generated by write_lines being the original paths rather than the localized ones has been fixed as well? I rewrote my tool to not use optional inputs, but it still failed even in v24 because it couldn't find those files:. ```; $ java -jar ~/bin/cromwell-24.jar run /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/aggregate_mafs.wdl tests/inputs.json; [2017-01-20 09:31:10,44] [info] Slf4jLogger started; [2017-01-20 09:31:10,52] [info] RUN sub-command; [2017-01-20 09:31:10,52] [info] WDL file: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/aggregate_mafs.wdl; [2017-01-20 09:31:10,52] [info] Inputs: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/inputs.json; [2017-01-20 09:31:10,58] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-01-20 09:31:10,63] [info] Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 submitted.; [2017-01-20 09:31:10,63] [info] SingleWorkflowRunnerActor: Workflow submitted 814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:11,29] [info] Running with database db.url = jdbc:hsqldb:mem:396f6af4-b493-451b-ad19-2042625bf63e;shutdown=false;hsqldb.tx=mvcc; [2017-01-20 09:31:15,99] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-01-20 09:31:16,00] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-01-20 09:31:16,03] [info] Metadata summary refreshing every 2 seconds.; [2017-01-20 09:31:16,06] [info] 1 new workflows fetched; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Starting workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Successfully started WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-01-20 09:31:16,30] [info] MaterializeWorkflowDescriptorActor [814c47aa]: Cal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:793,Testability,test,tests,793,"@cjllanwarne so the secondary issue of the file paths listed in the file generated by write_lines being the original paths rather than the localized ones has been fixed as well? I rewrote my tool to not use optional inputs, but it still failed even in v24 because it couldn't find those files:. ```; $ java -jar ~/bin/cromwell-24.jar run /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/aggregate_mafs.wdl tests/inputs.json; [2017-01-20 09:31:10,44] [info] Slf4jLogger started; [2017-01-20 09:31:10,52] [info] RUN sub-command; [2017-01-20 09:31:10,52] [info] WDL file: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/aggregate_mafs.wdl; [2017-01-20 09:31:10,52] [info] Inputs: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/inputs.json; [2017-01-20 09:31:10,58] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-01-20 09:31:10,63] [info] Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 submitted.; [2017-01-20 09:31:10,63] [info] SingleWorkflowRunnerActor: Workflow submitted 814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:11,29] [info] Running with database db.url = jdbc:hsqldb:mem:396f6af4-b493-451b-ad19-2042625bf63e;shutdown=false;hsqldb.tx=mvcc; [2017-01-20 09:31:15,99] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-01-20 09:31:16,00] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-01-20 09:31:16,03] [info] Metadata summary refreshing every 2 seconds.; [2017-01-20 09:31:16,06] [info] 1 new workflows fetched; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Starting workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Successfully started WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-01-20 09:31:16,30] [info] MaterializeWorkflowDescriptorActor [814c47aa]: Cal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:2402,Testability,test,test,2402,"7-01-20 09:31:16,03] [info] Metadata summary refreshing every 2 seconds.; [2017-01-20 09:31:16,06] [info] 1 new workflows fetched; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Starting workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Successfully started WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-01-20 09:31:16,30] [info] MaterializeWorkflowDescriptorActor [814c47aa]: Call-to-Backend assignments: aggregate_mafs_workflow.aggregate_mafs -> Local; [2017-01-20 09:31:16,48] [info] WorkflowExecutionActor-814c47aa-9d11-4c81-a08c-f2b77c002b46 [814c47aa]: Starting calls: aggregate_mafs_workflow.aggregate_mafs:NA:1; [2017-01-20 09:31:16,69] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: python /src/Merge_MAFs.py --suffix test ACC /root/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/execution/wdlarray-8012c813f9ee08681a0b1fb427a70b0d.tmp; [2017-01-20 09:31:16,70] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: executing: docker run --rm -v /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs:/root/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs -i broadgdac/aggregate_mafs:2 /bin/bash /root/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/execution/script; [2017-01-20 09:31:16,71] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: command: ""/bin/bash"" ""/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/execution/script.submit""; [2017-01-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4589,Testability,log,logging,4589,"79] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$po",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4661,Testability,log,log-dead-letters,4661,"4c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4689,Testability,log,log-dead-letters-during-shutdown,4689,"ggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.Sta",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:7889,Testability,test,tests,7889,"n$map$1.apply(Future.scala:237); 	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J1-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:83,Availability,down,down,83,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:103,Availability,error,error,103,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:197,Availability,error,error,197,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:109,Integrability,message,message,109,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:203,Integrability,message,messages,203,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:224,Integrability,message,message,224,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:220,Testability,log,log,220,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193:299,Availability,error,error,299,"FWIW, in the GATK world we just blanket refuse to support anything Windows. Every now and then we get a question from someone who edited a file manually on a Windows box -- but it happens *maybe* twice a year. I wouldn't advocate for putting a huge amount of effort into this beyond recognizing the error and providing an informative message if possible...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193:334,Integrability,message,message,334,"FWIW, in the GATK world we just blanket refuse to support anything Windows. Every now and then we get a question from someone who edited a file manually on a Windows box -- but it happens *maybe* twice a year. I wouldn't advocate for putting a huge amount of effort into this beyond recognizing the error and providing an informative message if possible...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274350415:1001,Deployability,update,updated,1001,"@LeeTL1220 In the file you sent me the lines look like this `/local/cga-fh/cga/Lee_Normal_Analysis/Pair/CESC-HSCX1005-TP-NB--/jobs/capture/mut/oncotate/job.83173721/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated`. In the ""fixed"" one you sent me the lines look like this `/dsde/data/test_dl_oxoq/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated`. Further, even running it through `dos2unix` doesn't change this fact. Was `full_m1_oncotated_list_pc.txt` the actual file which caused the problem? If so could either this have been a GIGO situation or something else in the WDL run putting the wrong paths in your file? I find it hard to believe that those paths are what you meant. Whatever is going on here I don't believe it has to do with DOS-style newline chars as that doesn't seem to matter, even when I forcibly insert them. I'm closing this issue as one way or the other it appears to be a misnomer. However let's continue to followup either here or in person and potentially open a new issue w/ updated info.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274350415
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274498966:1189,Deployability,update,updated,1189,"Are you running 0.24?. On Sun, Jan 22, 2017 at 1:52 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> In the file you sent me the; > lines look like this /local/cga-fh/cga/Lee_Normal_; > Analysis/Pair/CESC-HSCX1005-TP-NB--/jobs/capture/mut/; > oncotate/job.83173721/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated. In; > the ""fixed"" one you sent me the lines look like this; > /dsde/data/test_dl_oxoq/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated.; > Further, even running it through dos2unix doesn't change this fact.; >; > Was full_m1_oncotated_list_pc.txt the actual file which caused the; > problem? If so could either this have been a GIGO situation or something; > else in the WDL run putting the wrong paths in your file? I find it hard to; > believe that those paths are what you meant.; >; > Whatever is going on here I don't believe it has to do with DOS-style; > newline chars as that doesn't seem to matter, even when I forcibly insert; > them.; >; > I'm closing this issue as one way or the other it appears to be a; > misnomer. However let's continue to followup either here or in person and; > potentially open a new issue w/ updated info.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274350415>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk29mFygmQZGpLlqpvgZcjpAsa6BCks5rU6VtgaJpZM4LoLVh>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274498966
https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-278390963:41,Availability,failure,failure,41,"Within the Broad, this file will cause a failure: /dsde/data/test_dl_oxoq/v2/nt/tumor_list.original",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-278390963
https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273869187:75,Availability,down,down,75,"Looks like the data in mysql will be lost when the docker compose is shut 'down'. Is that true? If so, it would be good to document (or make it so) to mount in a volume in the mysql docker compose so the database survives a restart",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273869187
https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273878008:118,Availability,down,down,118,@kcibul I added a volume entry to mount the mysql data directory onto the host so the data survives a `docker-compose down`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273878008
https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898:447,Availability,error,errors,447,"@katevoss IIRC the intended behavior is that submitted files are stored as-is no matter what and then when we pick up the workflow we check to see if everything is valid. However @cjllanwarne noticed that we are actually validating one of the input files at actual submission time which led to two issues: a) there was a reason why we didn't want to do that in the first place, b) there was a suspicion that this could lead to timeouts instead of errors anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898
https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898:427,Safety,timeout,timeouts,427,"@katevoss IIRC the intended behavior is that submitted files are stored as-is no matter what and then when we pick up the workflow we check to see if everything is valid. However @cjllanwarne noticed that we are actually validating one of the input files at actual submission time which led to two issues: a) there was a reason why we didn't want to do that in the first place, b) there was a suspicion that this could lead to timeouts instead of errors anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898
https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898:221,Security,validat,validating,221,"@katevoss IIRC the intended behavior is that submitted files are stored as-is no matter what and then when we pick up the workflow we check to see if everything is valid. However @cjllanwarne noticed that we are actually validating one of the input files at actual submission time which led to two issues: a) there was a reason why we didn't want to do that in the first place, b) there was a suspicion that this could lead to timeouts instead of errors anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898
https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341:149,Availability,error,error,149,"Per my investigation of #1740 I can confirm the validation is running synchronous to submission, which it should not be. But the API is returning an error for malformed input and not just timing out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341
https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341:48,Security,validat,validation,48,"Per my investigation of #1740 I can confirm the validation is running synchronous to submission, which it should not be. But the API is returning an error for malformed input and not just timing out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341
https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328299188:81,Security,validat,validation,81,"@katevoss Without looking at it, I suspect this would be as easy as removing the validation check. I have a feeling we're also checking where we're supposed to be, making this check not only in the wrong location but also superfluous. My vague recollection was that this was added after the fact by a well intentioned do gooder.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328299188
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:527,Integrability,message,messages,527,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:317,Testability,log,logs,317,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:347,Testability,log,logs,347,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:1004,Usability,simpl,simple,1004,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281770857:48,Security,expose,expose,48,"I think the main problem is that in the log you expose a lot of akka internals that are not easy understand even for people who worked with akka, maybe several log levels will be good? I think by defaul all this akka-internal crap will be useless",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281770857
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281770857:40,Testability,log,log,40,"I think the main problem is that in the log you expose a lot of akka internals that are not easy understand even for people who worked with akka, maybe several log levels will be good? I think by defaul all this akka-internal crap will be useless",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281770857
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281770857:160,Testability,log,log,160,"I think the main problem is that in the log you expose a lot of akka internals that are not easy understand even for people who worked with akka, maybe several log levels will be good? I think by defaul all this akka-internal crap will be useless",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281770857
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971:395,Modifiability,variab,variables,395,"this has come up a few times in a few different issues and that multitude actually makes the larger point here. Internally we've been discussing how to handle this as an upcoming project. In particular the problem is that we have too many different user personas and trying to have a single form of log meet all of their needs is going to be useless. Log levels doesn't quite capture all of the variables that might be in play here as often what happens is that someone 99% of the time only wants to see form X but once in a while *really* needs to see form Y and it's useless if Y wasn't captured at all. We're going to be moving towards some sort of system where there are different sorts of logs and then everyone can be happy, or at least happier. That's probably at least a ""next quarter"" level of project, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971:299,Testability,log,log,299,"this has come up a few times in a few different issues and that multitude actually makes the larger point here. Internally we've been discussing how to handle this as an upcoming project. In particular the problem is that we have too many different user personas and trying to have a single form of log meet all of their needs is going to be useless. Log levels doesn't quite capture all of the variables that might be in play here as often what happens is that someone 99% of the time only wants to see form X but once in a while *really* needs to see form Y and it's useless if Y wasn't captured at all. We're going to be moving towards some sort of system where there are different sorts of logs and then everyone can be happy, or at least happier. That's probably at least a ""next quarter"" level of project, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971:351,Testability,Log,Log,351,"this has come up a few times in a few different issues and that multitude actually makes the larger point here. Internally we've been discussing how to handle this as an upcoming project. In particular the problem is that we have too many different user personas and trying to have a single form of log meet all of their needs is going to be useless. Log levels doesn't quite capture all of the variables that might be in play here as often what happens is that someone 99% of the time only wants to see form X but once in a while *really* needs to see form Y and it's useless if Y wasn't captured at all. We're going to be moving towards some sort of system where there are different sorts of logs and then everyone can be happy, or at least happier. That's probably at least a ""next quarter"" level of project, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971:694,Testability,log,logs,694,"this has come up a few times in a few different issues and that multitude actually makes the larger point here. Internally we've been discussing how to handle this as an upcoming project. In particular the problem is that we have too many different user personas and trying to have a single form of log meet all of their needs is going to be useless. Log levels doesn't quite capture all of the variables that might be in play here as often what happens is that someone 99% of the time only wants to see form X but once in a while *really* needs to see form Y and it's useless if Y wasn't captured at all. We're going to be moving towards some sort of system where there are different sorts of logs and then everyone can be happy, or at least happier. That's probably at least a ""next quarter"" level of project, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:49,Integrability,message,messages,49,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:568,Modifiability,variab,variables,568,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:21,Performance,Queue,Queue,21,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:469,Testability,log,log,469,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:521,Testability,Log,Log,521,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:885,Testability,log,logs,885,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:32,Usability,guid,guideline,32,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281830294:276,Testability,log,log,276,"@LeeTL1220 But not for all use cases. The problem is that we have too many different types of people coming in and saying ""XYZ is reasonable"" and they're not remotely compatible. The solution we're going with is to stop trying to be all things to all people with a monolithic log. It'll be some time before that happens, but that's the idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281830294
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-330627387:62,Testability,Log,Logs,62,I have transferred some comments from this issue over to the [Logs Feature Spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#).; Closing this in the meantime.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-330627387
https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077:94,Safety,Abort,Aborts,94,"@dvoet I'm going through the backlog and I was wondering if you're still seeing this problem. Aborts are a known issue for Cromwell but I wasn't sure if a newer version of Cromwell happened to do this better. If so, I'll close this out, if not I'll keep it around for when we fix aborts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077
https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077:280,Safety,abort,aborts,280,"@dvoet I'm going through the backlog and I was wondering if you're still seeing this problem. Aborts are a known issue for Cromwell but I wasn't sure if a newer version of Cromwell happened to do this better. If so, I'll close this out, if not I'll keep it around for when we fix aborts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077
https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291919797:213,Safety,Abort,Aborts,213,"Probably. On Wed, Apr 5, 2017 at 10:56 AM Kate Voss <notifications@github.com> wrote:. > @dvoet <https://github.com/dvoet> I'm going through the backlog and I was; > wondering if you're still seeing this problem. Aborts are a known issue for; > Cromwell but I wasn't sure if a newer version of Cromwell happened to do; > this better. If so, I'll close this out, if not I'll keep it around for; > when we fix aborts.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABc2tSUYfndPSfSEpNFx6Aq72x98Src7ks5rs6uVgaJpZM4Lpr-l>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291919797
https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291919797:408,Safety,abort,aborts,408,"Probably. On Wed, Apr 5, 2017 at 10:56 AM Kate Voss <notifications@github.com> wrote:. > @dvoet <https://github.com/dvoet> I'm going through the backlog and I was; > wondering if you're still seeing this problem. Aborts are a known issue for; > Cromwell but I wasn't sure if a newer version of Cromwell happened to do; > this better. If so, I'll close this out, if not I'll keep it around for; > when we fix aborts.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABc2tSUYfndPSfSEpNFx6Aq72x98Src7ks5rs6uVgaJpZM4Lpr-l>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291919797
https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-298697710:97,Deployability,hotfix,hotfix,97,"FYI @katevoss we are seeing this in FireCloud (Alpha environment, ""special snowflake Cromwell 26 hotfix 2"" aka 70741da6). Not often: on the order of 1 out of 10,000.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-298697710
https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496:38,Availability,failure,failures,38,"Saw it again on C26 snowflake:. ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Read timed out""; }],; ""message"": ""Google credentials are invalid: Read timed out""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496
https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496:87,Integrability,message,message,87,"Saw it again on C26 snowflake:. ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Read timed out""; }],; ""message"": ""Google credentials are invalid: Read timed out""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496
https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496:121,Integrability,message,message,121,"Saw it again on C26 snowflake:. ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Read timed out""; }],; ""message"": ""Google credentials are invalid: Read timed out""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496
https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398:82,Availability,error,error,82,"As a **user running workflows**, I want to **see a timing diagram or other useful error message when my workflow has failed before making any calls**, so that **I know why I don't see the timing diagram like I expect.**; - Effort: Small; - Risk: X-Small; - Business value: Small; - I haven't heard any mention of this for a while from customers or other internal folk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398
https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398:88,Integrability,message,message,88,"As a **user running workflows**, I want to **see a timing diagram or other useful error message when my workflow has failed before making any calls**, so that **I know why I don't see the timing diagram like I expect.**; - Effort: Small; - Risk: X-Small; - Business value: Small; - I haven't heard any mention of this for a while from customers or other internal folk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398
https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398:240,Safety,Risk,Risk,240,"As a **user running workflows**, I want to **see a timing diagram or other useful error message when my workflow has failed before making any calls**, so that **I know why I don't see the timing diagram like I expect.**; - Effort: Small; - Risk: X-Small; - Business value: Small; - I haven't heard any mention of this for a while from customers or other internal folk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398
https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441:38,Testability,test,test,38,@LeeTL1220 Do you have a reproducible test case? Otherwise we probably need to close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441
https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147:21,Deployability,configurat,configuration,21,"I have not used this configuration in some time. On Aug 6, 2017 14:39, ""Geraldine Van der Auwera"" <notifications@github.com>; wrote:. @LeeTL1220 <https://github.com/leetl1220> Do you have a reproducible test; case? Otherwise we probably need to close this. ‚Äî; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk2gj4A8fOPuWbRAQvNF1k1H9Ct9Aks5sVgh-gaJpZM4LrbMZ>; .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147
https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147:21,Modifiability,config,configuration,21,"I have not used this configuration in some time. On Aug 6, 2017 14:39, ""Geraldine Van der Auwera"" <notifications@github.com>; wrote:. @LeeTL1220 <https://github.com/leetl1220> Do you have a reproducible test; case? Otherwise we probably need to close this. ‚Äî; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk2gj4A8fOPuWbRAQvNF1k1H9Ct9Aks5sVgh-gaJpZM4LrbMZ>; .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147
https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147:203,Testability,test,test,203,"I have not used this configuration in some time. On Aug 6, 2017 14:39, ""Geraldine Van der Auwera"" <notifications@github.com>; wrote:. @LeeTL1220 <https://github.com/leetl1220> Do you have a reproducible test; case? Otherwise we probably need to close this. ‚Äî; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk2gj4A8fOPuWbRAQvNF1k1H9Ct9Aks5sVgh-gaJpZM4LrbMZ>; .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147
https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497:9,Deployability,update,updated,9,@kshakir updated the config value,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497
https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497:21,Modifiability,config,config,21,@kshakir updated the config value,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497
https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-276699905:137,Energy Efficiency,green,green,137,"Can you rebase and see if centaur on travis-ci/push is happier? I suspect your branch contains stale develop code, since travis-ci/pr is green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-276699905
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-277036104:73,Testability,test,test,73,"Note to fixer: . When fixing this, please also un-ignore the new centaur test `write_line_files`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-277036104
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911:105,Deployability,update,update,105,"@dheiman this is actually more of a feature request as this feature has not yet been implemented. I will update the issue accordingly, thanks for your feedback!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911:151,Usability,feedback,feedback,151,"@dheiman this is actually more of a feature request as this feature has not yet been implemented. I will update the issue accordingly, thanks for your feedback!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-288066284:63,Testability,test,tested,63,"@katevoss going with a suggestion I got from @LeeTL1220 I just tested running this with a local backend, not using docker, and that works. The issue is that it doesn't work for docker. Doesn't that imply that this is a bug rather than a feature request, as it has been implemented, just not with the docker runtime?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-288066284
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-289590686:20,Testability,test,testing,20,"@dheiman After some testing this seems fixed on 25, including running on docker. I would recommend upgrading to 25 and if the issue still persists feel free to re-open this issue !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-289590686
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-293401783:152,Availability,error,error,152,"I'm working on the [Workbench counterpart](https://broadinstitute.atlassian.net/browse/GAWB-1704) to this. I can reproduce an alternate version of this error on Production FireCloud, using Cromwell 25. This is what seems to be happening:; * write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; * file (a) gets localized; * file (b) does not get localized; * the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-293401783
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-293934640:27,Testability,test,test,27,"This includes the code and test data which, when the wdl was modified to run locally rather than with a docker, worked on cromwell-24. [aggregate_mafs.zip](https://github.com/broadinstitute/cromwell/files/920152/aggregate_mafs.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-293934640
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:215,Availability,failure,failure,215,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:328,Availability,error,error,328,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:404,Availability,error,error,404,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:531,Deployability,configurat,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,531,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:531,Modifiability,config,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,531,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-296700787:20,Deployability,update,updated,20,"Hey @dheiman - I've updated the JES behaviour of `write_lines` to match the local and SGE behaviour. . Since this seems to have unearthed a slight difference of opinion regarding the expected behaviour in a variety of situations, I made a list of scenarios in [this doc](https://docs.google.com/a/broadinstitute.org/document/d/1WWxtVwZQKrotvJLXfIflYOwidbasf-arDZUByv0Dt14/edit?usp=sharing) - and it'd be awesome if you would consider adding your opinions too! Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-296700787
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-296700787:295,Deployability,a/b,a/broadinstitute,295,"Hey @dheiman - I've updated the JES behaviour of `write_lines` to match the local and SGE behaviour. . Since this seems to have unearthed a slight difference of opinion regarding the expected behaviour in a variety of situations, I made a list of scenarios in [this doc](https://docs.google.com/a/broadinstitute.org/document/d/1WWxtVwZQKrotvJLXfIflYOwidbasf-arDZUByv0Dt14/edit?usp=sharing) - and it'd be awesome if you would consider adding your opinions too! Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-296700787
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031:117,Deployability,configurat,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,117,"I've tested both WDLs [in the forum post](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) on firecloud-dev, which is running Cromwell 27. Neither of them work. Reopening this issue, but there isn't anything for us over in FC to do here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031:117,Modifiability,config,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,117,"I've tested both WDLs [in the forum post](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) on firecloud-dev, which is running Cromwell 27. Neither of them work. Reopening this issue, but there isn't anything for us over in FC to do here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031:5,Testability,test,tested,5,"I've tested both WDLs [in the forum post](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) on firecloud-dev, which is running Cromwell 27. Neither of them work. Reopening this issue, but there isn't anything for us over in FC to do here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-311714438:147,Testability,test,test,147,@helgridly could you send me the WDL and inputs that failed so I can try to recreate this? I'm curious what's missing from our `write_lines_files` test case: (https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/write_lines_files/write_lines_files.wdl),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-311714438
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-311776716:173,Testability,test,tests,173,"I am way overwhelmed right now. The WDLs are in the forum post I linked. The inputs look like this:. ```; mafpaths; [""gs://fc-1a3eda20-6bb1-4337-a9fb-32766ad0fc0d/firecloud-tests/write_lines_bug/example1.maf"" ""gs://fc-1a3eda20-6bb1-4337-a9fb-32766ad0fc0d/firecloud-tests/write_lines_bug/example2.maf""]; out_suffix ""barbar""; out_prefix ""foofoo""; ```. If you have (or make) a gmail account that's not your Broad one that you can use on firecloud-dev, I can share the workspace with you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-311776716
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-311776716:265,Testability,test,tests,265,"I am way overwhelmed right now. The WDLs are in the forum post I linked. The inputs look like this:. ```; mafpaths; [""gs://fc-1a3eda20-6bb1-4337-a9fb-32766ad0fc0d/firecloud-tests/write_lines_bug/example1.maf"" ""gs://fc-1a3eda20-6bb1-4337-a9fb-32766ad0fc0d/firecloud-tests/write_lines_bug/example2.maf""]; out_suffix ""barbar""; out_prefix ""foofoo""; ```. If you have (or make) a gmail account that's not your Broad one that you can use on firecloud-dev, I can share the workspace with you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-311776716
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-519643915:58,Availability,error,error,58,Closing - we believe this is fixed and haven't heard this error pop up since. Closing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-519643915
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275517495:105,Usability,user-friendly,user-friendly,105,"@geoffjentry Admittedly, despite some inaccuracies you might have, the documentation on the site is very user-friendly, special thanks, hopefully, the style itself won't change. I turned to the spec because I needed simpler examples for my own purposes. Nonetheless, this issue is resolved, thank you :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275517495
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275517495:216,Usability,simpl,simpler,216,"@geoffjentry Admittedly, despite some inaccuracies you might have, the documentation on the site is very user-friendly, special thanks, hopefully, the style itself won't change. I turned to the spec because I needed simpler examples for my own purposes. Nonetheless, this issue is resolved, thank you :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275517495
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773:30,Availability,ping,ping,30,@anton-khodak Cool. I'm gonna ping @katevoss and @vdauwera so they see the feedback. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773:75,Usability,feedback,feedback,75,@anton-khodak Cool. I'm gonna ping @katevoss and @vdauwera so they see the feedback. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323:173,Deployability,update,update,173,"@anton-khodak I'm glad to hear that you find the docs user-friendly, can you elaborate what you find particularly helpful? I want to make sure I preserve it as I review and update the docs. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323:54,Usability,user-friendly,user-friendly,54,"@anton-khodak I'm glad to hear that you find the docs user-friendly, can you elaborate what you find particularly helpful? I want to make sure I preserve it as I review and update the docs. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-275703147:89,Availability,error,error,89,"`us-east1-a` is not a legitimate zone, but nevertheless Cromwell's handling of this user error should be better. https://cloud.google.com/compute/docs/regions-zones/regions-zones",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-275703147
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666:8,Availability,error,error,8,The new error message for this is `Task wf_hello.hello:NA:1 failed: error code 5. Message: Some(no zones available)`; Is it acceptable @katevoss ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666:68,Availability,error,error,68,The new error message for this is `Task wf_hello.hello:NA:1 failed: error code 5. Message: Some(no zones available)`; Is it acceptable @katevoss ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666:105,Availability,avail,available,105,The new error message for this is `Task wf_hello.hello:NA:1 failed: error code 5. Message: Some(no zones available)`; Is it acceptable @katevoss ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666:14,Integrability,message,message,14,The new error message for this is `Task wf_hello.hello:NA:1 failed: error code 5. Message: Some(no zones available)`; Is it acceptable @katevoss ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666:82,Integrability,Message,Message,82,The new error message for this is `Task wf_hello.hello:NA:1 failed: error code 5. Message: Some(no zones available)`; Is it acceptable @katevoss ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252:14,Availability,error,error,14,Can that nice error message be removed from its `Option` container?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252:20,Integrability,message,message,20,Can that nice error message be removed from its `Option` container?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-290504375:25,Availability,error,error,25,"It would be great if the error was more descriptive, including what the error is and what the user can do about it (if anything).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-290504375
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-290504375:72,Availability,error,error,72,"It would be great if the error was more descriptive, including what the error is and what the user can do about it (if anything).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-290504375
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329661768:131,Availability,error,error,131,"@Horneth I don't believe this has been improved any further, is that true? Do you have any sense of how often users encounter this error?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329661768
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391:188,Availability,error,error,188,I don't think it's been improved no. I have no idea how often users encounter this. It could be added as a low hanging fruit for User improvement though as it's not a big deal to make the error message more useful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391:194,Integrability,message,message,194,I don't think it's been improved no. I have no idea how often users encounter this. It could be added as a low hanging fruit for User improvement though as it's not a big deal to make the error message more useful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:57,Availability,error,error,57,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:63,Integrability,message,messages,63,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:227,Safety,Risk,Risk,227,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:51,Usability,clear,clear,51,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804:52,Availability,error,error,52,"Running this on Cromwell 35 on PAPI v2 returns this error:; ```Task w.t:NA:1 failed. The job was stopped before the command finished. PAPI error code 5. Execution failed: selecting zone: no regions/zones match request```. AC: For both the PAPI v1/v2 backends, add more context to this error. Something along the lines of...; ```Unable to start job because the zones defined in the runtime parameter zones: ""$zones"" doesn't match zones/regions supported by GCE. Please resubmit the job with a list of supported zones/regions by consulting a list of options here: https://cloud.google.com/compute/docs/regions-zones/```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804:139,Availability,error,error,139,"Running this on Cromwell 35 on PAPI v2 returns this error:; ```Task w.t:NA:1 failed. The job was stopped before the command finished. PAPI error code 5. Execution failed: selecting zone: no regions/zones match request```. AC: For both the PAPI v1/v2 backends, add more context to this error. Something along the lines of...; ```Unable to start job because the zones defined in the runtime parameter zones: ""$zones"" doesn't match zones/regions supported by GCE. Please resubmit the job with a list of supported zones/regions by consulting a list of options here: https://cloud.google.com/compute/docs/regions-zones/```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804:285,Availability,error,error,285,"Running this on Cromwell 35 on PAPI v2 returns this error:; ```Task w.t:NA:1 failed. The job was stopped before the command finished. PAPI error code 5. Execution failed: selecting zone: no regions/zones match request```. AC: For both the PAPI v1/v2 backends, add more context to this error. Something along the lines of...; ```Unable to start job because the zones defined in the runtime parameter zones: ""$zones"" doesn't match zones/regions supported by GCE. Please resubmit the job with a list of supported zones/regions by consulting a list of options here: https://cloud.google.com/compute/docs/regions-zones/```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804
https://github.com/broadinstitute/cromwell/issues/1917#issuecomment-275690267:25,Testability,log,log,25,@geoffjentry I grabbed a log of ctl-\ into a file ``screenlog.0`` and I confirmed that the entire output was there. Unfortunately...; ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$ egrep evaluateOutputs screenlog.0. lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$. ```. ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$ egrep ExecutionActor screenlog.0. lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$. ```. ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$ egrep -i postProcess screenlog.0. lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1917#issuecomment-275690267
https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-275730142:34,Testability,test,tests,34,"As far as I can tell, all Centaur tests in Travis use HSQLDB for Cromwell",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-275730142
https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-328590862:21,Testability,test,test,21,"Sounds like we don't test MySQL, HSQL, or CloudSQL ( #1726 ), is that right?; @cjllanwarne would the effort to test MySQL and HSQL be the same as testing CloudSQL? Who do we know is using MySQL and HSQL?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-328590862
https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-328590862:111,Testability,test,test,111,"Sounds like we don't test MySQL, HSQL, or CloudSQL ( #1726 ), is that right?; @cjllanwarne would the effort to test MySQL and HSQL be the same as testing CloudSQL? Who do we know is using MySQL and HSQL?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-328590862
https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-328590862:146,Testability,test,testing,146,"Sounds like we don't test MySQL, HSQL, or CloudSQL ( #1726 ), is that right?; @cjllanwarne would the effort to test MySQL and HSQL be the same as testing CloudSQL? Who do we know is using MySQL and HSQL?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-328590862
https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-328601540:7,Testability,test,testing,7,We are testing MySQL and HSQL but not CloudSQL. Unfortunately CloudSQL is what we actually use in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1920#issuecomment-328601540
https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999:666,Availability,echo,echo,666,"Crom support went back to redteam; here is the content from the dsde-docs issue:. ----. There's documentation in the CHANGELOG but nothing in the README, though what's in the CHANGELOG might suffice for the README. I don't know any more than this anyway, @Horneth is the expert. üòõ . * Add support for Google Private IPs through `noAddress` runtime attribute. If set to true, the VM will NOT be provided with a public IP address.; *Important*: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If it's not whitelisted and you set this attribute to true, the task will hang.; Defaults to `false`.; e.g:; ```; task {; command {; echo ""I'm private !""; }. runtime {; docker: ""ubuntu:latest""; noAddress: true; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999
https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999:487,Security,Access,Access,487,"Crom support went back to redteam; here is the content from the dsde-docs issue:. ----. There's documentation in the CHANGELOG but nothing in the README, though what's in the CHANGELOG might suffice for the README. I don't know any more than this anyway, @Horneth is the expert. üòõ . * Add support for Google Private IPs through `noAddress` runtime attribute. If set to true, the VM will NOT be provided with a public IP address.; *Important*: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If it's not whitelisted and you set this attribute to true, the task will hang.; Defaults to `false`.; e.g:; ```; task {; command {; echo ""I'm private !""; }. runtime {; docker: ""ubuntu:latest""; noAddress: true; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999
https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999:516,Security,Access,Access,516,"Crom support went back to redteam; here is the content from the dsde-docs issue:. ----. There's documentation in the CHANGELOG but nothing in the README, though what's in the CHANGELOG might suffice for the README. I don't know any more than this anyway, @Horneth is the expert. üòõ . * Add support for Google Private IPs through `noAddress` runtime attribute. If set to true, the VM will NOT be provided with a public IP address.; *Important*: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If it's not whitelisted and you set this attribute to true, the task will hang.; Defaults to `false`.; e.g:; ```; task {; command {; echo ""I'm private !""; }. runtime {; docker: ""ubuntu:latest""; noAddress: true; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999
https://github.com/broadinstitute/cromwell/issues/1924#issuecomment-275759774:47,Modifiability,config,config,47,Do you have a docker hub private token in your config ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924#issuecomment-275759774
https://github.com/broadinstitute/cromwell/issues/1928#issuecomment-276105148:37,Availability,down,download,37,"This tight-looping of the `could not download return code file, retrying:` is typical of a problem that's been fixed in C24. ; If you see the 'never finishing' problem again in C24+, it's probably due to a different cause so please repost with new logs and metadata files (sorry!). Closing this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1928#issuecomment-276105148
https://github.com/broadinstitute/cromwell/issues/1928#issuecomment-276105148:248,Testability,log,logs,248,"This tight-looping of the `could not download return code file, retrying:` is typical of a problem that's been fixed in C24. ; If you see the 'never finishing' problem again in C24+, it's probably due to a different cause so please repost with new logs and metadata files (sorry!). Closing this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1928#issuecomment-276105148
https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276198438:71,Integrability,wrap,wrappers,71,Looking forward to some walkthrough as well to understand the new Path wrappers but this is really cool.; Is there a way this could get us rid of the `xxxProxy` classes ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276198438
https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276229649:268,Integrability,inject,inject,268,"These changes are primarily focused on getting as much code into Standard as possible. Getting JES and SFS perfect wasn't a goal here though. There's a lot more work that could be done to tighten up each of those backends. Regarding JES/GCSFS: the proxy classes, that inject the custom file system providers, could likely be merged with `GcsPath`. I also have my eye on further cleaning up a lot of the path mapping, someday. For example JES calls something like `callRoot.resolve(path.stripPrefix(""/""))` in two different classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276229649
https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276229649:268,Security,inject,inject,268,"These changes are primarily focused on getting as much code into Standard as possible. Getting JES and SFS perfect wasn't a goal here though. There's a lot more work that could be done to tighten up each of those backends. Regarding JES/GCSFS: the proxy classes, that inject the custom file system providers, could likely be merged with `GcsPath`. I also have my eye on further cleaning up a lot of the path mapping, someday. For example JES calls something like `callRoot.resolve(path.stripPrefix(""/""))` in two different classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276229649
https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-277081924:57,Deployability,update,update,57,"Just a friendly reminder that the Changelog could use an update, possibly the Readme ü§ì",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-277081924
https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-276481524:35,Availability,error,errors,35,NB also check how Cromwell reports errors in imported WDL files,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-276481524
https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584:37,Availability,error,error,37,"@cjllanwarne I know you've made some error message improvements, was this one you fixed? Or is it still To-Do?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584
https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584:43,Integrability,message,message,43,"@cjllanwarne I know you've made some error message improvements, was this one you fixed? Or is it still To-Do?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584
https://github.com/broadinstitute/cromwell/issues/1934#issuecomment-327927401:50,Integrability,depend,dependency,50,@geoffjentry this also feels relevant to our repo dependency discussion.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1934#issuecomment-327927401
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:321,Availability,error,error,321,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:335,Availability,failure,failures,335,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:986,Availability,error,error,986,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:351,Integrability,message,message,351,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:268,Modifiability,config,config,268,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:808,Security,access,access,808,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276758340:168,Deployability,release,released,168,"There were indeed a few regressions around optionals introduced when we implemented the conditionals. They should be fixed in C25 (EDIT: I should probably say, not yet released!), so if your WDL has a pattern like anything in the following test case, it should be good: https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/optional_parameter/optional_parameter.wdl. I'll add another test for the optional file input since it's not a case I was aware of. I believe @kshakir has been fixing a lot of file path manipulations recently so it might also be good in C25.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276758340
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276758340:240,Testability,test,test,240,"There were indeed a few regressions around optionals introduced when we implemented the conditionals. They should be fixed in C25 (EDIT: I should probably say, not yet released!), so if your WDL has a pattern like anything in the following test case, it should be good: https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/optional_parameter/optional_parameter.wdl. I'll add another test for the optional file input since it's not a case I was aware of. I believe @kshakir has been fixing a lot of file path manipulations recently so it might also be good in C25.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276758340
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276758340:422,Testability,test,test,422,"There were indeed a few regressions around optionals introduced when we implemented the conditionals. They should be fixed in C25 (EDIT: I should probably say, not yet released!), so if your WDL has a pattern like anything in the following test case, it should be good: https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/optional_parameter/optional_parameter.wdl. I'll add another test for the optional file input since it's not a case I was aware of. I believe @kshakir has been fixing a lot of file path manipulations recently so it might also be good in C25.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276758340
https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276804216:67,Deployability,release,release,67,"I'm closing this ticket because the issue is addressed in our next release (and our develop branch, in case you're feeling brave). I've also added and started working on https://github.com/broadinstitute/cromwell/issues/1940 for the `File?` inputs bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276804216
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276766802:128,Performance,perform,performance,128,"@yfarjoun You're always using a DB, so what you're doing is using an in memory DB. Note that I've found that MySQL gives better performance. When you run against GCS are you also using the default in memory DB?. Are they both the same version? If so, what version. If develop, from when?. I did recently make some changes to develop that should tremendously help what this *might* be but there are also spots which are a lot more single threaded than one would like and it's possible you've found yourself in a situation where that's true on the SFS backend but not JES.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276766802
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276767681:60,Integrability,depend,dependence,60,"version 24.; didn't run this actual wdl in cloud due to the dependence on many (MANY!) local files. another thing I found was that there might be little ""object storage"" space left: ; ```; Heap; PSYoungGen total 1966592K, used 891894K [0x0000000580100000, 0x00000006a7d80000, 0x0000000800000000); eden space 1965568K, 45% used [0x0000000580100000,0x00000005b6715b20,0x00000005f8080000); from space 1024K, 90% used [0x00000006a7c80000,0x00000006a7d68000,0x00000006a7d80000); to space 1536K, 0% used [0x00000006a7a80000,0x00000006a7a80000,0x00000006a7c00000); ParOldGen total 2513408K, used 2436406K [0x0000000080200000, 0x0000000119880000, 0x0000000580100000); object space 2513408K, 96% used [0x0000000080200000,0x0000000114d4db58,0x0000000119880000); Metaspace used 54568K, capacity 55776K, committed 56112K, reserved 1097728K; class space used 7106K, capacity 7334K, committed 7472K, reserved 1048576K; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276767681
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276768648:439,Modifiability,rewrite,rewriteBatchedStatements,439,"It's worth a try. I've found that big (for a relatively modest ""big"") scatters of file paths end up chewing a lot of memory. In particular if you're globbing at all there are definitely some single threaded choke points (which is fine in a system running a ton of WFs, not so good if you're the only one) around the processing of those files. For both you and @meganshand it's worth trying upgrading to develop and making sure to include `rewriteBatchedStatements=true` to your JDBC URL. I think this only helps on MySQL however, I remember having reason to believe it didn't do anything on HSQL but I didn't try it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276768648
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289:49,Availability,down,down,49,"I'm trying it with 16g right now. It still slows down abruptly after completing the first task inside of the scatter. The first task is fast, but then the second task (which depends on the first) is slower.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289:174,Integrability,depend,depends,174,"I'm trying it with 16g right now. It still slows down abruptly after completing the first task inside of the scatter. The first task is fast, but then the second task (which depends on the first) is slower.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276773341:13,Usability,clear,clear,13,"Sorry, to be clear, the second task itself is not slow, but submitting it is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276773341
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276790934:11,Performance,throttle,throttle,11,"I tried to throttle to only 50 jobs at a time, but it didn't seem to help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276790934
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276963200:1020,Availability,down,down,1020,"Yeah, a github-based scan of the code just now does make me think that this is a workflow-level slowdown, which again is total crap if you're the only workflow in the system. I don't know why only 3k wide would manifest here though, I've run things 200k wide and not seen a problem in this spot, and I don't think the processing here would be a function of the inputs/outputs/etc . What I was getting at with the single vs multi-workflow thing though is that many of the cases like this that I've seen the solution is to parallelize the operation, but in a system running thousands of workflows you don't just want to spam a ton of threads to do this as you'll choke out everyone else. In contrast if you're a single workflow on an n-core box you'd want to use as many cores as you can for this. In the general case I don't know how to resolve that other than to set up a special execution context for these sorts of things, default them to having a a bunch of CPU and then letting someone like firecloud configure that down. That said, in this particular area we've previously seen some highly inefficient copying/comparisons going on so that's also a possible culprit here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276963200
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276963200:1005,Modifiability,config,configure,1005,"Yeah, a github-based scan of the code just now does make me think that this is a workflow-level slowdown, which again is total crap if you're the only workflow in the system. I don't know why only 3k wide would manifest here though, I've run things 200k wide and not seen a problem in this spot, and I don't think the processing here would be a function of the inputs/outputs/etc . What I was getting at with the single vs multi-workflow thing though is that many of the cases like this that I've seen the solution is to parallelize the operation, but in a system running thousands of workflows you don't just want to spam a ton of threads to do this as you'll choke out everyone else. In contrast if you're a single workflow on an n-core box you'd want to use as many cores as you can for this. In the general case I don't know how to resolve that other than to set up a special execution context for these sorts of things, default them to having a a bunch of CPU and then letting someone like firecloud configure that down. That said, in this particular area we've previously seen some highly inefficient copying/comparisons going on so that's also a possible culprit here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276963200
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989:4555,Performance,concurren,concurrent,4555,flowExecutionActor.scala:347); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989:4629,Performance,concurren,concurrent,4629,flowExecutionActor.scala:347); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989:4715,Performance,concurren,concurrent,4715,flowExecutionActor.scala:347); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989:4793,Performance,concurren,concurrent,4793,flowExecutionActor.scala:347); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989:3709,Testability,Log,LoggingFSM,3709,rkflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:347); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkj,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989:3789,Testability,Log,LoggingFSM,3789,orkflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:347); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276977989
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277106755:208,Energy Efficiency,Monitor,Monitor,208,is this related to long overheads I'm seeing in a 900 way scatter in firecloud (as I see that firecloud is on 24 now...). https://portal.firecloud.org/#workspaces/broad-ccdg-dev%3AFunctionallyEquivalent-CCDG/Monitor/97002137-7006-47a6-90c8-faf471f0b2d1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277106755
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277107529:77,Usability,simpl,simple,77,"Can't recreate this locally, albeit JES backend and not the same WDL, just a simple 10k wide scatter. (@yfarjoun this isn't an accusation to you, just a note on the ticket). I did notice that MaterializeWorkflowDescriptorActor takes *way* longer to build up w/ my wide scatter wdl than it used to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277107529
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329:57,Availability,error,error,57,"Another note to add that while i can't reproduce Yossi's error, he & I had previously identified `Scope.fullyQualifiedName` as a possible culprit. I decided to look at why MWDA is so much slower and that *is* being gated by the same function, to the tune of (at the time of this writing) 84.6% (and rising) of the total runtime so far.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329:250,Performance,tune,tune,250,"Another note to add that while i can't reproduce Yossi's error, he & I had previously identified `Scope.fullyQualifiedName` as a possible culprit. I decided to look at why MWDA is so much slower and that *is* being gated by the same function, to the tune of (at the time of this writing) 84.6% (and rising) of the total runtime so far.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114757:34,Availability,error,error,34,"Hah, mea culpa! That *is* Yossi's error!. I misread the logs earlier and had been unwittingly sitting at the same step that he's on. It just takes a lot wider of a scatter on my machine than the one he's on apparently. We have reason to believe that that value can't be fully static, but it might be able to be locked in after construction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114757
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114757:56,Testability,log,logs,56,"Hah, mea culpa! That *is* Yossi's error!. I misread the logs earlier and had been unwittingly sitting at the same step that he's on. It just takes a lot wider of a scatter on my machine than the one he's on apparently. We have reason to believe that that value can't be fully static, but it might be able to be locked in after construction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114757
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027:3981,Performance,concurren,concurrent,3981,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027:4055,Performance,concurren,concurrent,4055,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027:4141,Performance,concurren,concurrent,4141,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027:4219,Performance,concurren,concurrent,4219,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027:3135,Testability,Log,LoggingFSM,3135,ble.filter(Traversable.scala:104); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData.workflowCompletionStatus(WorkflowExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkj,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027:3215,Testability,Log,LoggingFSM,3215,ecycle.execution.WorkflowExecutionActorData.workflowCompletionStatus(WorkflowExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496229:195,Performance,bottleneck,bottleneck,195,"Cool thanks. Like I said it's be a couple of days before I can look at what, if anything that affected. When I do I'll look at the profiler again but it's pretty likely that we found a, not the, bottleneck",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496229
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277521031:164,Usability,clear,clear,164,"I'm not trying to say that's it (I haven't thought at all about the FC case) but it's possible. What I saw was that'd it would often take a half hour to an hour to clear out a 40-50k pool of jobs due to quotas. Now imagine if there were hundreds of thousands of jobs being tracked by FC, the problem is way worse. 2 things: I doubt that this was the cas in your example and yes google is aware that this sucks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277521031
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692:273,Performance,bottleneck,bottleneck,273,"Got a chance to look w/ a profiler. Situation definitely *is* improved over previous in that I had to dramatically increase the size of the scatter to get hung up again (this is on my laptop which is apparently way faster than whatever machine @yfarjoun is using). The new bottleneck appears to be ExecutionStore.arePrerequisitesDone, sitting at 99% and growing CPU usage. Specifically the `exists` call in ExecutionStore.isDone and the collect on `key.scope.upstream`. . Note that `isDone` was also the previous hotspot but it doesn't appear to be the FQN calculation any more, rather just the `exists` itself. . It's possible that there's still something we can do a la the FQN here but if not my concern is that this is going to take you into the ""something clever"" realm. BTW @yfarjoun whatever machine you're running this on is also part of your bottleneck. I was able to do a 40k scatter no problem on one of my laptop cores, then just threw in 200k which is what locked it up. If you can't do 1k perhaps retry on something not from the stone age? ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692:851,Performance,bottleneck,bottleneck,851,"Got a chance to look w/ a profiler. Situation definitely *is* improved over previous in that I had to dramatically increase the size of the scatter to get hung up again (this is on my laptop which is apparently way faster than whatever machine @yfarjoun is using). The new bottleneck appears to be ExecutionStore.arePrerequisitesDone, sitting at 99% and growing CPU usage. Specifically the `exists` call in ExecutionStore.isDone and the collect on `key.scope.upstream`. . Note that `isDone` was also the previous hotspot but it doesn't appear to be the FQN calculation any more, rather just the `exists` itself. . It's possible that there's still something we can do a la the FQN here but if not my concern is that this is going to take you into the ""something clever"" realm. BTW @yfarjoun whatever machine you're running this on is also part of your bottleneck. I was able to do a 40k scatter no problem on one of my laptop cores, then just threw in 200k which is what locked it up. If you can't do 1k perhaps retry on something not from the stone age? ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938:564,Performance,bottleneck,bottleneck,564,"I was running this on gsa4. Can you submit sge jobs from your laptop? can; your sctter test on gsa4 submitting to sge before you insult my beautiful; iMac? :-p. On Mon, Feb 6, 2017 at 6:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > Got a chance to look w/ a profiler. Situation definitely *is* improved; > over previous in that I had to dramatically increase the size of the; > scatter to get hung up again (this is on my laptop which is apparently way; > faster than whatever machine @yfarjoun <https://github.com/yfarjoun> is; > using).; >; > The new bottleneck appears to be ExecutionStore.arePrerequisitesDone,; > sitting at 99% and growing CPU usage. Specifically the exists call in; > ExecutionStore.isDone and the collect on key.scope.upstream.; >; > Note that isDone was also the previous hotspot but it doesn't appear to; > be the FQN calculation any more, rather just the exists itself.; >; > It's possible that there's still something we can do a la the FQN here but; > if not my concern is that this is going to take you into the ""something; > clever"" realm.; >; > BTW @yfarjoun <https://github.com/yfarjoun> whatever machine you're; > running this on is also part of your bottleneck. I was able to do a 40k; > scatter no problem on one of my laptop cores, then just threw in 200k which; > is what locked it up. If you can't do 1k perhaps retry on something not; > from the stone age? ;); >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0oXuof6pg3nhG_3qO-drfxxs4dEvks5rZ6zmgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938:1196,Performance,bottleneck,bottleneck,1196,"I was running this on gsa4. Can you submit sge jobs from your laptop? can; your sctter test on gsa4 submitting to sge before you insult my beautiful; iMac? :-p. On Mon, Feb 6, 2017 at 6:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > Got a chance to look w/ a profiler. Situation definitely *is* improved; > over previous in that I had to dramatically increase the size of the; > scatter to get hung up again (this is on my laptop which is apparently way; > faster than whatever machine @yfarjoun <https://github.com/yfarjoun> is; > using).; >; > The new bottleneck appears to be ExecutionStore.arePrerequisitesDone,; > sitting at 99% and growing CPU usage. Specifically the exists call in; > ExecutionStore.isDone and the collect on key.scope.upstream.; >; > Note that isDone was also the previous hotspot but it doesn't appear to; > be the FQN calculation any more, rather just the exists itself.; >; > It's possible that there's still something we can do a la the FQN here but; > if not my concern is that this is going to take you into the ""something; > clever"" realm.; >; > BTW @yfarjoun <https://github.com/yfarjoun> whatever machine you're; > running this on is also part of your bottleneck. I was able to do a 40k; > scatter no problem on one of my laptop cores, then just threw in 200k which; > is what locked it up. If you can't do 1k perhaps retry on something not; > from the stone age? ;); >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0oXuof6pg3nhG_3qO-drfxxs4dEvks5rZ6zmgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938:87,Testability,test,test,87,"I was running this on gsa4. Can you submit sge jobs from your laptop? can; your sctter test on gsa4 submitting to sge before you insult my beautiful; iMac? :-p. On Mon, Feb 6, 2017 at 6:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > Got a chance to look w/ a profiler. Situation definitely *is* improved; > over previous in that I had to dramatically increase the size of the; > scatter to get hung up again (this is on my laptop which is apparently way; > faster than whatever machine @yfarjoun <https://github.com/yfarjoun> is; > using).; >; > The new bottleneck appears to be ExecutionStore.arePrerequisitesDone,; > sitting at 99% and growing CPU usage. Specifically the exists call in; > ExecutionStore.isDone and the collect on key.scope.upstream.; >; > Note that isDone was also the previous hotspot but it doesn't appear to; > be the FQN calculation any more, rather just the exists itself.; >; > It's possible that there's still something we can do a la the FQN here but; > if not my concern is that this is going to take you into the ""something; > clever"" realm.; >; > BTW @yfarjoun <https://github.com/yfarjoun> whatever machine you're; > running this on is also part of your bottleneck. I was able to do a 40k; > scatter no problem on one of my laptop cores, then just threw in 200k which; > is what locked it up. If you can't do 1k perhaps retry on something not; > from the stone age? ;); >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277848692>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0oXuof6pg3nhG_3qO-drfxxs4dEvks5rZ6zmgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277887938
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572:180,Availability,down,down,180,"Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and release machinery? Please don't use it as an experimental pod racer or anything like that. If you take it down it affects user-facing systems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572:74,Deployability,release,release,74,"Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and release machinery? Please don't use it as an experimental pod racer or anything like that. If you take it down it affects user-facing systems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572:58,Testability,test,test,58,"Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and release machinery? Please don't use it as an experimental pod racer or anything like that. If you take it down it affects user-facing systems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902:324,Availability,down,down,324,"lets discuss this in a different channel. On Tue, Feb 7, 2017 at 7:27 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and; > release machinery? Please don't use it as an experimental pod racer or; > anything like that. If you take it down it affects user-facing systems.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0gK6NNq40ngord0qDCt-hwUDqLsYks5raGNCgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902:215,Deployability,release,release,215,"lets discuss this in a different channel. On Tue, Feb 7, 2017 at 7:27 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and; > release machinery? Please don't use it as an experimental pod racer or; > anything like that. If you take it down it affects user-facing systems.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0gK6NNq40ngord0qDCt-hwUDqLsYks5raGNCgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902:196,Testability,test,test,196,"lets discuss this in a different channel. On Tue, Feb 7, 2017 at 7:27 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and; > release machinery? Please don't use it as an experimental pod racer or; > anything like that. If you take it down it affects user-facing systems.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0gK6NNq40ngord0qDCt-hwUDqLsYks5raGNCgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:153,Performance,cache,cache,153,"I'm trying to run the same wdl on gsa5 now with the snapshot (cromwell-25-1d61047-SNAP.jar). I tried it with call caching and it is slow to retrieve the cache hits. One every 5 seconds or so. Here's the relevant jstack:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-34"" #99 prio=5 os_prio=0 tid=0x00002b4fa8024800 nid=0x174b2 runnable [0x00002b4da8502000]; java.lang.Thread.State: RUNNABLE; 	at scala.collection.Iterator$class.exists(Iterator.scala:919); 	at scala.collection.AbstractIterator.exists(Iterator.scala:1336); 	at scala.collection.IterableLike$class.exists(IterableLike.scala:77); 	at scala.collection.AbstractIterable.exists(Iterable.scala:54); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:88); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:87); 	at scala.collection.TraversableLike$$anonfun$filterImpl$1.apply(TraversableLike.scala:248); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247); 	at scala.collection.TraversableLike$class.filter(TraversableLike.scala:259); 	at scala.collection.AbstractTraversable.filter(Traversable.scala:104); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1(WorkflowExecutionActorData.scala:87); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$3.apply(WorkflowExecutionActorData.scala:93); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$3.apply(WorkflowExecutionActorData.scala:92); 	at scala.collection.TraversableLike$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:4140,Performance,concurren,concurrent,4140,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:4214,Performance,concurren,concurrent,4214,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:4300,Performance,concurren,concurrent,4300,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:4378,Performance,concurren,concurrent,4378,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:3294,Testability,Log,LoggingFSM,3294,ble.filter(Traversable.scala:104); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData.workflowCompletionStatus(WorkflowExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkj,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957:3374,Testability,Log,LoggingFSM,3374,ecycle.execution.WorkflowExecutionActorData.workflowCompletionStatus(WorkflowExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278438957
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397:192,Integrability,depend,depends,192,"I tried the same thing without call caching on gsa5, same version of cromwell as above. It's much faster, but still gets slower throughout the workflow. The scatter is made of three jobs each depends on the previous one. The first job, is submitted very quickly to SGE (same job was taking 5 secs per job to retrieve from call caching as per previous note). The second job is submitting faster than before, at 5 secs per job, with the following jstack:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-100"" #166 prio=5 os_prio=0 tid=0x00002b59440ad000 nid=0x17ac7 runnable [0x00002b5b700fe000]; java.lang.Thread.State: RUNNABLE; 	at scala.collection.Iterator$class.exists(Iterator.scala:919); 	at scala.collection.AbstractIterator.exists(Iterator.scala:1336); 	at scala.collection.IterableLike$class.exists(IterableLike.scala:77); 	at scala.collection.AbstractIterable.exists(Iterable.scala:54); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:88); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:87); 	at scala.collection.TraversableLike$$anonfun$filterImpl$1.apply(TraversableLike.scala:248); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247); 	at scala.collection.TraversableLike$class.filter(TraversableLike.scala:259); 	at scala.collection.AbstractTraversable.filter(Traversable.scala:104); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1(WorkflowExecutionActorData.scala:87); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecuti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397:4375,Performance,concurren,concurrent,4375,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397:4449,Performance,concurren,concurrent,4449,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397:4535,Performance,concurren,concurrent,4535,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397:4613,Performance,concurren,concurrent,4613,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397:3529,Testability,Log,LoggingFSM,3529,ble.filter(Traversable.scala:104); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData.workflowCompletionStatus(WorkflowExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkj,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397:3609,Testability,Log,LoggingFSM,3609,ecycle.execution.WorkflowExecutionActorData.workflowCompletionStatus(WorkflowExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292:3831,Performance,concurren,concurrent,3831,startRunnableScopes(WorkflowExecutionActor.scala:357); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:336); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:314); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292:3904,Performance,concurren,concurrent,3904,startRunnableScopes(WorkflowExecutionActor.scala:357); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:336); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:314); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292:3989,Performance,concurren,concurrent,3989,startRunnableScopes(WorkflowExecutionActor.scala:357); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:336); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:314); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292:4066,Performance,concurren,concurrent,4066,startRunnableScopes(WorkflowExecutionActor.scala:357); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:336); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:314); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292:2997,Testability,Log,LoggingFSM,2997,gine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:357); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:336); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:314); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292:3076,Testability,Log,LoggingFSM,3076,engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:357); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:336); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:314); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285708800:405,Modifiability,variab,variable,405,"I suggested in standup today that a batching solution like that in [WriteMetadataActor](https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/metadata/impl/WriteMetadataActor.scala) might help here. i.e. don't sweep the execution store every single time any job changes status, but instead sweep based on a timer and a `var shouldCheckForRunnableJobs: Boolean` variable. Whenever a job changes status `shouldCheckForRunnableJobs` would be assigned `true`. When the timer goes off and if that variable is `true`, check the store for runnable jobs. edit: everywhere I wrote ""changes status"" I meant ""changes to a terminal status""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285708800
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285708800:536,Modifiability,variab,variable,536,"I suggested in standup today that a batching solution like that in [WriteMetadataActor](https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/metadata/impl/WriteMetadataActor.scala) might help here. i.e. don't sweep the execution store every single time any job changes status, but instead sweep based on a timer and a `var shouldCheckForRunnableJobs: Boolean` variable. Whenever a job changes status `shouldCheckForRunnableJobs` would be assigned `true`. When the timer goes off and if that variable is `true`, check the store for runnable jobs. edit: everywhere I wrote ""changes status"" I meant ""changes to a terminal status""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285708800
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288457149:749,Energy Efficiency,monitor,monitor,749,"@horneth Talk to miguel as he was the last to look at this and would have the most up to date information. IIRC where this was left off is that there were a handful of known hotspots, all of the low hanging fruit had been picked and at this point we're talking fundamental changes to how things are being stored/tracked. As you can see from the numbers used it'll vary by the oomph your computer has but you'll want to find a number closer to the ""it starts happening here"" point vs some arbitrarily huge number as IIRC some of the initial chokepoints were less bad than deeper ones but at huge numbers were still horrible to wait through. You'll definitely want to become friends with JProfiler if you're not already friends with it. The lightbend monitor could also be helpful here (although I'm less sure of that for these purposes) but it's probably worth waiting until I'm back for that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288457149
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829:367,Security,access,access,367,"@yfarjoun @meganshand If I point you to a cromwell branch that might fix this problem, is this something you could easily test ?; I did some testing on my own and it's definitely better but hard to tell if it would really solve this without actually testing it for real.; If the answer is ""you can run this WDL yourself and check"" that's also fine. Just need to have access to all the inputs (and figure out how to get permission to run something on SGE..)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829:122,Testability,test,test,122,"@yfarjoun @meganshand If I point you to a cromwell branch that might fix this problem, is this something you could easily test ?; I did some testing on my own and it's definitely better but hard to tell if it would really solve this without actually testing it for real.; If the answer is ""you can run this WDL yourself and check"" that's also fine. Just need to have access to all the inputs (and figure out how to get permission to run something on SGE..)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829:141,Testability,test,testing,141,"@yfarjoun @meganshand If I point you to a cromwell branch that might fix this problem, is this something you could easily test ?; I did some testing on my own and it's definitely better but hard to tell if it would really solve this without actually testing it for real.; If the answer is ""you can run this WDL yourself and check"" that's also fine. Just need to have access to all the inputs (and figure out how to get permission to run something on SGE..)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829:250,Testability,test,testing,250,"@yfarjoun @meganshand If I point you to a cromwell branch that might fix this problem, is this something you could easily test ?; I did some testing on my own and it's definitely better but hard to tell if it would really solve this without actually testing it for real.; If the answer is ""you can run this WDL yourself and check"" that's also fine. Just need to have access to all the inputs (and figure out how to get permission to run something on SGE..)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288859881:95,Modifiability,config,config,95,Thanks ! Based on the Cromwell changelog for 25 I think you should be good to go with the same config. This is the branch: `cromwell-1938`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288859881
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289036596:141,Availability,down,down,141,"@Horneth I tried it with call-caching based on file path only, and while it was significantly faster than it used to be, it still got bogged down. I restarted it with call-caching turned off completely, and then things seemed to work quite well. . It's running over 1000 jobs with no ""Cromwell Overhead"" in the timing diagram. So I would say this works when call-caching is turned off, but with call-caching it is still slow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289036596
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289037124:181,Testability,test,test,181,"My $0.02 here is that this ticket doesn't imply ""with call caching"". The slowdowns will be completely unrelated and the problem which @yfarjoun doesn't involve call caching. If the test WDL went from ""super slow"" to ""AOK"" w/o call caching enabled I'd like to consider this closed. @meganshand WDYT?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289037124
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289040489:216,Performance,perform,performance,216,"@meganshand Sweet, I'm going to close. Out of curiosity do you have any before/after in terms of how long running the wdl took (again, sans call caching)? I'm trying to get a sense of the real world magnitude of the performance improvement for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289040489
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:252,Availability,down,down,252,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:291,Availability,fault,fault,291,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:94,Modifiability,variab,variable,94,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:172,Usability,clear,clear,172,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289051846:89,Performance,cache,cache,89,"@meganshand when call caching was on, you said it was using file paths, what about [call cache copying](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L214) ? Is it full copying or (soft/hard) linking ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289051846
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387:14,Performance,cache,cache,14,"@Horneth Call cache copying seems to be working! It's relatively slow during ""Backend is copying Cached Outputs"", but the other tasks that run don't get stuck in ""Queued in Cromwell"" or ""Cromwell Overhead"". So I'd say that it works with traditional (not just the file path) call caching. . I don't have a thread dump with file path CC on but I could rerun it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387:97,Performance,Cache,Cached,97,"@Horneth Call cache copying seems to be working! It's relatively slow during ""Backend is copying Cached Outputs"", but the other tasks that run don't get stuck in ""Queued in Cromwell"" or ""Cromwell Overhead"". So I'd say that it works with traditional (not just the file path) call caching. . I don't have a thread dump with file path CC on but I could rerun it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387:163,Performance,Queue,Queued,163,"@Horneth Call cache copying seems to be working! It's relatively slow during ""Backend is copying Cached Outputs"", but the other tasks that run don't get stuck in ""Queued in Cromwell"" or ""Cromwell Overhead"". So I'd say that it works with traditional (not just the file path) call caching. . I don't have a thread dump with file path CC on but I could rerun it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289095387
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289103663:39,Usability,clear,clear,39,With some help from @Horneth it became clear that I was using the wrong jar when I ran with file path call caching. Everything seems to be working much faster! Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289103663
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-1330458010:88,Usability,clear,clear,88,I met the same problem. How did you solve it?. > With some help from @Horneth it became clear that I was using the wrong jar when I ran with file path call caching. Everything seems to be working much faster! Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-1330458010
https://github.com/broadinstitute/cromwell/issues/1939#issuecomment-294157945:39,Integrability,message,message,39,"From @ruchim, suggestion for the usage message:. ```; run <WDL file> [<workflow inputs file>] [<workflow options file>] [<workflow metadata output path>] [<WDL zip file>] [<labels file>]. WDL file - WDL file that contains primary workflow to be run. Required. Inputs file - JSON file that has inputs that get passed to WDL file. Optional. Workflow options file - JSON file that contains workflow level options. Optional. Workflow metadata path - File path where workflow metadata is to be outputted. Optional (NOTE: provide an example??). WDL zip file - zip file containing workflows being imported by the primary workflow. Optional (Not required if primary workflow is importing workflows that exist inside the same directory where ; Cromwell is being run.) (NOTE: maybe that last line isn't required??). labels file - JSON file containing labels applied to all tasks in the WDL file. Optional; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1939#issuecomment-294157945
https://github.com/broadinstitute/cromwell/issues/1943#issuecomment-277035010:65,Testability,test,test,65,"The `Int` problem was a bug that's fixed in our develop (see our test cases here to see that they should cover this: https://github.com/broadinstitute/centaur/tree/develop/src/main/resources/standardTestCases/optional_parameter). Unfortunately, optional file inputs is also a known bug that I'm about to pick up. You can track that progress here: https://github.com/broadinstitute/cromwell/issues/1940",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943#issuecomment-277035010
https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277049483:99,Availability,failure,failure,99,"@ruchim The options part of this might duplicate that bug, but I'm not sure since I don't know the failure mode there. The non-localized paths part of this is maybe related to #1944 or could be original, not sure about that either. üò¶",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277049483
https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238:702,Availability,echo,echo,702,"Per discussion with @ruchim and @cjllanwarne, the coercion from `File` to `String` in task declarations can't possibly yield a localized path since wdl4s coercions simply don't have the information required to do this. We're beginning to think File to String coercions probably shouldn't be allowed at all, but I'll keep the issue of disallowing them out of this ticket. There's still an issue with the optional ""filling"" not being transferred between the optional File and optional String. Here's a WDL that reproduces the issue. `coerced_int` and `coerced_string` stringify as empty:. ```; task strings {; Int? int; String? string. Int? coerced_int = string; String? coerced_string = int. command {; echo int: ${int} string: ${string} coerced_int: ${coerced_int} coerced_string: ${coerced_string}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call strings { input: int = 1, string = ""2"" }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238
https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238:164,Usability,simpl,simply,164,"Per discussion with @ruchim and @cjllanwarne, the coercion from `File` to `String` in task declarations can't possibly yield a localized path since wdl4s coercions simply don't have the information required to do this. We're beginning to think File to String coercions probably shouldn't be allowed at all, but I'll keep the issue of disallowing them out of this ticket. There's still an issue with the optional ""filling"" not being transferred between the optional File and optional String. Here's a WDL that reproduces the issue. `coerced_int` and `coerced_string` stringify as empty:. ```; task strings {; Int? int; String? string. Int? coerced_int = string; String? coerced_string = int. command {; echo int: ${int} string: ${string} coerced_int: ${coerced_int} coerced_string: ${coerced_string}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call strings { input: int = 1, string = ""2"" }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238
https://github.com/broadinstitute/cromwell/issues/1946#issuecomment-277272371:126,Usability,simpl,simple,126,"As far as I can tell, Cromwell should just throw this out as invalid WDL, and definitely not trip over itself. Thanks for the simple example though, that's going to make it much easier to debug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1946#issuecomment-277272371
https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277369027:22,Testability,test,tests,22,:+1: a comment in the tests as to why we want to test those things would be awesome. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1949/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277369027
https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277369027:49,Testability,test,test,49,:+1: a comment in the tests as to why we want to test those things would be awesome. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1949/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277369027
https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277398535:263,Testability,test,tests,263,"The conf issue was discovered while reviewing the merged reference.conf with Kate. I was showing her the difference between reference and application confs. We noticed that cromwell's reference conf was attempting to override a value and silently failing. The db tests were just extra tests on top of the emergency fix from last friday. After the actual fix, I thought I'd add tests showing why the SqlConverters were converting empty lobs to nulls. While there, I put in a test showing a slick deadlock also.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277398535
https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277398535:285,Testability,test,tests,285,"The conf issue was discovered while reviewing the merged reference.conf with Kate. I was showing her the difference between reference and application confs. We noticed that cromwell's reference conf was attempting to override a value and silently failing. The db tests were just extra tests on top of the emergency fix from last friday. After the actual fix, I thought I'd add tests showing why the SqlConverters were converting empty lobs to nulls. While there, I put in a test showing a slick deadlock also.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277398535
https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277398535:377,Testability,test,tests,377,"The conf issue was discovered while reviewing the merged reference.conf with Kate. I was showing her the difference between reference and application confs. We noticed that cromwell's reference conf was attempting to override a value and silently failing. The db tests were just extra tests on top of the emergency fix from last friday. After the actual fix, I thought I'd add tests showing why the SqlConverters were converting empty lobs to nulls. While there, I put in a test showing a slick deadlock also.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277398535
https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277398535:474,Testability,test,test,474,"The conf issue was discovered while reviewing the merged reference.conf with Kate. I was showing her the difference between reference and application confs. We noticed that cromwell's reference conf was attempting to override a value and silently failing. The db tests were just extra tests on top of the emergency fix from last friday. After the actual fix, I thought I'd add tests showing why the SqlConverters were converting empty lobs to nulls. While there, I put in a test showing a slick deadlock also.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277398535
https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277409975:53,Testability,test,tests,53,"@kshakir Cool. I forgot that there were two types of tests, was asking about the conf one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277409975
https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706:47,Deployability,update,update,47,@delocalizer Hey sorry I forgot to give you an update on that and I don't know if you've seen it. You were right and the bug should be fixed now. It was due to a bug in the `better files` library we use and I filed a ticket on their github https://github.com/pathikrit/better-files/issues/115.; In the meantime I added a workaround.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706
https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-282145993:26,Deployability,update,update,26,"Thanks @Horneth , for the update and explanation and workaround fix!. ________________________________; From: Thib <notifications@github.com>; Sent: Friday, 24 February 2017 12:44 AM; To: broadinstitute/cromwell; Cc: Conrad; Mention; Subject: Re: [broadinstitute/cromwell] Using Local backend, localization of relative symbolic links doesn't work (#1950). @delocalizer<https://github.com/delocalizer> Hey sorry I forgot to give you an update on that and I don't know if you've seen it. You were right and the bug should be fixed now. It was due to a bug in the better files library we use and I filed a ticket on their github pathikrit/better-files#115<https://github.com/pathikrit/better-files/issues/115>.; In the meantime I added a workaround. ‚Äî; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AAx6ka7RU3i7jBQp7dQaZGPB1z6WtzlRks5rfY05gaJpZM4L1_PC>.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-282145993
https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-282145993:435,Deployability,update,update,435,"Thanks @Horneth , for the update and explanation and workaround fix!. ________________________________; From: Thib <notifications@github.com>; Sent: Friday, 24 February 2017 12:44 AM; To: broadinstitute/cromwell; Cc: Conrad; Mention; Subject: Re: [broadinstitute/cromwell] Using Local backend, localization of relative symbolic links doesn't work (#1950). @delocalizer<https://github.com/delocalizer> Hey sorry I forgot to give you an update on that and I don't know if you've seen it. You were right and the bug should be fixed now. It was due to a bug in the better files library we use and I filed a ticket on their github pathikrit/better-files#115<https://github.com/pathikrit/better-files/issues/115>.; In the meantime I added a workaround. ‚Äî; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AAx6ka7RU3i7jBQp7dQaZGPB1z6WtzlRks5rfY05gaJpZM4L1_PC>.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-282145993
https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488:410,Availability,avail,available,410,"More info:; - Again `WdlFile` is allowing a directory to slip in, while this isn't technically supported. For another example see #1935, and others I cannot locate at the moment.; - With an input file of `""""`, this directory equates to the present working directory.; - Cromwell is attempting to localize the whole directory.; - hard-link to a directory always fails, and since this is docker, soft-link isn't available.; - Cromwell is then copying everything in the pwd. Note, this includes everything under `cromwell-executions`, so it could be potentially large.; - Issuing a Control-C at this point doesn't interrupt the copy localization. I'm not sure how we'll implement aborting copying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488
https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488:677,Safety,abort,aborting,677,"More info:; - Again `WdlFile` is allowing a directory to slip in, while this isn't technically supported. For another example see #1935, and others I cannot locate at the moment.; - With an input file of `""""`, this directory equates to the present working directory.; - Cromwell is attempting to localize the whole directory.; - hard-link to a directory always fails, and since this is docker, soft-link isn't available.; - Cromwell is then copying everything in the pwd. Note, this includes everything under `cromwell-executions`, so it could be potentially large.; - Issuing a Control-C at this point doesn't interrupt the copy localization. I'm not sure how we'll implement aborting copying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488
https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344351642:64,Availability,error,error,64,We do a lot of files of filenames and I think we'd get the same error in the case of an empty line. Right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344351642
https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344357584:71,Usability,intuit,intuition,71,If I understand what's being asked I think so. I'm saying that more on intuition than hard evidence however,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344357584
https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-484219225:70,Testability,test,test,70,"Ensure this doesn't happen on the google backend. Also, add a centaur test for local and google.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-484219225
https://github.com/broadinstitute/cromwell/issues/1955#issuecomment-327927328:45,Integrability,depend,dependency,45,@geoffjentry this feels relevant to our repo dependency discussion.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1955#issuecomment-327927328
https://github.com/broadinstitute/cromwell/issues/1958#issuecomment-278106050:96,Testability,test,test,96,Confirmed broken. We should make sure to re-enable `missing_imports` in centaur as a regression test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958#issuecomment-278106050
https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059:171,Availability,reliab,reliably,171,"We feel this is another in a long series of weird concurrency issues which we see from time to time involving shared filesystems and fs synching. We've never been able to reliably reproduce these, but if someone can provide something here we can reopen and look at it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059
https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059:50,Performance,concurren,concurrency,50,"We feel this is another in a long series of weird concurrency issues which we see from time to time involving shared filesystems and fs synching. We've never been able to reliably reproduce these, but if someone can provide something here we can reopen and look at it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059
https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-291985959:82,Availability,error,errors,82,@cjllanwarne do you think this was resolved by your improvements to JES transient errors?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-291985959
https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-665698064:112,Security,access,access,112,"To accept this bug report, we would probably ask the user to try; ```; gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list ~/test_file; ```; on their local machine to make sure permissions are workable and it's really Cromwell that is screwing up. Unfortunately the user is no longer at Broad (hi Lee!) and I am not aware of a systemic problem in this area, so I will close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-665698064
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:240,Deployability,configurat,configuration,240,"```. 2017/02/07 15:40:50 I: Switching to status: pulling-image; 2017/02/07 15:40:50 I: Calling SetOperationStatus(pulling-image); 2017/02/07 15:40:50 I: SetOperationStatus(pulling-image) succeeded; 2017/02/07 15:40:50 I: Writing new Docker configuration file; 2017/02/07 15:40:50 I: Pulling image ""leetl1220/m1:bs""; 2017/02/07 15:41:48 I: Pulled image ""leetl1220/m1:bs"" successfully.; 2017/02/07 15:41:48 I: Switching to status: localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:240,Modifiability,config,configuration,240,"```. 2017/02/07 15:40:50 I: Switching to status: pulling-image; 2017/02/07 15:40:50 I: Calling SetOperationStatus(pulling-image); 2017/02/07 15:40:50 I: SetOperationStatus(pulling-image) succeeded; 2017/02/07 15:40:50 I: Writing new Docker configuration file; 2017/02/07 15:40:50 I: Pulling image ""leetl1220/m1:bs""; 2017/02/07 15:41:48 I: Pulled image ""leetl1220/m1:bs"" successfully.; 2017/02/07 15:41:48 I: Switching to status: localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1234,Security,access,access,1234," Writing new Docker configuration file; 2017/02/07 15:40:50 I: Pulling image ""leetl1220/m1:bs""; 2017/02/07 15:41:48 I: Pulled image ""leetl1220/m1:bs"" successfully.; 2017/02/07 15:41:48 I: Switching to status: localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list;",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1442,Security,access,access,1442,localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1649,Security,access,access,1649,h maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1837,Security,access,access,1837,a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_asse,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2073,Security,access,access,2073,surgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_lis,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2258,Security,access,access,2258,"nce/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2446,Security,Access,AccessDeniedException,2446,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2516,Security,access,access,2516,"us_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2554,Security,access,access,2554,"us_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2715,Security,access,access,2715,"ent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2900,Security,access,access,2900,"_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3066,Security,Access,AccessDeniedException,3066,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3136,Security,access,access,3136,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3173,Security,access,access,3173,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3327,Security,access,access,3327,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3512,Security,access,access,3512,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3700,Security,Access,AccessDeniedException,3700,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3770,Security,access,access,3770,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3808,Security,access,access,3808,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3969,Security,access,access,3969,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4154,Security,access,access,4154," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4320,Security,Access,AccessDeniedException,4320,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4390,Security,access,access,4390,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4427,Security,access,access,4427,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4581,Security,access,access,4581,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4766,Security,access,access,4766,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4954,Security,Access,AccessDeniedException,4954,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5024,Security,access,access,5024,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5062,Security,access,access,5062,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5223,Security,access,access,5223,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5408,Security,access,access,5408," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5574,Security,Access,AccessDeniedException,5574,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5644,Security,access,access,5644,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5681,Security,access,access,5681,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5835,Security,access,access,5835,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6020,Security,access,access,6020,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6208,Security,Access,AccessDeniedException,6208,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6278,Security,access,access,6278,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6316,Security,access,access,6316,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6477,Security,access,access,6477,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6662,Security,access,access,6662," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6828,Security,Access,AccessDeniedException,6828,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6898,Security,access,access,6898,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6935,Security,access,access,6935,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7089,Security,access,access,7089,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7274,Security,access,access,7274,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7462,Security,Access,AccessDeniedException,7462,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7532,Security,access,access,7532,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7570,Security,access,access,7570,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7731,Security,access,access,7731,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7916,Security,access,access,7916," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8082,Security,Access,AccessDeniedException,8082,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8152,Security,access,access,8152,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8189,Security,access,access,8189,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8343,Security,access,access,8343,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8528,Security,access,access,8528,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8716,Security,Access,AccessDeniedException,8716,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8786,Security,access,access,8786,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8824,Security,access,access,8824,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8985,Security,access,access,8985,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9170,Security,access,access,9170," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9336,Security,Access,AccessDeniedException,9336,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9406,Security,access,access,9406,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9443,Security,access,access,9443,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9597,Security,access,access,9597,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9782,Security,access,access,9782,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9970,Security,Access,AccessDeniedException,9970,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10040,Security,access,access,10040,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10078,Security,access,access,10078,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10239,Security,access,access,10239,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10424,Security,access,access,10424," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10590,Security,Access,AccessDeniedException,10590,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10660,Security,access,access,10660,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10697,Security,access,access,10697,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10851,Security,access,access,10851,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11036,Security,access,access,11036,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11224,Security,Access,AccessDeniedException,11224,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11294,Security,access,access,11294,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11332,Security,access,access,11332,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11493,Security,access,access,11493,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11678,Security,access,access,11678," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11844,Security,Access,AccessDeniedException,11844,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11914,Security,access,access,11914,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11951,Security,access,access,11951,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12105,Security,access,access,12105,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12290,Security,access,access,12290,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12478,Security,Access,AccessDeniedException,12478,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12548,Security,access,access,12548,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12586,Security,access,access,12586,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12747,Security,access,access,12747,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12932,Security,access,access,12932," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13098,Security,Access,AccessDeniedException,13098,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13168,Security,access,access,13168,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13205,Security,access,access,13205,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13359,Security,access,access,13359,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13544,Security,access,access,13544,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13732,Security,Access,AccessDeniedException,13732,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13802,Security,access,access,13802,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/pl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13840,Security,access,access,13840,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/pl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14001,Security,access,access,14001,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14186,Security,access,access,14186," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14352,Security,Access,AccessDeniedException,14352,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14422,Security,access,access,14422,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14459,Security,access,access,14459,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14562,Testability,log,log,14562,"orial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Je",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14584,Testability,log,log,14584," failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14840,Testability,log,log,14840," failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14862,Testability,log,log,14862,"m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence that it's happening.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278113743>,; > or mute the thread; > <https:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:15057,Testability,log,log,15057,"m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence that it's happening.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278113743>,; > or mute the thread; > <https:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:15079,Testability,log,log,15079,"-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence that it's happening.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278113743>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk1Cgl0Kn0A875jDa1oWmlt1Ukw41ks5raMY2gaJpZM4L50dE>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:15335,Testability,log,log,15335,"-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence that it's happening.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278113743>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk1Cgl0Kn0A875jDa1oWmlt1Ukw41ks5raMY2gaJpZM4L50dE>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:15357,Testability,log,log,15357,"-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence that it's happening.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278113743>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk1Cgl0Kn0A875jDa1oWmlt1Ukw41ks5raMY2gaJpZM4L50dE>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974
https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278124726:118,Testability,test,testdata,118,"And here is a different exception that should not retry (CommandException:; No URLs matched:; gs://broad-dsde-methods/testdata/crsp-public-bamsSM-74NEG.bam\nCommandException:; 1 file/object could not be transferred.\n)... Basically, the gs url was bad; -- no need to retry:. operations/EJfO9NChKxj-8IKPzZCav5kBIJ-XkZe9BioPcHJvZHVjdGlvblF1ZXVl. On Tue, Feb 7, 2017 at 3:06 PM, Lee Lichtenstein <; lichtens@broadinstitute.org> wrote:. > Here are a couple of operations IDs:; >; > operations/EKXs9cihKxitjp-C9L64yu8BIJ-XkZe9BioPcHJvZHVjdGlvblF1ZXVl; >; > operations/ENflycmhKxi6nric87SH0VUgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; >; >; >; > On Tue, Feb 7, 2017 at 2:56 PM, Jeff Gentry <notifications@github.com>; > wrote:; >; >> Discussed in person, this is a JES thing and @LeeTL1220; >> <https://github.com/LeeTL1220> is joining our call w/ them.; >>; >> ‚Äî; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278121257>,; >> or mute the thread; >> <https://github.com/notifications/unsubscribe-auth/ACDXkz_R2EPm8L_Cf0qsEuETVTXhXQqgks5raMxvgaJpZM4L50dE>; >> .; >>; >; >; >; > --; > Lee Lichtenstein; > Broad Institute; > 75 Ames Street, Room 7003EB; > Cambridge, MA 02142; > 617 714 8632 <(617)%20714-8632>; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278124726
https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-278176656:79,Availability,down,down,79,I don't disagree in spirit but I think this is a dangerous road to start going down and we'd need to manage it carefully. There are a *lot* of potential variations in the structure of xSV files (example: http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html) and there aren't too many people who manage to handle it gracefully - I don't have faith that we'll be one of the few who find the holy grail there.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-278176656
https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-278204995:103,Testability,mock,mock,103,"Hah no worries, @geoffjentry. I'm almost not offended that you just called my feature request ""crud"". (mock glare)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-278204995
https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-328599964:8,Performance,load,loads,8,"There's loads of scenarios where this would be useful; iirc the case I was working on when I made this request was that I had a FOFN that contained a list of samples with bam and index filepaths, and I wanted to load that in as input to a task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-328599964
https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-328599964:212,Performance,load,load,212,"There's loads of scenarios where this would be useful; iirc the case I was working on when I made this request was that I had a FOFN that contained a list of samples with bam and index filepaths, and I wanted to load that in as input to a task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-328599964
https://github.com/broadinstitute/cromwell/issues/1963#issuecomment-304968047:26,Availability,error,error,26,This is an issue with the error reporting. I have not checked (and I do not remember how to replicate).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963#issuecomment-304968047
https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-278388151:85,Performance,cache,cache,85,"Actually unrelated to docker, this is ""make sure if an output changes, we don't call cache using the modified file""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-278388151
https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:138,Performance,cache,cache,138,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054
https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:126,Safety,safe,safely,126,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054
https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:234,Safety,risk,risk,234,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054
https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:74,Security,hash,hash,74,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054
https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-345303342:33,Availability,robust,robust,33,"@katevoss It'd certainly be more robust and nice to have, however it's unlikely to be some huge fire for poeple",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-345303342
https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279000653:45,Testability,log,logs,45,I am seeing this NPE a ton in our production logs and could also be a cause of jobs generally not progressing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279000653
https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279007438:175,Testability,log,logging,175,Yeah I was going to try to take a look today. It shouldn't be too hard to localize as any `null` is going to be coming from Google's java library. It'll probably require some logging to get more data though.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279007438
https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279012967:278,Availability,error,error,278,One quick thing that could be done to see if the NPE is related to other undesired behavior. In JesApiQueryManager.handleTerminated I put in some logic a while back to automatically retry any polling actor which failed under the belief that it was going to be some goofy google error. That'd also be picking up actors which died via NPE. Changing it we're either specially handling NPE or being specific about what *is* being handled (which should be part of #1914 anyways) would make this better,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279012967
https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279012967:146,Testability,log,logic,146,One quick thing that could be done to see if the NPE is related to other undesired behavior. In JesApiQueryManager.handleTerminated I put in some logic a while back to automatically retry any polling actor which failed under the belief that it was going to be some goofy google error. That'd also be picking up actors which died via NPE. Changing it we're either specially handling NPE or being specific about what *is* being handled (which should be part of #1914 anyways) would make this better,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279012967
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986:73,Availability,down,down,73,":+1:. Very nice! I need to go back and re-read this after my brain cools down, but the tests give me a lot of confidence it's all working. üôÇ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1969/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986:87,Testability,test,tests,87,":+1:. Very nice! I need to go back and re-read this after my brain cools down, but the tests give me a lot of confidence it's all working. üôÇ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1969/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:366,Modifiability,config,config,366,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:553,Modifiability,config,config,553,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:760,Modifiability,config,config,760,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:31,Security,hash,hash,31,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:456,Security,hash,hash,456,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:520,Security,authenticat,authentication,520,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:195,Testability,log,logged,195,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:228,Testability,log,login,228,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:246,Testability,log,login,246,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577
https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765:116,Integrability,message,messages,116,@geoffjentry Closing this issue. I think I figured out what happened and it is okay. Very hard to decipher from log messages.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765
https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765:112,Testability,log,log,112,@geoffjentry Closing this issue. I think I figured out what happened and it is okay. Very hard to decipher from log messages.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765
https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327941555:57,Safety,Abort,Aborting,57,A very quick peek in the Rawls db for workflows still in Aborting gives me 92. The earliest one is from February so I'd _guess_ this is still a problem.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327941555
https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327944618:59,Safety,abort,aborting,59,The workflow d800c362-e8e7-48e2-bb08-3b88226307e9 has been aborting on cromwell1 since 2017-06-27. Rawls is still trying as of the time of me posting this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327944618
https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-342587383:336,Safety,abort,aborted,336,"Closing this, as far as I can tell this should not be happening anymore as per https://github.com/broadinstitute/cromwell/pull/2808 (unless the underlying jobs are indeed stuck in which case Cromwell will wait until they reach a terminal status). If that happens we talked about having a separate endpoint like ""just mark this workflow aborted anyway"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-342587383
https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279144686:30,Performance,optimiz,optimization,30,"üëç ; Just totally ToL for more optimization later, maybe if instead of having like now a `Map[JobKey, Status]`,; what we really want is more something like a `Map[Status, List[JobKey]]` ?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1977/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279144686
https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279423967:31,Modifiability,refactor,refactor,31,"My baseline for these kinds of refactor is ""does it pass our test suites""... so üëç but only once you've got all that sorted out. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1977/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279423967
https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279423967:61,Testability,test,test,61,"My baseline for these kinds of refactor is ""does it pass our test suites""... so üëç but only once you've got all that sorted out. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1977/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279423967
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:64,Availability,recover,recovered,64,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:64,Safety,recover,recovered,64,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:288,Security,authenticat,authentication,288,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:9,Testability,log,log,9,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:207,Testability,test,test,207,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:572,Testability,test,test,572,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:642,Testability,test,test,642,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:821,Testability,test,test,821,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:994,Testability,test,test,994,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1061,Testability,test,test,1061,"low that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1195,Testability,test,test,1195,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1262,Testability,test,test,1262,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1402,Testability,test,test,1402,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1469,Testability,test,test,1469,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1643,Testability,test,test,1643,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1678,Testability,test,test,1678,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1783,Testability,test,test,1783,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1849,Testability,log,log,1849,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1927,Testability,log,logs,1927,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:1956,Testability,log,logs,1956,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:2007,Testability,log,log,2007,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:2061,Testability,log,logs,2061,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:2112,Testability,log,log,2112,"FO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:150,Energy Efficiency,monitor,monitor,150,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:260,Energy Efficiency,monitor,monitor,260,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:330,Energy Efficiency,monitor,monitor,330,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:422,Energy Efficiency,monitor,monitor,422,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:492,Energy Efficiency,monitor,monitor,492,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:581,Energy Efficiency,monitor,monitor,581,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:757,Energy Efficiency,monitor,monitor,757,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:811,Energy Efficiency,monitor,monitor,811,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:941,Energy Efficiency,monitor,monitor,941,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:1055,Energy Efficiency,monitor,monitor,1055,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:62,Testability,log,log,62,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:628,Testability,log,log,628,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:1091,Testability,log,log,1091,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976:28,Availability,error,errors,28,@andy7i Those are FireCloud errors because of the resulting missing status. GAWB-1645 has been opened to be more resilient on our side (and thus make those errors go away); this ticket is for Cromwell to deal with why the statuses are missing in the first place.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976:113,Availability,resilien,resilient,113,@andy7i Those are FireCloud errors because of the resulting missing status. GAWB-1645 has been opened to be more resilient on our side (and thus make those errors go away); this ticket is for Cromwell to deal with why the statuses are missing in the first place.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976:156,Availability,error,errors,156,@andy7i Those are FireCloud errors because of the resulting missing status. GAWB-1645 has been opened to be more resilient on our side (and thus make those errors go away); this ticket is for Cromwell to deal with why the statuses are missing in the first place.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3890,Deployability,configurat,configuration,3890,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3475,Integrability,Message,Message,3475,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3890,Modifiability,config,configuration,3890,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:1169,Testability,test,test,1169,"ow submission system and nothing notices"", which in this case perhaps that happens before it registers itself to the metadata service, thus no status?. ```; Feb 13 10:22:04 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 15:22:04,998 cromwell-system-akka.dispatchers.engine-dispatcher-40 INFO - Workflows 3d01da76-98f9-4751-a3c0-efc61ef67030 submitted.; Feb 13 11:50:07 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:07,816 cromwell-system-akka.dispatchers.engine-dispatcher-8 INFO - WorkflowManagerActor Starting workflow UUID(3d01da76-98f9-4751-a3c0-efc61ef67030); Feb 13 11:50:07 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:07,816 cromwell-system-akka.dispatchers.engine-dispatcher-8 INFO - WorkflowManagerActor Successfully started WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030; Feb 13 11:50:16 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:16,621 cromwell-system-akka.dispatchers.backend-dispatcher-108 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; Feb 13 11:51:01 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:01,890 cromwell-system-akka.dispatchers.backend-dispatcher-114 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:1236,Testability,test,test,1236,"happens before it registers itself to the metadata service, thus no status?. ```; Feb 13 10:22:04 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 15:22:04,998 cromwell-system-akka.dispatchers.engine-dispatcher-40 INFO - Workflows 3d01da76-98f9-4751-a3c0-efc61ef67030 submitted.; Feb 13 11:50:07 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:07,816 cromwell-system-akka.dispatchers.engine-dispatcher-8 INFO - WorkflowManagerActor Starting workflow UUID(3d01da76-98f9-4751-a3c0-efc61ef67030); Feb 13 11:50:07 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:07,816 cromwell-system-akka.dispatchers.engine-dispatcher-8 INFO - WorkflowManagerActor Successfully started WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030; Feb 13 11:50:16 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:16,621 cromwell-system-akka.dispatchers.backend-dispatcher-108 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; Feb 13 11:51:01 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:01,890 cromwell-system-akka.dispatchers.backend-dispatcher-114 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:1490,Testability,test,test,1490,"a76-98f9-4751-a3c0-efc61ef67030 submitted.; Feb 13 11:50:07 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:07,816 cromwell-system-akka.dispatchers.engine-dispatcher-8 INFO - WorkflowManagerActor Starting workflow UUID(3d01da76-98f9-4751-a3c0-efc61ef67030); Feb 13 11:50:07 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:07,816 cromwell-system-akka.dispatchers.engine-dispatcher-8 INFO - WorkflowManagerActor Successfully started WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030; Feb 13 11:50:16 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:16,621 cromwell-system-akka.dispatchers.backend-dispatcher-108 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; Feb 13 11:51:01 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:01,890 cromwell-system-akka.dispatchers.backend-dispatcher-114 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:1557,Testability,test,test,1557,"pha102 docker/cromwell-app[837]: 2017-02-13 16:50:07,816 cromwell-system-akka.dispatchers.engine-dispatcher-8 INFO - WorkflowManagerActor Starting workflow UUID(3d01da76-98f9-4751-a3c0-efc61ef67030); Feb 13 11:50:07 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:07,816 cromwell-system-akka.dispatchers.engine-dispatcher-8 INFO - WorkflowManagerActor Successfully started WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030; Feb 13 11:50:16 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:50:16,621 cromwell-system-akka.dispatchers.backend-dispatcher-108 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; Feb 13 11:51:01 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:01,890 cromwell-system-akka.dispatchers.backend-dispatcher-114 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.respons",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:2056,Testability,test,test,2056,"well-app[837]: 2017-02-13 16:50:16,621 cromwell-system-akka.dispatchers.backend-dispatcher-108 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; Feb 13 11:51:01 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:01,890 cromwell-system-akka.dispatchers.backend-dispatcher-114 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:2123,Testability,test,test,2123,"kend-dispatcher-108 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; Feb 13 11:51:01 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:01,890 cromwell-system-akka.dispatchers.backend-dispatcher-114 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:2415,Testability,test,test,2415,"3 16:51:01,890 cromwell-system-akka.dispatchers.backend-dispatcher-114 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:2511,Testability,test,test,2511,"or [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:2616,Testability,test,test,2616,"ing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/use",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:2682,Testability,log,log,2682,":09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowAc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3140,Testability,log,logs,3140,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3169,Testability,log,logs,3169,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3220,Testability,log,log,3220,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3274,Testability,log,logs,3274,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3325,Testability,log,log,3325,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3847,Testability,log,logging,3847,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3919,Testability,log,log-dead-letters,3919,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:73,Availability,Reboot,Rebooting,73,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:53,Safety,abort,aborted,53,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:235,Testability,log,logs,235,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:105,Usability,clear,cleared,105,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279543377:10,Testability,test,testOnly,10,Try `sbt 'testOnly -- -l localdockertest'`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279543377
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906:164,Availability,Down,Downloaded,164,"@kshakir adding `-elocaldockertest` addressed the issue when I ran the tests locally. In Travis CI, the `non_root_default_user` test is failing with:. ```; Status: Downloaded newer image for mcovarr/notroot:v1; bin/bash: /cromwell-executions/woot/148f812b-028b-4264-82bd-ab2f089efe98/call-notroot/execution/script: Permission denied; ```. This test passes when I run it locally. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906:71,Testability,test,tests,71,"@kshakir adding `-elocaldockertest` addressed the issue when I ran the tests locally. In Travis CI, the `non_root_default_user` test is failing with:. ```; Status: Downloaded newer image for mcovarr/notroot:v1; bin/bash: /cromwell-executions/woot/148f812b-028b-4264-82bd-ab2f089efe98/call-notroot/execution/script: Permission denied; ```. This test passes when I run it locally. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906:128,Testability,test,test,128,"@kshakir adding `-elocaldockertest` addressed the issue when I ran the tests locally. In Travis CI, the `non_root_default_user` test is failing with:. ```; Status: Downloaded newer image for mcovarr/notroot:v1; bin/bash: /cromwell-executions/woot/148f812b-028b-4264-82bd-ab2f089efe98/call-notroot/execution/script: Permission denied; ```. This test passes when I run it locally. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906:344,Testability,test,test,344,"@kshakir adding `-elocaldockertest` addressed the issue when I ran the tests locally. In Travis CI, the `non_root_default_user` test is failing with:. ```; Status: Downloaded newer image for mcovarr/notroot:v1; bin/bash: /cromwell-executions/woot/148f812b-028b-4264-82bd-ab2f089efe98/call-notroot/execution/script: Permission denied; ```. This test passes when I run it locally. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503:200,Availability,error,error,200,@adamstruck That test tries to run a Docker image which has a default user which is not root (the test was created based on [this ticket](https://github.com/broadinstitute/cromwell/issues/472)). That error looks like the non-root user in the container doesn't have execute permission on the script that was created by Cromwell running as a different user.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503:17,Testability,test,test,17,@adamstruck That test tries to run a Docker image which has a default user which is not root (the test was created based on [this ticket](https://github.com/broadinstitute/cromwell/issues/472)). That error looks like the non-root user in the container doesn't have execute permission on the script that was created by Cromwell running as a different user.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503:98,Testability,test,test,98,@adamstruck That test tries to run a Docker image which has a default user which is not root (the test was created based on [this ticket](https://github.com/broadinstitute/cromwell/issues/472)). That error looks like the non-root user in the container doesn't have execute permission on the script that was created by Cromwell running as a different user.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280425784:25,Testability,test,tests,25,@kshakir all TES Centaur tests are now passing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280425784
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280730117:21,Availability,down,downloading,21,üëç once centaur isn't downloading from an external repo. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1979/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280730117
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280774491:77,Availability,down,downloaded,77,Thanks for the comments. I pushed the README changes and Funnel is now being downloaded from the broad's fork. . It looks like both the Local and Tes Centaur build are failing now though?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280774491
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262:89,Availability,error,error,89,"The Local build did fail, due to an unrelated hiccup. A restart of the build cleared the error. Meanwhile, **J**ES Centaur failed due to #1717, but **T**ES passed just fine. Everything looking great. Thanks again for all of your contributions!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262:77,Usability,clear,cleared,77,"The Local build did fail, due to an unrelated hiccup. A restart of the build cleared the error. Meanwhile, **J**ES Centaur failed due to #1717, but **T**ES passed just fine. Everything looking great. Thanks again for all of your contributions!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262
https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280022028:2005,Availability,echo,echo,2005," even create the `.list` file:; ```bash; #!/bin/sh; umask 0000; (; cd /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution; base_name=$(basename /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/picard_aggregation/DEV-7039/Pond-544027,_Standard_Kapa_LC/v1/Pond-544027,_Standard_Kapa_LC.bam .bam); java -Xmx4g -jar $picard CollectHsMetrics \; I=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/picard_aggregation/DEV-7039/Pond-544027,_Standard_Kapa_LC/v1/Pond-544027,_Standard_Kapa_LC.bam \; O=$base_name.hs_metrics \; PER_TARGET_COVERAGE=$base_name.per_target_coverage \; TI=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/ice-22.interval_list \; BI=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/ice-22.interval_list \; R=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; ); echo $? > /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution/rc.tmp; (; cd /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution. ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280022028
https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280026759:219,Security,access,access,219,"NB this didn't happen when the `[0]` wasn't part of the output declaration. If I had to guess, I'd say the FileEvaluator is looking for GlobFiles to expand, but is missing this one because it looks like an array member access rather that a GlobFile type.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280026759
https://github.com/broadinstitute/cromwell/issues/1982#issuecomment-280668311:164,Availability,echo,echo,164,"To correct the test case once this is fixed:. Add this task:; ```; task defined_in_task {; File? f; Boolean is_defined = defined(f); command {; ${true=""cat"" false=""echo no file"" is_defined} ${f}; }; runtime {; docker: ""ubuntu:latest""; }; output {; String out = read_string(stdout()); }; }; ```; Call it from inside the scatter:; ```; scatter (p in masked_indices) {; ...; call defined_in_task { input: f = mk_file.f }; }; ```; Add the output:; ```; Array[String] dit_out = defined_in_task.out; ```; Add expectations",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1982#issuecomment-280668311
https://github.com/broadinstitute/cromwell/issues/1982#issuecomment-280668311:15,Testability,test,test,15,"To correct the test case once this is fixed:. Add this task:; ```; task defined_in_task {; File? f; Boolean is_defined = defined(f); command {; ${true=""cat"" false=""echo no file"" is_defined} ${f}; }; runtime {; docker: ""ubuntu:latest""; }; output {; String out = read_string(stdout()); }; }; ```; Call it from inside the scatter:; ```; scatter (p in masked_indices) {; ...; call defined_in_task { input: f = mk_file.f }; }; ```; Add the output:; ```; Array[String] dit_out = defined_in_task.out; ```; Add expectations",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1982#issuecomment-280668311
https://github.com/broadinstitute/cromwell/issues/1983#issuecomment-279749511:73,Testability,test,test,73,"I believe this is fixed in `develop`. It looks just like the new centaur test `interpolation_additions` in https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/optional_parameter/optional_parameter.wdl. @noblem - FWIW, your input is not optional in that WDL, so your proposed fix of `-t ${input_table}` should be functionally identical to `${""-t "" + input_table}` in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1983#issuecomment-279749511
https://github.com/broadinstitute/cromwell/issues/1986#issuecomment-286130513:36,Deployability,release,release,36,"I believe this is complete with the release of Cromwell 25, is this ready to be closed @ruchim?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1986#issuecomment-286130513
https://github.com/broadinstitute/cromwell/issues/1988#issuecomment-280177392:16,Availability,error,error,16,Oops. Raised in error. This actually does work,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1988#issuecomment-280177392
https://github.com/broadinstitute/cromwell/issues/1990#issuecomment-303823955:75,Performance,cache,cached,75,This is about people wanting to know why their job (which should have call cached) did not call cache.; It's come up quite a few times and can be infuriating to debug,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1990#issuecomment-303823955
https://github.com/broadinstitute/cromwell/issues/1990#issuecomment-303823955:96,Performance,cache,cache,96,This is about people wanting to know why their job (which should have call cached) did not call cache.; It's come up quite a few times and can be infuriating to debug,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1990#issuecomment-303823955
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280379776:40,Availability,error,error,40,Adding @vdauwera's comment about adding error codes to GATK from [DSDE-docs #1742](https://github.com/broadinstitute/dsde-docs/issues/1742#issuecomment-280304238):; >We may be able to put in error codes for things like this in GATK4. Should ask David Roazen or Louis Bergelson.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280379776
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280379776:191,Availability,error,error,191,Adding @vdauwera's comment about adding error codes to GATK from [DSDE-docs #1742](https://github.com/broadinstitute/dsde-docs/issues/1742#issuecomment-280304238):; >We may be able to put in error codes for things like this in GATK4. Should ask David Roazen or Louis Bergelson.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280379776
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280384314:523,Availability,error,error,523,"Just a though --what if we added a ""retryOn"" attribute that took a boolean expression. Then we add a function like grep(pattern, var/file, mode) <see R for example function> which would probably be similar work and much more reusable. Then Jose could write something like. retryOn = grep(""(foo|bar"", $stderr, ""any""). But could also check any other element. Syntax is lousy typing from tiny keyboard. > On Feb 16, 2017, at 11:23 AM, Kate Voss <notifications@github.com> wrote:; > ; > Adding @vdauwera's comment about adding error codes to GATK from DSDE-docs #1742:; > ; > We may be able to put in error codes for things like this in GATK4. Should ask David Roazen or Louis Bergelson.; > ; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280384314
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280384314:597,Availability,error,error,597,"Just a though --what if we added a ""retryOn"" attribute that took a boolean expression. Then we add a function like grep(pattern, var/file, mode) <see R for example function> which would probably be similar work and much more reusable. Then Jose could write something like. retryOn = grep(""(foo|bar"", $stderr, ""any""). But could also check any other element. Syntax is lousy typing from tiny keyboard. > On Feb 16, 2017, at 11:23 AM, Kate Voss <notifications@github.com> wrote:; > ; > Adding @vdauwera's comment about adding error codes to GATK from DSDE-docs #1742:; > ; > We may be able to put in error codes for things like this in GATK4. Should ask David Roazen or Louis Bergelson.; > ; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280384314
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584:340,Availability,robust,robust,340,"This would be extremely useful for us. We're currently having to deal with several problems that would be helped by an automated retry ability. . The first problems is what David said, we have tools that can fail sometimes due to GCS issues and being able to restart when that happens would be useful. We're working on making our code more robust to that, but it's difficult to completely fix the problem. Having to restart a workflow with 10s of thousands of jobs because 2 failed is pretty annoying. The second problem is out of memory issues. We have thousands of jobs, and most will run with a small amount of memory, but some of them will need more. It's difficult to predict ahead of time which shards will need more since it's a function of the data rather than of the file size. Having a way to automatically retry these shards with increased memory would be really valuable since it would let us provision for the average shard rather than the worst case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584:673,Safety,predict,predict,673,"This would be extremely useful for us. We're currently having to deal with several problems that would be helped by an automated retry ability. . The first problems is what David said, we have tools that can fail sometimes due to GCS issues and being able to restart when that happens would be useful. We're working on making our code more robust to that, but it's difficult to completely fix the problem. Having to restart a workflow with 10s of thousands of jobs because 2 failed is pretty annoying. The second problem is out of memory issues. We have thousands of jobs, and most will run with a small amount of memory, but some of them will need more. It's difficult to predict ahead of time which shards will need more since it's a function of the data rather than of the file size. Having a way to automatically retry these shards with increased memory would be really valuable since it would let us provision for the average shard rather than the worst case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316791748:414,Energy Efficiency,reduce,reduce,414,"@katevoss This is actually really important, not just for @droazen and @lbergelson ... This issue has cost the Broad $$$ and analysts a lot of time. Not just the people on this issue. And putting retry code into the GATK (or any task for that matter) is bit arduous and actually a more expensive solution, especially when some random code path is missed. Also, retry on memory should do a lot for us to be able to reduce costs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316791748
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316800064:71,Availability,robust,robust,71,+1 on this feature (or one like it) -- it's really helpful for writing robust and cheap workflows,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316800064
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:378,Availability,down,download,378,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:425,Availability,error,errors,425,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:221,Energy Efficiency,monitor,monitoring,221,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:108,Performance,load,load,108,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:302,Safety,avoid,avoiding,302,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408:120,Availability,error,error,120,"As a **workflow runner**, I want **Cromwell to automatically retry my workflow with increased memory/disk/on a specific error code, etc**, so that I can **get my workflow to complete without having to manually intervene**.; - Effort: **?** @geoffjentry ; - Risk: **Medium**; - if users are unaware that they have retries set in ways that would cost them a lot the 2nd or tertiary run, i.e to double their memory, they could end up paying for a much more expensive VM when a smaller one would do; - Business value: **Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408:257,Safety,Risk,Risk,257,"As a **workflow runner**, I want **Cromwell to automatically retry my workflow with increased memory/disk/on a specific error code, etc**, so that I can **get my workflow to complete without having to manually intervene**.; - Effort: **?** @geoffjentry ; - Risk: **Medium**; - if users are unaware that they have retries set in ways that would cost them a lot the 2nd or tertiary run, i.e to double their memory, they could end up paying for a much more expensive VM when a smaller one would do; - Business value: **Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-344400450:60,Integrability,interface,interface,60,I'd encourage interested parties (e.g. @drozen) to directly interface w/ OpenWDL,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-344400450
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-344402249:122,Integrability,interface,interface,122,What does that mean? How do we get you guys to prioritize work on it?; (This is directed at Jeff's comment that we should interface elsewhere),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-344402249
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-357252477:0,Availability,Ping,Ping,0,Ping! We still would like this. Is it on the roadmap?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-357252477
https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-535943929:197,Deployability,update,updates,197,@guma44 we have moved to Jira and this ticket is in review at the moment on our new board. Please check out this ticket https://broadworkbench.atlassian.net/browse/BA-5933 or this Pull Request for updates https://github.com/broadinstitute/cromwell/pull/5180,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-535943929
https://github.com/broadinstitute/cromwell/issues/1995#issuecomment-305589282:78,Security,hash,hashes,78,@cjllanwarne is this still true ? From what I see the EJEA is waiting for the hashes to be written before reporting that it's finished,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1995#issuecomment-305589282
https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-282616647:177,Integrability,message,messages,177,"For the record, we use a custom backend that we specify as default, and specify local for some tasks with; ```; runtime {; backend: ""Local""; }; ```; and I've noticed those WARN messages in the logs but wasn't really concerned as everything works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-282616647
https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-282616647:193,Testability,log,logs,193,"For the record, we use a custom backend that we specify as default, and specify local for some tasks with; ```; runtime {; backend: ""Local""; }; ```; and I've noticed those WARN messages in the logs but wasn't really concerned as everything works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-282616647
https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-328541603:180,Safety,Risk,Risk,180,"As a **user**, I want **Cromwell to not check the backend attribute**, so that I can **not be distracted by strange warnings that aren't actually useful**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-328541603
https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487:165,Availability,error,error,165,"+1 ; The runtime.backend parameter is completely undocumented from what I can tell, but mentioned variously in forums and here on github. I tried to use it, saw the error, got extremely confused, and here I am hours later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487
https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487:49,Usability,undo,undocumented,49,"+1 ; The runtime.backend parameter is completely undocumented from what I can tell, but mentioned variously in forums and here on github. I tried to use it, saw the error, got extremely confused, and here I am hours later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487
https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:17,Availability,error,error,17,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016
https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:408,Availability,error,error,408,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016
https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:23,Integrability,message,message,23,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016
https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:414,Integrability,message,message,414,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016
https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:159,Testability,log,logs,159,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016
https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752:65,Deployability,update,update,65,Is this the SFS version of invalidate cache results? Can we also update the centaur test `invalidate_bad_caches` to include whatever causes this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752
https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752:38,Performance,cache,cache,38,Is this the SFS version of invalidate cache results? Can we also update the centaur test `invalidate_bad_caches` to include whatever causes this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752
https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752:84,Testability,test,test,84,Is this the SFS version of invalidate cache results? Can we also update the centaur test `invalidate_bad_caches` to include whatever causes this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949:72,Performance,cache,cache,72,"I was testing some call caching behaviors. Specifically I tried to call cache on local backend with the ""hash the file path"" method which would hash a file path even though the file doesn't exist. The job would then try to find a cache hit for this path even though the file doesn't exist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949:230,Performance,cache,cache,230,"I was testing some call caching behaviors. Specifically I tried to call cache on local backend with the ""hash the file path"" method which would hash a file path even though the file doesn't exist. The job would then try to find a cache hit for this path even though the file doesn't exist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949:105,Security,hash,hash,105,"I was testing some call caching behaviors. Specifically I tried to call cache on local backend with the ""hash the file path"" method which would hash a file path even though the file doesn't exist. The job would then try to find a cache hit for this path even though the file doesn't exist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949:144,Security,hash,hash,144,"I was testing some call caching behaviors. Specifically I tried to call cache on local backend with the ""hash the file path"" method which would hash a file path even though the file doesn't exist. The job would then try to find a cache hit for this path even though the file doesn't exist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949:6,Testability,test,testing,6,"I was testing some call caching behaviors. Specifically I tried to call cache on local backend with the ""hash the file path"" method which would hash a file path even though the file doesn't exist. The job would then try to find a cache hit for this path even though the file doesn't exist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:522,Deployability,Patch,Patch,522,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:596,Deployability,patch,patch,596,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:402,Modifiability,config,config,402,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:409,Modifiability,Config,ConfigHashingStrategy,409,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369
https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:270,Security,hash,hashing,270,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369
https://github.com/broadinstitute/cromwell/pull/2007#issuecomment-280730525:15,Testability,test,tests,15,"üëç, if the EJEA tests all say üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2007/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2007#issuecomment-280730525
https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-281697619:104,Availability,down,down,104,Side note to this ticket: There's also the issue on how to handle catastrophic DB issues - e.g. DB goes down for a period of time. I've taken that up myself as a lower priority path which shall eventually turn into an issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-281697619
https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-307434426:49,Availability,failure,failures,49,"I'm uping this as we've seen CloudSQL connection failures recently resulting in a variety of inconsistencies in workflow statuses, metadata and generally DB state.; This is going to be even more important as we move towards CaaS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-307434426
https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-321343728:53,Modifiability,config,config,53,"We should try to find a way to deprecate this in the config files, the combo of the reflection and the way that users just copy/paste whole blocks even if they're not modifying anything will make it hard to have this be purely cosmetic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-321343728
https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-321536662:128,Performance,load,loaded,128,"Yeah, that's the easy part :). Honestly a reasonable Definition Of Done could be ""all docs and the renaming of the reflectively loaded class(es)"" as the real problem is our user facing stuff - it's starting to cause confusion, but we might as well go all the way at that point.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-321536662
https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328190721:38,Performance,load,loaded,38,"@geoffjentry what is the reflectively loaded classes? If it's just the docs, I can do a quick ReadMe ""search and replace"" üòÅ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328190721
https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328217096:277,Modifiability,config,config,277,"The issue is that the way one loads a backend into Cromwell is to reference the implementing scala class, like [this](https://github.com/broadinstitute/cromwell/blob/f1955f963ee65ca9296f554376bd655b9529c10d/cromwell.examples.conf#L466). If we change the name of that class old config files will be broken. In theory we could deprecate it and have some sort of redirect but not sure offhand how that'd work. We have the same problem in #2440",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328217096
https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328217096:30,Performance,load,loads,30,"The issue is that the way one loads a backend into Cromwell is to reference the implementing scala class, like [this](https://github.com/broadinstitute/cromwell/blob/f1955f963ee65ca9296f554376bd655b9529c10d/cromwell.examples.conf#L466). If we change the name of that class old config files will be broken. In theory we could deprecate it and have some sort of redirect but not sure offhand how that'd work. We have the same problem in #2440",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-328217096
https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888:91,Deployability,Pipeline,Pipelines,91,"As a **user setting up Cromwell**, I want **only want to see references to Google Genomics Pipelines API**, so that **I know how to set up the Google backend, not some JES thing.**; - effort: small; - risk: small to medium; - business value: small; - may grow if it becomes confusing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888
https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888:201,Safety,risk,risk,201,"As a **user setting up Cromwell**, I want **only want to see references to Google Genomics Pipelines API**, so that **I know how to set up the Google backend, not some JES thing.**; - effort: small; - risk: small to medium; - business value: small; - may grow if it becomes confusing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888
https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-281811584:50,Usability,clear,clear,50,This sounds like a low priority bug as there is a clear workaround.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-281811584
https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-282584614:106,Availability,error,errors,106,"#2030 fixes this specific issue. There remains a greater tech debt of actor supervision, where unexpected errors like this shouldn't leave the workflows in a running state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-282584614
https://github.com/broadinstitute/cromwell/issues/2022#issuecomment-282117203:91,Testability,test,testing,91,"I agree, a local database is the way to do this, e.g. I can use a mysql database even when testing things locally.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2022#issuecomment-282117203
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:301,Availability,down,down,301,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:387,Modifiability,config,config,387,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:466,Modifiability,config,config,466,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:145,Performance,concurren,concurrent-job-limit,145,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:476,Performance,concurren,concurrent-job-limit,476,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:728,Safety,detect,detect,728,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:343,Usability,pause,pause,343,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:349,Usability,resume,resume,349,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360181260:24,Usability,Resume,Resume,24,"Another user story for ""Resume after last successful task"" feature:. > Allow them to rerun in place and not copy - i.e. reuse a submission folder and reference old results, bypassing call caching and need to copy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360181260
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:682,Performance,cache,cache,682,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:730,Performance,cache,cached,730,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:852,Performance,cache,cache,852,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:701,Security,hash,hashes,701,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:490,Usability,resume,resume,490,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:26,Performance,cache,cache,26,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:125,Testability,test,test,125,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:258,Testability,test,test,258,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:2,Usability,resume,resume,2,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626
https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282294665:30,Performance,load,load,30,"Have you run this under heavy load? If so, how does it react?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282294665
https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282397582:90,Performance,load,load,90,"@cjllanwarne You asked at standup what I was asking for when I asked about how it handles load. What I meant was if you submit stuff such that there are at least in the range of several thousand runs & status polls happening (ideally interspersed as well, not just a bunch of runs followed by a bunch of statuses) does something terrible happen? . IOW a week from now when thousands of things are flowing through via FC is there some obvious gotcha that we could have spotted now to save us some pain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2026#issuecomment-282397582
https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282367470:31,Usability,undo,undocumented,31,"The siblling knob is currently undocumented, I thought perhaps intentionally? I can certainly add to the reference.conf.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282367470
https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282369105:36,Testability,log,log,36,Which was the sibling? the workflow log copier?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282369105
https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282391104:54,Usability,undo,undocumented,54,"In terms of documenting the knob, I'm fine leaving it undocumented for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282391104
https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282771079:82,Usability,undo,undocumented,82,üëç... but do we at least have a list somewhere (or institutional knowledge) of our undocumented options?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2027/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282771079
https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282571582:61,Availability,repair,repair,61,"Fixes the workaround syntax in #1126, but doesn't completely repair the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282571582
https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282770276:64,Testability,test,tests,64,ToL: Is this centaurable? Or can we reconfigure centaur's Local tests so that this is verified?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282770276
https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282778691:13,Modifiability,variab,variable,13,"The `script` variable was busted, but docker was working in SFS land because, instead of `${script}` the local backend was using `${cwd}/execution/script` when launching docker jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282778691
https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954:143,Deployability,update,updated,143,"Re ""centaurable""-- I'm confident by updating the local backend to use `${script}` that centaur is actually exercising a third of the variables updated in patch. We could also change centaur to completely run on a new backend that also uses the `${out}` and `${err}` paths, as outlined in issue #1126. But because of that issue, this new centaur test would require a separate, new backend definition in `local_centaur.conf`. FYI: I still don't have a full fix for #1126, but this is a step in that direction. When I looked at the reference.conf, I noticed the local backend wasn't using `${script}`, wanted to know why, and discovered this small issue. This patch will also make the workaround in that ticket work as expected, instead of pointing to paths outside the docker container like `/Users/kshakir/<path>` now pointing paths inside like `/cromwell_root/<path>`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954
https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954:154,Deployability,patch,patch,154,"Re ""centaurable""-- I'm confident by updating the local backend to use `${script}` that centaur is actually exercising a third of the variables updated in patch. We could also change centaur to completely run on a new backend that also uses the `${out}` and `${err}` paths, as outlined in issue #1126. But because of that issue, this new centaur test would require a separate, new backend definition in `local_centaur.conf`. FYI: I still don't have a full fix for #1126, but this is a step in that direction. When I looked at the reference.conf, I noticed the local backend wasn't using `${script}`, wanted to know why, and discovered this small issue. This patch will also make the workaround in that ticket work as expected, instead of pointing to paths outside the docker container like `/Users/kshakir/<path>` now pointing paths inside like `/cromwell_root/<path>`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954
https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954:657,Deployability,patch,patch,657,"Re ""centaurable""-- I'm confident by updating the local backend to use `${script}` that centaur is actually exercising a third of the variables updated in patch. We could also change centaur to completely run on a new backend that also uses the `${out}` and `${err}` paths, as outlined in issue #1126. But because of that issue, this new centaur test would require a separate, new backend definition in `local_centaur.conf`. FYI: I still don't have a full fix for #1126, but this is a step in that direction. When I looked at the reference.conf, I noticed the local backend wasn't using `${script}`, wanted to know why, and discovered this small issue. This patch will also make the workaround in that ticket work as expected, instead of pointing to paths outside the docker container like `/Users/kshakir/<path>` now pointing paths inside like `/cromwell_root/<path>`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954
https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954:133,Modifiability,variab,variables,133,"Re ""centaurable""-- I'm confident by updating the local backend to use `${script}` that centaur is actually exercising a third of the variables updated in patch. We could also change centaur to completely run on a new backend that also uses the `${out}` and `${err}` paths, as outlined in issue #1126. But because of that issue, this new centaur test would require a separate, new backend definition in `local_centaur.conf`. FYI: I still don't have a full fix for #1126, but this is a step in that direction. When I looked at the reference.conf, I noticed the local backend wasn't using `${script}`, wanted to know why, and discovered this small issue. This patch will also make the workaround in that ticket work as expected, instead of pointing to paths outside the docker container like `/Users/kshakir/<path>` now pointing paths inside like `/cromwell_root/<path>`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954
https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954:345,Testability,test,test,345,"Re ""centaurable""-- I'm confident by updating the local backend to use `${script}` that centaur is actually exercising a third of the variables updated in patch. We could also change centaur to completely run on a new backend that also uses the `${out}` and `${err}` paths, as outlined in issue #1126. But because of that issue, this new centaur test would require a separate, new backend definition in `local_centaur.conf`. FYI: I still don't have a full fix for #1126, but this is a step in that direction. When I looked at the reference.conf, I noticed the local backend wasn't using `${script}`, wanted to know why, and discovered this small issue. This patch will also make the workaround in that ticket work as expected, instead of pointing to paths outside the docker container like `/Users/kshakir/<path>` now pointing paths inside like `/cromwell_root/<path>`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954
https://github.com/broadinstitute/cromwell/issues/2029#issuecomment-342580733:150,Energy Efficiency,monitor,monitored,150,"Fixed by https://github.com/broadinstitute/cromwell/pull/2808, although other jobs are not stopped when a job fails, they're left as is and are being monitored until they reach completion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2029#issuecomment-342580733
https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283056934:25,Usability,clear,clearer,25,It would be nice to have clearer docs on Spark backend (what should be put to wdl and what should be given when running java -jar cromwell.jar instead of default application.conf),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283056934
https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283210838:15,Deployability,update,update,15,@jainh Can you update / help clarify the readme docs for spark?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283210838
https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134:20,Deployability,update,update,20,"@antonkulaga I will update Readme to point right configuration, however on `what should be given when running java -jar Cromwell.jar` it doesn't need any additional runtime attributes besides uncommenting Spark backend configuration from `reference.conf` and on `what should be put to wdl` is referred here : [WDL](https://github.com/broadinstitute/cromwell#sample-wdl)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134
https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134:49,Deployability,configurat,configuration,49,"@antonkulaga I will update Readme to point right configuration, however on `what should be given when running java -jar Cromwell.jar` it doesn't need any additional runtime attributes besides uncommenting Spark backend configuration from `reference.conf` and on `what should be put to wdl` is referred here : [WDL](https://github.com/broadinstitute/cromwell#sample-wdl)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134
https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134:219,Deployability,configurat,configuration,219,"@antonkulaga I will update Readme to point right configuration, however on `what should be given when running java -jar Cromwell.jar` it doesn't need any additional runtime attributes besides uncommenting Spark backend configuration from `reference.conf` and on `what should be put to wdl` is referred here : [WDL](https://github.com/broadinstitute/cromwell#sample-wdl)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134
https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134:49,Modifiability,config,configuration,49,"@antonkulaga I will update Readme to point right configuration, however on `what should be given when running java -jar Cromwell.jar` it doesn't need any additional runtime attributes besides uncommenting Spark backend configuration from `reference.conf` and on `what should be put to wdl` is referred here : [WDL](https://github.com/broadinstitute/cromwell#sample-wdl)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134
https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134:219,Modifiability,config,configuration,219,"@antonkulaga I will update Readme to point right configuration, however on `what should be given when running java -jar Cromwell.jar` it doesn't need any additional runtime attributes besides uncommenting Spark backend configuration from `reference.conf` and on `what should be put to wdl` is referred here : [WDL](https://github.com/broadinstitute/cromwell#sample-wdl)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522:206,Deployability,release,release,206,"@mcovarr ; because docker does not run without root on Linux. Although, there is a workaround:; ```; sudo usermod -aG docker $USER; ```; But after doing this it did not solve a problem. I am using the last release because I have dependency problems when building from master.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522:229,Integrability,depend,dependency,229,"@mcovarr ; because docker does not run without root on Linux. Although, there is a workaround:; ```; sudo usermod -aG docker $USER; ```; But after doing this it did not solve a problem. I am using the last release because I have dependency problems when building from master.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:328,Deployability,release,release,328,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:503,Deployability,release,release,503,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:546,Deployability,release,release,546,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:453,Modifiability,config,config,453,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:110,Security,secur,security,110,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:119,Security,secur,security,119,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283050869:272,Deployability,release,release,272,"@antonkulaga Internally we're not allowed to use docker on our unix machines for exactly this reason, so handling this problem was never prioritized by our product ownership. We had to deal with it for our work on the upcoming AWS backend, as @mcovarr pointed out. The 25 release should be out in a matter of days",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283050869
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283063200:32,Modifiability,config,config,32,"@antonkulaga that PR is for the config backend (Local, SGE, etc) and specifically *not* JES. I see now it is a bit confusing how I worded the description in the PR... üò¶",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283063200
https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-285912645:24,Deployability,update,update,24,"Perfect, thanks for the update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-285912645
https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282703575:43,Deployability,patch,patched,43,Yeah I would agree that's a bug. If you've patched this locally we would certainly welcome a PR! üòÑ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282703575
https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047:172,Availability,redundant,redundant,172,"I don't actually know what the fix is because I don't know the intent behind exporting custom `TMPDIR` into the shell environment. I could just delete that line ‚Äî it seems redundant to me, I can't think why the command shell `TMPDIR` has to equal `java.io.tmpdir` ‚Äî but I don't know if it's there to fix some other issue that I don't know about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047
https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047:172,Safety,redund,redundant,172,"I don't actually know what the fix is because I don't know the intent behind exporting custom `TMPDIR` into the shell environment. I could just delete that line ‚Äî it seems redundant to me, I can't think why the command shell `TMPDIR` has to equal `java.io.tmpdir` ‚Äî but I don't know if it's there to fix some other issue that I don't know about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845:192,Testability,log,logging,192,"@knoblett Haven't you already filed a similar issue before?. Long story short: We have a plan, it's unlikely to happen in the near future. Because there are so many different requests for how logging should be handled and they can't all happen in a single log we'll be providing multiple logs each catered do different archetypical user/operator personas. I suggest you chat with @katevoss when she gets back, we had talked about her setting up some focus groups involving the different sorts of people so we can start refining what these various logs would actually be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845:256,Testability,log,log,256,"@knoblett Haven't you already filed a similar issue before?. Long story short: We have a plan, it's unlikely to happen in the near future. Because there are so many different requests for how logging should be handled and they can't all happen in a single log we'll be providing multiple logs each catered do different archetypical user/operator personas. I suggest you chat with @katevoss when she gets back, we had talked about her setting up some focus groups involving the different sorts of people so we can start refining what these various logs would actually be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845:288,Testability,log,logs,288,"@knoblett Haven't you already filed a similar issue before?. Long story short: We have a plan, it's unlikely to happen in the near future. Because there are so many different requests for how logging should be handled and they can't all happen in a single log we'll be providing multiple logs each catered do different archetypical user/operator personas. I suggest you chat with @katevoss when she gets back, we had talked about her setting up some focus groups involving the different sorts of people so we can start refining what these various logs would actually be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845:547,Testability,log,logs,547,"@knoblett Haven't you already filed a similar issue before?. Long story short: We have a plan, it's unlikely to happen in the near future. Because there are so many different requests for how logging should be handled and they can't all happen in a single log we'll be providing multiple logs each catered do different archetypical user/operator personas. I suggest you chat with @katevoss when she gets back, we had talked about her setting up some focus groups involving the different sorts of people so we can start refining what these various logs would actually be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282747248:164,Testability,log,logging,164,"I spoke with her via email last week while in Belgium. She asked me to file this ticket when I had the chance, so that it could be discussed as a part of the whole logging issue you guys have been discussing. I know this is already on your radar, but I just wanted to offer another perspective on a specific part of the logging as it came up during the workshop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282747248
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282747248:320,Testability,log,logging,320,"I spoke with her via email last week while in Belgium. She asked me to file this ticket when I had the chance, so that it could be discussed as a part of the whole logging issue you guys have been discussing. I know this is already on your radar, but I just wanted to offer another perspective on a specific part of the logging as it came up during the workshop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282747248
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332957318:87,Testability,log,logs,87,"@geoffjentry what kind of effort do you think it would take to color-code parts of the logs? ; Obviously we would not get it all done with the first go, but I bet @knoblett could list the top 5 things she would like colored for easier logging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332957318
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332957318:235,Testability,log,logging,235,"@geoffjentry what kind of effort do you think it would take to color-code parts of the logs? ; Obviously we would not get it all done with the first go, but I bet @knoblett could list the top 5 things she would like colored for easier logging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332957318
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332978633:125,Testability,log,logs,125,Color coding is easy to add. Be careful what you wish for. This is only actually viable in that world where we have multiple logs for multiple types of people. We've already intentionally removed color highlighting from server mode's logs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332978633
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332978633:234,Testability,log,logs,234,Color coding is easy to add. Be careful what you wish for. This is only actually viable in that world where we have multiple logs for multiple types of people. We've already intentionally removed color highlighting from server mode's logs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332978633
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648:251,Safety,Risk,Risk,251,"As a **user looking at logs** I want **the logs to be color-coded**, so that **I can easily debug my workflow and get the info I'm looking for**.; - Effort: Medium; - While the color-coding is easy, deciding what to color-code is more complicated.; - Risk: X-Small; - Business value: Medium; - Logs are a known issue. Maybe color coding them is an easy, first step to improving them",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648:23,Testability,log,logs,23,"As a **user looking at logs** I want **the logs to be color-coded**, so that **I can easily debug my workflow and get the info I'm looking for**.; - Effort: Medium; - While the color-coding is easy, deciding what to color-code is more complicated.; - Risk: X-Small; - Business value: Medium; - Logs are a known issue. Maybe color coding them is an easy, first step to improving them",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648:43,Testability,log,logs,43,"As a **user looking at logs** I want **the logs to be color-coded**, so that **I can easily debug my workflow and get the info I'm looking for**.; - Effort: Medium; - While the color-coding is easy, deciding what to color-code is more complicated.; - Risk: X-Small; - Business value: Medium; - Logs are a known issue. Maybe color coding them is an easy, first step to improving them",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648
https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648:294,Testability,Log,Logs,294,"As a **user looking at logs** I want **the logs to be color-coded**, so that **I can easily debug my workflow and get the info I'm looking for**.; - Effort: Medium; - While the color-coding is easy, deciding what to color-code is more complicated.; - Risk: X-Small; - Business value: Medium; - Logs are a known issue. Maybe color coding them is an easy, first step to improving them",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:59,Availability,FAILURE,FAILURE,59,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:100,Availability,FAILURE,FAILURE,100,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:112,Availability,FAILURE,FAILURES,112,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:124,Availability,FAILURE,FAILURE,124,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:71,Integrability,message,message,71,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:228,Integrability,message,message,228,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:79,Availability,failure,failures,79,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:189,Availability,Error,Error,189,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:62,Integrability,message,messages,62,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:95,Integrability,message,message,95,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:127,Integrability,message,message,127,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:178,Integrability,message,message,178,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:155,Security,authenticat,authentication,155,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:203,Security,access,access,203,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:136,Availability,failure,failures,136,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:224,Availability,error,error,224,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:627,Availability,failure,failure,627,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:660,Availability,failure,failures,660,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:721,Availability,failure,failure,721,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:840,Availability,error,errors,840,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1320,Availability,failure,failures,1320,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1413,Availability,Error,Error,1413,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1580,Availability,failure,failures,1580,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1738,Availability,failure,failures,1738,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1769,Availability,reliab,reliably,1769,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:900,Deployability,Pipeline,Pipeline,900,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1096,Deployability,Pipeline,Pipeline,1096,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:120,Integrability,message,message,120,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:152,Integrability,message,message,152,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:238,Integrability,Message,Message,238,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:886,Integrability,message,message,886,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1082,Integrability,message,message,1082,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1366,Integrability,message,message,1366,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1402,Integrability,message,message,1402,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1469,Integrability,message,message,1469,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:297,Modifiability,config,config,297,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1427,Security,access,access,1427,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1497,Security,authenticat,authentication,1497,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926:131,Availability,failure,failure,131,"@cjllanwarne. Yes, flattening the messages would definitely make things better. The other thing to address is the ""timestamp"" and ""failure"" format shown in my previous comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926
https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926:34,Integrability,message,messages,34,"@cjllanwarne. Yes, flattening the messages would definitely make things better. The other thing to address is the ""timestamp"" and ""failure"" format shown in my previous comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839921:0,Availability,Ping,Pinging,0,Pinging @ansingh7115 @katevoss @abaumann for thoughts on a priority for this...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839921
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839997:15,Availability,ping,ping,15,"Oh go on, I'll ping @geoffjentry too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839997
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842176:41,Availability,failure,failures,41,"Right now we are trying to display these failures to users in firecloud because they are currently suppressed and you can only see them if you inspect the json. I'd prefer our UI to not show raw JSON, so we want to format the responses. For now, we could format the known cases and show raw JSON otherwise and hope there aren't other raw JSON formats other than this. This is definitely not a showstopper for us, but not just a nice to have, I'd say it's a P2 on a scale from P1 to P3 with P0 for showstoppers. . We could also flatten ourselves, but there's the other case where it says ""timestamp"" and the singular ""failure"", so it's not always consistent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842176
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842176:617,Availability,failure,failure,617,"Right now we are trying to display these failures to users in firecloud because they are currently suppressed and you can only see them if you inspect the json. I'd prefer our UI to not show raw JSON, so we want to format the responses. For now, we could format the known cases and show raw JSON otherwise and hope there aren't other raw JSON formats other than this. This is definitely not a showstopper for us, but not just a nice to have, I'd say it's a P2 on a scale from P1 to P3 with P0 for showstoppers. . We could also flatten ourselves, but there's the other case where it says ""timestamp"" and the singular ""failure"", so it's not always consistent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842176
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842811:62,Integrability,message,message,62,"The one consistent thing should be: there should always be a ""message"" element in each entry, regardless of new/old format",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842811
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282843011:23,Availability,failure,failure,23,"Oh, apart from the old failure/timestamp that doesn't seem to be used anywhere any more",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282843011
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:0,Availability,Ping,Pinging,0,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:147,Availability,failure,failures,147,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:230,Availability,failure,failures,230,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:179,Integrability,message,message,179,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:247,Integrability,message,message,247,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:281,Usability,simpl,simple,281,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:24,Availability,failure,failures,24,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:38,Availability,failure,failure,38,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:74,Availability,failure,failures,74,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:228,Availability,failure,failures,228,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:278,Availability,failure,failures,278,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:88,Integrability,message,message,88,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:242,Integrability,message,message,242,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:292,Integrability,message,message,292,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236
https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646:136,Availability,avail,available,136,"We probably also want to say where to copy it to, e.g. ; ```; When using Spark backend, copy the Spark configuration in reference.conf (available under core/src/main/resources) into the main application.conf (in src/main/resources):; ```. I think we probably also don't want to have a copy/pasted version in this file, since it's unlikely to be updated if reference.conf is changed? Up to you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646
https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646:103,Deployability,configurat,configuration,103,"We probably also want to say where to copy it to, e.g. ; ```; When using Spark backend, copy the Spark configuration in reference.conf (available under core/src/main/resources) into the main application.conf (in src/main/resources):; ```. I think we probably also don't want to have a copy/pasted version in this file, since it's unlikely to be updated if reference.conf is changed? Up to you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646
https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646:345,Deployability,update,updated,345,"We probably also want to say where to copy it to, e.g. ; ```; When using Spark backend, copy the Spark configuration in reference.conf (available under core/src/main/resources) into the main application.conf (in src/main/resources):; ```. I think we probably also don't want to have a copy/pasted version in this file, since it's unlikely to be updated if reference.conf is changed? Up to you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646
https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646:103,Modifiability,config,configuration,103,"We probably also want to say where to copy it to, e.g. ; ```; When using Spark backend, copy the Spark configuration in reference.conf (available under core/src/main/resources) into the main application.conf (in src/main/resources):; ```. I think we probably also don't want to have a copy/pasted version in this file, since it's unlikely to be updated if reference.conf is changed? Up to you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646
https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285094026:86,Modifiability,config,config,86,Hi @jainh - I don't really like this. What we tell users to do is to take the bits of config they want to override and put them in a separate file and to invoke cromwell with the `-Dconfig.file=....` flag. Aside: I notice that this isn't quite what it says in the [README](https://github.com/broadinstitute/cromwell#configuring-cromwell) but that's a separate topic which I'll address myself.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285094026
https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285094026:316,Modifiability,config,configuring-cromwell,316,Hi @jainh - I don't really like this. What we tell users to do is to take the bits of config they want to override and put them in a separate file and to invoke cromwell with the `-Dconfig.file=....` flag. Aside: I notice that this isn't quite what it says in the [README](https://github.com/broadinstitute/cromwell#configuring-cromwell) but that's a separate topic which I'll address myself.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285094026
https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285116661:2,Deployability,update,updated,2,I updated README in my PR #2058,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285116661
https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283404662:27,Availability,failure,failure,27,This is what a (simulated) failure looks like for the two areas edited by this PR:; https://travis-ci.org/broadinstitute/cromwell/builds/206678315,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283404662
https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283414270:118,Testability,test,test,118,"I agree with the [referenced ticket](https://github.com/broadinstitute/cromwell/issues/1717), I would much prefer the test be marked ignored if we didn't even try to run it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283414270
https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987:259,Energy Efficiency,reduce,reduces,259,"Travis doesn't have ignored-test-results afaik, nor support for a dynamic yaml test matrix based on internal-vs.-external contributions. If one knows of a way to do what the ticket wants, ignore vs. pass, I'll happily incorporate that fix. Otherwise, this PR reduces red-fatigue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987
https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987:28,Testability,test,test-results,28,"Travis doesn't have ignored-test-results afaik, nor support for a dynamic yaml test matrix based on internal-vs.-external contributions. If one knows of a way to do what the ticket wants, ignore vs. pass, I'll happily incorporate that fix. Otherwise, this PR reduces red-fatigue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987
https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987:79,Testability,test,test,79,"Travis doesn't have ignored-test-results afaik, nor support for a dynamic yaml test matrix based on internal-vs.-external contributions. If one knows of a way to do what the ticket wants, ignore vs. pass, I'll happily incorporate that fix. Otherwise, this PR reduces red-fatigue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987
https://github.com/broadinstitute/cromwell/issues/2047#issuecomment-284272631:34,Testability,test,tests,34,See above for the missing centaur tests.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2047#issuecomment-284272631
https://github.com/broadinstitute/cromwell/issues/2047#issuecomment-288433555:0,Testability,Test,Test,0,Test merged:; https://github.com/broadinstitute/centaur/commit/c377c658ec47cf3c0acf7966d3f56eaa27362d2c,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2047#issuecomment-288433555
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:379,Performance,cache,cache,379,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:448,Performance,cache,cache,448,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:494,Performance,cache,cache,494,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:60,Security,hash,hash,60,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:374,Security,hash,hash,374,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:443,Security,hash,hash,443,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:482,Security,hash,hash,482,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:545,Security,hash,hashes,545,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:437,Usability,clear,clear,437,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment üò¢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-284004128:57,Performance,cache,cache-size,57,"For the second point, it's already possible, setting the cache-size to 0 will disable the cache here:; https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L150",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-284004128
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-284004128:90,Performance,cache,cache,90,"For the second point, it's already possible, setting the cache-size to 0 will disable the cache here:; https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L150",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-284004128
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:517,Availability,down,downstream,517,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:498,Performance,perform,performed,498,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:73,Security,hash,hash,73,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:478,Security,hash,hash,478,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:214,Availability,down,down,214,"Thoughts for a Monday Tech Talk‚Ñ¢Ô∏è:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:260,Availability,recover,recovers,260,"Thoughts for a Monday Tech Talk‚Ñ¢Ô∏è:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:260,Safety,recover,recovers,260,"Thoughts for a Monday Tech Talk‚Ñ¢Ô∏è:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:693,Safety,avoid,avoid,693,"Thoughts for a Monday Tech Talk‚Ñ¢Ô∏è:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:127,Security,hash,hashes,127,"Thoughts for a Monday Tech Talk‚Ñ¢Ô∏è:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:431,Security,hash,hash,431,"Thoughts for a Monday Tech Talk‚Ñ¢Ô∏è:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:495,Security,hash,hash,495,"Thoughts for a Monday Tech Talk‚Ñ¢Ô∏è:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497
https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-283862074:79,Testability,test,test,79,"Awesome! üëç. #2047 should stay open though until we have the centaur regression test setup, mentioned in the ticket. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2049/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-283862074
https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-283940732:67,Availability,down,down,67,:+1: Thank you @delocalizer this is great! üòÑ Can you please squash down to one commit before we merge?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2049/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-283940732
https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-284017153:53,Deployability,release,release,53,I've merged this as-is to make sure it gets into the release! üéâ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-284017153
https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133:75,Availability,error,error,75,"@katevoss in your absence I've marked this as low. It's mainly a ""terrible error messages"" bug (you might consider this more important!). OTOH, the lack of failure recording and the EJEA crashing does concern me and might indicate a bigger problem under the surface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133
https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133:156,Availability,failure,failure,156,"@katevoss in your absence I've marked this as low. It's mainly a ""terrible error messages"" bug (you might consider this more important!). OTOH, the lack of failure recording and the EJEA crashing does concern me and might indicate a bigger problem under the surface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133
https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133:81,Integrability,message,messages,81,"@katevoss in your absence I've marked this as low. It's mainly a ""terrible error messages"" bug (you might consider this more important!). OTOH, the lack of failure recording and the EJEA crashing does concern me and might indicate a bigger problem under the surface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133
https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744:77,Availability,error,error,77,@cjllanwarne Might this have been addressed in the recent (c28?) EJEA/better error messages push? We had some FC reports of similar things that were reported resolved after the FC cromwell was bumped to 28 iirc.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744
https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744:83,Integrability,message,messages,83,@cjllanwarne Might this have been addressed in the recent (c28?) EJEA/better error messages push? We had some FC reports of similar things that were reported resolved after the FC cromwell was bumped to 28 iirc.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744
https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040:17,Deployability,update,update,17,"@cjllanwarne any update on whether this is still an issue? I know aborts is a tangled mess, is this still part of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040
https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040:66,Safety,abort,aborts,66,"@cjllanwarne any update on whether this is still an issue? I know aborts is a tangled mess, is this still part of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040
https://github.com/broadinstitute/cromwell/issues/2052#issuecomment-284161081:8,Deployability,release,release,8,"The new release jar has been renamed and updated, thanks for the catch!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2052#issuecomment-284161081
https://github.com/broadinstitute/cromwell/issues/2052#issuecomment-284161081:41,Deployability,update,updated,41,"The new release jar has been renamed and updated, thanks for the catch!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2052#issuecomment-284161081
https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367:807,Availability,ping,pinging,807,"I'm thinking perhaps this should be address instead through the use of docker User Namespaces and described in the documentation (a critical part of the solution!). When docker runs a container as root, it isolates the user/groups from the host definitions and instead remaps them. This could be important to a user for two reasons. Once, files written as root in the container will instead be written as this remapped user. Second, since IO on the underlying host VM is currently done as root, the container can read files as root. So if you mounted in /etc/passwd you could read/write on top of that. For a good tutorial see:. E.g. http://blog.aquasec.com/docker-1.10-user-namespace. For more details see:. https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-user-namespace-options. Also pinging @davidbernick for more thoughts from a security perspective",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367
https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367:854,Security,secur,security,854,"I'm thinking perhaps this should be address instead through the use of docker User Namespaces and described in the documentation (a critical part of the solution!). When docker runs a container as root, it isolates the user/groups from the host definitions and instead remaps them. This could be important to a user for two reasons. Once, files written as root in the container will instead be written as this remapped user. Second, since IO on the underlying host VM is currently done as root, the container can read files as root. So if you mounted in /etc/passwd you could read/write on top of that. For a good tutorial see:. E.g. http://blog.aquasec.com/docker-1.10-user-namespace. For more details see:. https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-user-namespace-options. Also pinging @davidbernick for more thoughts from a security perspective",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367
https://github.com/broadinstitute/cromwell/issues/2054#issuecomment-323150716:39,Availability,error,error,39,@Horneth I have run into the disk full error several times and the workflow appropriately fails,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2054#issuecomment-323150716
https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313:26,Deployability,configurat,configuration,26,make sure to document the configuration in readme and add a sentence that this exists and the changelog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313
https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313:26,Modifiability,config,configuration,26,make sure to document the configuration in readme and add a sentence that this exists and the changelog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313
https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-285151638:134,Energy Efficiency,schedul,schedule,134,"@Horneth oh right. Yeah, lets do it as in person unless no one is interested in which case disregard. If it's too much of a hassle to schedule around me feel free to not bother and I'll attend if I can.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-285151638
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:551,Deployability,release,release,551,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:655,Deployability,release,release,655,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:154,Performance,load,load,154,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:496,Performance,load,load,496,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:920,Usability,responsiv,responsiveness,920,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542:234,Deployability,configurat,configuration,234,"That plot is the load on the Cromwell server itself? . From what's you've said, and the top output, it looks like your tasks are exeucting on that server as well. You mentioned that you're running SFS, can you tell us more about your configuration? Are you dispatching to PBS but the execution host is the cromwell server itself (where the sync commands are running)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542:234,Modifiability,config,configuration,234,"That plot is the load on the Cromwell server itself? . From what's you've said, and the top output, it looks like your tasks are exeucting on that server as well. You mentioned that you're running SFS, can you tell us more about your configuration? Are you dispatching to PBS but the execution host is the cromwell server itself (where the sync commands are running)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542:17,Performance,load,load,17,"That plot is the load on the Cromwell server itself? . From what's you've said, and the top output, it looks like your tasks are exeucting on that server as well. You mentioned that you're running SFS, can you tell us more about your configuration? Are you dispatching to PBS but the execution host is the cromwell server itself (where the sync commands are running)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023:25,Performance,load,load,25,"Good point - that is the load on cromwell server itself; default backend is PBS (very like SGE) but some of the simple tasks in the workflow (`mkdir`, `uuidgen`) specify `backend: ""Local""` in the WDL `runtime` block.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023:112,Usability,simpl,simple,112,"Good point - that is the load on cromwell server itself; default backend is PBS (very like SGE) but some of the simple tasks in the workflow (`mkdir`, `uuidgen`) specify `backend: ""Local""` in the WDL `runtime` block.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336:269,Availability,echo,echo,269,"Regardless of where the task is executing though, that `sync` is expensive, and the more cores and memory you have on execution nodes, and the less work the `command` is actually doing, the worse things become when lots of tasks get allocated to a given node.; Simple ""echo Hello, World"" tasks occupy one of our 56-core execution nodes for about 30 seconds when 56 of them are allocated to a node, nearly the whole time spent in sync. ; The answer could well be ""then stop doing that"", but I hope not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336:233,Energy Efficiency,allocate,allocated,233,"Regardless of where the task is executing though, that `sync` is expensive, and the more cores and memory you have on execution nodes, and the less work the `command` is actually doing, the worse things become when lots of tasks get allocated to a given node.; Simple ""echo Hello, World"" tasks occupy one of our 56-core execution nodes for about 30 seconds when 56 of them are allocated to a node, nearly the whole time spent in sync. ; The answer could well be ""then stop doing that"", but I hope not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336:377,Energy Efficiency,allocate,allocated,377,"Regardless of where the task is executing though, that `sync` is expensive, and the more cores and memory you have on execution nodes, and the less work the `command` is actually doing, the worse things become when lots of tasks get allocated to a given node.; Simple ""echo Hello, World"" tasks occupy one of our 56-core execution nodes for about 30 seconds when 56 of them are allocated to a node, nearly the whole time spent in sync. ; The answer could well be ""then stop doing that"", but I hope not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336:261,Usability,Simpl,Simple,261,"Regardless of where the task is executing though, that `sync` is expensive, and the more cores and memory you have on execution nodes, and the less work the `command` is actually doing, the worse things become when lots of tasks get allocated to a given node.; Simple ""echo Hello, World"" tasks occupy one of our 56-core execution nodes for about 30 seconds when 56 of them are allocated to a node, nearly the whole time spent in sync. ; The answer could well be ""then stop doing that"", but I hope not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:76,Availability,down,down,76,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:243,Availability,echo,echo,243,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:98,Deployability,integrat,integration,98,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:98,Integrability,integrat,integration,98,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:110,Testability,test,tests,110,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:223,Testability,test,tests,223,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284814964:113,Testability,test,test,113,Might we experiment with [unbuffer](http://unix.stackexchange.com/questions/25372/turn-off-buffering-in-pipe) to test this hypothesis?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284814964
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:3,Deployability,update,update,3,"An update...; It looks like the performance of `sync` ‚Äî run on command line entirely outside the context of cromwell ‚Äî that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:407,Deployability,configurat,configuration,407,"An update...; It looks like the performance of `sync` ‚Äî run on command line entirely outside the context of cromwell ‚Äî that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:407,Modifiability,config,configuration,407,"An update...; It looks like the performance of `sync` ‚Äî run on command line entirely outside the context of cromwell ‚Äî that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:32,Performance,perform,performance,32,"An update...; It looks like the performance of `sync` ‚Äî run on command line entirely outside the context of cromwell ‚Äî that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:642,Performance,perform,performance,642,"An update...; It looks like the performance of `sync` ‚Äî run on command line entirely outside the context of cromwell ‚Äî that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:684,Performance,perform,performance,684,"An update...; It looks like the performance of `sync` ‚Äî run on command line entirely outside the context of cromwell ‚Äî that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482:111,Deployability,integrat,integration,111,"@delocalizer We're starting to consider that the issue is in tooling, specifically in the tools we use for our integration tests. Since you are as far as I know the largest user of the shared file system backend(s), to what degree do you trust that tools are flushing when they're complete?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482:111,Integrability,integrat,integration,111,"@delocalizer We're starting to consider that the issue is in tooling, specifically in the tools we use for our integration tests. Since you are as far as I know the largest user of the shared file system backend(s), to what degree do you trust that tools are flushing when they're complete?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482:123,Testability,test,tests,123,"@delocalizer We're starting to consider that the issue is in tooling, specifically in the tools we use for our integration tests. Since you are as far as I know the largest user of the shared file system backend(s), to what degree do you trust that tools are flushing when they're complete?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482
https://github.com/broadinstitute/cromwell/issues/2060#issuecomment-329487219:169,Availability,error,error,169,"@cjllanwarne I know we've added interpolation for certain instances, is this still relevant? I can add this to our list of doc things to fix, but I agree that adding an error/warning would be good too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2060#issuecomment-329487219
https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090:103,Deployability,release,release,103,"After discussing with @abaumann and @geoffjentry, we are going to plan this for Cromwell 27, our first release of Q4 (April-or-so). Once this is complete, the A-Team will be able to use the Docker Hash library for their own features.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090
https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090:197,Security,Hash,Hash,197,"After discussing with @abaumann and @geoffjentry, we are going to plan this for Cromwell 27, our first release of Q4 (April-or-so). Once this is complete, the A-Team will be able to use the Docker Hash library for their own features.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090
https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793:34,Integrability,inject,injecting,34,"@cjllanwarne The problem with not injecting, and creating separate tasks, is that you either have to clone the entire repo again for each of the tasks to extract the version information etc.., or clone it once and pass around the execution dir of the corresponding task which is even more horrible IMO.; Unless there's another way I'm missing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793
https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793:34,Security,inject,injecting,34,"@cjllanwarne The problem with not injecting, and creating separate tasks, is that you either have to clone the entire repo again for each of the tasks to extract the version information etc.., or clone it once and pass around the execution dir of the corresponding task which is even more horrible IMO.; Unless there's another way I'm missing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793
https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:72,Deployability,release,release,72,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325
https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:165,Deployability,hotfix,hotfix,165,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325
https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:202,Integrability,inject,injection,202,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325
https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:202,Security,inject,injection,202,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325
https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:225,Testability,test,tested,225,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325
https://github.com/broadinstitute/cromwell/issues/2066#issuecomment-287111821:48,Testability,test,test,48,Note to fixer: also investigate why the centaur test `missing_imports` failed to catch this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2066#issuecomment-287111821
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:192,Deployability,configurat,configurations,192,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:269,Deployability,configurat,configuration,269,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:192,Modifiability,config,configurations,192,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:269,Modifiability,config,configuration,269,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:500,Modifiability,extend,extends,500,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:973,Modifiability,Config,Config,973,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:1366,Modifiability,Config,Config,1366,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:1390,Modifiability,config,config,1390,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:1707,Modifiability,portab,portable,1707,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:127,Usability,clear,clearer,127,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > ‚Ä¶; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-289041622:97,Usability,guid,guide,97,"Yes, this explains my issue. I suggest including a description of runtime parameters in the user guide: https://software.broadinstitute.org/wdl/userguide/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-289041622
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-289053131:28,Usability,feedback,feedback,28,"@MatthewMah thanks for your feedback, I'm in the process of updating the Cromwell docs and information about the runtime parameters will be a part of that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-289053131
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798:40,Modifiability,config,config,40,"After discussion, we will resurrect the config named [`backend.backendsAllowed`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-29d3e359e75a05cf433ad3abe5f194a8L85), [`backend.allowedBackends`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-59ba4e6691e949675b4ccaa65a906e1dL22), or maybe just `backend.allowed` to match the style of the config named `backend.default`. Whatever the name, only backends found in this explicit list will be loaded. By default, the reference list will only contain ""Local"". Thus, after upgrading, cromwell will default back to running _only_ the Local backend, until admins/users re-enable the other backends by overriding the list.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798:435,Modifiability,config,config,435,"After discussion, we will resurrect the config named [`backend.backendsAllowed`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-29d3e359e75a05cf433ad3abe5f194a8L85), [`backend.allowedBackends`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-59ba4e6691e949675b4ccaa65a906e1dL22), or maybe just `backend.allowed` to match the style of the config named `backend.default`. Whatever the name, only backends found in this explicit list will be loaded. By default, the reference list will only contain ""Local"". Thus, after upgrading, cromwell will default back to running _only_ the Local backend, until admins/users re-enable the other backends by overriding the list.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798:536,Performance,load,loaded,536,"After discussion, we will resurrect the config named [`backend.backendsAllowed`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-29d3e359e75a05cf433ad3abe5f194a8L85), [`backend.allowedBackends`](https://github.com/broadinstitute/cromwell/commit/9faa735df0039652b124ecc0b7e7e5ffa62fd0ff#diff-59ba4e6691e949675b4ccaa65a906e1dL22), or maybe just `backend.allowed` to match the style of the config named `backend.default`. Whatever the name, only backends found in this explicit list will be loaded. By default, the reference list will only contain ""Local"". Thus, after upgrading, cromwell will default back to running _only_ the Local backend, until admins/users re-enable the other backends by overriding the list.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288155798
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:14,Deployability,upgrade,upgrade,14,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:179,Deployability,release,release,179,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:267,Deployability,update,update,267,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:72,Modifiability,config,configure,72,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:155,Usability,clear,clearly,155,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-423236846:98,Modifiability,config,config,98,"For those coming across this issue, there is already an ""alternative"" `disable`. Add this to your config file and the ""Local"" backend will be disabled. . ```hocon; backend.providers.Local.config.root: /dev/null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-423236846
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-423236846:188,Modifiability,config,config,188,"For those coming across this issue, there is already an ""alternative"" `disable`. Add this to your config file and the ""Local"" backend will be disabled. . ```hocon; backend.providers.Local.config.root: /dev/null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-423236846
https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287195148:122,Modifiability,config,config,122,"This line will allow cromwell to start, but will fail all the workflows on a Local backend:. ```; backend.providers.Local.config.root = ""/dev/null""; ```. Tested on 24_hotfix (489f66b) and develop (b1039f7).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287195148
https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287195148:154,Testability,Test,Tested,154,"This line will allow cromwell to start, but will fail all the workflows on a Local backend:. ```; backend.providers.Local.config.root = ""/dev/null""; ```. Tested on 24_hotfix (489f66b) and develop (b1039f7).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287195148
https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287393971:0,Deployability,Deploy,Deployed,0,"Deployed in the FC dev repo, PR 349.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287393971
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-287444838:0,Availability,Ping,Ping,0,Ping me when it's ready for review.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-287444838
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870:105,Availability,avail,available,105,"Hey Thibault, can you let me know your timeline for this? I'm going on vacation in a week but want to be available to you to discuss anything you have questions about and also available to give feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870:176,Availability,avail,available,176,"Hey Thibault, can you let me know your timeline for this? I'm going on vacation in a week but want to be available to you to discuss anything you have questions about and also available to give feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870:194,Usability,feedback,feedback,194,"Hey Thibault, can you let me know your timeline for this? I'm going on vacation in a week but want to be available to you to discuss anything you have questions about and also available to give feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289556076:143,Usability,feedback,feedback,143,"Hey, I have started thinking about it but haven't written anything yet. I'll see if I can draft a first version this week at least to get your feedback on it before you go on vacation :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289556076
https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222:19,Deployability,update,update,19,@Horneth Could you update w/ a description of what's being fixed/improved/etc here and/or link to an issue? I can see the code but need help wrapping my head around what it's all doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222
https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222:141,Integrability,wrap,wrapping,141,@Horneth Could you update w/ a description of what's being fixed/improved/etc here and/or link to an issue? I can see the code but need help wrapping my head around what it's all doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:31,Availability,avail,available,31,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:820,Availability,Error,Error,820,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:845,Performance,load,load,845,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:26,Testability,log,logs,26,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:346,Testability,log,log,346,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:370,Testability,log,log,370,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287830506:144,Integrability,message,message,144,Yeah so it looks like the exact same thing happened. It can't delocalize the output file because it was never created. In the JES logs the same message as before happened again:. ```; 2017/03/20 16:44:51 I: Docker file /cromwell_root/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar maps to host location /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar.; 2017/03/20 16:44:51 I: Copying gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar to /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; 2017/03/20 16:44:51 I: Running command: sudo gsutil -q -m cp gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287830506
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287830506:130,Testability,log,logs,130,Yeah so it looks like the exact same thing happened. It can't delocalize the output file because it was never created. In the JES logs the same message as before happened again:. ```; 2017/03/20 16:44:51 I: Docker file /cromwell_root/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar maps to host location /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar.; 2017/03/20 16:44:51 I: Copying gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar to /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; 2017/03/20 16:44:51 I: Running command: sudo gsutil -q -m cp gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287830506
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-289777291:27,Deployability,update,update,27,@meganshand thanks for the update! I'll lower this in priority.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-289777291
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146:254,Availability,error,error,254,"I think this has just bitten me as well. I am reading from a collaborator's bucket of several hundred terabytes that has underscores in the bucket name. I _think_ this is resulting in the input files being unhashable and thus disabling call-caching. The error I see (minus a real bucket path) is:. ```; [2017-05-15 17:08:12,44] [error] a05af6bd:Pre_Merge_SV.Extract_Reads:21:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File input_cram hash. Caused by java.lang.IllegalArgumentException: Could not find suitable filesystem among Gcs to parse gs://bucket_with_underscores/my.cram.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146:329,Availability,error,error,329,"I think this has just bitten me as well. I am reading from a collaborator's bucket of several hundred terabytes that has underscores in the bucket name. I _think_ this is resulting in the input files being unhashable and thus disabling call-caching. The error I see (minus a real bucket path) is:. ```; [2017-05-15 17:08:12,44] [error] a05af6bd:Pre_Merge_SV.Extract_Reads:21:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File input_cram hash. Caused by java.lang.IllegalArgumentException: Could not find suitable filesystem among Gcs to parse gs://bucket_with_underscores/my.cram.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146:383,Availability,error,error,383,"I think this has just bitten me as well. I am reading from a collaborator's bucket of several hundred terabytes that has underscores in the bucket name. I _think_ this is resulting in the input files being unhashable and thus disabling call-caching. The error I see (minus a real bucket path) is:. ```; [2017-05-15 17:08:12,44] [error] a05af6bd:Pre_Merge_SV.Extract_Reads:21:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File input_cram hash. Caused by java.lang.IllegalArgumentException: Could not find suitable filesystem among Gcs to parse gs://bucket_with_underscores/my.cram.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146:378,Security,Hash,Hash,378,"I think this has just bitten me as well. I am reading from a collaborator's bucket of several hundred terabytes that has underscores in the bucket name. I _think_ this is resulting in the input files being unhashable and thus disabling call-caching. The error I see (minus a real bucket path) is:. ```; [2017-05-15 17:08:12,44] [error] a05af6bd:Pre_Merge_SV.Extract_Reads:21:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File input_cram hash. Caused by java.lang.IllegalArgumentException: Could not find suitable filesystem among Gcs to parse gs://bucket_with_underscores/my.cram.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146:491,Security,hash,hash,491,"I think this has just bitten me as well. I am reading from a collaborator's bucket of several hundred terabytes that has underscores in the bucket name. I _think_ this is resulting in the input files being unhashable and thus disabling call-caching. The error I see (minus a real bucket path) is:. ```; [2017-05-15 17:08:12,44] [error] a05af6bd:Pre_Merge_SV.Extract_Reads:21:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File input_cram hash. Caused by java.lang.IllegalArgumentException: Could not find suitable filesystem among Gcs to parse gs://bucket_with_underscores/my.cram.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308527429:48,Testability,log,logic,48,The current `JesExpressionFunctions#preMapping` logic does not distinguish between a path that fails to parse as a valid GCS URI for being relative to the call root and a path that fails to parse for being a fully qualified `gs://uri_with_underscores/file.txt` containing invalid characters in the bucket name.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308527429
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174:250,Availability,error,error,250,I should have made this explicit in the previous comment: . I don't currently see a way we can support underscores in bucket names as long as we're using Google's GCS NIO filesystem. But I do think Cromwell can and should fail with useful and timely error messages when presented with bucket names that will not work.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174
https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174:256,Integrability,message,messages,256,I should have made this explicit in the previous comment: . I don't currently see a way we can support underscores in bucket names as long as we're using Google's GCS NIO filesystem. But I do think Cromwell can and should fail with useful and timely error messages when presented with bucket names that will not work.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-287905079:38,Testability,test,tests,38,"Yes. Running as part of our automated tests in Travis. On Mar 20, 2017 17:13, ""Thib"" <notifications@github.com> wrote:. > Is this single workflow mode ?; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-287899957>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0l0I2ZeZd5Jsy015muaziLf0FK2ks5rnuwTgaJpZM4Mi6Mp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-287905079
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429:20,Availability,error,error,20,"@LeeTL1220 Does the error have the effect on the actual final status of the workflow ? Or does it cause cromwell to exit with a non 0 exit code ? I wasn't able to reproduce the exact same error but I've had similar ones and I've got a branch that should fix it, if you want to try it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429:188,Availability,error,error,188,"@LeeTL1220 Does the error have the effect on the actual final status of the workflow ? Or does it cause cromwell to exit with a non 0 exit code ? I wasn't able to reproduce the exact same error but I've had similar ones and I've got a branch that should fix it, if you want to try it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289471493:179,Availability,error,error,179,"No effect on final status. Just exits with non-zero. On Mon, Mar 27, 2017 at 9:58 AM, Thib <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Does the error have the effect; > on the actual final status of the workflow ? Or does it cause cromwell to; > exit with a non 0 exit code ? I wasn't able to reproduce the exact same; > error but I've had similar ones and I've got a branch that should fix it,; > if you want to try it out.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2yfUNo1FrM2Y7ftpxQSHOsOwCc3ks5rp8BpgaJpZM4Mi6Mp>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289471493
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289471493:356,Availability,error,error,356,"No effect on final status. Just exits with non-zero. On Mon, Mar 27, 2017 at 9:58 AM, Thib <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Does the error have the effect; > on the actual final status of the workflow ? Or does it cause cromwell to; > exit with a non 0 exit code ? I wasn't able to reproduce the exact same; > error but I've had similar ones and I've got a branch that should fix it,; > if you want to try it out.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2yfUNo1FrM2Y7ftpxQSHOsOwCc3ks5rp8BpgaJpZM4Mi6Mp>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289471493
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542:115,Availability,error,errors,115,"@LeeTL1220 so I haven't been able to get cromwell to exit with a non 0 exit code, although I do get a bunch of log errors like yours. I created a branch that shuts the system down in a cleaner way, which I believe is the reason for this error : `cromwell-2079`.; If you can test it out and see if it makes any difference that would be very useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542:175,Availability,down,down,175,"@LeeTL1220 so I haven't been able to get cromwell to exit with a non 0 exit code, although I do get a bunch of log errors like yours. I created a branch that shuts the system down in a cleaner way, which I believe is the reason for this error : `cromwell-2079`.; If you can test it out and see if it makes any difference that would be very useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542:237,Availability,error,error,237,"@LeeTL1220 so I haven't been able to get cromwell to exit with a non 0 exit code, although I do get a bunch of log errors like yours. I created a branch that shuts the system down in a cleaner way, which I believe is the reason for this error : `cromwell-2079`.; If you can test it out and see if it makes any difference that would be very useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542:111,Testability,log,log,111,"@LeeTL1220 so I haven't been able to get cromwell to exit with a non 0 exit code, although I do get a bunch of log errors like yours. I created a branch that shuts the system down in a cleaner way, which I believe is the reason for this error : `cromwell-2079`.; If you can test it out and see if it makes any difference that would be very useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542
https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542:274,Testability,test,test,274,"@LeeTL1220 so I haven't been able to get cromwell to exit with a non 0 exit code, although I do get a bunch of log errors like yours. I created a branch that shuts the system down in a cleaner way, which I believe is the reason for this error : `cromwell-2079`.; If you can test it out and see if it makes any difference that would be very useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542
https://github.com/broadinstitute/cromwell/pull/2080#issuecomment-289107247:28,Deployability,update,updated-names,28,"Besides concurring with the updated-names request, LGTM üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2080/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2080#issuecomment-289107247
https://github.com/broadinstitute/cromwell/pull/2083#issuecomment-288469021:16,Deployability,patch,patch,16,"This is a small patch on the patch of #2067. Yes, labels and the sql converters both need better unit and or centaur tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2083#issuecomment-288469021
https://github.com/broadinstitute/cromwell/pull/2083#issuecomment-288469021:29,Deployability,patch,patch,29,"This is a small patch on the patch of #2067. Yes, labels and the sql converters both need better unit and or centaur tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2083#issuecomment-288469021
https://github.com/broadinstitute/cromwell/pull/2083#issuecomment-288469021:117,Testability,test,tests,117,"This is a small patch on the patch of #2067. Yes, labels and the sql converters both need better unit and or centaur tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2083#issuecomment-288469021
https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288756294:92,Integrability,depend,depend,92,"@Horneth what do you think the effort would be to add retries to the WDL functions? Does it depend on the function? This might get prioritized as a part of Joint Calling, but for now I'll leave it out of the retries improvement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288756294
https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933:441,Availability,failure,failures,441,"I think the real solution would be to make WDLExpression evaluation truly asynchronous, which is not trivial.; The simpler solution to retry evaluations as a whole I would say is not that much work, although it happens in a bunch of places and some might be trickier than others. I would say it's small enough that we could try to squeeze it in 26, which would be nice because we've been seeing an increasingly large number of transient gcs failures lately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933
https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933:115,Usability,simpl,simpler,115,"I think the real solution would be to make WDLExpression evaluation truly asynchronous, which is not trivial.; The simpler solution to retry evaluations as a whole I would say is not that much work, although it happens in a bunch of places and some might be trickier than others. I would say it's small enough that we could try to squeeze it in 26, which would be nice because we've been seeing an increasingly large number of transient gcs failures lately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933
https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912:209,Availability,robust,robustness,209,@katevoss Specifically my concern has always been providing the ability to execute wdl functions in a controlled fashion via a worker pool (perhaps not even in the same JVM as the main engine) for scalability/robustness reasons. As it stands now a cromwell server could get crushed by a ton of these things happening all at once,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912
https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912:197,Performance,scalab,scalability,197,@katevoss Specifically my concern has always been providing the ability to execute wdl functions in a controlled fashion via a worker pool (perhaps not even in the same JVM as the main engine) for scalability/robustness reasons. As it stands now a cromwell server could get crushed by a ton of these things happening all at once,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912
https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-329788962:413,Performance,load,load,413,"a worker pool is a technique where you have a fixed size pool of workers taking work off a pile instead of just doing everything the moment it wants to be done. it allows you to have better control over system resources, at the cost of needing to handle ""what happens if the pile itself consumes too many system resources?"" (the latter is one reason why you sometimes see references to things being dropped under load)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-329788962
https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517:18,Integrability,depend,dependency,18,"You mentioned the dependency of job store simpletons on job store entries, is that still a complication here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517
https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517:42,Usability,simpl,simpletons,42,"You mentioned the dependency of job store simpletons on job store entries, is that still a complication here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517
https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288825795:211,Usability,simpl,simpletons,211,"Yes, it makes it more difficult to batch but I'm sure there's some slick magic that can make it happen ?; But yeah pretty much those events arrive in the form of. `JobStoreEntry -> List(JobStoreSimpleton)`. The simpletons have a FK mapping to their job store entry, which means job store entries have to be inserted first and the PK collected before simpletons can be inserted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288825795
https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288825795:350,Usability,simpl,simpletons,350,"Yes, it makes it more difficult to batch but I'm sure there's some slick magic that can make it happen ?; But yeah pretty much those events arrive in the form of. `JobStoreEntry -> List(JobStoreSimpleton)`. The simpletons have a FK mapping to their job store entry, which means job store entries have to be inserted first and the PK collected before simpletons can be inserted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288825795
https://github.com/broadinstitute/cromwell/issues/2086#issuecomment-288824418:258,Performance,load,load,258,"Right, so when I put in the batched metadata i ran into exactly this. I talked to bernick and the tl;dr was that he felt CloudSQL would be able to take this much further (hooray for our production) but any MySQL instance would need proper tuning to handle a load like this, and that that point it'd be a YMMV situation where every setup would be a bit different. If you're poking around this space try pointing at a CloudSQL and see if that helps, I never did verify that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2086#issuecomment-288824418
https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274:383,Availability,echo,echo,383,"@geoffjentry I ran this wdl on local backend, call caching off, and I mocked the backend response to immediately return success instead of running the job. So all jobs finish immediately and at the same time. It's not a realistic use case but it's an easy way to push cromwell hard in terms of execution performance for large scatter. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; runtime {; docker: ""ubuntu""; }; }. workflow wf_hello {; String wf_hello_input = ""world""; Array[Int] s = range(200000); scatter (i in s) {; call hello {input: addressee = wf_hello_input }; }; }; ```. Here are the results:. | Branch | JobStore Writes | ExecutionTime |; |------------|-----------------|-------------------------------------------------------------|; | Develop | On | Still computing runnable calls after 30' - no shard started |; | ThisBranch | On | 8' |; | ThisBranch | Off | 1'30"" |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274
https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274:304,Performance,perform,performance,304,"@geoffjentry I ran this wdl on local backend, call caching off, and I mocked the backend response to immediately return success instead of running the job. So all jobs finish immediately and at the same time. It's not a realistic use case but it's an easy way to push cromwell hard in terms of execution performance for large scatter. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; runtime {; docker: ""ubuntu""; }; }. workflow wf_hello {; String wf_hello_input = ""world""; Array[Int] s = range(200000); scatter (i in s) {; call hello {input: addressee = wf_hello_input }; }; }; ```. Here are the results:. | Branch | JobStore Writes | ExecutionTime |; |------------|-----------------|-------------------------------------------------------------|; | Develop | On | Still computing runnable calls after 30' - no shard started |; | ThisBranch | On | 8' |; | ThisBranch | Off | 1'30"" |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274
https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274:70,Testability,mock,mocked,70,"@geoffjentry I ran this wdl on local backend, call caching off, and I mocked the backend response to immediately return success instead of running the job. So all jobs finish immediately and at the same time. It's not a realistic use case but it's an easy way to push cromwell hard in terms of execution performance for large scatter. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; runtime {; docker: ""ubuntu""; }; }. workflow wf_hello {; String wf_hello_input = ""world""; Array[Int] s = range(200000); scatter (i in s) {; call hello {input: addressee = wf_hello_input }; }; }; ```. Here are the results:. | Branch | JobStore Writes | ExecutionTime |; |------------|-----------------|-------------------------------------------------------------|; | Develop | On | Still computing runnable calls after 30' - no shard started |; | ThisBranch | On | 8' |; | ThisBranch | Off | 1'30"" |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659:15,Safety,abort,abort,15,a big issue w/ abort is not that it doesn't claim it aborted in the response but all hell breaks loose internally :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659:53,Safety,abort,aborted,53,a big issue w/ abort is not that it doesn't claim it aborted in the response but all hell breaks loose internally :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885:202,Safety,abort,abort,202,"ToL about a few ways to assert deeper on this -- for JES, you could label; the VM with a random number (so you can find it later), then assert you can; find it out of band using gcloud commands, do the abort, then assert it; goes away. Similar for the local backend using a command with some unique; value that you can grep a ps for. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Mar 23, 2017 at 3:37 PM, Jeff Gentry <notifications@github.com>; wrote:. > a big issue w/ abort is not that it doesn't claim it aborted in the; > response but all hell breaks loose internally :); >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gzQqYfBKEOyRH6s2_KiWyfqyG647ks5rosn9gaJpZM4MnQcP>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885:604,Safety,abort,abort,604,"ToL about a few ways to assert deeper on this -- for JES, you could label; the VM with a random number (so you can find it later), then assert you can; find it out of band using gcloud commands, do the abort, then assert it; goes away. Similar for the local backend using a command with some unique; value that you can grep a ps for. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Mar 23, 2017 at 3:37 PM, Jeff Gentry <notifications@github.com>; wrote:. > a big issue w/ abort is not that it doesn't claim it aborted in the; > response but all hell breaks loose internally :); >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gzQqYfBKEOyRH6s2_KiWyfqyG647ks5rosn9gaJpZM4MnQcP>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885:642,Safety,abort,aborted,642,"ToL about a few ways to assert deeper on this -- for JES, you could label; the VM with a random number (so you can find it later), then assert you can; find it out of band using gcloud commands, do the abort, then assert it; goes away. Similar for the local backend using a command with some unique; value that you can grep a ps for. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Mar 23, 2017 at 3:37 PM, Jeff Gentry <notifications@github.com>; wrote:. > a big issue w/ abort is not that it doesn't claim it aborted in the; > response but all hell breaks loose internally :); >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gzQqYfBKEOyRH6s2_KiWyfqyG647ks5rosn9gaJpZM4MnQcP>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885:24,Testability,assert,assert,24,"ToL about a few ways to assert deeper on this -- for JES, you could label; the VM with a random number (so you can find it later), then assert you can; find it out of band using gcloud commands, do the abort, then assert it; goes away. Similar for the local backend using a command with some unique; value that you can grep a ps for. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Mar 23, 2017 at 3:37 PM, Jeff Gentry <notifications@github.com>; wrote:. > a big issue w/ abort is not that it doesn't claim it aborted in the; > response but all hell breaks loose internally :); >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gzQqYfBKEOyRH6s2_KiWyfqyG647ks5rosn9gaJpZM4MnQcP>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885:136,Testability,assert,assert,136,"ToL about a few ways to assert deeper on this -- for JES, you could label; the VM with a random number (so you can find it later), then assert you can; find it out of band using gcloud commands, do the abort, then assert it; goes away. Similar for the local backend using a command with some unique; value that you can grep a ps for. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Mar 23, 2017 at 3:37 PM, Jeff Gentry <notifications@github.com>; wrote:. > a big issue w/ abort is not that it doesn't claim it aborted in the; > response but all hell breaks loose internally :); >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gzQqYfBKEOyRH6s2_KiWyfqyG647ks5rosn9gaJpZM4MnQcP>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885:214,Testability,assert,assert,214,"ToL about a few ways to assert deeper on this -- for JES, you could label; the VM with a random number (so you can find it later), then assert you can; find it out of band using gcloud commands, do the abort, then assert it; goes away. Similar for the local backend using a command with some unique; value that you can grep a ps for. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Mar 23, 2017 at 3:37 PM, Jeff Gentry <notifications@github.com>; wrote:. > a big issue w/ abort is not that it doesn't claim it aborted in the; > response but all hell breaks loose internally :); >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gzQqYfBKEOyRH6s2_KiWyfqyG647ks5rosn9gaJpZM4MnQcP>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635:43,Safety,abort,aborts,43,@Horneth how about this ticket for testing aborts?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635:35,Testability,test,testing,35,@Horneth how about this ticket for testing aborts?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-342579064:16,Safety,abort,abort,16,centaur now has abort tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-342579064
https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-342579064:22,Testability,test,tests,22,centaur now has abort tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-342579064
https://github.com/broadinstitute/cromwell/issues/2089#issuecomment-329662656:31,Testability,test,tested,31,"@Horneth I believe this is now tested as a part of restart testing, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2089#issuecomment-329662656
https://github.com/broadinstitute/cromwell/issues/2089#issuecomment-329662656:59,Testability,test,testing,59,"@Horneth I believe this is now tested as a part of restart testing, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2089#issuecomment-329662656
https://github.com/broadinstitute/cromwell/issues/2089#issuecomment-329783404:209,Testability,test,tested,209,"No this is not, there's some prep-work that was done to make it (maybe) possible to do through centaur but currently the same Cromwell runs before and after the shutdown so there's no database migration being tested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2089#issuecomment-329783404
https://github.com/broadinstitute/cromwell/pull/2091#issuecomment-289031598:129,Performance,perform,performance,129,"üëç . Massive ToL (as in, this just reminded me of something I saw the other day). The akka docs somewhere make the point that for performance reasons one wants to completely handle an exception as close to the call site as possible and then transform into some other object which won't ever do stack unwinding and such. We often do the opposite, passing exceptions all the way back out and then handling them there. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2091/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2091#issuecomment-289031598
https://github.com/broadinstitute/cromwell/pull/2092#issuecomment-289249569:20,Integrability,depend,dependency,20,"Still fighting some dependency issues, there are two tests where I can only seem to get one or the other working. closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2092#issuecomment-289249569
https://github.com/broadinstitute/cromwell/pull/2092#issuecomment-289249569:53,Testability,test,tests,53,"Still fighting some dependency issues, there are two tests where I can only seem to get one or the other working. closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2092#issuecomment-289249569
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957:33,Availability,redundant,redundant,33,"I think some of the test code is redundant with SwaggerServiceSpec but I don't understand swagger well enough to opine. @kshakir - it looks like you did a lot of the swagger work (albeit a long time ago), any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957:33,Safety,redund,redundant,33,"I think some of the test code is redundant with SwaggerServiceSpec but I don't understand swagger well enough to opine. @kshakir - it looks like you did a lot of the swagger work (albeit a long time ago), any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957:20,Testability,test,test,20,"I think some of the test code is redundant with SwaggerServiceSpec but I don't understand swagger well enough to opine. @kshakir - it looks like you did a lot of the swagger work (albeit a long time ago), any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:4,Availability,redundant,redundant,4,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:302,Integrability,rout,route,302,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:659,Integrability,wrap,wrapping,659,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:671,Integrability,rout,route,671,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:696,Integrability,wrap,wrapped,696,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:704,Integrability,rout,routes,704,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:4,Safety,redund,redundant,4,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:14,Testability,test,test,14,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:137,Testability,test,tests,137,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:187,Testability,Test,Test,187,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:271,Testability,Test,Test,271,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:370,Testability,Test,Test,370,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065
https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-289526079:79,Security,hash,hashes,79,"I'd suggest titling this something like ""Consistently resolve Docker labels to hashes within a workflow"" that captures the point of this, if in fact that is the point. üòÑ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-289526079
https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:133,Availability,failure,failure,133,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726
https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:237,Security,hash,hash,237,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726
https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:309,Security,hash,hash,309,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726
https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:455,Security,hash,hash,455,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726
https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:542,Security,hash,hash,542,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726
https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:727,Security,hash,hash,727,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726
https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:766,Testability,Test,Test,766,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726
https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289779903:156,Testability,test,test,156,"I'm not sure if this can fix #2066, since we were already in a `Try`? Although this does look a lot neater... Did you get to the bottom of why the original test wasn't catching this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289779903
https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572:47,Performance,load,load,47,"@cjllanwarne the reason was that `WdlNamespace.load` was throwing an `ValidationException` which unfortunately is a `Throwable` but not an `Exception`, which is why there's also a PR in lenthall to make `AggregatedException` an `Exception`... I can't find a `missing_import` test in centaur though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572
https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572:70,Security,Validat,ValidationException,70,"@cjllanwarne the reason was that `WdlNamespace.load` was throwing an `ValidationException` which unfortunately is a `Throwable` but not an `Exception`, which is why there's also a PR in lenthall to make `AggregatedException` an `Exception`... I can't find a `missing_import` test in centaur though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572
https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572:275,Testability,test,test,275,"@cjllanwarne the reason was that `WdlNamespace.load` was throwing an `ValidationException` which unfortunately is a `Throwable` but not an `Exception`, which is why there's also a PR in lenthall to make `AggregatedException` an `Exception`... I can't find a `missing_import` test in centaur though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572
https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289807967:71,Testability,test,test,71,@Horneth it's in `src/main/resources/standardTestCases/missing_imports.test`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289807967
https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289779759:159,Integrability,message,message,159,"Couple of Qs (either way it's a better situation than current):. - Is it possible for write requests to enter into the metadata service after the check events message?; - What happens if the DB is still busy writing data (as you noted last week, that can get backed up)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289779759
https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289781672:188,Integrability,message,message,188,"To @geoffjentry's first point, should there be a quiescence period in SWRA? i.e. after finding there are no pending writes it could transition to a ReChecking state, send itself a ReCheck message on a timer, and check again. To the second point, admittedly I didn't check this but I assume the transaction has committed after which Cromwell can't really make any additional durability guarantees?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289781672
https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289783755:165,Integrability,message,messages,165,My gut feeling is to be extremely relaxed about the first point. The WorkflowExecutionActor has already exited at this point so nothing exists that would be sending messages to the metadata service. The only problem would be if messages get reordered somehow which IMO is unlikely,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289783755
https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289783755:228,Integrability,message,messages,228,My gut feeling is to be extremely relaxed about the first point. The WorkflowExecutionActor has already exited at this point so nothing exists that would be sending messages to the metadata service. The only problem would be if messages get reordered somehow which IMO is unlikely,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289783755
https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289810027:156,Availability,failure,failures,156,"First point: Looking at the code again, the only thing that can possibly be sent to metadata after the status of the workflow becomes terminal are workflow failures and end time, due to the ordering of the `onTransition` blocks in the WorkflowActor. I swapped them in the last commit, so from now on nothing should be sent to the metadata after the terminal status event is sent.; I'm happy to add an extra check for robustness but I don't believe it to be necessary. Second point: If the DB is busy writing data, the `WriteMetadataActor` will be in state `WritingToDb`, in which case it will always reply with `HasPendingWrites`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289810027
https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289810027:417,Availability,robust,robustness,417,"First point: Looking at the code again, the only thing that can possibly be sent to metadata after the status of the workflow becomes terminal are workflow failures and end time, due to the ordering of the `onTransition` blocks in the WorkflowActor. I swapped them in the last commit, so from now on nothing should be sent to the metadata after the terminal status event is sent.; I'm happy to add an extra check for robustness but I don't believe it to be necessary. Second point: If the DB is busy writing data, the `WriteMetadataActor` will be in state `WritingToDb`, in which case it will always reply with `HasPendingWrites`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289810027
https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291512744:27,Testability,test,testing,27,":+1: Minor questions, Nice testing. Travis looks like its harassing you... [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2102/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291512744
https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:314,Availability,error,error,314,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185
https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:271,Integrability,message,message,271,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185
https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:337,Performance,race condition,race condition,337,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185
https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:27,Testability,log,logging,27,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185
https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:388,Testability,test,tests,388,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185
https://github.com/broadinstitute/cromwell/pull/2104#issuecomment-290064284:31,Testability,test,testing,31,@horneth do you still have the testing environment you were using for scale stuff recently? I'd be curious to see a before & after just to make sure there's not something else lurking here besides that one bug,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2104#issuecomment-290064284
https://github.com/broadinstitute/cromwell/pull/2104#issuecomment-290145773:50,Testability,test,test,50,Ready for merging assuming it passes a regression test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2104#issuecomment-290145773
https://github.com/broadinstitute/cromwell/issues/2106#issuecomment-305232962:59,Deployability,release,released,59,@ruchim I believe this is now supported as of Cromwell 27 (released yesterday).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2106#issuecomment-305232962
https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640:24,Availability,error,error,24,"Is the problem that the error message is unhelpful, or that there is an error in the migration that is blocking?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640
https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640:72,Availability,error,error,72,"Is the problem that the error message is unhelpful, or that there is an error in the migration that is blocking?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640
https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640:30,Integrability,message,message,30,"Is the problem that the error message is unhelpful, or that there is an error in the migration that is blocking?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640
https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290506409:10,Availability,error,error,10,@katevoss error that is blocking the migration,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290506409
https://github.com/broadinstitute/cromwell/issues/2109#issuecomment-329663337:86,Testability,test,testing,86,@geoffjentry is this Epic still useful?; @ndbolliger this Epic has tickets related to testing that I made after we talked with you the first time,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2109#issuecomment-329663337
https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508:98,Availability,recover,recovered,98,What is being done is not exactly what this ticket says but it does test that at least one job is recovered properly during centaur tests.; I think this can be closed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508
https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508:98,Safety,recover,recovered,98,What is being done is not exactly what this ticket says but it does test that at least one job is recovered properly during centaur tests.; I think this can be closed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508
https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508:68,Testability,test,test,68,What is being done is not exactly what this ticket says but it does test that at least one job is recovered properly during centaur tests.; I think this can be closed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508
https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508:132,Testability,test,tests,132,What is being done is not exactly what this ticket says but it does test that at least one job is recovered properly during centaur tests.; I think this can be closed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508
https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:26,Deployability,integrat,integration,26,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247
https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:59,Energy Efficiency,Green,Green,59,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247
https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:26,Integrability,integrat,integration,26,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247
https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:38,Testability,test,tests,38,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247
https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:56,Availability,error,error,56,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999
https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:62,Integrability,message,message,62,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999
https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:105,Modifiability,config,config,105,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999
https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:140,Modifiability,config,config,140,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999
https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:161,Modifiability,config,config,161,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999
https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291:81,Availability,failure,failure,81,"üëç but it'd be nice to get travis to go green before merging, even if the current failure is unrelated to this change. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2121/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291
https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291:39,Energy Efficiency,green,green,39,"üëç but it'd be nice to get travis to go green before merging, even if the current failure is unrelated to this change. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2121/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291
https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-291249221:23,Modifiability,plugin,plugins,23,The ownership of those plugins is murky at best. I don't disagree but it's not a black & white thing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-291249221
https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495:606,Availability,avail,available,606,"# Re: Sublime support. ## Added support for if/then/else; Runtime was already present?. ## Easier Sublime Installation; In order to streamline Sublime installation, I [created a package](https://github.com/broadinstitute/wdl-sublime-syntax-highlighter) for their package manager Package Control. ## Requires moving those syntax highlighter files out of wdl repository; Sadly the package hosting mechanism requires the syntax files at the root of a repo and I didn't think the wdl repo would like that. As soon as [this PR](https://github.com/wbond/package_control_channel/pull/6579) is merged it should be available and much easier to install from Sublime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495
https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495:106,Deployability,Install,Installation,106,"# Re: Sublime support. ## Added support for if/then/else; Runtime was already present?. ## Easier Sublime Installation; In order to streamline Sublime installation, I [created a package](https://github.com/broadinstitute/wdl-sublime-syntax-highlighter) for their package manager Package Control. ## Requires moving those syntax highlighter files out of wdl repository; Sadly the package hosting mechanism requires the syntax files at the root of a repo and I didn't think the wdl repo would like that. As soon as [this PR](https://github.com/wbond/package_control_channel/pull/6579) is merged it should be available and much easier to install from Sublime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495
https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495:151,Deployability,install,installation,151,"# Re: Sublime support. ## Added support for if/then/else; Runtime was already present?. ## Easier Sublime Installation; In order to streamline Sublime installation, I [created a package](https://github.com/broadinstitute/wdl-sublime-syntax-highlighter) for their package manager Package Control. ## Requires moving those syntax highlighter files out of wdl repository; Sadly the package hosting mechanism requires the syntax files at the root of a repo and I didn't think the wdl repo would like that. As soon as [this PR](https://github.com/wbond/package_control_channel/pull/6579) is merged it should be available and much easier to install from Sublime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495
https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495:635,Deployability,install,install,635,"# Re: Sublime support. ## Added support for if/then/else; Runtime was already present?. ## Easier Sublime Installation; In order to streamline Sublime installation, I [created a package](https://github.com/broadinstitute/wdl-sublime-syntax-highlighter) for their package manager Package Control. ## Requires moving those syntax highlighter files out of wdl repository; Sadly the package hosting mechanism requires the syntax files at the root of a repo and I didn't think the wdl repo would like that. As soon as [this PR](https://github.com/wbond/package_control_channel/pull/6579) is merged it should be available and much easier to install from Sublime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495
https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-330277323:46,Deployability,update,updated,46,Closing this as sublime text and vim-wdl were updated. There remains an issue w/ intellij captured [here](https://github.com/broadinstitute/winstanley/issues/10) so that remains open.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-330277323
https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096:94,Availability,echo,echo,94,"I can't reproduce this. I ran the following WDL without issues:. ```; task test {; command {; echo ""hello""; }; output {; String o = read_string(stdout()); }; }. workflow workflow_test {; call test as aliased_test; output {; aliased_test.*; }; }; ```. @yfarjoun do you happen to have the full WDL ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096
https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096:75,Testability,test,test,75,"I can't reproduce this. I ran the following WDL without issues:. ```; task test {; command {; echo ""hello""; }; output {; String o = read_string(stdout()); }; }. workflow workflow_test {; call test as aliased_test; output {; aliased_test.*; }; }; ```. @yfarjoun do you happen to have the full WDL ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096
https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096:192,Testability,test,test,192,"I can't reproduce this. I ran the following WDL without issues:. ```; task test {; command {; echo ""hello""; }; output {; String o = read_string(stdout()); }; }. workflow workflow_test {; call test as aliased_test; output {; aliased_test.*; }; }; ```. @yfarjoun do you happen to have the full WDL ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096
https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305614974:10,Deployability,pipeline,pipeline,10,"The E Pam pipeline builder has actually gotten pretty good at making graph visualizations, you should try that yossi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305614974
https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599:153,Availability,error,error,153,Here's the URL to the EPAM pipeline builder: http://pb.opensource.epam.com/. I assume this can be closed but crommers can feel free to reopen if I am in error,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599
https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599:27,Deployability,pipeline,pipeline,27,Here's the URL to the EPAM pipeline builder: http://pb.opensource.epam.com/. I assume this can be closed but crommers can feel free to reopen if I am in error,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599
https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-291577666:196,Security,Access,Access,196,"@geoffjentry as a part of your spec-ing of CromIAm, we'd like to add to the Role vs. Permission chart on [this page in confluence](https://broadinstitute.atlassian.net/wiki/display/GAWB/Workspace+Access+Control+-+User+experience), with the FireCloud roles and what actions they can take in Cromwell. We'd like you (and @cjllanwarne) to add the Cromwell actions, and Tiffany and I can help with deciding what roles have permission to do the actions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-291577666
https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-345318111:82,Security,authenticat,authenticate,82,"As a **user running workflows through a hosted Cromwell**, I want **to be able to authenticate and authorize permissions to my workflows**, so that **my labmates and I can view and take action on the workflows for which we have the right permissions**. - Remaining work: adding Collections to the /Query endpoint",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-345318111
https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-345318111:99,Security,authoriz,authorize,99,"As a **user running workflows through a hosted Cromwell**, I want **to be able to authenticate and authorize permissions to my workflows**, so that **my labmates and I can view and take action on the workflows for which we have the right permissions**. - Remaining work: adding Collections to the /Query endpoint",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-345318111
https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212:191,Safety,Risk,Risk,191,"As a **user accessing CaaS**, I want **to query a Collection of workflows**, so that I can **view the status of many workflows at once and get the results quickly**.; - Effort: **Medium**; - Risk: **Small to Medium**; - Business value: **Medium to Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212
https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212:12,Security,access,accessing,12,"As a **user accessing CaaS**, I want **to query a Collection of workflows**, so that I can **view the status of many workflows at once and get the results quickly**.; - Effort: **Medium**; - Risk: **Small to Medium**; - Business value: **Medium to Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212
https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-291508178:131,Performance,cache,cache,131,"@Thib; - Are there docs somewhere? So that I can see how to disable the lookup.; - Doesn't this leave SGE open to false alarm call cache hits when using; tags?. On Tue, Apr 4, 2017 at 9:34 AM, Thib <notifications@github.com> wrote:. > *@Horneth* commented on this pull request.; > ------------------------------; >; > In src/bin/travis/resources/centaur.inputs; > <https://github.com/broadinstitute/cromwell/pull/2139#discussion_r109662641>; > :; >; > > @@ -1,7 +1,7 @@; > {; > ""centaur_workflow.centaur.cromwell_jar"":""gs://cloud-cromwell-dev/travis-centaur/CROMWELL_JAR"",; > ""centaur_workflow.centaur.centaur_branch"":""CENTAUR_BRANCH"",; > - ""centaur_workflow.centaur.conf"":""gs://cloud-cromwell-dev/travis-centaur/multiBackend.conf"",; > + ""centaur_workflow.centaur.conf"":""gs://cloud-cromwell-dev/travis-centaur/multiBackendDockerLookup.conf"",; >; > Revert before merging; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2139#pullrequestreview-30778280>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkya-uKJyJCAUfDENTCs3BFzbwoY3ks5rskbLgaJpZM4MyKO1>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-291508178
https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784:331,Deployability,update,updated,331,"@LeeTL1220 there is doc in the README and CHANGELOG (in this PR) on how to disable it.; For SGE, since it doesn't honor the docker runtime attribute I don't think there can be false positive because of that. ; On a backend that does honor the docker attribute, if a tag is used, then yes it can yield false positives if the tag is updated, since Cromwell won't lookup the hash.; There can be false negatives though on SGE, if you change the value of the docker attribute in the WDL, it won't call cache, although it could because SGE will ignore the docker value anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784
https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784:497,Performance,cache,cache,497,"@LeeTL1220 there is doc in the README and CHANGELOG (in this PR) on how to disable it.; For SGE, since it doesn't honor the docker runtime attribute I don't think there can be false positive because of that. ; On a backend that does honor the docker attribute, if a tag is used, then yes it can yield false positives if the tag is updated, since Cromwell won't lookup the hash.; There can be false negatives though on SGE, if you change the value of the docker attribute in the WDL, it won't call cache, although it could because SGE will ignore the docker value anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784
https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784:372,Security,hash,hash,372,"@LeeTL1220 there is doc in the README and CHANGELOG (in this PR) on how to disable it.; For SGE, since it doesn't honor the docker runtime attribute I don't think there can be false positive because of that. ; On a backend that does honor the docker attribute, if a tag is used, then yes it can yield false positives if the tag is updated, since Cromwell won't lookup the hash.; There can be false negatives though on SGE, if you change the value of the docker attribute in the WDL, it won't call cache, although it could because SGE will ignore the docker value anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784
https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-291680472:129,Modifiability,config,config,129,@francares At this point I just took a quick skim so nothing substantive to say but in general I love that this is moving to the config backend. I was curious if you could provide more explanation on what motivates the changes to the runtime attr structure?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-291680472
https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-293009241:68,Security,validat,validation,68,"@geoffjentry I did those modifications to runtime attributes memory validation in order to re-use same behavior/code for disk attribute. HtCondor, SGE and TES use same way (data unit) to define disk capacity which is the same way to define memory size as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-293009241
https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294217336:97,Modifiability,config,config,97,@francares sorry about the delay. There are separate tickets filed to converge documentation for config based backends so I wouldn't worry about that too much. :+1: assuming the tests pass. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2141/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294217336
https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294217336:178,Testability,test,tests,178,@francares sorry about the delay. There are separate tickets filed to converge documentation for config based backends so I wouldn't worry about that too much. :+1: assuming the tests pass. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2141/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294217336
https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294218303:15,Energy Efficiency,green,green,15,Also :+1: once green. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2141/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294218303
https://github.com/broadinstitute/cromwell/issues/2145#issuecomment-297725072:60,Testability,mock,mock,60,Closing this issue as @Horneth has successfully run the 10K mock WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2145#issuecomment-297725072
https://github.com/broadinstitute/cromwell/issues/2152#issuecomment-292029467:64,Availability,error,errors,64,The test_read_data.wdl has lines commented out that resulted in errors I'm reporting here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2152#issuecomment-292029467
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:46,Deployability,hotfix,hotfix,46,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:97,Safety,abort,aborts,97,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:22,Usability,clear,clear,22,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292596812:40,Deployability,hotfix,hotfix,40,"@mcovarr I should have made that on the hotfix one, it's more in how that's communicated (and if they even care)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292596812
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655:81,Availability,error,errors,81,Spoke to @mcovarr in-person. My take on the goal of this PR is to catch database errors related to aborts and either report them back to the sender or log them. :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2154/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655:99,Safety,abort,aborts,99,Spoke to @mcovarr in-person. My take on the goal of this PR is to catch database errors related to aborts and either report them back to the sender or log them. :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2154/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655:151,Testability,log,log,151,Spoke to @mcovarr in-person. My take on the goal of this PR is to catch database errors related to aborts and either report them back to the sender or log them. :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2154/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655
https://github.com/broadinstitute/cromwell/issues/2162#issuecomment-293007654:35,Deployability,hotfix,hotfix,35,I think this should be fixed in 25 hotfix or develop. I entered #1945 a while back for what sounds like the same issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2162#issuecomment-293007654
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293584947:24,Security,validat,validate,24,Sounds like a potential validate endpoint.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293584947
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293587582:317,Availability,fault,faulty,317,"Waaaaay back when @yfarjoun asked for a ""dry run"" feature which was similar. We got lost in the weeds as I was involved. I remember at the time noting that this wouldn't be particularly useful since you only check things which were fully resolvable at submission time but that was based on what I now believe to be a faulty assumption, that typically only the entrypoint calls would have resolvable inputs at submission time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293587582
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966:29,Availability,fault,faulty,29,"Ah that assumption is indeed faulty -- in a full pipeline we can have upward of 20 inputs that may be used at various points, some only in the last few tasks to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966:49,Deployability,pipeline,pipeline,49,"Ah that assumption is indeed faulty -- in a full pipeline we can have upward of 20 inputs that may be used at various points, some only in the last few tasks to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293602394:102,Testability,test,testing,102,I think the only case figure really where we would have hundreds of static files would be if you were testing the primary inputs in a workflow that scatters over many things. But we can document that that is not supported and have some kind of cut off.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293602394
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332205964:46,Security,validat,validation,46,"@vdauwera is this a use case for #2652, add a validation endpoint?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332205964
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:60,Availability,avail,available,60,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:206,Performance,Queue,Queue,206,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:89,Security,validat,validation,89,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390
https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:159,Security,validat,validation,159,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390
https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293331032:79,Testability,test,tested,79,"Some general documentation would be helpful here as to what is and isn't being tested, known limitations etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293331032
https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293645068:100,Energy Efficiency,green,green,100,My main concern here is relying on it as a proof of validity until it's guaranteed that it won't go green if it's not picking up workflows in flight.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293645068
https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293933413:43,Security,validat,validate,43,"@geoffjentry @mcovarr this script doesn't *validate* anything at all, so yeah... The implication of that is, a person currently has to follow along, and make sure that things are happening at appropriate times. The comments are sort-of deliberately scary to encourage that :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293933413
https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293678971:281,Security,expose,expose,281,"If preemptible is requested, cost is being considered a higher priority; than time. Preempting within 10-20 minutes is not going to rack up much; cost. I could imagine a default cap of 10-20 for the number of early retries. If; the user cares a lot about time, it may be useful to expose it as a; parameter, but this seems less important for the common case. On Wed, Apr 12, 2017 at 2:46 PM, Jeff Gentry <notifications@github.com>; wrote:. > So what if a user actually means they only want to preempt N times no; > matter what? How do we know which the user really means?; >; > Sometimes it's about time, not cost.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293671865>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFK12asbeR0RAQpx0AGY7zQuqqlWtUIcks5rvRwTgaJpZM4M70XZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293678971
https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293684749:65,Performance,tune,tuned,65,@gsaksena Not necessarily. I know that there are people who have tuned their preemption number specifically thinking about total time on top of cost. . I'm not saying that this is a bad idea but it's not necessarily as cut & dry as you're making out.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293684749
https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592:534,Energy Efficiency,charge,charge,534,"From the point of view of the wider user community (which reaches far outside the Broad's walls), it would be difficult to justify (not to mention communicate and support) an implicit retry mechanism that would effectively override the request stated by the user in their WDL. So we would have to expose that second setting, but then that increases the technical complexity that we need to maintain and support as well. Additionally, this would be vulnerable to business decisions by Google -- for example, what if they change the no-charge duration cutoff in response to a sudden dramatic increase of retries on early-preemption jobs? . Generally speaking I believe the best thing we can do for the user community is provide a transparent way for people to understand what are the odds and tradeoff of preemption, and to control the setting depending on their time & cost constraints (ie how much they're willing to gamble).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592
https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592:842,Integrability,depend,depending,842,"From the point of view of the wider user community (which reaches far outside the Broad's walls), it would be difficult to justify (not to mention communicate and support) an implicit retry mechanism that would effectively override the request stated by the user in their WDL. So we would have to expose that second setting, but then that increases the technical complexity that we need to maintain and support as well. Additionally, this would be vulnerable to business decisions by Google -- for example, what if they change the no-charge duration cutoff in response to a sudden dramatic increase of retries on early-preemption jobs? . Generally speaking I believe the best thing we can do for the user community is provide a transparent way for people to understand what are the odds and tradeoff of preemption, and to control the setting depending on their time & cost constraints (ie how much they're willing to gamble).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592
https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592:297,Security,expose,expose,297,"From the point of view of the wider user community (which reaches far outside the Broad's walls), it would be difficult to justify (not to mention communicate and support) an implicit retry mechanism that would effectively override the request stated by the user in their WDL. So we would have to expose that second setting, but then that increases the technical complexity that we need to maintain and support as well. Additionally, this would be vulnerable to business decisions by Google -- for example, what if they change the no-charge duration cutoff in response to a sudden dramatic increase of retries on early-preemption jobs? . Generally speaking I believe the best thing we can do for the user community is provide a transparent way for people to understand what are the odds and tradeoff of preemption, and to control the setting depending on their time & cost constraints (ie how much they're willing to gamble).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293673942:21,Integrability,depend,depends,21,The current behavior depends on if this time out would cause GCP to return a preemption signal or another termination signal. They're not the same buckets of retries.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293673942
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422:99,Availability,error,error,99,"We'd need a way to detect that a job was timed out rather than genuinely preempted (either another error code or by analyzing the total run time).; We'd also need a special case in the ""start this as a preemptible VM?"" logic to not start the subsequent job preemptibly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422:19,Safety,detect,detect,19,"We'd need a way to detect that a job was timed out rather than genuinely preempted (either another error code or by analyzing the total run time).; We'd also need a special case in the ""start this as a preemptible VM?"" logic to not start the subsequent job preemptibly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422:219,Testability,log,logic,219,"We'd need a way to detect that a job was timed out rather than genuinely preempted (either another error code or by analyzing the total run time).; We'd also need a special case in the ""start this as a preemptible VM?"" logic to not start the subsequent job preemptibly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:68,Deployability,upgrade,upgraded,68,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:464,Deployability,upgrade,upgraded,464,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:184,Safety,avoid,avoid,184,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:276,Safety,Risk,Risk,276,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:369,Safety,timeout,timeout,369,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:490,Availability,failure,failure,490,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:599,Availability,failure,failures,599,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:94,Deployability,Pipeline,Pipelines,94,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:281,Safety,timeout,timeout,281,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:318,Safety,timeout,timeout,318,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:342,Safety,timeout,timeout,342,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:467,Safety,timeout,timeout,467,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:502,Safety,avoid,avoided,502,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:664,Safety,timeout,timeout,664,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563
https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499987555:173,Security,access,access,173,@mbookman our backlog has been moved to Jira. Here is the link to this [ticket](https://broadworkbench.atlassian.net/browse/BA-2168) ; You will need to create an account to access this ticket. I defer to @ruchim for priority of this feature.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499987555
https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:118,Deployability,update,updates,118,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559
https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:191,Deployability,update,updates,191,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559
https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:221,Deployability,update,updates,221,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559
https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:280,Deployability,update,updates,280,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559
https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:574,Deployability,update,updates,574,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559
https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:165,Performance,load,load,165,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559
https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016:19,Availability,down,downside,19,"@kcibul what's the downside of just splitting every interval in the original set (N=2500) into 4 even pieces? Is it just that we won't be able to ensure that each quarter is equally balanced? Because rather than spending effort balancing shards, I'd rather optimize the GATK code. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016
https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016:257,Performance,optimiz,optimize,257,"@kcibul what's the downside of just splitting every interval in the original set (N=2500) into 4 even pieces? Is it just that we won't be able to ensure that each quarter is equally balanced? Because rather than spending effort balancing shards, I'd rather optimize the GATK code. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016
https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294230149:306,Usability,simpl,simply,306,"That might be a reasonable place to start. Part of the confusion (that I just want someone to get right!) is what that list should be. There's v2 and v3, which are ""right"" but only about 1000 shards. Then there are several flavors of what was used to do the 20k, which is the ~20k shards. . So the task is simply getting someone who knows to (a) pick the best starting point (something with the right territory and somewhat even) and (b) do the splitting . Thinking out loud... Do you think that the balance of the shards changes with the number of samples? That is... could you take a 100-sample call set and use it to empirically balance the shards and then that would scale to 50k? or does the distribution change significantly as you add more samples?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294230149
https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-295265998:1386,Safety,predict,predicting,1386,"I talked to @dshiga about how gs://broad-gotc-dev-storage/dshiga/wgs_split_even_tiledb.hg38.v3.interval_list was created. If you want an interval list for hg38 then that is the best interval list we have created thus far. `v1` is our first crack at creating an interval list that was ""even"" in terms of run times starting with histograms created from the 1000 genomes vcfs (converted from hg19). `v2` is `v1` after taking the 15 longest running shards from the callset we used `v1` on and chopped them into 10 equal parts by bases. `v3` was created once we realized that `v2` nor `v1` covered all of the genome territory that our calling intervals cover so we added the parts of the genome that were missing (they're small so they shouldn't be problem causing). If you wanted to split this interval list without any kind of model helping you make smart splits you will probably run into what we did with gnomad of having to iteratively split the same intervals multiple times to hit acceptable run times which is no fun. . You can always overfit the problem and just make a crazy number of intervals where you can guarantee that even in the worst case for each of those intervals, it will still be under some run time you want to beat but that probably isn't the best strategy for many reasons. Alot of it has to do with the content of the callset which is something we are not good at predicting. The callset we used `v2` on had one interval that was multiple times longer than the next longest shard but in the callset we used `v1` on that same interval was one of the faster ones. Both callsets had around the same number of samples. Currently we are always retroactively ""fixing"" our interval list",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-295265998
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295843178:80,Availability,error,error,80,Hi @cowmoo - this appears to be retrying across all backends no matter what the error was. Is that what you intended for this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295843178
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647:584,Modifiability,config,configs,584,"Hi Jeff, . The short answer is I don't know about the overall Cromwell world. . The longer answer is: it's intended for me to get around transient issues involving local filesystems and GridEngine dispatcher. . We'd either run into paths not found when it's a network mount issue; or submitting jobs to a GridEngine queue (e.g., UGER) where the job might get killed after a certain time-period or if it takes up too much resources, . I understand that Cromwell already have retry logic that deals with I/O issues or pre-emptible VMs in the GCP world. I'm not sure how to organize the configs and the code to harmonize these two retry world's, so I'll leave it to you except to state that we do want some kind of ""Retry"" in the GridEngine use case. . Thanks,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647:316,Performance,queue,queue,316,"Hi Jeff, . The short answer is I don't know about the overall Cromwell world. . The longer answer is: it's intended for me to get around transient issues involving local filesystems and GridEngine dispatcher. . We'd either run into paths not found when it's a network mount issue; or submitting jobs to a GridEngine queue (e.g., UGER) where the job might get killed after a certain time-period or if it takes up too much resources, . I understand that Cromwell already have retry logic that deals with I/O issues or pre-emptible VMs in the GCP world. I'm not sure how to organize the configs and the code to harmonize these two retry world's, so I'll leave it to you except to state that we do want some kind of ""Retry"" in the GridEngine use case. . Thanks,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647:480,Testability,log,logic,480,"Hi Jeff, . The short answer is I don't know about the overall Cromwell world. . The longer answer is: it's intended for me to get around transient issues involving local filesystems and GridEngine dispatcher. . We'd either run into paths not found when it's a network mount issue; or submitting jobs to a GridEngine queue (e.g., UGER) where the job might get killed after a certain time-period or if it takes up too much resources, . I understand that Cromwell already have retry logic that deals with I/O issues or pre-emptible VMs in the GCP world. I'm not sure how to organize the configs and the code to harmonize these two retry world's, so I'll leave it to you except to state that we do want some kind of ""Retry"" in the GridEngine use case. . Thanks,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295900647
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296215555:124,Availability,failure,failure,124,"@cowmoo is there a way for Cromwell to tell from the result of running the ""begin execution"" command, or from the resulting failure, that the task should be retryable?. I like the idea of retrying certain things, but a blanket ""retry everything n times, regardless of problem"" is probably going to annoy more people than it helps (especially if they're paying for it directly in cloud compute!). One thing we previously considered was a ""retryOnStdoutRegex"" attribute, maybe something like:; ```; command {; # ...; }; runtime {; retryOnStdoutRegex: ""path not found:""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296215555
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296226126:157,Performance,queue,queues,157,"Hi Chris, . This would work for us as long as the retryOnStdoutRegex allows for just blanket retries (since it's free for us to do it on internal GridEngine queues), . Thanks for the follow-up, ; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296226126
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296802795:288,Modifiability,config,config,288,"Hi @cowmoo and @MatthewMah (I think, I know this came up recently and I think it was you). This PR goes beyond the scope of what we're comfortable with as it paints with a pretty broad brush. If either of you were interested in implementing something which retried job submission for the config backend only (e.g. SGE, SLURM, etc) we'd be interested in that. We'll likely get to it ourselves at some point but not immediately. If that's something either of you (or anyone else coming across this) are interested in let us know and we can point you in the right direction. It should be relatively straightforward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296802795
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:436,Availability,error,errors,436,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:462,Availability,error,errors,462,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:984,Availability,error,error-caused-by-a-job-submission-failure,984,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:69,Deployability,update,updated,69,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:801,Deployability,configurat,configuration-not-working,801,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:220,Modifiability,config,configure,220,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:621,Modifiability,config,configs,621,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:801,Modifiability,config,configuration-not-working,801,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510116881:168,Safety,timeout,timeout,168,"@genomics-geek Having just spent some time last week on this (also on SGE), I believe you need this:; https://cromwell.readthedocs.io/en/stable/backends/HPC/#exit-code-timeout",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510116881
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:89,Availability,error,errors,89,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:145,Availability,down,down,145,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:2020,Availability,alive,alive,2020,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:424,Integrability,interface,interface,424,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:339,Modifiability,config,config,339,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:682,Modifiability,config,config,682,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:689,Modifiability,Config,ConfigBackendLifecycleActorFactory,689,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:726,Modifiability,config,config,726,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1392,Modifiability,config,config,1392,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1399,Modifiability,Config,ConfigBackendLifecycleActorFactory,1399,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1436,Modifiability,config,config,1436,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:502,Performance,cache,cache-results,502,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:736,Performance,concurren,concurrent-job-limit,736,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1509,Performance,concurren,concurrent-job-limit,1509,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1486,Safety,timeout,timeout-seconds,1486,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1275,Security,hash,hashing-strategy,1275,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:162,Availability,error,error,162,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:182,Availability,error,error,182,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:197,Availability,error,error,197,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:213,Availability,error,error,213,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:138,Integrability,message,message,138,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335
https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:154,Safety,timeout,timeout,154,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:124,Availability,down,down,124,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:756,Availability,down,download,756,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:618,Deployability,integrat,integrate,618,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:618,Integrability,integrat,integrate,618,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:935,Testability,test,test,935,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-301208260:19,Availability,ping,ping,19,"awesome! Yeah just ping on here when you are ready, and I'll be glad to help :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-301208260
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279:290,Safety,Risk,Risk,290,"As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**.; - Effort: ** @geoffjentry ? **; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330348650:121,Deployability,install,installing,121,"I can also offer to help, in whatever form is useful! If you just need to use / pull, then Singularity image support via installing it should fit the bill. Users can use Github to host images via Singularity Hub. If you want to host your own registry, then Singularity Registry is the way to go! Let me know if I can help, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330348650
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888:159,Deployability,pipeline,pipelines,159,I'm checking out WDL/Cromwell at the moment and this feature would make Cromwell definitely more interesting. It would make it much easier to run reproducible pipelines without relying on docker. (Docker is a no go on our cluster because it gives users root access.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888:258,Security,access,access,258,I'm checking out WDL/Cromwell at the moment and this feature would make Cromwell definitely more interesting. It would make it much easier to run reproducible pipelines without relying on docker. (Docker is a no go on our cluster because it gives users root access.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186:43,Deployability,integrat,integration,43,"I just found out that Cromwell-Singularity integration will be on the agenda on Winter Codefest 2018, starting tomorrow! See https://docs.google.com/document/d/1RlDUWRFqMcy4V2vvkA1_ENsVo6TXge2wIO_Nf73Itk0/edit#heading=h.xg79ql4rt605. You can join in (also remotely) by checking this file: https://docs.google.com/spreadsheets/d/1o4xDUgl2iu_CgFuDpB1swtG8XVZK3aifvKlhh5qagyI/edit#gid=0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186:43,Integrability,integrat,integration,43,"I just found out that Cromwell-Singularity integration will be on the agenda on Winter Codefest 2018, starting tomorrow! See https://docs.google.com/document/d/1RlDUWRFqMcy4V2vvkA1_ENsVo6TXge2wIO_Nf73Itk0/edit#heading=h.xg79ql4rt605. You can join in (also remotely) by checking this file: https://docs.google.com/spreadsheets/d/1o4xDUgl2iu_CgFuDpB1swtG8XVZK3aifvKlhh5qagyI/edit#gid=0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052:90,Deployability,integrat,integration,90,"I also encountered the udocker-singularity route in the discussion on cwltool singularity integration. Maybe it is an idea to take a closer look on the udocker-singularity implementation as a starting point for workflow tool singularity usage. . Or maybe not, because you will lose HPC friendly singularity features this way!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052:43,Integrability,rout,route,43,"I also encountered the udocker-singularity route in the discussion on cwltool singularity integration. Maybe it is an idea to take a closer look on the udocker-singularity implementation as a starting point for workflow tool singularity usage. . Or maybe not, because you will lose HPC friendly singularity features this way!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052:90,Integrability,integrat,integration,90,"I also encountered the udocker-singularity route in the discussion on cwltool singularity integration. Maybe it is an idea to take a closer look on the udocker-singularity implementation as a starting point for workflow tool singularity usage. . Or maybe not, because you will lose HPC friendly singularity features this way!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310220:29,Security,access,accessible,29,"@geoffjentry In case this is accessible, can you point me to the udocker singularity work you mentioned?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310220
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358345068:129,Integrability,rout,route,129,"With udocker running in proot vs Singularity running in chroot, some HPC performance/IB/GPUcapability issues might occur in this route.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358345068
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358345068:73,Performance,perform,performance,73,"With udocker running in proot vs Singularity running in chroot, some HPC performance/IB/GPUcapability issues might occur in this route.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358345068
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385569620:26,Deployability,update,update,26,@geoffjentry is there any update on udocker support or is that already works with some tricks ?. I got same question for singularity as well.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385569620
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385579273:331,Integrability,wrap,wrap,331,"hey I noticed that you guys use Google Cloud? http://cromwell.readthedocs.io/en/develop/wf_options/Google/ I have a builder that runs here, so there might be some synthesis between the two, although I'm not super familiar with Cromwell. If you just need to use Singularity containers your best bet is to do a singularity pull (and wrap these commands into your workflow functions, allowing the user to specify the container uri). if there is more of a service that someone is running with cromwell and you want to dip into the storage directly (and would use the API en masse) then we could try this --> https://cloud.google.com/storage/docs/requester-pays",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385579273
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411567369:172,Deployability,update,updates,172,hey friends! Just wanted to poke here again that this is still badly wanted / needed / desired / dreamed of / prayed for / sacrificial lambs... (you get the idea :P _) Any updates? Can I help?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411567369
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411568103:369,Availability,down,downstream,369,"Hi @vsoch - the first problem to solve is how to represent the usage of singularity in one's WDL (not sure how CWL does it, will need to look). This is being discussed in the [OpenWDL group](https://github.com/openwdl/wdl/pull/237) so if you have thoughts here that'd be very welcome. . For instance, is there a way to express ""run this container"" but not be locking a downstream WDL user into Singularity vs Docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411568103
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375:267,Deployability,pipeline,pipeline,267,"I'm not great / experienced with Cromwell, and to be honest I'm not sure what native support would mean. What I was trying is to just treat a singularity container like an executable, and add it as a Local backend, sort of like this --> https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. That works to run the analysis step (in a singularity container) just using singularity like any executable. I don't totally understand the job_id so there is a bug, but my colleague @bek is going to take a look! The container is run to produce the output, so that's a good start at least (and probably I'm missing something huge here). So to answer your question... in my wdl at least, I'm just using the same local commands. It looks the same as it would running any Local backend configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375:811,Deployability,configurat,configuration,811,"I'm not great / experienced with Cromwell, and to be honest I'm not sure what native support would mean. What I was trying is to just treat a singularity container like an executable, and add it as a Local backend, sort of like this --> https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. That works to run the analysis step (in a singularity container) just using singularity like any executable. I don't totally understand the job_id so there is a bug, but my colleague @bek is going to take a look! The container is run to produce the output, so that's a good start at least (and probably I'm missing something huge here). So to answer your question... in my wdl at least, I'm just using the same local commands. It looks the same as it would running any Local backend configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375:811,Modifiability,config,configuration,811,"I'm not great / experienced with Cromwell, and to be honest I'm not sure what native support would mean. What I was trying is to just treat a singularity container like an executable, and add it as a Local backend, sort of like this --> https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. That works to run the analysis step (in a singularity container) just using singularity like any executable. I don't totally understand the job_id so there is a bug, but my colleague @bek is going to take a look! The container is run to produce the output, so that's a good start at least (and probably I'm missing something huge here). So to answer your question... in my wdl at least, I'm just using the same local commands. It looks the same as it would running any Local backend configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732:21,Availability,down,down,21,Yeah doesn't it come down to:. ```bash; # singularity; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; ```; vs; ```bash; # docker; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=docker cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/docker.json; ```; so you use the same *.wdl but just choose a different backend / and workflow opts?. ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732:161,Testability,test,test,161,Yeah doesn't it come down to:. ```bash; # singularity; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; ```; vs; ```bash; # docker; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=docker cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/docker.json; ```; so you use the same *.wdl but just choose a different backend / and workflow opts?. ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732:178,Testability,TEST,TEST-YEAST,178,Yeah doesn't it come down to:. ```bash; # singularity; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; ```; vs; ```bash; # docker; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=docker cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/docker.json; ```; so you use the same *.wdl but just choose a different backend / and workflow opts?. ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732:365,Testability,test,test,365,Yeah doesn't it come down to:. ```bash; # singularity; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; ```; vs; ```bash; # docker; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=docker cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/docker.json; ```; so you use the same *.wdl but just choose a different backend / and workflow opts?. ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732:382,Testability,TEST,TEST-YEAST,382,Yeah doesn't it come down to:. ```bash; # singularity; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; ```; vs; ```bash; # docker; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=docker cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/docker.json; ```; so you use the same *.wdl but just choose a different backend / and workflow opts?. ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1074,Deployability,pipeline,pipeline,1074,"ngularity container vs a docker container - but if that's not really the case I've definitely been overcomplicating the matter. I'll admit that I've never been comfortable in my understanding of Singularity. If you are using a container, it definitely is an ""either / or"" in the sense that getting one working inside the other is pretty challenging. The reason a Dockerized cromwell doesn't work on a host (to submit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:730,Integrability,depend,dependency,730,"> I think I've been viewing Singularity & Docker as more of an ""either/or"" in that perhaps a task would require a singularity container vs a docker container - but if that's not really the case I've definitely been overcomplicating the matter. I'll admit that I've never been comfortable in my understanding of Singularity. If you are using a container, it definitely is an ""either / or"" in the sense that getting one working inside the other is pretty challenging. The reason a Dockerized cromwell doesn't work on a host (to submit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1837,Integrability,interface,interface,1837,"mit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to be more of a site specific situation, like hwat you're showing here?. I don't think it would be site specific (if the container is singularity, it would largely be the same, a container_uri and then some args to it). The only reason I have two sections is because I was trying out two ways to do it. Neither of them fully work (at least according to cromwell) because I don't know what that job_id business it :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1306,Modifiability,plugin,plugin,1306,"een comfortable in my understanding of Singularity. If you are using a container, it definitely is an ""either / or"" in the sense that getting one working inside the other is pretty challenging. The reason a Dockerized cromwell doesn't work on a host (to submit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to be more of a site specific situation, like hwat you're showing here?. I don't think it would be site specific (if the container is singularity, it would la",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1544,Modifiability,plugin,plugin,1544,"mit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to be more of a site specific situation, like hwat you're showing here?. I don't think it would be site specific (if the container is singularity, it would largely be the same, a container_uri and then some args to it). The only reason I have two sections is because I was trying out two ways to do it. Neither of them fully work (at least according to cromwell) because I don't know what that job_id business it :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1745,Modifiability,plugin,plugin,1745,"mit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to be more of a site specific situation, like hwat you're showing here?. I don't think it would be site specific (if the container is singularity, it would largely be the same, a container_uri and then some args to it). The only reason I have two sections is because I was trying out two ways to do it. Neither of them fully work (at least according to cromwell) because I don't know what that job_id business it :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1801,Modifiability,plugin,plugin,1801,"mit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to be more of a site specific situation, like hwat you're showing here?. I don't think it would be site specific (if the container is singularity, it would largely be the same, a container_uri and then some args to it). The only reason I have two sections is because I was trying out two ways to do it. Neither of them fully work (at least according to cromwell) because I don't know what that job_id business it :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412188725:18,Usability,clear,clear,18,"Hi @vsoch - to be clear, what I mean is this ... If I'm writing a WDL and I want to put some container in the `runtime` block, should **I** be opinionated as to if it's singularity or docker or should that be up to the person running the WDL? I used to view it as the former, but now I think it's the latter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412188725
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412201049:164,Usability,clear,clear,164,"Wouldn't it be up to the person running the wdl? If it's not up to me, how I am empowered to say I am using slurm vs a container environment like kubernetes? to be clear I've only used Cromwell a day and a half so I'm not the right person to answer this question. I'm trying to understand how Singularity would fit in beyond being a binary executable (that might work in several environments). I think @bek might be able to weigh in?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412201049
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363:192,Deployability,configurat,configuration,192,"Had a convo w/ Seth yesterday and looked into a few similar things (e.g. cwltool's support). I think the proper plan is as follows:. - Explore the path you've been looking at, by changing the configuration of a Cromwell backend to use Singularity instead of docker, but just for docker containers. This would cover the most common use cases; - Separately continue the [conversation at OpenWDL](https://github.com/openwdl/wdl/pull/237) to explore what support for native Singularity containers might look like in WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363:192,Modifiability,config,configuration,192,"Had a convo w/ Seth yesterday and looked into a few similar things (e.g. cwltool's support). I think the proper plan is as follows:. - Explore the path you've been looking at, by changing the configuration of a Cromwell backend to use Singularity instead of docker, but just for docker containers. This would cover the most common use cases; - Separately continue the [conversation at OpenWDL](https://github.com/openwdl/wdl/pull/237) to explore what support for native Singularity containers might look like in WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412309341:277,Deployability,update,updates,277,"Aye aye! I don't know scala, but I found the [developer docs](http://cromwell.readthedocs.io/en/develop/Building/) and I know how to use GIthub, so I'm ready to go, lol. I likely won't start this weekend (I have a few projects I'm working on!) but next week for sure. I'll put updates, troubles, and other musings here - thanks in advance for your help :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412309341
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720:254,Availability,down,down,254,"Hi @vsoch - it shouldn't require too much of a deep dive into the scala, we know that it already can be made to work with `udocker` by just changing the configuration like you've done. Let me know if you've not seen the udocker example and I'll track it down for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720:153,Deployability,configurat,configuration,153,"Hi @vsoch - it shouldn't require too much of a deep dive into the scala, we know that it already can be made to work with `udocker` by just changing the configuration like you've done. Let me know if you've not seen the udocker example and I'll track it down for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720:153,Modifiability,config,configuration,153,"Hi @vsoch - it shouldn't require too much of a deep dive into the scala, we know that it already can be made to work with `udocker` by just changing the configuration like you've done. Let me know if you've not seen the udocker example and I'll track it down for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:1639,Availability,echo,echo,1639,"he Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File syste",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:291,Deployability,install,install,291,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:775,Deployability,configurat,configuration,775,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2625,Deployability,configurat,configuration,2625," /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will pro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2689,Deployability,configurat,configuration,2689,"ell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:1032,Energy Efficiency,schedul,scheduled,1032,"hat I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:650,Modifiability,Config,ConfigBackend,650,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:708,Modifiability,config,config,708,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:715,Modifiability,Config,ConfigBackendLifecycleActorFactory,715,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:775,Modifiability,config,configuration,775,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:790,Modifiability,config,config,790,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:1680,Modifiability,config,configures,1680,"config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration speci",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2625,Modifiability,config,configuration,2625," /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will pro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2689,Modifiability,config,configuration,2689,"ell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:837,Performance,concurren,concurrent,837,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:855,Performance,concurren,concurrent-job-limit,855,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2946,Performance,cache,cached,2946,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:3167,Security,hash,hash,3167,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:3222,Security,hash,hash,3222,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:3395,Security,hash,hashed,3395,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:3403,Security,hash,hashing-strategy,3403,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:3573,Security,hash,hash,3573,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:3653,Security,hash,hashing,3653,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542:256,Deployability,pipeline,pipeline,256,"Cool thanks! So just to verify - I don't actually need to touch any scala, this is just a custom backend.conf for singularity (most of which I've already got a good start on?) This would simplify things quite a bit! Is this then provided in the workflow / pipeline or with cromwell here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542:187,Usability,simpl,simplify,187,"Cool thanks! So just to verify - I don't actually need to touch any scala, this is just a custom backend.conf for singularity (most of which I've already got a good start on?) This would simplify things quite a bit! Is this then provided in the workflow / pipeline or with cromwell here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778:154,Deployability,configurat,configuration,154,"@vsoch That's the theory. Let me know if that doesn't seem to be working for you and we can go from there. The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people **never** want to use actual Docker. However, there are a few buts to the above .... - A Cromwell server can have multiple backends, and workflows can be directed to specific backends, for the case where one sometimes wants it on and sometimes not . - None of this will cover the case where a user **really* wants Singularity (as in an actual Singularity container) instead of the ""use singularity to run a docker container"" model. We'll need to address this separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778:154,Modifiability,config,configuration,154,"@vsoch That's the theory. Let me know if that doesn't seem to be working for you and we can go from there. The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people **never** want to use actual Docker. However, there are a few buts to the above .... - A Cromwell server can have multiple backends, and workflows can be directed to specific backends, for the case where one sometimes wants it on and sometimes not . - None of this will cover the case where a user **really* wants Singularity (as in an actual Singularity container) instead of the ""use singularity to run a docker container"" model. We'll need to address this separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413025139:68,Deployability,update,update,68,"gotcha! I'll dig into this and try for a first shot, will send back update when I break I mean, dip in my toe a big more.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413025139
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076:152,Deployability,Configurat,ConfigurationFiles,152,"This is the standard way to configure cromwell, to provide your own .conf file. Best to start [here](http://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076:28,Modifiability,config,configure,28,"This is the standard way to configure cromwell, to provide your own .conf file. Best to start [here](http://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076:152,Modifiability,Config,ConfigurationFiles,152,"This is the standard way to configure cromwell, to provide your own .conf file. Best to start [here](http://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413196352:263,Availability,fault,fault,263,"I am trying to use udocker to locally run the gatk4-germline-snps-indels, using the wdl and json file offered by its GitHub page: https://github.com/gatk-workflows/gatk4-germline-snps-indels.; I am locally running it with docker, until now seems working well, no fault report.; I read udocker intro, it said I can use it to pull or run docker image (maybe my understand is wrong). ; What should I do to use udocker replace docker for this task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413196352
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:159,Deployability,configurat,configuration,159,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:355,Deployability,pipeline,pipeline,355,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:403,Deployability,pipeline,pipeline,403,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:514,Deployability,pipeline,pipeline,514,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:650,Deployability,configurat,configuration,650,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:772,Deployability,integrat,integrated,772,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:837,Deployability,configurat,configuration,837,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:883,Deployability,pipeline,pipeline,883,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:772,Integrability,integrat,integrated,772,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:159,Modifiability,config,configuration,159,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:650,Modifiability,config,configuration,650,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:837,Modifiability,config,configuration,837,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397:210,Deployability,configurat,configuration,210,@vsoch Hi - I think what you're looking for is [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf) which is where we put examples like this. So if there's a configuration for a backend which works we'd put it in there so we could point people at it. Does that make sense?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397:210,Modifiability,config,configuration,210,@vsoch Hi - I think what you're looking for is [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf) which is where we put examples like this. So if there's a configuration for a backend which works we'd put it in there so we could point people at it. Does that make sense?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055:86,Deployability,integrat,integrate,86,"Ah gotcha! To summarize:. - no changes are being made to the (scala) cromwell code to integrate a backend; - the specification of the backend still happens on the level of the pipeline, via the backend.conf; - of which we can provide an example from the `cromwell.examples.conf`. So I just need to write that example :) Did I get that right this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055:176,Deployability,pipeline,pipeline,176,"Ah gotcha! To summarize:. - no changes are being made to the (scala) cromwell code to integrate a backend; - the specification of the backend still happens on the level of the pipeline, via the backend.conf; - of which we can provide an example from the `cromwell.examples.conf`. So I just need to write that example :) Did I get that right this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055:86,Integrability,integrat,integrate,86,"Ah gotcha! To summarize:. - no changes are being made to the (scala) cromwell code to integrate a backend; - the specification of the backend still happens on the level of the pipeline, via the backend.conf; - of which we can provide an example from the `cromwell.examples.conf`. So I just need to write that example :) Did I get that right this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413943140:117,Integrability,bridg,bridge,117,@vsoch if in fact this scheme is going to work you have it right. It might **not** be feasible but we can cross that bridge if/when we get there,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413943140
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413957437:95,Testability,test,testing,95,Yep sounds good. I‚Äôll do this PR after the requested changes to add the Dockerfile development testing go through. Thanks for the clarification!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413957437
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:3909,Availability,down,down,3909," (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been so confusing?. We get Singularity confused with Docker, because they are both containers. Same thing right? Sort of, but not exactly. Docker is a container technology, but actually it's older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's confusing. The distinction is that although Singularity is also a container, Singularity is **not** like Docker because it doesn't have the fully developed services API (yet!). This problem is hard because the language for Singularity containers communicating between one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the softwa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6690,Availability,down,down,6690,"ularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9916,Availability,echo,echo,9916,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6188,Deployability,configurat,configuration,6188," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6283,Deployability,integrat,integrate,6283," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8579,Deployability,pipeline,pipeline,8579," general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But i",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9784,Deployability,configurat,configuration,9784,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:4321,Integrability,wrap,wrapped,4321," older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's confusing. The distinction is that although Singularity is also a container, Singularity is **not** like Docker because it doesn't have the fully developed services API (yet!). This problem is hard because the language for Singularity containers communicating between one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the software or components inside because my host just needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") inst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:4795,Integrability,depend,dependency,4795,"etween one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the software or components inside because my host just needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity its",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6283,Integrability,integrat,integrate,6283," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6188,Modifiability,config,configuration,6188," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9217,Modifiability,layers,layers,9217,"is is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that witho",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9784,Modifiability,config,configuration,9784,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6027,Performance,load,load,6027,"t needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:2575,Safety,redund,redundancy,2575,"ry, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it mean for Singularity to be a part of Cromwell. I first logically thought it would mean a backend, because the basic exec / run commands for Singularity don't change much (but arguments do!). But it doesn't fit well here because it's missing that API to make it a fully fledged service. To those familiar with Singularity, this is the instance command group (and not running containers as images). Then I thought it was really more of a workflow executable. But if this is the case, why is it special at all? It doesn't really fit because there is still going to be a lot of redundancy in specifying the ""singularity run <container> <args> bit over and over again. So I think (eventually) all these use cases could fit into cromwell,. - running a singularity container as an executable with a backend like slurm; - running a singularity container as an executable on with Local (host) backend; - running a container as a backend as a container instance (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been so confusing?. We get Singularity confused with Docker, because they are both containers. Same thing right? Sort of, but not exactly. Docker is a container technology, but actually it's older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6889,Security,validat,validated,6889,"ant this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the deve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:54,Testability,test,testing,54,"Hey everyone!. I've been thinking more about this and testing, and I want to offer my thoughts here. ; I think overall my conclusions are:. - We are trying to shove Singularity in as a backend **and** a workflow component, it's one or the other; - It's probably more appropriately the latter - a command you would put into a workflow (e.g., like python, any binary really) because services and standards (OCI) aren't fully developed.; - The time is soon, but it's not now, to define a Singularity backend; - For now, give users examples of just using containers as executables, nothing special. TLDR let's not try shoving a dog into a cat hole because the ears look similar. They are two different technologies, the latter (Singularity) is probably going to do great things for Cromwell **because** it is a single binary (and not a collection of tarballs) but we need that version 3.0 with OCI compliance to really have a well formulated language for Cromwell to talk to, period. I can go into more detail. First, let's define the parties involved:. ## Definitions. - **cromwell** is a workflow executor. It understands backends, and workflows. The backends run the workflows, and cromwell is just a manager for that.; - **backend** is an API really for services. The basic needs for this API are generally ""start, ""stop"", ""status,"" etc., and other kinds of ""controller"" commands for a particular executable. You have to be able to list what is going on, and get PIDs, and issue stop and status commands for the guts inside.; - **executable** is a script, binary, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:2056,Testability,log,logically,2056," backends, and workflows. The backends run the workflows, and cromwell is just a manager for that.; - **backend** is an API really for services. The basic needs for this API are generally ""start, ""stop"", ""status,"" etc., and other kinds of ""controller"" commands for a particular executable. You have to be able to list what is going on, and get PIDs, and issue stop and status commands for the guts inside.; - **executable** is a script, binary, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it mean for Singularity to be a part of Cromwell. I first logically thought it would mean a backend, because the basic exec / run commands for Singularity don't change much (but arguments do!). But it doesn't fit well here because it's missing that API to make it a fully fledged service. To those familiar with Singularity, this is the instance command group (and not running containers as images). Then I thought it was really more of a workflow executable. But if this is the case, why is it special at all? It doesn't really fit because there is still going to be a lot of redundancy in specifying the ""singularity run <container> <args> bit over and over again. So I think (eventually) all these use cases could fit into cromwell,. - running a singularity container as an executable with a backend like slurm; - running a singularity container as an executable on with Local (host) backend; - running a container as a backend as a container instance (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8001,Testability,test,tests,8001," new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://gi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8629,Testability,test,test,8629," to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8995,Testability,test,tests,8995," merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8167,Usability,learn,learn,8167," data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:10109,Usability,simpl,simple,10109,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:10861,Usability,simpl,simple,10861," Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the workflow part to the technologies that big players are building already. This definitely isn't a ""throw hands in the air"" sort of deal, because most of this stuff is working already it seems? I don't know if this perspective is useful, but as a new person (outsider) I wanted to offer it because if I'm confused and find this hard, probably others are too. And minimally it's good for awareness and discussion? I'm definitely happy to help however I can! But I'd really like to not try shoving dogs into cat holes, it's a very messy business. :cat: :dog: :hole: :sos:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6023,Availability,down,down,6023," clean API for services, only the first two really; > make sense. Singularity is not special. It's just a binary.; > Why has it been so confusing?; >; > We get Singularity confused with Docker, because they are both containers.; > Same thing right? Sort of, but not exactly. Docker is a container; > technology, but actually it's older and has had time to develop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing that is the container you run in."" But it's confusing. The; > distinction is that although Singularity is also a container, Singularity; > is *not* like Docker because it doesn't have the fully developed services; > API (yet!). This problem is hard because the language for Singularity; > containers communicating between one another, and even to the host, is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8924,Availability,down,down,8924,"ropose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12312,Availability,echo,echo,12312,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8398,Deployability,configurat,configuration,8398,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8496,Deployability,integrat,integrate,8496,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10872,Deployability,pipeline,pipeline,10872,"taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any k",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10914,Deployability,pipeline,pipeline,10914," on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub:/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12174,Deployability,configurat,configuration,12174,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1206,Integrability,wrap,wrapper,1206,"y could have a role as a backend for; workflow systems, but it's ineffective to take that idea as a starting; point. I really agree that it's best to lay that idea to rest and focus on; the biggest impact / low hanging fruit . To be honest, Singularity as a workflow componetn is exactly the way I've; been using Singularity in real life, whereas the idea to use it as a; workflow backbone always remained ... just an idea. This is not because; Singularity lacks potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > co",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6452,Integrability,wrap,wrapped,6452,"lop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing that is the container you run in."" But it's confusing. The; > distinction is that although Singularity is also a container, Singularity; > is *not* like Docker because it doesn't have the fully developed services; > API (yet!). This problem is hard because the language for Singularity; > containers communicating between one another, and even to the host, is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. I wouldn't; > need to care about the software or components inside because my host just; > needs to run Singularity. A container should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6947,Integrability,depend,dependency,6947," is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. I wouldn't; > need to care about the software or components inside because my host just; > needs to run Singularity. A container should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8496,Integrability,integrat,integrate,8496,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1031,Modifiability,variab,variables,1031,"I fully agree Vanessa!!! I don't think this is surrendering, it's finding; the solution that has been standing in plain sight all the time. At some point in the future Singularity could have a role as a backend for; workflow systems, but it's ineffective to take that idea as a starting; point. I really agree that it's best to lay that idea to rest and focus on; the biggest impact / low hanging fruit . To be honest, Singularity as a workflow componetn is exactly the way I've; been using Singularity in real life, whereas the idea to use it as a; workflow backbone always remained ... just an idea. This is not because; Singularity lacks potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8398,Modifiability,config,configuration,8398,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:11592,Modifiability,layers,layers,11592,"ity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it get",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12174,Modifiability,config,configuration,12174,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8228,Performance,load,load,8228,"should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:4643,Safety,redund,redundancy,4643,"written all the magic into, that takes some input arguments (data,; > poutputs, thresholds, etc.) and ""does the scientific thing"" to return to; > the workflow manager (cromwell) that is controlling its run via the backend.; >; > What does Singularity + Cromwell look like?; >; > People keep saying these two together, and I've been struggling to figure; > it out. I've been doing a lot of work trying to do that. What does it mean; > for Singularity to be a part of Cromwell. I first logically thought it; > would mean a backend, because the basic exec / run commands for Singularity; > don't change much (but arguments do!). But it doesn't fit well here because; > it's missing that API to make it a fully fledged service. To those familiar; > with Singularity, this is the instance command group (and not running; > containers as images). Then I thought it was really more of a workflow; > executable. But if this is the case, why is it special at all? It doesn't; > really fit because there is still going to be a lot of redundancy in; > specifying the ""singularity run bit over and over again. So I think; > (eventually) all these use cases could fit into cromwell,; >; > - running a singularity container as an executable with a backend like; > slurm; > - running a singularity container as an executable on with Local; > (host) backend; > - running a container as a backend as a container instance (via its; > API); >; > but for now, without a clean API for services, only the first two really; > make sense. Singularity is not special. It's just a binary.; > Why has it been so confusing?; >; > We get Singularity confused with Docker, because they are both containers.; > Same thing right? Sort of, but not exactly. Docker is a container; > technology, but actually it's older and has had time to develop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1747,Security,access,access,1747," potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > component, it's one or the other; > - It's probably more appropriately the latter - a command you would; > put into a workflow (e.g., like python, any binary really) because services; > and standards (OCI) aren't fully developed.; > - The time is soon, but it's not now, to define a Singularity backend; > - For now, give users examples of just using containers as; > executables, nothing special.; >; > TLDR let's not try shoving a dog into a cat hole because the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:9132,Security,validat,validated,9132," to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it eas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:2008,Testability,test,testing,2008,"give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > component, it's one or the other; > - It's probably more appropriately the latter - a command you would; > put into a workflow (e.g., like python, any binary really) because services; > and standards (OCI) aren't fully developed.; > - The time is soon, but it's not now, to define a Singularity backend; > - For now, give users examples of just using containers as; > executables, nothing special.; >; > TLDR let's not try shoving a dog into a cat hole because the ears look; > similar. They are two different technologies, the latter (Singularity) is; > probably going to do great things for Cromwell *because* it is a single; > binary (and not a collection of tarballs) but we need that version 3.0 with; > OCI compliance to really have a well formulated language for Cromwell to; > talk to, period.; >; > I can go into more",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:4103,Testability,log,logically,4103,"n the workflows, and cromwell is just a manager; > for that.; > - *backend* is an API really for services. The basic needs for this; > API are generally ""start, ""stop"", ""status,"" etc., and other kinds of; > ""controller"" commands for a particular executable. You have to be able to; > list what is going on, and get PIDs, and issue stop and status commands for; > the guts inside.; > - *executable* is a script, binary, etc. that the scientist has; > written all the magic into, that takes some input arguments (data,; > poutputs, thresholds, etc.) and ""does the scientific thing"" to return to; > the workflow manager (cromwell) that is controlling its run via the backend.; >; > What does Singularity + Cromwell look like?; >; > People keep saying these two together, and I've been struggling to figure; > it out. I've been doing a lot of work trying to do that. What does it mean; > for Singularity to be a part of Cromwell. I first logically thought it; > would mean a backend, because the basic exec / run commands for Singularity; > don't change much (but arguments do!). But it doesn't fit well here because; > it's missing that API to make it a fully fledged service. To those familiar; > with Singularity, this is the instance command group (and not running; > containers as images). Then I thought it was really more of a workflow; > executable. But if this is the case, why is it special at all? It doesn't; > really fit because there is still going to be a lot of redundancy in; > specifying the ""singularity run bit over and over again. So I think; > (eventually) all these use cases could fit into cromwell,; >; > - running a singularity container as an executable with a backend like; > slurm; > - running a singularity container as an executable on with Local; > (host) backend; > - running a container as a backend as a container instance (via its; > API); >; > but for now, without a clean API for services, only the first two really; > make sense. Singularity is not special. It's ju",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10276,Testability,test,tests,10276,"; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10970,Testability,test,test,10970," on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub:/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:11361,Testability,test,tests,11361,"o I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10462,Usability,learn,learn,10462,"that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12511,Usability,simpl,simple,12511,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:13297,Usability,simpl,simple,13297,"ng takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container that understands its data. You point the container at a; > dataset and run it. You outsource the workflow part to the technologies; > that big players are building already.; >; > This definitely isn't a ""throw hands in the air"" sort of deal, because; > most of this stuff is working already it seems? I don't know if this; > perspective is useful, but as a new person (outsider) I wanted to offer it; > because if I'm confused and find this hard, probably others are too. And; > minimally it's good for awareness and discussion? I'm definitely happy to; > help however I can! But I'd really like to not try shoving dogs into cat; > holes, it's a very messy business. üê± üê∂ üï≥ üÜò; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214>,; > or mute the thread; > <https://github.com/notifications/unsubscri",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:150,Availability,echo,echo,150,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:455,Modifiability,config,config,455,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:462,Modifiability,Config,ConfigBackendLifecycleActorFactory,462,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:499,Modifiability,config,config,499,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:195,Testability,test,test,195,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:260,Testability,test,test,260,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191
https://github.com/broadinstitute/cromwell/issues/2179#issuecomment-296784331:177,Security,access,access,177,@knoblett I replied directly to the forum post. . This is already part of Cromwell (FC actually duplicates us!). The user needs to start up Cromwell in server mode and can then access the timing diagram at:; ```; http://<HOST>:<PORT>/api/workflows/v1/<WORKFLOWID>/timing; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2179#issuecomment-296784331
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-294986870:141,Performance,queue,queue,141,"Unlike the stacktrace suggests this is not specifically related to the `refreshMetadataSummaryEntries`, it's just a consequence of the slick queue being overflowed.; See https://github.com/slick/slick/issues/1183 and https://github.com/slick/slick/issues/1683",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-294986870
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297413274:61,Performance,cache,cache,61,This starts happening again on a 20k wide scatter (with call cache read OFF). Metadata can be lost as writes can fail without failing the workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297413274
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297416609:11,Performance,queue,queueSize,11,Is upping `queueSize` the answer here?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297416609
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297418125:410,Testability,log,logs,410,"It will probably help a little bit, but won't guarantee that it's not going to happen. My concern is more about the fact that we can lose random metadata batches. Really any DB query can fail, some of them it's not that bad like summarizing metadata, others are fatal to the workflow, which is bad but at least fails the workflow, some are silent like fail to write metadata (silent as in you'll see it in the logs but your metadata will be incomplete without a way to know really what's missing)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297418125
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297419330:106,Availability,down,down,106,Well that's what happens when we design something in a way where that's semi-intentional :) We should sit down and figure out how to work all of this in a way which doesn't tie up the whole system (i.e. the reason we went down this path in the first place),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297419330
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297419330:222,Availability,down,down,222,Well that's what happens when we design something in a way where that's semi-intentional :) We should sit down and figure out how to work all of this in a way which doesn't tie up the whole system (i.e. the reason we went down this path in the first place),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297419330
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-313531180:320,Availability,robust,robust,320,"We should leave this open. This is basically the same thing @danbills has been poking at for Firecloud but we weren't able to reproduce it. For their side of things we discovered that they weren't taking advantage of metadata batching, which they're going to change. It likely won't *solve* the issue but should make it robust enough that they don't see it anymore. However the underlying problem is still lurking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-313531180
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-332211258:180,Security,attack,attack,180,I believe the related google doc is [Slick Heartburn](https://docs.google.com/document/d/11CHJzI-rQJWJ2XZWjPo1WUr8CiqCYn_vv_a5Raprw9U/edit).; @geoffjentry have we chosen a plan of attack yet?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-332211258
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329:116,Integrability,message,message,116,"What is the recommendation for resolving this problem?; I am getting the following:. ```; {; u'status': u'fail',; u'message': u'Task slick.basic.BasicBackend$DatabaseDef$$anon$2@2dbcf781 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@6dbdf3be[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 550175]'; }; ```. when calling the `query` endpoint.; It happens episodically. If I call `query` again, it often responds just fine. I'm particularly curious about the message indicating:. ```; queued tasks = 1000; ```. There is not much going on with this instance:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":24,""jobs"":115}. $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""33-215cca9-SNAP""}; ```. How should I interpret having 1000 queued tasks?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329:504,Integrability,message,message,504,"What is the recommendation for resolving this problem?; I am getting the following:. ```; {; u'status': u'fail',; u'message': u'Task slick.basic.BasicBackend$DatabaseDef$$anon$2@2dbcf781 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@6dbdf3be[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 550175]'; }; ```. when calling the `query` endpoint.; It happens episodically. If I call `query` again, it often responds just fine. I'm particularly curious about the message indicating:. ```; queued tasks = 1000; ```. There is not much going on with this instance:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":24,""jobs"":115}. $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""33-215cca9-SNAP""}; ```. How should I interpret having 1000 queued tasks?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329:297,Performance,queue,queued,297,"What is the recommendation for resolving this problem?; I am getting the following:. ```; {; u'status': u'fail',; u'message': u'Task slick.basic.BasicBackend$DatabaseDef$$anon$2@2dbcf781 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@6dbdf3be[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 550175]'; }; ```. when calling the `query` endpoint.; It happens episodically. If I call `query` again, it often responds just fine. I'm particularly curious about the message indicating:. ```; queued tasks = 1000; ```. There is not much going on with this instance:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":24,""jobs"":115}. $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""33-215cca9-SNAP""}; ```. How should I interpret having 1000 queued tasks?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329:530,Performance,queue,queued,530,"What is the recommendation for resolving this problem?; I am getting the following:. ```; {; u'status': u'fail',; u'message': u'Task slick.basic.BasicBackend$DatabaseDef$$anon$2@2dbcf781 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@6dbdf3be[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 550175]'; }; ```. when calling the `query` endpoint.; It happens episodically. If I call `query` again, it often responds just fine. I'm particularly curious about the message indicating:. ```; queued tasks = 1000; ```. There is not much going on with this instance:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":24,""jobs"":115}. $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""33-215cca9-SNAP""}; ```. How should I interpret having 1000 queued tasks?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329:804,Performance,queue,queued,804,"What is the recommendation for resolving this problem?; I am getting the following:. ```; {; u'status': u'fail',; u'message': u'Task slick.basic.BasicBackend$DatabaseDef$$anon$2@2dbcf781 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@6dbdf3be[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 550175]'; }; ```. when calling the `query` endpoint.; It happens episodically. If I call `query` again, it often responds just fine. I'm particularly curious about the message indicating:. ```; queued tasks = 1000; ```. There is not much going on with this instance:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":24,""jobs"":115}. $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""33-215cca9-SNAP""}; ```. How should I interpret having 1000 queued tasks?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394782296:362,Performance,queue,queued,362,"I think I have resolved this.; My `query` call always includes a workflow name, but I had been issuing an unrestricted query and doing the filtering client side. When I change the query call to filter on `name`, it returns successfully on a consistent basis.; So I interpret this to mean that it was the `query` call itself that was generating a large number of queued tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394782296
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:97,Availability,error,error,97,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. üôÇ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:103,Integrability,message,message,103,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. üôÇ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:318,Performance,queue,queue,318,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. üôÇ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:11,Usability,clear,clear,11,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. üôÇ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516
https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295010496:156,Integrability,message,message,156,@katevoss Assuming @Horneth thinks it'd be easy to add this I think we should. It's pretty irritating when it happens as it tends to come in storms and the message sounds a lot scarier than it really is.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295010496
https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139:55,Availability,down,down,55,"It should be fairly easy to add yes, we can also drill down deep in the caused by chain or not depending on how wide/specific we want to be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139
https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139:95,Integrability,depend,depending,95,"It should be fairly easy to add yes, we can also drill down deep in the caused by chain or not depending on how wide/specific we want to be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139
https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-302100731:89,Availability,error,error-caused-by-a-job-submission-failure,89,This was also reported [http://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure#latest](here) by @MatthewMah,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-302100731
https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-316074843:60,Modifiability,variab,variable,60,"The silver lining is ; ""If you declare an execution context variable in an actor using `context.dispatcher`, don't make it a lazy val"". I remember removing the lazy in a couple places so I think we can close this and re-open it if it happens again.; The underlying problem still exists in that if an actor starts some work asynchronously and then stops without waiting for it, the asynchronous work might never be acted upon. But this could manifest itself in unpredicatable ways.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-316074843
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:322,Availability,Down,Downloading,322,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:448,Availability,down,download,448,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:826,Availability,avail,available,826,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:311,Deployability,install,installed,311,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:365,Deployability,INSTALL,INSTALL,365,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:151,Performance,cache,cache,151,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:214,Performance,perform,performance,214,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:247,Performance,cache,cache,247,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-349763129:204,Availability,avail,available,204,"We have a very similar use case. We'd like to be able to run a different annotator that has a massive pile of data sources ~20gb. We want an easy way to package different sets of test files and make them available for people to use with our docker image, without having to make a 20gb docker image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-349763129
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-349763129:179,Testability,test,test,179,"We have a very similar use case. We'd like to be able to run a different annotator that has a massive pile of data sources ~20gb. We want an easy way to package different sets of test files and make them available for people to use with our docker image, without having to make a 20gb docker image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-349763129
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-373077355:346,Modifiability,config,configuring-options-to-run-containers,346,"Hello @vdauwera , I have a similar use case in Cromwell that I think this could cover. We specifically hope we can mount the `type=tmpfs` volume. This creates a ram disk which we use to unpack data that has tens of thousands of files very quickly. . Google describes how to do this in their docs; https://cloud.google.com/compute/docs/containers/configuring-options-to-run-containers#mounting_tmpfs_file_system_as_a_data_volume. We have had success using this in our Slurm Cromwell by launching the docker docker through `submit` ourselves and giving the docker run the parameter to mount . `${'--mount type=tmpfs,destination='+mount_tmpfs}`. It would be great if declaring a tmpfs mount point could also be supported by cromwell in google cloud submissions. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-373077355
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-413995155:170,Availability,avail,available,170,"+1 on `tmpfs`. Currently, we have to create a directory under `/dev/` and rely on the assumption that that directory gets mounted by default as a `tmpfs` with 1/2 of the available RAM (at least on GCP). This is obviously not ideal. Delocalization of such files is problematic as well. Our use case is exactly the same, to unpack/process tens or hundreds of thousands of small files (in a BCL). Doing so with any ""normal"" disk is much slower than with RAM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-413995155
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-477680950:163,Security,expose,expose,163,"Hi -- I know this is an old issue, but has there been any further discussion on how to mount persistent disks? We're using PAPIv2 as the backend, and we'd like to expose reference databases (stored as filesystems) to our docker containers via a mounted volume.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-477680950
https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-477972220:77,Deployability,Pipeline,Pipelines,77,"Thanks @Selonka! That looks like a nifty workaround. Does this work with the Pipelines API backend, or just with a local backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-477972220
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296316723:173,Security,hash,hash,173,"I spoke to @droazen and we'll need a 2nd jar as well, for the GATK protected (at least for now). The repo there is `broadinstitute/gatk-protected`. He will provide a commit hash later this evening",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296316723
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:181,Deployability,integrat,integration,181,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:181,Integrability,integrat,integration,181,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:462,Performance,perform,performance,462,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:193,Testability,test,tests,193,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337:39,Deployability,update,updated,39,"I've made the builds of both toolsets, updated the dockerfile (see dsde-pipelines branch kc_jg_turbocharge), built and pushed the docker image for our testing (kcibul/tiledb-with-gcloud:2.2.5-1492828987) in the JG WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337:72,Deployability,pipeline,pipelines,72,"I've made the builds of both toolsets, updated the dockerfile (see dsde-pipelines branch kc_jg_turbocharge), built and pushed the docker image for our testing (kcibul/tiledb-with-gcloud:2.2.5-1492828987) in the JG WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337:151,Testability,test,testing,151,"I've made the builds of both toolsets, updated the dockerfile (see dsde-pipelines branch kc_jg_turbocharge), built and pushed the docker image for our testing (kcibul/tiledb-with-gcloud:2.2.5-1492828987) in the JG WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-297770669:6,Deployability,update,updated,6,"We've updated gatk-protected to remove the JSON requirement for reading from GenomicsDB in `GenotypeGVCFs`, and also made it so that you can use the same jar to run both the `GenomicsDBImport` tool and `GenotypeGVCFs`. The commit you want is `8812780144561904e6e529c4673c3770076f543b` in `https://github.com/broadinstitute/gatk-protected`. Updated clone/build commands:. ```; git clone git@github.com:broadinstitute/gatk-protected.git gatk-protected; cd gatk-protected; git checkout 8812780144561904e6e529c4673c3770076f543b; ./gradlew clean localJar; Jar will be in build/libs with a name like gatk-protected-package-*-SNAPSHOT-local.jar; You can use this jar to run either GenomicsDBImport or GenotypeGVCFs (no need for a second jar); ```. We're still waiting on a PR from Intel to add the --batchSize argument to the GenomicsDBImport tool, which will be necessary to control memory usage with many samples, but otherwise we should be good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-297770669
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-297770669:340,Deployability,Update,Updated,340,"We've updated gatk-protected to remove the JSON requirement for reading from GenomicsDB in `GenotypeGVCFs`, and also made it so that you can use the same jar to run both the `GenomicsDBImport` tool and `GenotypeGVCFs`. The commit you want is `8812780144561904e6e529c4673c3770076f543b` in `https://github.com/broadinstitute/gatk-protected`. Updated clone/build commands:. ```; git clone git@github.com:broadinstitute/gatk-protected.git gatk-protected; cd gatk-protected; git checkout 8812780144561904e6e529c4673c3770076f543b; ./gradlew clean localJar; Jar will be in build/libs with a name like gatk-protected-package-*-SNAPSHOT-local.jar; You can use this jar to run either GenomicsDBImport or GenotypeGVCFs (no need for a second jar); ```. We're still waiting on a PR from Intel to add the --batchSize argument to the GenomicsDBImport tool, which will be necessary to control memory usage with many samples, but otherwise we should be good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-297770669
https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-313503722:110,Deployability,pipeline,pipelines,110,"It's happened for the 11k run, but the changes to the joint genotyping workflow haven't been merged into dsde-pipelines/master yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-313503722
https://github.com/broadinstitute/cromwell/issues/2197#issuecomment-313503625:110,Deployability,pipeline,pipelines,110,"It's happened for the 11k run, but the changes to the joint genotyping workflow haven't been merged into dsde-pipelines/master yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2197#issuecomment-313503625
https://github.com/broadinstitute/cromwell/pull/2198#issuecomment-296384021:639,Security,hash,hash,639,"Yes that surprised me too. The numbers are averages so it's possible that it kinds of flattens out around ~20/30ms, and the 10000 happened to have slightly better runs. It does it 30 times for each of the sizes. The minimum time value would be a better indicator maybe. Edit: Did the same with min times. There's still a weird timing where 8K was longer.; The main difference between the 2 approaches is that before we would go through the list of ""done"" keys one by one until we find the one we want. Which means the longer the list the longer the time in average. However now we go through it once to create a map, and then it becomes a hash lookup every other time. for develop:. <img width=""326"" alt=""screen shot 2017-04-22 at 12 28 58 pm"" src=""https://cloud.githubusercontent.com/assets/2978948/25306250/5dcc9d1c-2757-11e7-8987-0aaec4d4a038.png"">. and this branch:; <img width=""323"" alt=""screen shot 2017-04-22 at 12 23 06 pm"" src=""https://cloud.githubusercontent.com/assets/2978948/25306253/68140aa8-2757-11e7-8483-f6f2770c031d.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2198#issuecomment-296384021
https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346:168,Availability,failure,failures,168,Note to self: Metadata field ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```. Should be:; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:message; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```; And: ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:causedBy[]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: NULL; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346
https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346:510,Availability,failure,failures,510,Note to self: Metadata field ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```. Should be:; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:message; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```; And: ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:causedBy[]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: NULL; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346
https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346:855,Availability,failure,failures,855,Note to self: Metadata field ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```. Should be:; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:message; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```; And: ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:causedBy[]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: NULL; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346
https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346:522,Integrability,message,message,522,Note to self: Metadata field ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```. Should be:; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:message; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```; And: ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:causedBy[]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: NULL; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346
https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297183874:21,Deployability,hotfix,hotfix,21,Will back-port to 26-hotfix once the develop version gets the seal of approval.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297183874
https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297187333:11,Availability,failure,failure,11,:+1: total failure. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2203/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297187333
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:17,Availability,error,error,17,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:122,Availability,error,error,122,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:200,Availability,ERROR,ERROR,200,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:346,Availability,ERROR,ERROR,346,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:23,Integrability,message,message,23,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:128,Integrability,message,messages,128,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:170,Performance,load,load,170,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:316,Performance,load,load,316,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298098365:476,Availability,error,errors,476,"Yes, it would also be nice. Right now, most of the wasted time in my pipelines is due to the things like this: when something in cromwell crashes and I spend a lot of time figuring out what exactly.; P.S.; To be honest, I do not understand why you started to create a language with a lot of problems to solve ( like this one) instead of just providing better declarative scala DSL (I think it is even possible to make it look almost like current wdl syntax) where tooling and errors are solved by scala ecosystem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298098365
https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298098365:69,Deployability,pipeline,pipelines,69,"Yes, it would also be nice. Right now, most of the wasted time in my pipelines is due to the things like this: when something in cromwell crashes and I spend a lot of time figuring out what exactly.; P.S.; To be honest, I do not understand why you started to create a language with a lot of problems to solve ( like this one) instead of just providing better declarative scala DSL (I think it is even possible to make it look almost like current wdl syntax) where tooling and errors are solved by scala ecosystem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298098365
https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855:48,Availability,down,down,48,I think just an artifact of everything shutting down quickly and some messages ending up floating in space. Not a big deal just polluting the logs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855
https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855:70,Integrability,message,messages,70,I think just an artifact of everything shutting down quickly and some messages ending up floating in space. Not a big deal just polluting the logs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855
https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855:142,Testability,log,logs,142,I think just an artifact of everything shutting down quickly and some messages ending up floating in space. Not a big deal just polluting the logs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855
https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-298001488:66,Integrability,message,message,66,"I think I covered this in another issue ""don't bother sending the message that nobody's waiting for"". I'll see if I can find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-298001488
https://github.com/broadinstitute/cromwell/issues/2215#issuecomment-396068773:39,Availability,error,error,39,@LeeTL1220 Do you still encounter this error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215#issuecomment-396068773
https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298043978:11,Performance,queue,queue,11,"Are those ""queue in cromwell"" part of a subworkflow ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298043978
https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298047657:92,Performance,queue,queue,92,"Yes. On Fri, Apr 28, 2017 at 12:27 PM, Thib <notifications@github.com> wrote:. > Are those ""queue in cromwell"" part of a subworkflow ?; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298043978>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkzhPtkBvD1-WA1YWqvHZk5BJpFDwks5r0hNVgaJpZM4NLf9E>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2216#issuecomment-298047657
https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298047072:205,Usability,simpl,simple,205,"The issue is in the reference.conf. It still uses the old convention. Even if you override the values in your own conf file, the reference.conf will trigger the deprecation exception. This issue should be simple as correcting the reference.conf.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298047072
https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:1189,Energy Efficiency,monitor,monitor,1189,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016
https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:411,Modifiability,rewrite,rewriteBatchedStatements,411,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016
https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:760,Safety,avoid,avoid,760,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016
https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:462,Security,password,password,462,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016
https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110122:81,Modifiability,config,config,81,"Although, that mysql example does look wrong (but shouldn't affect/override real config file values)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110122
https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093:163,Deployability,configurat,configuration,163,"Not it isn't, I believe @mcovarr is working on something that should make this ""go away"". ; In the meantime you can try to increase `database.db.queueSize` in the configuration. Results are not guaranteed though it's just giving slick more room but it might still fail. The default is 1000.; Also how big is your workflow ? Must be large to hit this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093
https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093:163,Modifiability,config,configuration,163,"Not it isn't, I believe @mcovarr is working on something that should make this ""go away"". ; In the meantime you can try to increase `database.db.queueSize` in the configuration. Results are not guaranteed though it's just giving slick more room but it might still fail. The default is 1000.; Also how big is your workflow ? Must be large to hit this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093
https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093:145,Performance,queue,queueSize,145,"Not it isn't, I believe @mcovarr is working on something that should make this ""go away"". ; In the meantime you can try to increase `database.db.queueSize` in the configuration. Results are not guaranteed though it's just giving slick more room but it might still fail. The default is 1000.; Also how big is your workflow ? Must be large to hit this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093
https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060272:207,Availability,failure,failure,207,"yeah that's a lot, we just found out about this issue a week ago while testing large scale for joint genotyping but looks like ""large scale"" is quickly becoming ""normal scale"".; Definitely keep filing those failure modes as they come up !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060272
https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060272:71,Testability,test,testing,71,"yeah that's a lot, we just found out about this issue a week ago while testing large scale for joint genotyping but looks like ""large scale"" is quickly becoming ""normal scale"".; Definitely keep filing those failure modes as they come up !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060272
https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060547:71,Integrability,Depend,Depends,71,@Horneth No problem. I *might* scale up to 200k jobs in this workflow. Depends on how many samples I need.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060547
https://github.com/broadinstitute/cromwell/issues/2221#issuecomment-299583979:156,Availability,robust,robust,156,@geoffjentry would this fit under the current work with labels that @ruchim is doing? I agree that it's a good idea and it would help make labels even more robust and useful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2221#issuecomment-299583979
https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-300302414:101,Deployability,release,release,101,"@cjllanwarne this wasn't fixed in your PR, right? What would be involved in fixing this for the next release? I don't want to hold it up but improving our magic release would be nice :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-300302414
https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-300302414:161,Deployability,release,release,161,"@cjllanwarne this wasn't fixed in your PR, right? What would be involved in fixing this for the next release? I don't want to hold it up but improving our magic release would be nice :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-300302414
https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-312347678:39,Deployability,release,release,39,This seems to be fixed with the latest release wdl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-312347678
https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298670361:130,Security,validat,validation,130,"This looks like a more specific version of https://github.com/broadinstitute/cromwell/issues/2209 - to work around this I bet the validation will work without the string interpolation:; ```; output {; File gvcf = gvcf; File gvcf_index = gvcf + "".tbi""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298670361
https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298679419:110,Security,validat,validate,110,"Right, the issue is that the name of the output is wrong, so `gvcf` doesn't exist. It would be nice for it to validate before running to tell the user that though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298679419
https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298740549:83,Safety,detect,detected,83,"Yeah, my point is, this might also be an instance of circular references not being detected (i.e. we should catch things like `File gvcf = gvcf` regardless of where it appears)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298740549
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:72,Availability,ERROR,ERROR,72,"And again on a new run, with something that looks very similar... ```; [ERROR] [05/01/2017 21:06:38.897] [cromwell-system-akka.dispatchers.engine-dispatcher-86] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1700,Availability,ERROR,ERROR,1700,"h.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2003,Availability,error,error,2003,"$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"",",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2018,Integrability,Message,Message,2018,"tor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:611,Performance,concurren,concurrent,611,"And again on a new run, with something that looks very similar... ```; [ERROR] [05/01/2017 21:06:38.897] [cromwell-system-akka.dispatchers.engine-dispatcher-86] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1066,Performance,concurren,concurrent,1066,[ERROR] [05/01/2017 21:06:38.897] [cromwell-system-akka.dispatchers.engine-dispatcher-86] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1387,Performance,concurren,concurrent,1387,"n.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1460,Performance,concurren,concurrent,1460,"ala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1545,Performance,concurren,concurrent,1545,"tor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/7",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1622,Performance,concurren,concurrent,1622,"CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Trace",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:4310,Safety,timeout,timeout,4310,"n Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:5190,Safety,timeout,timeout,5190,"n Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:3100,Security,validat,validate,3100,"ull_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; ret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2063,Testability,log,logs,2063,"tor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2128,Testability,test,test-dl-oxoq-full,2128,"tor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2358,Testability,log,log,2358,"tor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2380,Testability,log,log,2380,"forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback();",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2437,Testability,test,test-dl-oxoq-full,2437,"forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback();",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:26,Availability,failure,failures,26,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:109,Availability,error,error,109,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:61,Integrability,message,message,61,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:124,Integrability,Message,Message,124,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:2455,Safety,timeout,timeout,2455,"b/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n: ""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:3371,Safety,timeout,timeout,3371,"b/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n: ""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:1213,Security,validat,validate,1213,"_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:169,Testability,log,logs,169,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:233,Testability,test,test-dl-oxoq-full,233,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:462,Testability,log,log,462,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:484,Testability,log,log,484,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:541,Testability,test,test-dl-oxoq-full,541,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298670203:26,Availability,failure,failure,26,"So the real reason of the failure I'm afraid is the JES one, which I don't think we can do much for except tell them about it ?; The Cromwell NPE is still a bug but I believe is an artifact of the job failing combined with the fact that the DB is swamped.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298670203
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298733460:62,Availability,down,down,62,"@Horneth I pruned my number of jobs dramatically.... I am now down to <5k. And I am still getting this error and very slow call caching. @katevoss Can we tell google, if the crashes are on their end?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298733460
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298733460:103,Availability,error,error,103,"@Horneth I pruned my number of jobs dramatically.... I am now down to <5k. And I am still getting this error and very slow call caching. @katevoss Can we tell google, if the crashes are on their end?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298733460
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:6,Availability,ERROR,ERROR,6,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:309,Availability,error,error,309,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:324,Integrability,Message,Message,324,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:2616,Safety,timeout,timeout,2616,"e/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```. This is a gsutil stacktrace. JES tried to copy the logs and failed, hence failing the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:3496,Safety,timeout,timeout,3496,"usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```. This is a gsutil stacktrace. JES tried to copy the logs and failed, hence failing the job and the workflow. They might want to retry this - although we've been telling them to stop retrying too much on some things so I don't know. @geoffjentry and @cjllanwarne were talking about it on slack maybe they have an opinion. For call caching: it will get slower and slower. Basically the more jobs you run the slower it's going to be... I'm working on something to fix that but it's not in develop yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:1406,Security,validat,validate,1406,"ull_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; ret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:369,Testability,log,logs,369,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:434,Testability,test,test-dl-oxoq-full,434,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:664,Testability,log,log,664,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:686,Testability,log,log,686,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:743,Testability,test,test-dl-oxoq-full,743,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:3575,Testability,log,logs,3575,"usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```. This is a gsutil stacktrace. JES tried to copy the logs and failed, hence failing the job and the workflow. They might want to retry this - although we've been telling them to stop retrying too much on some things so I don't know. @geoffjentry and @cjllanwarne were talking about it on slack maybe they have an opinion. For call caching: it will get slower and slower. Basically the more jobs you run the slower it's going to be... I'm working on something to fix that but it's not in develop yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:115,Availability,ERROR,ERROR,115,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:420,Availability,error,error,420,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:435,Integrability,Message,Message,435,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:2779,Safety,timeout,timeout,2779,"01, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callback(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; > return c_gce.Metadata().Project(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; > gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; > return func(*args, **kwargs), None; > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; > return gce_read.ReadNoProxy(uri); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; > request, timeout=timeout_property).read(); > File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; > response = self._open(req, data); > File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; > '_open', req); > File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; > result = func(*args); > File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; > return self.do_open(httplib.HTTPConnection, req); > File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; > r = h.getresponse(buffering=True); > File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; > response.begin(); > File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; > version, status, reason = self._read_status(); > File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; > line = self.fp.readline(); > File ""/usr/lib/python2.7/socket.py"", line 447, in readline; > data = self._sock.recv(self._rbufsize); > socket.timeout: timed out; > :; >; > This is a gsutil stacktrace. JES tried to cop",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:3697,Safety,timeout,timeout,3697,"/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; > request, timeout=timeout_property).read(); > File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; > response = self._open(req, data); > File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; > '_open', req); > File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; > result = func(*args); > File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; > return self.do_open(httplib.HTTPConnection, req); > File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; > r = h.getresponse(buffering=True); > File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; > response.begin(); > File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; > version, status, reason = self._read_status(); > File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; > line = self.fp.readline(); > File ""/usr/lib/python2.7/socket.py"", line 447, in readline; > data = self._sock.recv(self._rbufsize); > socket.timeout: timed out; > :; >; > This is a gsutil stacktrace. JES tried to copy the logs and failed, hence; > failing the job and the workflow. They might want to retry this - although; > we've been telling them to stop retrying too much on some things so I don't; > know. @geoffjentry <https://github.com/geoffjentry> and @cjllanwarne; > <https://github.com/cjllanwarne> were talking about it on slack maybe; > they have an opinion.; >; > For call caching: it will get slower and slower. Basically the more jobs; > you run the slower it's going to be... I'm working on something to fix that; > but it's not in develop yet.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk6vHuB6D3iMODjERjQgj6h_SG4z2ks5r15JkgaJpZM4NNP8f>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:1537,Security,validat,validate,1537,"with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callback(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; > return c_gce.Metadata().Project(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; > gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; > return func(*args, **kwargs), None; > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoPr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:480,Testability,log,logs,480,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:547,Testability,test,test-dl-oxoq-full,547,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:779,Testability,log,log,779,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:801,Testability,log,log,801,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:858,Testability,test,test-dl-oxoq-full,858,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:3778,Testability,log,logs,3778,"/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; > request, timeout=timeout_property).read(); > File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; > response = self._open(req, data); > File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; > '_open', req); > File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; > result = func(*args); > File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; > return self.do_open(httplib.HTTPConnection, req); > File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; > r = h.getresponse(buffering=True); > File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; > response.begin(); > File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; > version, status, reason = self._read_status(); > File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; > line = self.fp.readline(); > File ""/usr/lib/python2.7/socket.py"", line 447, in readline; > data = self._sock.recv(self._rbufsize); > socket.timeout: timed out; > :; >; > This is a gsutil stacktrace. JES tried to copy the logs and failed, hence; > failing the job and the workflow. They might want to retry this - although; > we've been telling them to stop retrying too much on some things so I don't; > know. @geoffjentry <https://github.com/geoffjentry> and @cjllanwarne; > <https://github.com/cjllanwarne> were talking about it on slack maybe; > they have an opinion.; >; > For call caching: it will get slower and slower. Basically the more jobs; > you run the slower it's going to be... I'm working on something to fix that; > but it's not in develop yet.; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk6vHuB6D3iMODjERjQgj6h_SG4z2ks5r15JkgaJpZM4NNP8f>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298917663:71,Availability,failure,failures,71,But I'm having trouble running even 500 tasks without one of these JES failures. Is the fact that I using pre-emptible instances matter?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298917663
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298935140:42,Availability,failure,failures,42,"Hmm I don't know, I don't think so. Those failures started popping up recently I believe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298935140
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648:43,Availability,failure,failure,43,@LeeTL1220 I'm seeing the same log copying failure in our test suite actually. So it very likely isn't your doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648:31,Testability,log,log,31,@LeeTL1220 I'm seeing the same log copying failure in our test suite actually. So it very likely isn't your doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648:58,Testability,test,test,58,@LeeTL1220 I'm seeing the same log copying failure in our test suite actually. So it very likely isn't your doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:359,Availability,error,error,359,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:756,Performance,throttle,throttled,756,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:189,Safety,timeout,timeout,189,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:791,Safety,timeout,timeouts,791,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728:59,Availability,failure,failures,59,"@Horneth @patmagee I'm going to have to enable continue on failures, except I am not sure that is viable.; @Horneth Can I tell cromwell to continue even on these failures? These are not non-zero return codes, right?; ; I actually can't just rerun the jobs and expect any progress. Due to the high rate of these failures and that I scatter over quite a few jobs, nothing gets to complete before one job fails. If I were to make my scatter narrower, I'd probably still get failures due, since each job would take substantially longer. . In other words, I can't proceed unless there is a viable ""continue on failure"" option (or a failure in my reasoning above). And even then, I am likely to proceed slowly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728:162,Availability,failure,failures,162,"@Horneth @patmagee I'm going to have to enable continue on failures, except I am not sure that is viable.; @Horneth Can I tell cromwell to continue even on these failures? These are not non-zero return codes, right?; ; I actually can't just rerun the jobs and expect any progress. Due to the high rate of these failures and that I scatter over quite a few jobs, nothing gets to complete before one job fails. If I were to make my scatter narrower, I'd probably still get failures due, since each job would take substantially longer. . In other words, I can't proceed unless there is a viable ""continue on failure"" option (or a failure in my reasoning above). And even then, I am likely to proceed slowly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728:311,Availability,failure,failures,311,"@Horneth @patmagee I'm going to have to enable continue on failures, except I am not sure that is viable.; @Horneth Can I tell cromwell to continue even on these failures? These are not non-zero return codes, right?; ; I actually can't just rerun the jobs and expect any progress. Due to the high rate of these failures and that I scatter over quite a few jobs, nothing gets to complete before one job fails. If I were to make my scatter narrower, I'd probably still get failures due, since each job would take substantially longer. . In other words, I can't proceed unless there is a viable ""continue on failure"" option (or a failure in my reasoning above). And even then, I am likely to proceed slowly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728:471,Availability,failure,failures,471,"@Horneth @patmagee I'm going to have to enable continue on failures, except I am not sure that is viable.; @Horneth Can I tell cromwell to continue even on these failures? These are not non-zero return codes, right?; ; I actually can't just rerun the jobs and expect any progress. Due to the high rate of these failures and that I scatter over quite a few jobs, nothing gets to complete before one job fails. If I were to make my scatter narrower, I'd probably still get failures due, since each job would take substantially longer. . In other words, I can't proceed unless there is a viable ""continue on failure"" option (or a failure in my reasoning above). And even then, I am likely to proceed slowly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728:605,Availability,failure,failure,605,"@Horneth @patmagee I'm going to have to enable continue on failures, except I am not sure that is viable.; @Horneth Can I tell cromwell to continue even on these failures? These are not non-zero return codes, right?; ; I actually can't just rerun the jobs and expect any progress. Due to the high rate of these failures and that I scatter over quite a few jobs, nothing gets to complete before one job fails. If I were to make my scatter narrower, I'd probably still get failures due, since each job would take substantially longer. . In other words, I can't proceed unless there is a viable ""continue on failure"" option (or a failure in my reasoning above). And even then, I am likely to proceed slowly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728:627,Availability,failure,failure,627,"@Horneth @patmagee I'm going to have to enable continue on failures, except I am not sure that is viable.; @Horneth Can I tell cromwell to continue even on these failures? These are not non-zero return codes, right?; ; I actually can't just rerun the jobs and expect any progress. Due to the high rate of these failures and that I scatter over quite a few jobs, nothing gets to complete before one job fails. If I were to make my scatter narrower, I'd probably still get failures due, since each job would take substantially longer. . In other words, I can't proceed unless there is a viable ""continue on failure"" option (or a failure in my reasoning above). And even then, I am likely to proceed slowly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:414,Modifiability,extend,extended,414,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:363,Performance,concurren,concurrent,363,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:511,Performance,concurren,concurrent,511,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:84,Safety,timeout,timeouts,84,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:117,Testability,log,log,117,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858:274,Availability,error,errors,274,"@LeeTL1220 @patmagee I'd been paying less attention to this than I should have. I mentioned to @LeeTL1220 yesterday that the general issue of ""sometimes logs fail to upload and it doesn't retry a ton - but Google knows this"". However looking at the frequency and the actual errors makes me think I need to ping Google to make sure they know about the errors themselves. Perhaps there's something more fundamentally wrong going on that they're blissfully unaware of.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858:306,Availability,ping,ping,306,"@LeeTL1220 @patmagee I'd been paying less attention to this than I should have. I mentioned to @LeeTL1220 yesterday that the general issue of ""sometimes logs fail to upload and it doesn't retry a ton - but Google knows this"". However looking at the frequency and the actual errors makes me think I need to ping Google to make sure they know about the errors themselves. Perhaps there's something more fundamentally wrong going on that they're blissfully unaware of.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858:351,Availability,error,errors,351,"@LeeTL1220 @patmagee I'd been paying less attention to this than I should have. I mentioned to @LeeTL1220 yesterday that the general issue of ""sometimes logs fail to upload and it doesn't retry a ton - but Google knows this"". However looking at the frequency and the actual errors makes me think I need to ping Google to make sure they know about the errors themselves. Perhaps there's something more fundamentally wrong going on that they're blissfully unaware of.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858
https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858:153,Testability,log,logs,153,"@LeeTL1220 @patmagee I'd been paying less attention to this than I should have. I mentioned to @LeeTL1220 yesterday that the general issue of ""sometimes logs fail to upload and it doesn't retry a ton - but Google knows this"". However looking at the frequency and the actual errors makes me think I need to ping Google to make sure they know about the errors themselves. Perhaps there's something more fundamentally wrong going on that they're blissfully unaware of.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858
https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178:710,Availability,error,error,710,"It turns out the original task above didn't work when `combined_gvcf=""gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz""` even in V24. I updated the task to:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. The thing is I still don't know why the first task in the comment above doesn't work. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178
https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178:197,Deployability,update,updated,197,"It turns out the original task above didn't work when `combined_gvcf=""gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz""` even in V24. I updated the task to:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. The thing is I still don't know why the first task in the comment above doesn't work. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178
https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178:716,Integrability,message,message,716,"It turns out the original task above didn't work when `combined_gvcf=""gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz""` even in V24. I updated the task to:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. The thing is I still don't know why the first task in the comment above doesn't work. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178
https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-414067237:89,Deployability,update,updated,89,"@meganshand if this is still happening on a modern cromwell, please open a new ticket w/ updated info",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-414067237
https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235:215,Availability,down,down,215,@jmthibault79 Adding this to the retry list would retry the user's job. Is that really what you're advocating for?. This is coming from JES. They've actually been requested (by both Firecloud & other users) to tune down the retries that they do on the gsutil up/downloading.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235
https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235:262,Availability,down,downloading,262,@jmthibault79 Adding this to the retry list would retry the user's job. Is that really what you're advocating for?. This is coming from JES. They've actually been requested (by both Firecloud & other users) to tune down the retries that they do on the gsutil up/downloading.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235
https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235:210,Performance,tune,tune,210,@jmthibault79 Adding this to the retry list would retry the user's job. Is that really what you're advocating for?. This is coming from JES. They've actually been requested (by both Firecloud & other users) to tune down the retries that they do on the gsutil up/downloading.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235
https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503:368,Availability,failure,failure,368,"(noting here that a few Cromwellians & Workbenchers had a face conversation about this and general retry policy). My goal is to enable a user to say ""run my workflow and if JES has a random hiccup, try again and keep going"". The details of where in the stack we should do this, and under which conditions, are unclear to me. So I guess what I want here is to note one failure case for FireCloud. As for the actual action taken, that depends on the policy we decide on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503
https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503:433,Integrability,depend,depends,433,"(noting here that a few Cromwellians & Workbenchers had a face conversation about this and general retry policy). My goal is to enable a user to say ""run my workflow and if JES has a random hiccup, try again and keep going"". The details of where in the stack we should do this, and under which conditions, are unclear to me. So I guess what I want here is to note one failure case for FireCloud. As for the actual action taken, that depends on the policy we decide on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503
https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298731424:169,Security,authoriz,authorization,169,"@jmthibault79 One of our standing rules is to never reinvoke an entire jes job for someone w/o their consent (i.e. ""cost them money""). It's possible to wire in some pre-authorization but that should be a cross-PO type conversation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298731424
https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094:34,Availability,error,error,34,"This is an exceptionally annoying error, any more thoughts on how to potentially fix this? Should we go through the pipelines team?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094
https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094:116,Deployability,pipeline,pipelines,116,"This is an exceptionally annoying error, any more thoughts on how to potentially fix this? Should we go through the pipelines team?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094
https://github.com/broadinstitute/cromwell/issues/2237#issuecomment-298782585:129,Deployability,pipeline,pipelines,129,"This is GREAT news!. Sorry I meant ""cross cloud platform"", not ""cross cloud"" compatibility. It has a lot to do with that - if my pipelines are in WDL, I can't use them e.g. on the SevenBridges platform; if they are in CWL, I can't use them in FireCloud (currently). Anyways, thanks for the positive reply and keep up the great work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2237#issuecomment-298782585
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126:72,Availability,failure,failure,72,Rebased and a minor migration fix added. I also removed the non-default failure and error handling on the migration but did not add in a special error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126:84,Availability,error,error,84,Rebased and a minor migration fix added. I also removed the non-default failure and error handling on the migration but did not add in a special error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126:145,Availability,error,error,145,Rebased and a minor migration fix added. I also removed the non-default failure and error handling on the migration but did not add in a special error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126:151,Integrability,message,message,151,Rebased and a minor migration fix added. I also removed the non-default failure and error handling on the migration but did not add in a special error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300170214:67,Performance,cache,cache,67,"Perhaps I missed it, but it seems like we are still publishing the cache hit results as soon as we get a hit, not after copying succeeds? Maybe it wasn't meant to be a part of this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300170214
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806:20,Performance,cache,cache,20,"~~@ruchim writing a cache result (its hashes and its results) should be functionally atomic (i.e. either you get a cache hit, or you don't, and if you do, then you can copy all the results immediately).; If that's not part of this PR then it should 100% be part of Cromwell 27 (IMO)~~. Oops, I misread. You're talking about cache hit vs cache miss in the metadata if copying results fails? If the metadata is overwriting itself then ""current state of play"" is probably ok?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806:115,Performance,cache,cache,115,"~~@ruchim writing a cache result (its hashes and its results) should be functionally atomic (i.e. either you get a cache hit, or you don't, and if you do, then you can copy all the results immediately).; If that's not part of this PR then it should 100% be part of Cromwell 27 (IMO)~~. Oops, I misread. You're talking about cache hit vs cache miss in the metadata if copying results fails? If the metadata is overwriting itself then ""current state of play"" is probably ok?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806:324,Performance,cache,cache,324,"~~@ruchim writing a cache result (its hashes and its results) should be functionally atomic (i.e. either you get a cache hit, or you don't, and if you do, then you can copy all the results immediately).; If that's not part of this PR then it should 100% be part of Cromwell 27 (IMO)~~. Oops, I misread. You're talking about cache hit vs cache miss in the metadata if copying results fails? If the metadata is overwriting itself then ""current state of play"" is probably ok?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806:337,Performance,cache,cache,337,"~~@ruchim writing a cache result (its hashes and its results) should be functionally atomic (i.e. either you get a cache hit, or you don't, and if you do, then you can copy all the results immediately).; If that's not part of this PR then it should 100% be part of Cromwell 27 (IMO)~~. Oops, I misread. You're talking about cache hit vs cache miss in the metadata if copying results fails? If the metadata is overwriting itself then ""current state of play"" is probably ok?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806
https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806:38,Security,hash,hashes,38,"~~@ruchim writing a cache result (its hashes and its results) should be functionally atomic (i.e. either you get a cache hit, or you don't, and if you do, then you can copy all the results immediately).; If that's not part of this PR then it should 100% be part of Cromwell 27 (IMO)~~. Oops, I misread. You're talking about cache hit vs cache miss in the metadata if copying results fails? If the metadata is overwriting itself then ""current state of play"" is probably ok?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806
https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584:95,Deployability,update,updated,95,"Hi @antonkulaga - you're right, and likely more than just Pairs. It's been a while since we've updated the plugin so it's not totally on board with newer WDL constructs at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584
https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584:107,Modifiability,plugin,plugin,107,"Hi @antonkulaga - you're right, and likely more than just Pairs. It's been a while since we've updated the plugin so it's not totally on board with newer WDL constructs at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584
https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595288:31,Deployability,update,updated,31,@ruchim @danbills did this get updated in the intellij repository?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595288
https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595634:43,Usability,clear,clear,43,"@geoffjentry Unclear, I was only aiming to clear the closed PR from the board. Reopening and I'll let @danbills close this whenever appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595634
https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303736986:0,Modifiability,Plugin,Plugin,0,"Plugin has been submitted ""for approval"" by @cjllanwarne , will close the ticket when it's approved.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303736986
https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515:145,Deployability,release,releases,145,"Bloom filter is a test of ""have we seen this before?"" on a huge set. We get one for free from Google Guava [here](https://google.github.io/guava/releases/22.0/api/docs/com/google/common/hash/BloomFilter.html)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515
https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515:186,Security,hash,hash,186,"Bloom filter is a test of ""have we seen this before?"" on a huge set. We get one for free from Google Guava [here](https://google.github.io/guava/releases/22.0/api/docs/com/google/common/hash/BloomFilter.html)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515
https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515:18,Testability,test,test,18,"Bloom filter is a test of ""have we seen this before?"" on a huge set. We get one for free from Google Guava [here](https://google.github.io/guava/releases/22.0/api/docs/com/google/common/hash/BloomFilter.html)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515
https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385:183,Energy Efficiency,efficient,efficient,183,"What @danbills said. It's a probabilistic data structure which can tell you either ""I've probably seen this before"" or ""I've definitely *not* seen this before"" in a very space & time efficient manner. So in a case like this where there are a lot of misses we could use it to do a quick ""should I even bother hitting the DB"" sanity test. . I'd want to see that this is actually a bottleneck before proceeding, this was just an idea I had that I wanted to keep around for memory's sake in case we do see it be a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385
https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385:379,Performance,bottleneck,bottleneck,379,"What @danbills said. It's a probabilistic data structure which can tell you either ""I've probably seen this before"" or ""I've definitely *not* seen this before"" in a very space & time efficient manner. So in a case like this where there are a lot of misses we could use it to do a quick ""should I even bother hitting the DB"" sanity test. . I'd want to see that this is actually a bottleneck before proceeding, this was just an idea I had that I wanted to keep around for memory's sake in case we do see it be a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385
https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385:510,Performance,bottleneck,bottleneck,510,"What @danbills said. It's a probabilistic data structure which can tell you either ""I've probably seen this before"" or ""I've definitely *not* seen this before"" in a very space & time efficient manner. So in a case like this where there are a lot of misses we could use it to do a quick ""should I even bother hitting the DB"" sanity test. . I'd want to see that this is actually a bottleneck before proceeding, this was just an idea I had that I wanted to keep around for memory's sake in case we do see it be a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385
https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385:331,Testability,test,test,331,"What @danbills said. It's a probabilistic data structure which can tell you either ""I've probably seen this before"" or ""I've definitely *not* seen this before"" in a very space & time efficient manner. So in a case like this where there are a lot of misses we could use it to do a quick ""should I even bother hitting the DB"" sanity test. . I'd want to see that this is actually a bottleneck before proceeding, this was just an idea I had that I wanted to keep around for memory's sake in case we do see it be a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385
https://github.com/broadinstitute/cromwell/issues/2250#issuecomment-323157942:221,Testability,mock,mock,221,"I am not a bum, I'm a jerk. This one is me. This was more a feature request to be able to prepare objects that have non-string values, including Files. #2283 has a more detailed discussion and links to some work I did to mock up what structs (or revamped objects) could look like.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2250#issuecomment-323157942
https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751:18,Availability,error,error,18,Should I keep the error message to `JES` or should it say `Pipelies API`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751
https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751:24,Integrability,message,message,24,Should I keep the error message to `JES` or should it say `Pipelies API`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751
https://github.com/broadinstitute/cromwell/issues/2252#issuecomment-299852998:37,Security,hash,hashes,37,Cromwell 27 will be able to retrieve hashes for public images stored on quay.io,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2252#issuecomment-299852998
https://github.com/broadinstitute/cromwell/issues/2254#issuecomment-300552698:35,Security,hash,hash,35,"You're specifying both a tag and a hash, which you're right docker recognizes as a valid image (surprisingly I must say). We should fix Cromwell to validate that correctly. In the meantime if you use `quay.io/biocontainers/star@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b` it should work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2254#issuecomment-300552698
https://github.com/broadinstitute/cromwell/issues/2254#issuecomment-300552698:148,Security,validat,validate,148,"You're specifying both a tag and a hash, which you're right docker recognizes as a valid image (surprisingly I must say). We should fix Cromwell to validate that correctly. In the meantime if you use `quay.io/biocontainers/star@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b` it should work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2254#issuecomment-300552698
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300800287:149,Availability,down,down,149,@cjllanwarne I can't find it now (perhaps it has since been closed as a won't fix) but this has come up before. If you're open to trying to track it down close one as a dupe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300800287
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300922007:451,Integrability,wrap,wrapper,451,"I don't think we override the entrypoint that's correct. If you're using a ""ConfigBackend"" technically you can choose what the docker command is so you could do something like ; ```; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash ${script}; """"""; ```. And in your WDL command call whatever the original entrypoint of your docker was (`/opt/FastQC/wrapper.sh` in your case).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300922007
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300922007:76,Modifiability,Config,ConfigBackend,76,"I don't think we override the entrypoint that's correct. If you're using a ""ConfigBackend"" technically you can choose what the docker command is so you could do something like ; ```; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash ${script}; """"""; ```. And in your WDL command call whatever the original entrypoint of your docker was (`/opt/FastQC/wrapper.sh` in your case).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300922007
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-301098364:87,Modifiability,config,config,87,"Oh @antonkulaga @Horneth, so the fix to this is to add `--entrypoint /bin/bash` to our config examples, so that we can guarantee overwriting whatever the docker is trying to do with `bash` (which Cromwell assumes)? That sounds sensible enough to me!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-301098364
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247:1897,Modifiability,config,config,1897,"ssue. PR #2280 fixes part of the problem, but only for the local backend. I am trying to use the official Picard docker:; https://hub.docker.com/r/broadinstitute/picard/~/dockerfile/. Here is a simplified version of my WDL for illustration purposes:. task picard{; command {; CollectAlignmentSummaryMetrics; }; runtime{; docker:""broadinstitute/picard""; }; }. workflow picardwf {; call picard; }. If I locally do; `docker run broadinstitute/picard CollectAlignmentSummaryMetrics`. I get the behavior I expect -- in this simplified example, it prints out the help command for CollectAlignmentSummaryMetrics. (The Dockerfile specifies an entrypoint that calls a script in the container and passes ""CollectAlignmentSummaryMetrics"" to it.). When running with the JES backend, however, I get:; '/tmp/ggp-298770331' is not a valid command. See PicardCommandLine -h for more information.'. Cromwell has inserted a file path immediately after the image name in the docker run command:; `Running command: docker run -v /tmp/ggp-298770331:/tmp/ggp-298770331 -v /mnt/local-disk:/cromwell_root -e exec=/cromwell_root/exec.sh -e picard-rc.txt=/cromwell_root/picard-rc.txt -e __extra_config_gcs_path=gs://<path_redacted> broadinstitute/picard@sha256:1ddf5888182718c55054c96dca6a5d65d23a703f98e66832400e4837d04854a7 /tmp/ggp-298770331`. I don't think this default behavior makes sense for any backend. It should either override the entrypoint by default (as the local backend now does) or -- this would be much better -- it would work with the existing entrypoint. A lot of published containers specify an entrypoint, which means when running in Cromwell they can't really be used as intended with that entrypoint. To get them running at all requires changing Cromwell's config to ignore the entrypoint, and then if you want to use the entrypoint you have to re-specify it as part of the command. Could you please consider changing the default behavior to make it compatible with entrypoints out of the box?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247:336,Usability,simpl,simplified,336,"Greetings, my illustrious Cromwellian friends!. I find your workflow engine indispensable but am still running into problems related to this issue. PR #2280 fixes part of the problem, but only for the local backend. I am trying to use the official Picard docker:; https://hub.docker.com/r/broadinstitute/picard/~/dockerfile/. Here is a simplified version of my WDL for illustration purposes:. task picard{; command {; CollectAlignmentSummaryMetrics; }; runtime{; docker:""broadinstitute/picard""; }; }. workflow picardwf {; call picard; }. If I locally do; `docker run broadinstitute/picard CollectAlignmentSummaryMetrics`. I get the behavior I expect -- in this simplified example, it prints out the help command for CollectAlignmentSummaryMetrics. (The Dockerfile specifies an entrypoint that calls a script in the container and passes ""CollectAlignmentSummaryMetrics"" to it.). When running with the JES backend, however, I get:; '/tmp/ggp-298770331' is not a valid command. See PicardCommandLine -h for more information.'. Cromwell has inserted a file path immediately after the image name in the docker run command:; `Running command: docker run -v /tmp/ggp-298770331:/tmp/ggp-298770331 -v /mnt/local-disk:/cromwell_root -e exec=/cromwell_root/exec.sh -e picard-rc.txt=/cromwell_root/picard-rc.txt -e __extra_config_gcs_path=gs://<path_redacted> broadinstitute/picard@sha256:1ddf5888182718c55054c96dca6a5d65d23a703f98e66832400e4837d04854a7 /tmp/ggp-298770331`. I don't think this default behavior makes sense for any backend. It should either override the entrypoint by default (as the local backend now does) or -- this would be much better -- it would work with the existing entrypoint. A lot of published containers specify an entrypoint, which means when running in Cromwell they can't really be used as intended with that entrypoint. To get them running at all requires changing Cromwell's config to ignore the entrypoint, and then if you want to use the entrypoint you have to re-specify it as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247:661,Usability,simpl,simplified,661,"Greetings, my illustrious Cromwellian friends!. I find your workflow engine indispensable but am still running into problems related to this issue. PR #2280 fixes part of the problem, but only for the local backend. I am trying to use the official Picard docker:; https://hub.docker.com/r/broadinstitute/picard/~/dockerfile/. Here is a simplified version of my WDL for illustration purposes:. task picard{; command {; CollectAlignmentSummaryMetrics; }; runtime{; docker:""broadinstitute/picard""; }; }. workflow picardwf {; call picard; }. If I locally do; `docker run broadinstitute/picard CollectAlignmentSummaryMetrics`. I get the behavior I expect -- in this simplified example, it prints out the help command for CollectAlignmentSummaryMetrics. (The Dockerfile specifies an entrypoint that calls a script in the container and passes ""CollectAlignmentSummaryMetrics"" to it.). When running with the JES backend, however, I get:; '/tmp/ggp-298770331' is not a valid command. See PicardCommandLine -h for more information.'. Cromwell has inserted a file path immediately after the image name in the docker run command:; `Running command: docker run -v /tmp/ggp-298770331:/tmp/ggp-298770331 -v /mnt/local-disk:/cromwell_root -e exec=/cromwell_root/exec.sh -e picard-rc.txt=/cromwell_root/picard-rc.txt -e __extra_config_gcs_path=gs://<path_redacted> broadinstitute/picard@sha256:1ddf5888182718c55054c96dca6a5d65d23a703f98e66832400e4837d04854a7 /tmp/ggp-298770331`. I don't think this default behavior makes sense for any backend. It should either override the entrypoint by default (as the local backend now does) or -- this would be much better -- it would work with the existing entrypoint. A lot of published containers specify an entrypoint, which means when running in Cromwell they can't really be used as intended with that entrypoint. To get them running at all requires changing Cromwell's config to ignore the entrypoint, and then if you want to use the entrypoint you have to re-specify it as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-324949866:303,Modifiability,flexible,flexible,303,"Hi @dshiga - unfortunately there's currently a limitation with PAPI, see what @kshakir wrote [here](https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026). It's expected that PAPI will overcome these limitations over the next couple of quarters and then we can change to be more flexible, but until then there's not much that we can do.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-324949866
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-410760636:80,Testability,test,test,80,@dshiga We expect this to be fixed in PAPI v2 backend. Have you had a chance to test this in v2 yet? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-410760636
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-438696562:439,Deployability,install,installed,439,"@TMiguelT I'm largely channeling the thoughts [in this paper](https://f1000research.com/articles/7-742/v1). The main issue I have w/ `ENTRYPOINT` is that it makes things less explicit in terms of what's happening when one reads the workflow descriptor. . The other main argument which tends to come up is that this effectively locks the workflow to using the docker container, vs systems where one could ensure the appropriate software is installed. This one is definitely a real world use case but I feel less strongly about it as the use of the container is part of the selling point.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-438696562
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-478755517:85,Deployability,pipeline,pipelines,85,"Looks like this is still an issue, correct? I‚Äôve just stumbled upon it in one of our pipelines that uses PAPIv2 (on Cromwell 36). The pipeline step uses a Docker image based on `openjdk:13-alpine`, which does have its own entrypoint that we‚Äôd like Cromwell to ignore. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-478755517
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-478755517:134,Deployability,pipeline,pipeline,134,"Looks like this is still an issue, correct? I‚Äôve just stumbled upon it in one of our pipelines that uses PAPIv2 (on Cromwell 36). The pipeline step uses a Docker image based on `openjdk:13-alpine`, which does have its own entrypoint that we‚Äôd like Cromwell to ignore. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-478755517
https://github.com/broadinstitute/cromwell/pull/2257#issuecomment-300918183:75,Usability,usab,usability,75,"Other than my pie-in-the-sky ""wouldn't it be nice"" comment about the API's usability, looks good üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2257/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2257#issuecomment-300918183
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532:56,Deployability,release,release,56,"I'm wondering if it'd make more sense to add it to the [release wdl](https://github.com/broadinstitute/cromwell/blob/3e577223845ee8d20cab38590579b65fa73fe64e/release/release_workflow.wdl). @tomkinsc what do you think, yo'ure just looking for this to be applied to official releases, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532:158,Deployability,release,release,158,"I'm wondering if it'd make more sense to add it to the [release wdl](https://github.com/broadinstitute/cromwell/blob/3e577223845ee8d20cab38590579b65fa73fe64e/release/release_workflow.wdl). @tomkinsc what do you think, yo'ure just looking for this to be applied to official releases, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532:273,Deployability,release,releases,273,"I'm wondering if it'd make more sense to add it to the [release wdl](https://github.com/broadinstitute/cromwell/blob/3e577223845ee8d20cab38590579b65fa73fe64e/release/release_workflow.wdl). @tomkinsc what do you think, yo'ure just looking for this to be applied to official releases, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301189076:106,Deployability,release,release,106,"That sounds great. For the community at large would probably be nice to open a PR to bioconda on each new release with an updated recipe, but it could also push builds to its own channel on anaconda.org (probably easier).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301189076
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301189076:122,Deployability,update,updated,122,"That sounds great. For the community at large would probably be nice to open a PR to bioconda on each new release with an updated recipe, but it could also push builds to its own channel on anaconda.org (probably easier).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301189076
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815:89,Deployability,release,release,89,@tomkinsc What we're going to do is time allowing try to do it by hand for this upcoming release and either way make it part of our automated process for the following release. I'm assuming you're willing to provide a helpful hand if we need some guidance on the specifics?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815:168,Deployability,release,release,168,@tomkinsc What we're going to do is time allowing try to do it by hand for this upcoming release and either way make it part of our automated process for the following release. I'm assuming you're willing to provide a helpful hand if we need some guidance on the specifics?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815:247,Usability,guid,guidance,247,@tomkinsc What we're going to do is time allowing try to do it by hand for this upcoming release and either way make it part of our automated process for the following release. I'm assuming you're willing to provide a helpful hand if we need some guidance on the specifics?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815
https://github.com/broadinstitute/cromwell/issues/2261#issuecomment-301093231:104,Testability,log,logic,104,"How would this be as a temporary enabler (I think this will be easier/faster than modifying the scatter logic itself):. ```; Map[X,Y] map # from elsewhere; List[Pair[X,Y]] listOfPair = map # Coerce to a list of pairs; scatter (pair in listOfPair) {; # do the scatter; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2261#issuecomment-301093231
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292:117,Deployability,release,release,117,@vdauwera Can you explain the situation? I'm not clear what the exact feature request is. It won't make it into this release but we can see about next (Cromwell 28).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292:49,Usability,clear,clear,49,@vdauwera Can you explain the situation? I'm not clear what the exact feature request is. It won't make it into this release but we can see about next (Cromwell 28).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265:71,Integrability,depend,depending,71,"The tasks we run tend to have variable memory and storage requirements depending on the dataset we're processing in any given run. It would be nice to be able to set just minimum values and have Cromwell calculate what it should actually request based on ""some logic"" relating to the size of the input -> where the ""some logic"" is the difficult bit of course. For some tasks we have pretty good expectations of how the needs will relate to inputs, eg if I'm just copying over the same data with minor changes, but for others it could be hairy. . Frankly I don't think this should be made a priority, because my naive impression is that it will be really hard to do well, and the result will be a convenience, but nothing earth-shattering. There's a lot of other stuff I would want to have first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265:30,Modifiability,variab,variable,30,"The tasks we run tend to have variable memory and storage requirements depending on the dataset we're processing in any given run. It would be nice to be able to set just minimum values and have Cromwell calculate what it should actually request based on ""some logic"" relating to the size of the input -> where the ""some logic"" is the difficult bit of course. For some tasks we have pretty good expectations of how the needs will relate to inputs, eg if I'm just copying over the same data with minor changes, but for others it could be hairy. . Frankly I don't think this should be made a priority, because my naive impression is that it will be really hard to do well, and the result will be a convenience, but nothing earth-shattering. There's a lot of other stuff I would want to have first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265:261,Testability,log,logic,261,"The tasks we run tend to have variable memory and storage requirements depending on the dataset we're processing in any given run. It would be nice to be able to set just minimum values and have Cromwell calculate what it should actually request based on ""some logic"" relating to the size of the input -> where the ""some logic"" is the difficult bit of course. For some tasks we have pretty good expectations of how the needs will relate to inputs, eg if I'm just copying over the same data with minor changes, but for others it could be hairy. . Frankly I don't think this should be made a priority, because my naive impression is that it will be really hard to do well, and the result will be a convenience, but nothing earth-shattering. There's a lot of other stuff I would want to have first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265:321,Testability,log,logic,321,"The tasks we run tend to have variable memory and storage requirements depending on the dataset we're processing in any given run. It would be nice to be able to set just minimum values and have Cromwell calculate what it should actually request based on ""some logic"" relating to the size of the input -> where the ""some logic"" is the difficult bit of course. For some tasks we have pretty good expectations of how the needs will relate to inputs, eg if I'm just copying over the same data with minor changes, but for others it could be hairy. . Frankly I don't think this should be made a priority, because my naive impression is that it will be really hard to do well, and the result will be a convenience, but nothing earth-shattering. There's a lot of other stuff I would want to have first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301274387:159,Usability,learn,learn,159,"A suggestion I have would be to get people using expressions for memory and; disk size. I believe we already have a size expression for files, don't; we? Then learn what people are doing commonly and make that easy!. Magic is hard to do well -- discovering some common patterns and making; that is is... easier. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Sat, May 13, 2017 at 1:34 PM, Jeff Gentry <notifications@github.com>; wrote:. > It sounded like they wanted something more automagical but yeah; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301262719>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gw1uBQmH-POG0YZKp4XLsuf8p_V9ks5r5em6gaJpZM4NZ55B>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301274387
https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174:103,Performance,cache,cache,103,"This is from the point of view of the need on the FireCloud side:; In my opinion, Cromwell should call cache based upon it's best effort to know everything that is different. Clearly you could write a WDL that does the same thing in the end with different inputs (e.g. more memory is given than before), but I'd rather err on the side of no false positives than trying to make sure Cromwell is too clever. This really feels more like sugar in workbench where we could make some best guesses based upon what our users do or simply organize things in a way that it's less confusing/less effort for our users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174
https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174:175,Usability,Clear,Clearly,175,"This is from the point of view of the need on the FireCloud side:; In my opinion, Cromwell should call cache based upon it's best effort to know everything that is different. Clearly you could write a WDL that does the same thing in the end with different inputs (e.g. more memory is given than before), but I'd rather err on the side of no false positives than trying to make sure Cromwell is too clever. This really feels more like sugar in workbench where we could make some best guesses based upon what our users do or simply organize things in a way that it's less confusing/less effort for our users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174
https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174:523,Usability,simpl,simply,523,"This is from the point of view of the need on the FireCloud side:; In my opinion, Cromwell should call cache based upon it's best effort to know everything that is different. Clearly you could write a WDL that does the same thing in the end with different inputs (e.g. more memory is given than before), but I'd rather err on the side of no false positives than trying to make sure Cromwell is too clever. This really feels more like sugar in workbench where we could make some best guesses based upon what our users do or simply organize things in a way that it's less confusing/less effort for our users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301221420:230,Modifiability,config,configurable,230,"Hah . I'd prefer to provide a clean way to specify any collection of files (see CWLs secondary files concept) and then syntactic sugar in the form of specialized types, e.g. BamFile which knows to look for an index . Having it be configurable at the Cromwell level implies a potential lack of portability for WDLs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301221420
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301221420:293,Modifiability,portab,portability,293,"Hah . I'd prefer to provide a clean way to specify any collection of files (see CWLs secondary files concept) and then syntactic sugar in the form of specialized types, e.g. BamFile which knows to look for an index . Having it be configurable at the Cromwell level implies a potential lack of portability for WDLs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301221420
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301222671:50,Modifiability,portab,portability,50,"Oh hmm very good point, hadn't viewed it from the portability angle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-301222671
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317839668:145,Integrability,Depend,Depending,145,"@davidbenjamin has an interesting proposal for user-defined / explicit sets of params for WDL: https://github.com/broadinstitute/wdl/issues/102. Depending on how ""[CWL support](https://github.com/broadinstitute/cromwell/milestone/20)"" addresses the [secondaryFiles](http://www.commonwl.org/v1.0/Workflow.html#File) mentioned above, it's supposed that similar WDL features will follow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317839668
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944:97,Security,access,accessory,97,"FYI this was a key item in feedback from our WDL sessions in the UK workshops; having to specify accessory files is a big source of annoyance. Not that it's any surprise, but we're definitely getting confirmation from real users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944:27,Usability,feedback,feedback,27,"FYI this was a key item in feedback from our WDL sessions in the UK workshops; having to specify accessory files is a big source of annoyance. Not that it's any surprise, but we're definitely getting confirmation from real users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317859657:45,Usability,learn,learned,45,We should certainly heed the lesson that CWL learned to provide both the concepts of directory and secondary files. They wound up implementing the former because people were also trying to do that and shoehorning it into the latter.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317859657
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317861526:388,Usability,learn,learned,388,"+100 on support for secondary files! BAM + Index, VCF + Index would be; super helpful!. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Jul 25, 2017 at 4:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > We should certainly heed the lesson that CWL learned to provide both the; > concepts of directory and secondary files. They wound up implementing the; > former because people were also trying to do that and shoehorning it into; > the latter.; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317859657>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g1ghUrXGroGI7qjHMH_N5z75osBjks5sRk2sgaJpZM4NZ6CY>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317861526
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-463843884:135,Usability,simpl,simpler,135,"Is there any progress on secondary files, I see we have structs which I could probably use but I'm looking for a concept that makes it simpler to pick up index files rather than writing more globs and more mappings. We got directory support in WDL (https://github.com/openwdl/wdl/pull/241) and in Cromwell (https://github.com/broadinstitute/cromwell/pull/3980). . I understand that the language and the engine are different, but Cromwell has some concept of these secondary files as the CWL implementation supports it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-463843884
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464284636:216,Usability,clear,clear,216,"@illusional While I cannot speak on behalf of the cromwell team on what they are implementing, I can say that there has been no discussions around secondary files for WDL. My inclination is that we will try to steer clear of it within WDL. However I encourage you to create an issue or make a PR in the WDL repo suggesting this change and we can allow the community to determine wtheher or not it should be something supported.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464284636
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564:159,Modifiability,portab,portable,159,I would agree w/ @patmagee that this is a matter for the OpenWDL group. Any Cromwell-level constructs to get at the underlying functionality would require non-portable WDLs to be written. I'll tag @cjllanwarne in case he has any clever ideas on how to express the concept in portable WDL in a less sucky way. I disagree with @patmagee that WDL should steer clear of the concept - IMO not doing this in the first place was one of the larger mistakes we made in the early days of WDL. Perhaps something with `Object`. We're seeing something similar play out in GA4GH land w/ DRS ... the concept of a file bundle seems inescapable and it's not quite the same thing as `Directory`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564:275,Modifiability,portab,portable,275,I would agree w/ @patmagee that this is a matter for the OpenWDL group. Any Cromwell-level constructs to get at the underlying functionality would require non-portable WDLs to be written. I'll tag @cjllanwarne in case he has any clever ideas on how to express the concept in portable WDL in a less sucky way. I disagree with @patmagee that WDL should steer clear of the concept - IMO not doing this in the first place was one of the larger mistakes we made in the early days of WDL. Perhaps something with `Object`. We're seeing something similar play out in GA4GH land w/ DRS ... the concept of a file bundle seems inescapable and it's not quite the same thing as `Directory`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564:357,Usability,clear,clear,357,I would agree w/ @patmagee that this is a matter for the OpenWDL group. Any Cromwell-level constructs to get at the underlying functionality would require non-portable WDLs to be written. I'll tag @cjllanwarne in case he has any clever ideas on how to express the concept in portable WDL in a less sucky way. I disagree with @patmagee that WDL should steer clear of the concept - IMO not doing this in the first place was one of the larger mistakes we made in the early days of WDL. Perhaps something with `Object`. We're seeing something similar play out in GA4GH land w/ DRS ... the concept of a file bundle seems inescapable and it's not quite the same thing as `Directory`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736:410,Modifiability,portab,portable,410,"Yeah from an end user POV it is still a pain not to have a file bundle; concept, and it is something I wish we had at the WDL level. On Sat, Feb 16, 2019 at 9:32 AM Jeff Gentry <notifications@github.com>; wrote:. > I would agree w/ @patmagee <https://github.com/patmagee> that this is a; > matter for the OpenWDL group. Any Cromwell-level constructs to get at the; > underlying functionality would require non-portable WDLs to be written.; > I'll tag @cjllanwarne <https://github.com/cjllanwarne> in case he has any; > clever ideas on how to express the concept in portable WDL in a less sucky; > way.; >; > I disagree with @patmagee <https://github.com/patmagee> that WDL should; > steer clear of the concept - IMO not doing this in the first place was one; > of the larger mistakes we made in the early days of WDL. Perhaps something; > with Object. We're seeing something similar play out in GA4GH land w/ DRS; > ... the concept of a file bundle seems inescapable and it's not quite the; > same thing as Directory; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnwEyiT-eFgW3NMiY39SRU8MvDG3L6Gks5vOBZpgaJpZM4NZ6CY>; > .; >; -- ; Geraldine A. Van der Auwera, Ph.D.; Associate Director of Outreach and Communications; Data Sciences Platform; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736:565,Modifiability,portab,portable,565,"Yeah from an end user POV it is still a pain not to have a file bundle; concept, and it is something I wish we had at the WDL level. On Sat, Feb 16, 2019 at 9:32 AM Jeff Gentry <notifications@github.com>; wrote:. > I would agree w/ @patmagee <https://github.com/patmagee> that this is a; > matter for the OpenWDL group. Any Cromwell-level constructs to get at the; > underlying functionality would require non-portable WDLs to be written.; > I'll tag @cjllanwarne <https://github.com/cjllanwarne> in case he has any; > clever ideas on how to express the concept in portable WDL in a less sucky; > way.; >; > I disagree with @patmagee <https://github.com/patmagee> that WDL should; > steer clear of the concept - IMO not doing this in the first place was one; > of the larger mistakes we made in the early days of WDL. Perhaps something; > with Object. We're seeing something similar play out in GA4GH land w/ DRS; > ... the concept of a file bundle seems inescapable and it's not quite the; > same thing as Directory; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnwEyiT-eFgW3NMiY39SRU8MvDG3L6Gks5vOBZpgaJpZM4NZ6CY>; > .; >; -- ; Geraldine A. Van der Auwera, Ph.D.; Associate Director of Outreach and Communications; Data Sciences Platform; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736:689,Usability,clear,clear,689,"Yeah from an end user POV it is still a pain not to have a file bundle; concept, and it is something I wish we had at the WDL level. On Sat, Feb 16, 2019 at 9:32 AM Jeff Gentry <notifications@github.com>; wrote:. > I would agree w/ @patmagee <https://github.com/patmagee> that this is a; > matter for the OpenWDL group. Any Cromwell-level constructs to get at the; > underlying functionality would require non-portable WDLs to be written.; > I'll tag @cjllanwarne <https://github.com/cjllanwarne> in case he has any; > clever ideas on how to express the concept in portable WDL in a less sucky; > way.; >; > I disagree with @patmagee <https://github.com/patmagee> that WDL should; > steer clear of the concept - IMO not doing this in the first place was one; > of the larger mistakes we made in the early days of WDL. Perhaps something; > with Object. We're seeing something similar play out in GA4GH land w/ DRS; > ... the concept of a file bundle seems inescapable and it's not quite the; > same thing as Directory; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnwEyiT-eFgW3NMiY39SRU8MvDG3L6Gks5vOBZpgaJpZM4NZ6CY>; > .; >; -- ; Geraldine A. Van der Auwera, Ph.D.; Associate Director of Outreach and Communications; Data Sciences Platform; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464532451:271,Security,access,accessory,271,"Thanks @patmagee, @geoffjentry and all for directing me to the correct place, I've created a discussion over at https://github.com/openwdl/wdl/issues/289 as a place to have the conversation. If anyone finds this conversation, I'd love to see any thoughts you have on how accessory files may be specified in WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464532451
https://github.com/broadinstitute/cromwell/issues/2270#issuecomment-301503310:96,Security,validat,validation,96,Probably linked to https://github.com/broadinstitute/cromwell/issues/1886; Retrying credentials validation here https://github.com/broadinstitute/cromwell/blob/develop/filesystems/gcs/src/main/scala/cromwell/filesystems/gcs/auth/GoogleAuthMode.scala#L66 should fix all those issues,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270#issuecomment-301503310
https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-302416277:16,Testability,test,test,16,"Could we have a test in `MaterializeWorkflowDescriptorActorSpec` along the lines of `it should ""correctly identify all misformed input file names in an input json""`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-302416277
https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-302416979:36,Testability,test,test,36,":+1: Seconding the idea of adding a test, otherwise LGTM. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2271/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-302416979
https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-302427879:9,Testability,test,test,9,üëç with a test. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2271/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-302427879
https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-303425275:5,Testability,test,test,5,Re-üëç test looks good.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-303425275
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778:37,Deployability,update,update,37,Could you hold off a bit? There's an update in flight to the plugin.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778:61,Modifiability,plugin,plugin,61,Could you hold off a bit? There's an update in flight to the plugin.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302113476:140,Deployability,update,update,140,"Sure. Otherwise, seems fine. On May 17, 2017 10:42, ""Jeff Gentry"" <notifications@github.com> wrote:. > Could you hold off a bit? There's an update in flight to the plugin.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk59bJp_C1jcwAeUotUZ-mtu2w9_3ks5r6wdqgaJpZM4Nd63a>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302113476
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302113476:164,Modifiability,plugin,plugin,164,"Sure. Otherwise, seems fine. On May 17, 2017 10:42, ""Jeff Gentry"" <notifications@github.com> wrote:. > Could you hold off a bit? There's an update in flight to the plugin.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk59bJp_C1jcwAeUotUZ-mtu2w9_3ks5r6wdqgaJpZM4Nd63a>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302113476
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-313750304:64,Modifiability,plugin,plugin,64,"@LeeTL1220 Is this still happening w/ the latest version of the plugin? Also, if you didn't notice, pycharm works now too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-313750304
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393:37,Deployability,update,update,37,"@cjllanwarne I know you just made an update to the IntelliJ plugin, did you fix this too?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393:60,Modifiability,plugin,plugin,60,"@cjllanwarne I know you just made an update to the IntelliJ plugin, did you fix this too?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982664:162,Deployability,update,update,162,"This is fixed. On Sep 20, 2017 5:12 PM, ""Kate Voss"" <notifications@github.com> wrote:. > @cjllanwarne <https://github.com/cjllanwarne> I know you just made an; > update to the IntelliJ plugin, did you fix this too?; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAKFyC05-lHHzULkbRCfoBisrrWy8cMvks5skX-jgaJpZM4Nd63a>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982664
https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982664:185,Modifiability,plugin,plugin,185,"This is fixed. On Sep 20, 2017 5:12 PM, ""Kate Voss"" <notifications@github.com> wrote:. > @cjllanwarne <https://github.com/cjllanwarne> I know you just made an; > update to the IntelliJ plugin, did you fix this too?; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAKFyC05-lHHzULkbRCfoBisrrWy8cMvks5skX-jgaJpZM4Nd63a>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982664
https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444:97,Deployability,integrat,integration,97,This is great! We should wait for https://github.com/broadinstitute/centaur/pull/193 (fixing our integration tests) to merge but otherwise looking good to me!; üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2280/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444
https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444:97,Integrability,integrat,integration,97,This is great! We should wait for https://github.com/broadinstitute/centaur/pull/193 (fixing our integration tests) to merge but otherwise looking good to me!; üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2280/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444
https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444:109,Testability,test,tests,109,This is great! We should wait for https://github.com/broadinstitute/centaur/pull/193 (fixing our integration tests) to merge but otherwise looking good to me!; üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2280/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444
https://github.com/broadinstitute/cromwell/issues/2281#issuecomment-327807436:179,Availability,alive,alive,179,"from @cjllanwarne:. > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2281#issuecomment-327807436
https://github.com/broadinstitute/cromwell/issues/2281#issuecomment-327807436:243,Availability,alive,alive,243,"from @cjllanwarne:. > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2281#issuecomment-327807436
https://github.com/broadinstitute/cromwell/pull/2282#issuecomment-302831033:59,Energy Efficiency,green,greened,59,A big thank you to @cjllanwarne for his Centaur magic that greened the Travis builds on this PR! üá¨üáß,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2282#issuecomment-302831033
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-325748082:305,Testability,mock,mock,305,Patrick -- thanks for looking at this. I'd like to use Objects to group together related items so we can pass them as a single sample to processes. This enables things like batch or tumor/normal calling where you have multiple samples together and want to bundle each of items with each other. I've got a mock up of how I'd use it in a real somatic workflow here:. https://github.com/bcbio/test_bcbio_cwl/blob/master/somatic/somatic-wdl/main_somatic.wdl. The issue with current objects is that the items are typed so platforms can't identify files for localizing them. Happy to provide more details if it would help.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-325748082
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-326138202:671,Usability,Clear,Clearly,671,"I am currently experimenting with wdl4s object support in [dxWDL](https://github.com/dnanexus-rnd/dxWDL). The first use case is for representing JSON objects; it turns out that the syntax is very similar. I don't know if that is a coincidence. Presently, I am having trouble with the WDL object typing. For example, the workflow below: . ```; workflow wf_complex {; Object z = {""a"": 3, ""b"": 1}. output {; Int sum = z.a + z.b; }; }; ```; When evaluated with Cromwell; ```; java -jar cromwell-29.jar run wf_complex.wdl; ```; reports: ; ```; ""outputs"": {; ""wf_complex.sum"": 31; }; ```; However, I would think `a` and `b` are integers, and the addition result should be `4`. Clearly, Cromwell thinks these are strings, and the addition concatenates `3` to `1` instead. Had `z` been a JSON object, the values would have been interpreted as integers, as intended. . 1) Can the experts shed light on the situation? ; 2) How do I create a WDL object literal?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-326138202
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-327884056:79,Modifiability,enhance,enhance,79,"+1 on this :) I think the idea of `struct` would be very useful to potentially enhance/replace the Object type. May want the order of the type and variable name to be consistent with other WDL, e.g. instead of:. ```; struct MyType {; o_f: File; x: Array[String]; }; ```. ```; struct MyType {; File of; Array[String] x; }; ```. IMHO, the word `struct` is nice since it pays homage to C and it seems like a fairly nice correspondence. May want to consider `tuple` as well. Finally, I think a nice benefit of this is that it could have nice correspondence with records in CWL and thus potentially a nice representation in the WOM to handle both.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-327884056
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-327884056:147,Modifiability,variab,variable,147,"+1 on this :) I think the idea of `struct` would be very useful to potentially enhance/replace the Object type. May want the order of the type and variable name to be consistent with other WDL, e.g. instead of:. ```; struct MyType {; o_f: File; x: Array[String]; }; ```. ```; struct MyType {; File of; Array[String] x; }; ```. IMHO, the word `struct` is nice since it pays homage to C and it seems like a fairly nice correspondence. May want to consider `tuple` as well. Finally, I think a nice benefit of this is that it could have nice correspondence with records in CWL and thus potentially a nice representation in the WOM to handle both.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-327884056
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059:491,Availability,down,down,491,"Although now that I've said that I know that we *do* have a use case where something like this is being requested. They want a typed set of key/value pairs, but the thing that they really want is to be able to define some boundaries (e.g. ""Foo"" is a number between 1 and 10) and to have the static analysis fail to validate the workflow if one of these are a workflow input and the values are wrong. Now that I type that out, having refinement types in WDL seems like a bad path to be going down. I should verify that's *really* what they want or if I read too much into an example they gave.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059:315,Security,validat,validate,315,"Although now that I've said that I know that we *do* have a use case where something like this is being requested. They want a typed set of key/value pairs, but the thing that they really want is to be able to define some boundaries (e.g. ""Foo"" is a number between 1 and 10) and to have the static analysis fail to validate the workflow if one of these are a workflow input and the values are wrong. Now that I type that out, having refinement types in WDL seems like a bad path to be going down. I should verify that's *really* what they want or if I read too much into an example they gave.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:537,Energy Efficiency,efficient,efficiently,537,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:52,Security,validat,validation,52,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:896,Usability,simpl,simplify,896,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656
https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-302877669:26,Availability,error,error,26,"In other PC I got another error "" docker: command not found"". Probably it is because it tries to run docker inside cromwell docker container.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-302877669
https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-345821084:249,Energy Efficiency,schedul,scheduled,249,"@antonkulaga I am reviewing the backlog systematically over the next few months so I am using the PO Cleanup label to keep track of Github issues that I am planning to review next or have already reviewed. It's not a reflection of it being fixed or scheduled, it's just for my tracking purposes. . I haven't reviewed this issue yet, good to know that it is still not functioning as intended. . @geoffjentry Is there a workaround to using Docker-Compose?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-345821084
https://github.com/broadinstitute/cromwell/issues/2285#issuecomment-332234941:23,Testability,log,logs,23,@danbills are the call logs still not localizing on TES?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2285#issuecomment-332234941
https://github.com/broadinstitute/cromwell/issues/2285#issuecomment-332319549:19,Testability,test,test,19,I wasn't unable to test w/ latest funnel TES implementation so I suspect that not only is this not fixed but more things are broken,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2285#issuecomment-332319549
https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107:246,Availability,avail,available,246,Am wondering if these limits should be specified in the conf file. They should be generous and static but I could envision a scenario where one would want to override read_json or something. In that scenario a config change would be nice to have available,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107
https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107:210,Modifiability,config,config,210,Am wondering if these limits should be specified in the conf file. They should be generous and static but I could envision a scenario where one would want to override read_json or something. In that scenario a config change would be nice to have available,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107
https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303434488:125,Deployability,configurat,configuration,125,"Yeah, it was probably lost in the back & forth on that thread but IIRC the numbers were minimums which could be increased by configuration",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303434488
https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303434488:125,Modifiability,config,configuration,125,"Yeah, it was probably lost in the back & forth on that thread but IIRC the numbers were minimums which could be increased by configuration",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303434488
https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303748565:22,Modifiability,Config,Config,22,"closing as I""m adding Config feature so technically ""in progress"". Not 100% comfortable w/ this workflow but we'll see how it goes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303748565
https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-304015691:7,Modifiability,config,config,7,Is the config change worth a changelog/readme comment?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-304015691
https://github.com/broadinstitute/cromwell/issues/2294#issuecomment-303737006:61,Deployability,update,updated,61,This happened because we haven't published a wdltool with an updated wdl4s for a long time. Closing this issue but we should watch this next time we publish.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2294#issuecomment-303737006
https://github.com/broadinstitute/cromwell/issues/2297#issuecomment-310204092:181,Availability,failure,failure,181,"Suspicion confirmed! There is a collector key for each scatter that is a part of the execution store in Cromwell--and it's being abandoned in the ""NotStarted"" state when there is a failure in the Scatter in ContinueWhilePossible mode. Because there is a ""NotStarted"" key in the store--the workflow remains in the ""Running"" state with a job that's not really an executable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2297#issuecomment-310204092
https://github.com/broadinstitute/cromwell/issues/2300#issuecomment-332235119:48,Security,access,access,48,@cjllanwarne can you explain why users use Pair access?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2300#issuecomment-332235119
https://github.com/broadinstitute/cromwell/issues/2308#issuecomment-326322691:305,Integrability,interface,interfaces,305,"tl;dr there's a lot more work to do here and this should not be closed. I made subprojects for `wom`, `wdl`, and `cwl`, but in reality all the WDL stuff was dumped into `wom` rather than `wdl` where it properly belongs. WDL and WOM are currently very entangled, and Cromwell is talking exclusively to WDL interfaces rather than the WOM interfaces it should use.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2308#issuecomment-326322691
https://github.com/broadinstitute/cromwell/issues/2308#issuecomment-326322691:336,Integrability,interface,interfaces,336,"tl;dr there's a lot more work to do here and this should not be closed. I made subprojects for `wom`, `wdl`, and `cwl`, but in reality all the WDL stuff was dumped into `wom` rather than `wdl` where it properly belongs. WDL and WOM are currently very entangled, and Cromwell is talking exclusively to WDL interfaces rather than the WOM interfaces it should use.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2308#issuecomment-326322691
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305898548:490,Security,hash,hashed,490,"Is the FoFN itself important, or is it more about inputting things without localizing them? We were considering earlier something like a `FileRef` type:; ```; workflow foo {; Array[Int] indices; scatter (i in indices) {; call mkfile { input: mkfile_input = i }; }; call use_files { input: fileRefs = mkfile.f }; }. task mkfile {; Int mkfile_input; command { ... }; output {; File f = outfile; }; }. task use_files {; Array[FileRef] fileRefs; command { ... }; }; ```. The `FileRef` would be hashed like a `File` for call caching but not localized by Cromwell to run the task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305898548
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1218,Availability,robust,robust,1218,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:873,Deployability,pipeline,pipeline,873,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1492,Performance,cache,cache,1492,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1619,Performance,cache,cache-miss,1619,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1609,Safety,avoid,avoid,1609,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1755,Safety,avoid,avoidance,1755,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:593,Security,access,access,593,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1728,Security,hash,hashed,1728,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:779,Usability,resume,resume,779,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013:1197,Security,hash,hashes,1197,"On your last question, yeah that's basically it (although there was a request by Lee to refer to it as a `Path` and there was even some discussion that perhaps it should be cloud URL only so e.g. `CloudPath`). Despite the nomenclature the idea would be a type which would be treated as a String except that Cromwell would understand that it represents a file for things like call caching and handle them appropriately. Unlike a `File` the underlying path is never localized, it's always maintained as-is. Coercion between `File` and this new type would be seamless. I'll use `FileRef` for examples w/o necessarily endorsing that term. In a much simpler example where you have a single `FileRef` it'd be treated just like a `String` when it came to a command block, e.g. task foo {; FileRef bar; File baz. command {; grep ${bar} ${baz}; }; }. This would be grepping the delocalized path referenced by `bar` in the contents of the localized file `baz`. For the `Array[FileRef]`/`writelines()` examples, the belief was that if the writelines was in the command block (or the declaration? crap, now I forget which) that call caching would work as desired as the individual `FileRef`s would have their hashes checked *prior* to the FOFN generation. Moving away from your specific situation, I alluded to `CloudPath` above. The reason this came up in a separate context was e.g. GATK4's NIO capability where one doesn't want to be localizing files but does want call caching to be in effect. So there was a request for this concept of a file like thing which stays where it originally is. Some tricky edge cases start coming up when you're talking about actual local files for this and all the examples people came up with were people using cloud based storage, thus .....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013:645,Usability,simpl,simpler,645,"On your last question, yeah that's basically it (although there was a request by Lee to refer to it as a `Path` and there was even some discussion that perhaps it should be cloud URL only so e.g. `CloudPath`). Despite the nomenclature the idea would be a type which would be treated as a String except that Cromwell would understand that it represents a file for things like call caching and handle them appropriately. Unlike a `File` the underlying path is never localized, it's always maintained as-is. Coercion between `File` and this new type would be seamless. I'll use `FileRef` for examples w/o necessarily endorsing that term. In a much simpler example where you have a single `FileRef` it'd be treated just like a `String` when it came to a command block, e.g. task foo {; FileRef bar; File baz. command {; grep ${bar} ${baz}; }; }. This would be grepping the delocalized path referenced by `bar` in the contents of the localized file `baz`. For the `Array[FileRef]`/`writelines()` examples, the belief was that if the writelines was in the command block (or the declaration? crap, now I forget which) that call caching would work as desired as the individual `FileRef`s would have their hashes checked *prior* to the FOFN generation. Moving away from your specific situation, I alluded to `CloudPath` above. The reason this came up in a separate context was e.g. GATK4's NIO capability where one doesn't want to be localizing files but does want call caching to be in effect. So there was a request for this concept of a file like thing which stays where it originally is. Some tricky edge cases start coming up when you're talking about actual local files for this and all the examples people came up with were people using cloud based storage, thus .....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305989869:177,Security,hash,hashed,177,"To clarify @geoffjentry: if `write_lines` is in a declaration (e.g. `File fofn = write_lines(file_refs)`) then that's evaluated and used for CC (and with changeable paths being hashed, CC will miss, just like today). . So to make CC work as you'd want in this new scheme, the `write_lines` would have to be in the command block (e.g. `command { ./foo.sh write_lines(file_refs) }`).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305989869
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305992413:133,Availability,down,down,133,"Aside: I don't think having that sort of subtlety in any language, much less wdl, is particularly awesome. Aside part deux: If we go down this path we should think carefully how it is worded in the spec. WDL shouldn't be aware of call caching, so dropping in a feature that's effectively purely for call caching needs some explanation beyond that :) The whole cloud path/not for localization seems like the right official reason. This is of course assuming that this whole scheme gets @kcibul what he needs in the first place.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305992413
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305999959:266,Performance,cache,cache,266,"Just realized that last comment was especially well related how `write_lines` interacts with CC, maybe we can reconsider how CC interprets actual input values vs. Declarations (i.e. if the inputs are the same then any declarations will be the same too, so let's not cache expression results?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305999959
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035:192,Availability,echo,echo,192,"Hmm we would still need to cache at least the ""unevaluated expression"" I think, whatever that means. Otherwise what about this : . ```; task t1 {; String a; String b = ""hello"" + a; command {; echo ${b}; }; }. task t1 {; String a; String b = a + ""hello""; command {; echo ${b}; }; }. workflow w {; call t1 { input: a = ""a"" }; call t2 { input: a = ""a"" }; }; ```. Same input / command but the result will be different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035:265,Availability,echo,echo,265,"Hmm we would still need to cache at least the ""unevaluated expression"" I think, whatever that means. Otherwise what about this : . ```; task t1 {; String a; String b = ""hello"" + a; command {; echo ${b}; }; }. task t1 {; String a; String b = a + ""hello""; command {; echo ${b}; }; }. workflow w {; call t1 { input: a = ""a"" }; call t2 { input: a = ""a"" }; }; ```. Same input / command but the result will be different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035:27,Performance,cache,cache,27,"Hmm we would still need to cache at least the ""unevaluated expression"" I think, whatever that means. Otherwise what about this : . ```; task t1 {; String a; String b = ""hello"" + a; command {; echo ${b}; }; }. task t1 {; String a; String b = a + ""hello""; command {; echo ${b}; }; }. workflow w {; call t1 { input: a = ""a"" }; call t2 { input: a = ""a"" }; }; ```. Same input / command but the result will be different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:542,Security,hash,hash,542,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:5,Usability,resume,resume,5,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:92,Usability,resume,resume,92,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:146,Usability,resume,resume,146,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:276,Usability,resume,resume,276,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305527188:7,Availability,ping,ping,7,"Please ping @geoffjentry before picking up this ticket, he has some ideas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305527188
https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042:408,Availability,down,down,408,"What I was going to suggest was looking at [WES](https://github.com/ga4gh/workflow-execution-schemas). . - Conceptually it's what we need to do anyways (""Hey, here's a workflow and it's of type X""); - We've signed on to support WES at a future date anyways. It's possible that it doesn't map particularly cleanly and/or doesn't make sense for some other reason but IMO it'd be good to at least poke at going down this path first. There's protobuf & swagger in that repo I linked. If there are complaints/feedback about the API and not the Cromwell implications of the API, let me know and we can surface them w/ the appropriate folks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042
https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042:504,Usability,feedback,feedback,504,"What I was going to suggest was looking at [WES](https://github.com/ga4gh/workflow-execution-schemas). . - Conceptually it's what we need to do anyways (""Hey, here's a workflow and it's of type X""); - We've signed on to support WES at a future date anyways. It's possible that it doesn't map particularly cleanly and/or doesn't make sense for some other reason but IMO it'd be good to at least poke at going down this path first. There's protobuf & swagger in that repo I linked. If there are complaints/feedback about the API and not the Cromwell implications of the API, let me know and we can surface them w/ the appropriate folks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042
https://github.com/broadinstitute/cromwell/issues/2316#issuecomment-332235465:17,Deployability,update,update,17,"@cjllanwarne any update on this issue? Is it still to-do?; Also, what is it for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2316#issuecomment-332235465
https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-305813608:0,Safety,Abort,Abort,0,Abort is definitely not idempotent; I don't think it would be appropriate to make this a GET.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-305813608
https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-307442614:12,Safety,safe,safe,12,"Er I meant ""safe"" where I said ""idempotent"", but still this should remain a POST. https://stackoverflow.com/questions/1254132/so-why-should-we-use-post-instead-of-get-for-posting-data",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-307442614
https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198:119,Availability,error,error,119,I highly recommend using the dsde-toolbox method as it gives you access to the `vault-edit` command which is much less error prone,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198
https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198:65,Security,access,access,65,I highly recommend using the dsde-toolbox method as it gives you access to the `vault-edit` command which is much less error prone,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672:107,Security,Authoriz,Authorization,107,"Per @davidbernick's comments on Slack, this guide should include how Cromwell administrators should set up Authorization, Authentication, Encryption, and Persistent Databases. This guide will be created during the [Doc-A-Thon](https://docs.google.com/document/d/1M5u-ESSpt_eM0ORsvIu2AoOtvYXImPmAKoyvXFo4p9s/edit) in November.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672:122,Security,Authenticat,Authentication,122,"Per @davidbernick's comments on Slack, this guide should include how Cromwell administrators should set up Authorization, Authentication, Encryption, and Persistent Databases. This guide will be created during the [Doc-A-Thon](https://docs.google.com/document/d/1M5u-ESSpt_eM0ORsvIu2AoOtvYXImPmAKoyvXFo4p9s/edit) in November.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672:138,Security,Encrypt,Encryption,138,"Per @davidbernick's comments on Slack, this guide should include how Cromwell administrators should set up Authorization, Authentication, Encryption, and Persistent Databases. This guide will be created during the [Doc-A-Thon](https://docs.google.com/document/d/1M5u-ESSpt_eM0ORsvIu2AoOtvYXImPmAKoyvXFo4p9s/edit) in November.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672:44,Usability,guid,guide,44,"Per @davidbernick's comments on Slack, this guide should include how Cromwell administrators should set up Authorization, Authentication, Encryption, and Persistent Databases. This guide will be created during the [Doc-A-Thon](https://docs.google.com/document/d/1M5u-ESSpt_eM0ORsvIu2AoOtvYXImPmAKoyvXFo4p9s/edit) in November.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672:181,Usability,guid,guide,181,"Per @davidbernick's comments on Slack, this guide should include how Cromwell administrators should set up Authorization, Authentication, Encryption, and Persistent Databases. This guide will be created during the [Doc-A-Thon](https://docs.google.com/document/d/1M5u-ESSpt_eM0ORsvIu2AoOtvYXImPmAKoyvXFo4p9s/edit) in November.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005:201,Deployability,Configurat,ConfigurationFiles,201,The Cromwell Developer's guide is here!; - [Quick Start](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/); - [How to Configure](http://cromwell.readthedocs.io/en/develop/tutorials/ConfigurationFiles/); - [Google Pipelines API quick start](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/); - [Timing Diagrams](http://cromwell.readthedocs.io/en/develop/tutorials/TimingDiagrams/); - [Setup a persistent server](http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/); - [Setup on HPC cluster](http://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/); - [Run in Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/). Checkout the full [Cromwell Docs!](http://cromwell.rtfd.io),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005:233,Deployability,Pipeline,Pipelines,233,The Cromwell Developer's guide is here!; - [Quick Start](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/); - [How to Configure](http://cromwell.readthedocs.io/en/develop/tutorials/ConfigurationFiles/); - [Google Pipelines API quick start](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/); - [Timing Diagrams](http://cromwell.readthedocs.io/en/develop/tutorials/TimingDiagrams/); - [Setup a persistent server](http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/); - [Setup on HPC cluster](http://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/); - [Run in Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/). Checkout the full [Cromwell Docs!](http://cromwell.rtfd.io),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005:138,Modifiability,Config,Configure,138,The Cromwell Developer's guide is here!; - [Quick Start](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/); - [How to Configure](http://cromwell.readthedocs.io/en/develop/tutorials/ConfigurationFiles/); - [Google Pipelines API quick start](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/); - [Timing Diagrams](http://cromwell.readthedocs.io/en/develop/tutorials/TimingDiagrams/); - [Setup a persistent server](http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/); - [Setup on HPC cluster](http://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/); - [Run in Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/). Checkout the full [Cromwell Docs!](http://cromwell.rtfd.io),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005:201,Modifiability,Config,ConfigurationFiles,201,The Cromwell Developer's guide is here!; - [Quick Start](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/); - [How to Configure](http://cromwell.readthedocs.io/en/develop/tutorials/ConfigurationFiles/); - [Google Pipelines API quick start](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/); - [Timing Diagrams](http://cromwell.readthedocs.io/en/develop/tutorials/TimingDiagrams/); - [Setup a persistent server](http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/); - [Setup on HPC cluster](http://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/); - [Run in Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/). Checkout the full [Cromwell Docs!](http://cromwell.rtfd.io),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005:25,Usability,guid,guide,25,The Cromwell Developer's guide is here!; - [Quick Start](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/); - [How to Configure](http://cromwell.readthedocs.io/en/develop/tutorials/ConfigurationFiles/); - [Google Pipelines API quick start](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/); - [Timing Diagrams](http://cromwell.readthedocs.io/en/develop/tutorials/TimingDiagrams/); - [Setup a persistent server](http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/); - [Setup on HPC cluster](http://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/); - [Run in Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/). Checkout the full [Cromwell Docs!](http://cromwell.rtfd.io),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005
https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332640764:72,Security,hash,hash,72,@Horneth can you explain more about what the problem is with the Docker hash lookups?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332640764
https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637:174,Availability,down,down,174,"We currently support dockerhub, google container registry and quay as far as looking up hashes is concerned. With the way things are currently, if one of those services goes down or has a high error rate forcing us to retry a lot, all lookups will potentially be slowed down even if they target a service that is doing fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637
https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637:193,Availability,error,error,193,"We currently support dockerhub, google container registry and quay as far as looking up hashes is concerned. With the way things are currently, if one of those services goes down or has a high error rate forcing us to retry a lot, all lookups will potentially be slowed down even if they target a service that is doing fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637
https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637:270,Availability,down,down,270,"We currently support dockerhub, google container registry and quay as far as looking up hashes is concerned. With the way things are currently, if one of those services goes down or has a high error rate forcing us to retry a lot, all lookups will potentially be slowed down even if they target a service that is doing fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637
https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637:88,Security,hash,hashes,88,"We currently support dockerhub, google container registry and quay as far as looking up hashes is concerned. With the way things are currently, if one of those services goes down or has a high error rate forcing us to retry a lot, all lookups will potentially be slowed down even if they target a service that is doing fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637
https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399:158,Performance,perform,performance,158,"As a **user running workflows**, I want **Cromwell to split up its docker hashes by registry**, so that **if one registry is slow, that it doesn't affect the performance of the other registries**.; - Effort: Small to medium; - Risk: Small; - Business value: Small to medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399
https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399:227,Safety,Risk,Risk,227,"As a **user running workflows**, I want **Cromwell to split up its docker hashes by registry**, so that **if one registry is slow, that it doesn't affect the performance of the other registries**.; - Effort: Small to medium; - Risk: Small; - Business value: Small to medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399
https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399:74,Security,hash,hashes,74,"As a **user running workflows**, I want **Cromwell to split up its docker hashes by registry**, so that **if one registry is slow, that it doesn't affect the performance of the other registries**.; - Effort: Small to medium; - Risk: Small; - Business value: Small to medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306233790:22,Performance,cache,cached-to,22,Should we confirm the cached-to files are still where they are supposed to be and accessible?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306233790
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306233790:82,Security,access,accessible,82,Should we confirm the cached-to files are still where they are supposed to be and accessible?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306233790
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306236760:145,Security,hash,hashing,145,"SFS does not have a ""use original path"" AFAIK. It has ""copy"", ""hard-link"" and ""soft-link"".; Those are ""duplication strategies"".; There are also ""hashing strategies"", for file, which can be ""hash file content"" or ""hash file path"". With an optional ""use sibling md5 if it exists"".; I guess the question is, how much of all that do we want to standardize.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306236760
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306236760:190,Security,hash,hash,190,"SFS does not have a ""use original path"" AFAIK. It has ""copy"", ""hard-link"" and ""soft-link"".; Those are ""duplication strategies"".; There are also ""hashing strategies"", for file, which can be ""hash file content"" or ""hash file path"". With an optional ""use sibling md5 if it exists"".; I guess the question is, how much of all that do we want to standardize.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306236760
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306236760:213,Security,hash,hash,213,"SFS does not have a ""use original path"" AFAIK. It has ""copy"", ""hard-link"" and ""soft-link"".; Those are ""duplication strategies"".; There are also ""hashing strategies"", for file, which can be ""hash file content"" or ""hash file path"". With an optional ""use sibling md5 if it exists"".; I guess the question is, how much of all that do we want to standardize.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306236760
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306249534:127,Security,hash,hash,127,"Linking vs not doing anything still does have an impact regarding call caching.; For example, currently on SFS if you use the ""hash the file path"" strategy, then you have to use symlink in order for it to work otherwise we have no way to know the original file path and hence call caching breaks. If we provided the ""don't do anything option"" on SFS too it would be another way to get call caching to work with ""file path hashing"".; I'm just saying there is a soup of options regarding hashing and file duplication that would be nice to get as coherent as possible (even though like you said linking on GCS doesn't make sense). And I agree that all of this might very well be an implementation detail :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306249534
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306249534:422,Security,hash,hashing,422,"Linking vs not doing anything still does have an impact regarding call caching.; For example, currently on SFS if you use the ""hash the file path"" strategy, then you have to use symlink in order for it to work otherwise we have no way to know the original file path and hence call caching breaks. If we provided the ""don't do anything option"" on SFS too it would be another way to get call caching to work with ""file path hashing"".; I'm just saying there is a soup of options regarding hashing and file duplication that would be nice to get as coherent as possible (even though like you said linking on GCS doesn't make sense). And I agree that all of this might very well be an implementation detail :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306249534
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306249534:486,Security,hash,hashing,486,"Linking vs not doing anything still does have an impact regarding call caching.; For example, currently on SFS if you use the ""hash the file path"" strategy, then you have to use symlink in order for it to work otherwise we have no way to know the original file path and hence call caching breaks. If we provided the ""don't do anything option"" on SFS too it would be another way to get call caching to work with ""file path hashing"".; I'm just saying there is a soup of options regarding hashing and file duplication that would be nice to get as coherent as possible (even though like you said linking on GCS doesn't make sense). And I agree that all of this might very well be an implementation detail :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306249534
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306282239:134,Availability,down,down,134,Discussed in person w/ @Horneth - we'll keep the implementation specific split for now. It seems likely that in the future it'll boil down to a cloud/not-cloud split but at the moment it's SFS/PAPI,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306282239
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816:40,Performance,cache,cached-to,40,@geoffjentry probably better to treat a cached-to 404 the same as a failed cache hit copy so we can fall back to trying another cache hit rather than re-running the job.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816:75,Performance,cache,cache,75,@geoffjentry probably better to treat a cached-to 404 the same as a failed cache hit copy so we can fall back to trying another cache hit rather than re-running the job.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816:128,Performance,cache,cache,128,@geoffjentry probably better to treat a cached-to 404 the same as a failed cache hit copy so we can fall back to trying another cache hit rather than re-running the job.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306308816
https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-375766475:118,Performance,cache,cache,118,@geoffjentry does this mean that users can choose to link to the results rather than copy the results when they get a cache hit?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-375766475
https://github.com/broadinstitute/cromwell/issues/2333#issuecomment-313467275:13,Safety,abort,abort,13,"@ruchim Did ""abort by label"" functionality go into your work on Cromwell 28? If not that's fine, just wanted to check if I should close this or reprioritize.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2333#issuecomment-313467275
https://github.com/broadinstitute/cromwell/issues/2334#issuecomment-314076887:49,Security,access,access,49,"I've seen issues before where people struggle to access entries in pairs of pairs. I know we're planning another round of WDL fixup soon. My interim advice would be to begin by extracting the first layer of left and right as separate declarations in the scatter, e.g. ```; scatter(p in pairs); Pair[A,B] lefts = p.left; Pair[C,D] rights = p.right. call x { input: i = lefts.left}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2334#issuecomment-314076887
https://github.com/broadinstitute/cromwell/pull/2335#issuecomment-306906462:28,Integrability,rout,route,28,"If we're going the metadata route, which I think I like too, given that w're ok with the fact that it will likely nearly double the size of the full metadata response, I think there's no point merging this because half of it will be deleted in the next PR no ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2335#issuecomment-306906462
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:68,Deployability,integrat,integrated,68,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:449,Deployability,integrat,integrated,449,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:23,Energy Efficiency,Green,Green,23,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:68,Integrability,integrat,integrated,68,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:449,Integrability,integrat,integrated,449,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:722,Modifiability,config,configure,722,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:810,Modifiability,config,configure,810,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:959,Modifiability,config,config,959,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:1289,Performance,load,load,1289,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:113,Testability,test,test,113,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:191,Testability,test,test,191,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:366,Testability,test,test,366,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:460,Testability,test,test,460,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:805,Testability,test,test,805,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:852,Testability,test,test,852,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:940,Testability,test,test,940,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:995,Testability,test,test,995,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:1214,Testability,test,test,1214,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:1330,Testability,test,test,1330,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history üò° so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:314,Availability,down,downstream,314,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:49,Performance,cache,cache,49,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:140,Performance,cache,cache,140,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:79,Security,access,accessing,79,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:146,Security,hash,hashes,146,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781:240,Availability,failure,failures,240,"Interesting point!. I like the ~~idea~~ philosophy of CC tables being engine only, and queries being completely and solely calculable from metadata. It would probably mean ~~piping~~ forwarding all CC hashes, toggles of ""allowResultReuse"", failures to copy results, etc to the metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781:209,Deployability,toggle,toggles,209,"Interesting point!. I like the ~~idea~~ philosophy of CC tables being engine only, and queries being completely and solely calculable from metadata. It would probably mean ~~piping~~ forwarding all CC hashes, toggles of ""allowResultReuse"", failures to copy results, etc to the metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781:201,Security,hash,hashes,201,"Interesting point!. I like the ~~idea~~ philosophy of CC tables being engine only, and queries being completely and solely calculable from metadata. It would probably mean ~~piping~~ forwarding all CC hashes, toggles of ""allowResultReuse"", failures to copy results, etc to the metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781
https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-326085665:5,Performance,cache,cache,5,Call cache diffing did end up being written against metadata so what this ticket is asking for is no longer needed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-326085665
https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786:26,Deployability,update,update,26,"@IsanEmory thanks for the update, that's good to know. ; @ruchim can you tell me a bit more about what's going on to cause the message to show up?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786
https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786:127,Integrability,message,message,127,"@IsanEmory thanks for the update, that's good to know. ; @ruchim can you tell me a bit more about what's going on to cause the message to show up?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786
https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332242945:193,Integrability,message,message,193,There is an open ticket on the akka github for this: https://github.com/akka/akka-http/issues/907; It's out of our control AFAICT. We've taken every precaution to shutdown things in order. The message is harmless although confusing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332242945
https://github.com/broadinstitute/cromwell/issues/2344#issuecomment-314125428:94,Integrability,interface,interface,94,"@gemlam3 No they're different. This ticket is for the REST API, #2345 is for the command line interface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2344#issuecomment-314125428
https://github.com/broadinstitute/cromwell/issues/2346#issuecomment-325698981:5,Integrability,depend,depending,5,"Well depending on how different a given language becomes between versions we might want to route to different processor implementations. But again, when N > 2.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2346#issuecomment-325698981
https://github.com/broadinstitute/cromwell/issues/2346#issuecomment-325698981:91,Integrability,rout,route,91,"Well depending on how different a given language becomes between versions we might want to route to different processor implementations. But again, when N > 2.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2346#issuecomment-325698981
https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116:367,Availability,down,downstream,367,"@mcovarr that's true, I think it would fail eventually anyway whenever something tries to access those files (or not if nothing does ?) ? And I thought that one of the assumption in this ""no-copy"" mode is that we expect the files to be relatively immutable anyway. . But I also agree that it would be cleaner to fail the ""caching"" if they don't exist rather than the downstream task failing by itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116
https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116:90,Security,access,access,90,"@mcovarr that's true, I think it would fail eventually anyway whenever something tries to access those files (or not if nothing does ?) ? And I thought that one of the assumption in this ""no-copy"" mode is that we expect the files to be relatively immutable anyway. . But I also agree that it would be cleaner to fail the ""caching"" if they don't exist rather than the downstream task failing by itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116
https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307426337:61,Availability,avail,available,61,Yeah and we can fall back to try another cache hit if one is available. @geoffjentry feel free to leave the totem on my desk.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307426337
https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307426337:41,Performance,cache,cache,41,Yeah and we can fall back to try another cache hit if one is available. @geoffjentry feel free to leave the totem on my desk.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307426337
https://github.com/broadinstitute/cromwell/issues/2350#issuecomment-326413750:46,Testability,test,test,46,this is actually done as it's how the restart test works,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2350#issuecomment-326413750
https://github.com/broadinstitute/cromwell/pull/2357#issuecomment-310082912:41,Deployability,update,updated,41,Oh looks like the API client needs to be updated too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2357#issuecomment-310082912
https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:1047,Performance,cache,cache,1047,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081
https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:431,Security,hash,hash,431,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081
https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:501,Security,hash,hash,501,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081
https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:526,Security,hash,hash,526,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081
https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:547,Security,hash,hashes,547,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081
https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:983,Security,hash,hash,983,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081
https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:1198,Security,hash,hash,1198,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718:292,Energy Efficiency,schedul,scheduler,292,Hi @seandavi - you'll want to use the `concurrent-job-limit` field in your backend config (see the `reference.conf` for more details) which was put in for exactly this reason. It's a crude implement but the local backend was intended more as a debug/noodling around backend and not a full on scheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718:83,Modifiability,config,config,83,Hi @seandavi - you'll want to use the `concurrent-job-limit` field in your backend config (see the `reference.conf` for more details) which was put in for exactly this reason. It's a crude implement but the local backend was intended more as a debug/noodling around backend and not a full on scheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718:39,Performance,concurren,concurrent-job-limit,39,Hi @seandavi - you'll want to use the `concurrent-job-limit` field in your backend config (see the `reference.conf` for more details) which was put in for exactly this reason. It's a crude implement but the local backend was intended more as a debug/noodling around backend and not a full on scheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-326080916:54,Performance,concurren,concurrent-job-limit,54,"The cpu and memory usage points are not addressed by `concurrent-job-limit`, so I've labeled this for PO review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-326080916
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117:140,Safety,risk,risk,140,@seandavi are you still interested in being able to limit CPU and memory when running on local? ; @geoffjentry what would be the effort and risk for adding these parameters?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333242232:440,Safety,risk,risk,440,"It would be a nice feature, particularly for folks with a single large box,; but it is not something that I see myself using regularly, to be honest. On Fri, Sep 29, 2017 at 5:16 PM, Kate Voss <notifications@github.com> wrote:. > @seandavi <https://github.com/seandavi> are you still interested in being; > able to limit CPU and memory when running on local?; > @geoffjentry <https://github.com/geoffjentry> what would be the effort; > and risk for adding these parameters?; >; > ‚Äî; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE2iSooOX2P5Vj9bQYMoikzonywc7ks5snV4YgaJpZM4N9Ob6>; > .; >. -- ; Sean Davis, MD, PhD; Center for Cancer Research; National Cancer Institute; National Institutes of Health; Bethesda, MD 20892; https://seandavi.github.io/; https://twitter.com/seandavis12",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333242232
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:508,Availability,echo,echo,508,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:791,Testability,test,test,791,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:63,Usability,simpl,simple,63,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:501,Usability,simpl,simple,501,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:182,Integrability,interface,interface,182,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:34,Performance,concurren,concurrent-job-limit,34,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:730,Performance,concurren,concurrent-job-limit,730,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:431,Safety,safe,safely,431,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310670892:58,Testability,test,test,58,"üëç . I don't know if we already have that, but a ticket to test this would be nice I think. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2369/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310670892
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310722510:87,Modifiability,enhance,enhancementy,87,@Horneth I don't think there is such a ticket currently but that makes sense as a test enhancementy sort of thing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310722510
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310722510:82,Testability,test,test,82,@Horneth I don't think there is such a ticket currently but that makes sense as a test enhancementy sort of thing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310722510
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311425862:34,Availability,ping,pinging,34,If you haven't already it's worth pinging @vdauwera to let her know of the incoming change,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311425862
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311427326:35,Availability,ping,pinged,35,"@vdauwera you have officially been pinged. üòÑ . As I mentioned above I don't have strong opinions about where the documentation ends up, but I put it here for now so I could edit it. If the content remains here we should either take down or mark as ""pre-28"" the content that's on the forums.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311427326
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311427326:232,Availability,down,down,232,"@vdauwera you have officially been pinged. üòÑ . As I mentioned above I don't have strong opinions about where the documentation ends up, but I put it here for now so I could edit it. If the content remains here we should either take down or mark as ""pre-28"" the content that's on the forums.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311427326
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311445886:53,Availability,Ping,Ping,53,"Did someone say breaking changes? *starts sweating*. Ping is much appreciated. Could we plan to go over the changes together so that you can help us identify what needs to change in our user docs / course materials? . Re: location of docs, @katevoss and I have been discussing this a fair amount, and the latest proto-consensus we came to was that the forum is ultimately not the right place for Cromwell docs proper (as opposed to the end-user/n00b materials that Comms produces) in part because it's too detached from the codebase itself. Based on the audience and technical constraints, we think something like ReadTheDocs will be a more suitable platform. Kate is obviously going to be otherwise occupied for several more weeks, but I might be able to take a stab at prototyping what a ""Read the (Cromwell) Docs"" solution would look and feel like in a week or two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311445886
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311446204:9,Usability,clear,clear,9,So to be clear: moving docs from the forum back to here is a step in the right direction in any case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311446204
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311461964:85,Usability,feedback,feedback,85,"@vdauwera I'd be happy to go over the changes in person, and I'd also appreciate any feedback you have on the changes to the `README.md` here. üòÑ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311461964
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312978827:24,Availability,ping,ping,24,@mcovarr Can you please ping me when you guys are close to revisiting this? I don't have bandwidth right now but would definitely like to go over this before it hits. Do you have a sense of timeline?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312978827
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980064:50,Deployability,release,release,50,"@vdauwera We deliberately kept this out of the 28 release pending discussion. We'd certainly like to include this in 29, but that release won't go out for a while. I'd be happy to meet with you this week to discuss.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980064
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980064:130,Deployability,release,release,130,"@vdauwera We deliberately kept this out of the 28 release pending discussion. We'd certainly like to include this in 29, but that release won't go out for a while. I'd be happy to meet with you this week to discuss.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980064
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980238:95,Deployability,release,release,95,"@vdauwera I think it's basically ready to go now, we didn't want to go live until after the 28 release last week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980238
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313215779:78,Deployability,update,update,78,oh i see that's great -- i never liked positional args. thanks! we'll plan to update docs accordingly -- when do you expect to release this/29?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313215779
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313215779:127,Deployability,release,release,127,oh i see that's great -- i never liked positional args. thanks! we'll plan to update docs accordingly -- when do you expect to release this/29?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313215779
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313241466:56,Deployability,update,update,56,"Sounds good to me, thanks. I'll make a ticket for us to update our wdl user docs with a note about the upcoming change; y'all go ahead with this as you like.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313241466
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487:42,Deployability,update,update,42,I posted a heads up in the WDL blog; will update WDL user docs when v29 is released. Any update on its ETA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487:75,Deployability,release,released,75,I posted a heads up in the WDL blog; will update WDL user docs when v29 is released. Any update on its ETA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487:89,Deployability,update,update,89,I posted a heads up in the WDL blog; will update WDL user docs when v29 is released. Any update on its ETA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487
https://github.com/broadinstitute/cromwell/pull/2372#issuecomment-310696609:258,Integrability,message,message,258,"ToL: I was wondering whether 404 is the right code here, but I think it probably is... . My concern is that ""can't diff because your job is pre-28"" should be distinguishable from ""can't diff because your job doesn't exist"". EDIT: I think it's OK because the message string is appropriately different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2372#issuecomment-310696609
https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937:251,Availability,down,down,251,"More precisely, come up with a mechanism that ; 1) Gets all ""workflow-related"" actors to stop doing more work and make sure all DB write operations have been sent to ensure consistency.; 2) Waits for all DB write actors to empty their queue; 3) Shuts down the JVM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937
https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937:235,Performance,queue,queue,235,"More precisely, come up with a mechanism that ; 1) Gets all ""workflow-related"" actors to stop doing more work and make sure all DB write operations have been sent to ensure consistency.; 2) Waits for all DB write actors to empty their queue; 3) Shuts down the JVM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937
https://github.com/broadinstitute/cromwell/pull/2376#issuecomment-310902032:41,Testability,test,test,41,Results from the ExecutionStoreBenchmark test:. Develop; ```; [info] Parameters(size -> 1000): 2.4676951944444445; [info] Parameters(size -> 2000): 5.349590333333333; [info] Parameters(size -> 3000): 5.831528583333333; [info] Parameters(size -> 4000): 7.982331194444441; [info] Parameters(size -> 5000): 9.749072833333331; [info] Parameters(size -> 6000): 11.938545027777783; [info] Parameters(size -> 7000): 17.60429; [info] Parameters(size -> 8000): 23.686682388888883; [info] Parameters(size -> 9000): 24.980248888888887; [info] Parameters(size -> 10000): 22.37216330555555; ```. f57cf54 (what was reviewed); ```; [info] Parameters(size -> 1000): 2.2082013611111115; [info] Parameters(size -> 2000): 5.0982545833333335; [info] Parameters(size -> 3000): 5.249347527777778; [info] Parameters(size -> 4000): 6.8714216666666665; [info] Parameters(size -> 5000): 8.793947694444444; [info] Parameters(size -> 6000): 11.467324361111112; [info] Parameters(size -> 7000): 16.03606438888889; [info] Parameters(size -> 8000): 21.666372750000004; [info] Parameters(size -> 9000): 23.281565222222227; [info] Parameters(size -> 10000): 19.686238916666667; ```. c6c5051 (edits based on PR suggestions); ```; [info] Parameters(size -> 1000): 2.486097944444445; [info] Parameters(size -> 2000): 5.519608888888889; [info] Parameters(size -> 3000): 5.755341555555556; [info] Parameters(size -> 4000): 8.09441233333333; [info] Parameters(size -> 5000): 10.278697527777778; [info] Parameters(size -> 6000): 12.225090000000002; [info] Parameters(size -> 7000): 16.643762055555552; [info] Parameters(size -> 8000): 23.817459083333333; [info] Parameters(size -> 9000): 24.26833838888888; [info] Parameters(size -> 10000): 22.043697416666664; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2376#issuecomment-310902032
https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-332641113:46,Availability,error,error,46,@danbills can you explain more about when the error occurs? Do you have an idea of how much effort it would take to fix?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-332641113
https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092:252,Deployability,patch,patching,252,"As a **user running workflows**, I want to **see my stderr output even when Cromwell gets a ""FileNotFound"" response** so that I can **debug my workflow**.; - Effort: Small to Medium; - We're not sure of the exact way to fix it, so for now we have been patching the issue.; - Risk: Small; - Business value: Small; - There is a workaround, to manually look up the stderr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092
https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092:275,Safety,Risk,Risk,275,"As a **user running workflows**, I want to **see my stderr output even when Cromwell gets a ""FileNotFound"" response** so that I can **debug my workflow**.; - Effort: Small to Medium; - We're not sure of the exact way to fix it, so for now we have been patching the issue.; - Risk: Small; - Business value: Small; - There is a workaround, to manually look up the stderr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092
https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336165593:3,Usability,intuit,intuition,3,"My intuition is what @Horneth said, that this is an artifact of something we've seen before. It is possible to sorta-fix-it-fix-it (e.g. write out a ""i'm totally done"" file at the very end and not read anything until then) but that has its own problems, including some which lead to the ""sorta-"" prefix there. It's worth noting that this isn't a problem unique to Cromwell, it pops up fairly regularly with these things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336165593
https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336166266:54,Deployability,patch,patching,54,In that case is it a won't-fix situation? We can keep patching if necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336166266
https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731:104,Availability,down,downstream,104,"Also, regardless of where things stand, this will *not* be part of the upcoming release. I want to give downstream users a chance to more thoroughly vet this in case there are subtle changes not picked up by our testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731
https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731:80,Deployability,release,release,80,"Also, regardless of where things stand, this will *not* be part of the upcoming release. I want to give downstream users a chance to more thoroughly vet this in case there are subtle changes not picked up by our testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731
https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731:212,Testability,test,testing,212,"Also, regardless of where things stand, this will *not* be part of the upcoming release. I want to give downstream users a chance to more thoroughly vet this in case there are subtle changes not picked up by our testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731
https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311795514:229,Performance,cache,cache,229,"Ok, now a legit PR. @mcovarr does your thumb still hold? Changes were fairly minimal since you looked but feel free to comment. @Horneth you already did some digging and some of the followup changes involved pulling in your call cache 404 stuff, can you take a look?. @ruchim can you take a look at your label stuff?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311795514
https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311529864:28,Performance,queue,queue,28,It may be worth setting the queue size to something huge or even unlimited as a stop gap measure. In the settings conf file set put in database -> db `queueSize = -1`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311529864
https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311529864:151,Performance,queue,queueSize,151,It may be worth setting the queue size to something huge or even unlimited as a stop gap measure. In the settings conf file set put in database -> db `queueSize = -1`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311529864
https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510:74,Availability,reliab,reliably,74,"One thing I'd like to see before trying stuff is to have a setup that can reliably reproduce the problem and allows us to at least sorta kinda measure how effective our stopgaps & solution attempts are. . For instance, is it possible that cranking the queue size causes something else to fall over? It'd be good to know that before diving in. (Probably not, but just trying to make the point)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510
https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510:252,Performance,queue,queue,252,"One thing I'd like to see before trying stuff is to have a setup that can reliably reproduce the problem and allows us to at least sorta kinda measure how effective our stopgaps & solution attempts are. . For instance, is it possible that cranking the queue size causes something else to fall over? It'd be good to know that before diving in. (Probably not, but just trying to make the point)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510
https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332594156:133,Testability,test,tested,133,"funnel is the open source implementation of Task Execution Schemas (TES). Seen here: https://github.com/ohsu-comp-bio/funnel. I just tested latest version and had a bad time, so this ticket is probably more work than bumping a version number.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332594156
https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270:95,Deployability,release,release,95,@danbills I'd be happy to help debug any funnel issues you run into. Were you testing the v0.2 release or the latest on our master branch?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270
https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270:78,Testability,test,testing,78,@danbills I'd be happy to help debug any funnel issues you run into. Were you testing the v0.2 release or the latest on our master branch?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270
https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332612226:51,Deployability,release,release,51,"@adamstruck Thanks for the offer! I was using v0.2 release, and I'm guessing it's something cromwell is doing and not you guys :). I will look again later this week and check back in.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332612226
https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515:195,Safety,Risk,Risk,195,"As a **user running a backend via TES**, I want **Cromwell to test the latest TES version as exemplified by Funnel**, so that **regressions are caught in testing**.; * Effort: Small to Medium; * Risk: Small; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515
https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515:62,Testability,test,test,62,"As a **user running a backend via TES**, I want **Cromwell to test the latest TES version as exemplified by Funnel**, so that **regressions are caught in testing**.; * Effort: Small to Medium; * Risk: Small; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515
https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515:154,Testability,test,testing,154,"As a **user running a backend via TES**, I want **Cromwell to test the latest TES version as exemplified by Funnel**, so that **regressions are caught in testing**.; * Effort: Small to Medium; * Risk: Small; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515
https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-424942895:88,Testability,test,tests,88,Probably not. We should consider having both a TESK and Funnel based TES set of centaur tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-424942895
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-332633298:31,Deployability,release,release,31,@Horneth is this issue to auto-release wdltool when we release Cromwell?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-332633298
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-332633298:55,Deployability,release,release,55,@Horneth is this issue to auto-release wdltool when we release Cromwell?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-332633298
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:302,Availability,error,error,302,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:46,Deployability,release,released,46,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:89,Deployability,release,release,89,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:118,Deployability,release,released,118,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:262,Deployability,release,release,262,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:166,Safety,Risk,Risk,166,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:288,Safety,risk,risk,288,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089
https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-424933416:18,Deployability,release,release,18,I believe we auto release womtool now. @Horneth please re-open if I missed something.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-424933416
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405:7,Deployability,release,release,7,It's a release improvement ticket. Currently the release WDL creates a github release even if the build fails on the master branch right after merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405:49,Deployability,release,release,49,It's a release improvement ticket. Currently the release WDL creates a github release even if the build fails on the master branch right after merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405:78,Deployability,release,release,78,It's a release improvement ticket. Currently the release WDL creates a github release even if the build fails on the master branch right after merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:78,Deployability,release,release,78,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:144,Deployability,release,release,144,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:330,Deployability,release,released,330,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:110,Energy Efficiency,green,green,110,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:193,Safety,risk,risk,193,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964
https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-424931955:94,Deployability,release,release,94,I believe this has been done already: https://github.com/broadinstitute/cromwell/blob/develop/release/release_workflow.wdl#L24-L33. @Horneth please re-open if I misunderstood.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-424931955
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333240777:61,Deployability,release,release,61,@Horneth why is it useful to have one github account for the release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333240777
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563:83,Security,secur,security,83,Because the alternative is to use personal github tokens which is not great from a security and re-usability perspective,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563:99,Usability,usab,usability,99,Because the alternative is to use personal github tokens which is not great from a security and re-usability perspective,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648:46,Deployability,release,release,46,"As a **Cromwell dev**, I want **to be able to release Cromwell with the same Github account**, so that **I don't have to use my personal github token.**. - effort: small; - risk: small; - business value: small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648:173,Safety,risk,risk,173,"As a **Cromwell dev**, I want **to be able to release Cromwell with the same Github account**, so that **I don't have to use my personal github token.**. - effort: small; - risk: small; - business value: small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935:37,Deployability,release,release,37,"@rsasch I believe you're next on the release rotation. This might be a nice thing to do prior to the next release. Once an account has been generated, its likely that the release checklist needs a first step of ""generate a token for this user"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935:106,Deployability,release,release,106,"@rsasch I believe you're next on the release rotation. This might be a nice thing to do prior to the next release. Once an account has been generated, its likely that the release checklist needs a first step of ""generate a token for this user"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935:171,Deployability,release,release,171,"@rsasch I believe you're next on the release rotation. This might be a nice thing to do prior to the next release. Once an account has been generated, its likely that the release checklist needs a first step of ""generate a token for this user"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935
https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333240686:43,Deployability,release,release,43,@Horneth what else needs to happen for the release to be a push button process?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333240686
https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:98,Availability,avail,available,98,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328
https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:144,Deployability,release,release,144,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328
https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:157,Energy Efficiency,monitor,monitoring,157,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328
https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:118,Security,access,access,118,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328
https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-414084949:15,Safety,safe,safe,15,@Horneth Is it safe to say that this is close-able? We can add new tickets for specific change requests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-414084949
https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-331550505:74,Deployability,release,release,74,@Horneth are there any more issues for this Epic? ; @gemlam3 do we have a release improvement selected for the next release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-331550505
https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-331550505:116,Deployability,release,release,116,@Horneth are there any more issues for this Epic? ; @gemlam3 do we have a release improvement selected for the next release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-331550505
https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332001490:190,Deployability,release,release,190,Here is the [Google Doc](https://docs.google.com/document/d/14cOvS3zdG5R_54R5PuM83piE1QjzTzCk8-NMKOx_Qp0/edit?ts=59692ac4) with existing ideas. . @danbills did you make any other issues for release improvement?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332001490
https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:58,Deployability,release,release,58,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666
https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:116,Deployability,release,release,116,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666
https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:138,Integrability,depend,depends,138,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666
https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:175,Integrability,depend,depends,175,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666
https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:162,Safety,Risk,Risk,162,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666
https://github.com/broadinstitute/cromwell/pull/2408#issuecomment-312443688:2,Availability,down,download,2,![download](https://user-images.githubusercontent.com/961771/27764039-f12b841e-5e5d-11e7-9c9e-2d12766fbacd.jpg). üëç . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2408/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2408#issuecomment-312443688
https://github.com/broadinstitute/cromwell/issues/2410#issuecomment-333265726:37,Deployability,pipeline,pipelines,37,"We have RNA-Seq and de-novo assembly pipelines with many sub-workflows, forgetting to include a file or two is a common mistake that people make in the lab. Another inconvenience is that we have sub workflows in subfolders, and we do not know how to both keep references (incl. subfolders) to sub-workflows in the main WDL script (so IntelliJ idea WDL plugin can check that they are correct) but at the same time - send them as a lot of files via REST. I think being able to pack everything into one zip archive(preserving subfolder structure) and send it to REST API will allow tracking relative paths properly.; And the last argument is that sending only one file (instead of 5-15) to REST API is way more convenient for users!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2410#issuecomment-333265726
https://github.com/broadinstitute/cromwell/issues/2410#issuecomment-333265726:352,Modifiability,plugin,plugin,352,"We have RNA-Seq and de-novo assembly pipelines with many sub-workflows, forgetting to include a file or two is a common mistake that people make in the lab. Another inconvenience is that we have sub workflows in subfolders, and we do not know how to both keep references (incl. subfolders) to sub-workflows in the main WDL script (so IntelliJ idea WDL plugin can check that they are correct) but at the same time - send them as a lot of files via REST. I think being able to pack everything into one zip archive(preserving subfolder structure) and send it to REST API will allow tracking relative paths properly.; And the last argument is that sending only one file (instead of 5-15) to REST API is way more convenient for users!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2410#issuecomment-333265726
https://github.com/broadinstitute/cromwell/issues/2411#issuecomment-333239831:42,Testability,test,testing,42,@geoffjentry what would be the effort for testing this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2411#issuecomment-333239831
https://github.com/broadinstitute/cromwell/issues/2413#issuecomment-313128986:31,Availability,error,error,31,"Ups, looks like I've made type error, closing this",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2413#issuecomment-313128986
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761:12,Safety,abort,aborting,12,"As a **user aborting workflows**, I want to **get a 403 status after aborting a workflow**, so that **I know that my workflow is going to abort**.; * Effort: Small; * Decide whether we want to keep the 403 status; * @geoffjentry what does the 403 status gain us or users?; * Risk: Small; * Business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761:69,Safety,abort,aborting,69,"As a **user aborting workflows**, I want to **get a 403 status after aborting a workflow**, so that **I know that my workflow is going to abort**.; * Effort: Small; * Decide whether we want to keep the 403 status; * @geoffjentry what does the 403 status gain us or users?; * Risk: Small; * Business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761:138,Safety,abort,abort,138,"As a **user aborting workflows**, I want to **get a 403 status after aborting a workflow**, so that **I know that my workflow is going to abort**.; * Effort: Small; * Decide whether we want to keep the 403 status; * @geoffjentry what does the 403 status gain us or users?; * Risk: Small; * Business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761:275,Safety,Risk,Risk,275,"As a **user aborting workflows**, I want to **get a 403 status after aborting a workflow**, so that **I know that my workflow is going to abort**.; * Effort: Small; * Decide whether we want to keep the 403 status; * @geoffjentry what does the 403 status gain us or users?; * Risk: Small; * Business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:362,Availability,error,error,362,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:434,Energy Efficiency,energy,energy,434,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:58,Safety,abort,abort,58,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:153,Usability,Clear,Clearly,153,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379
https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332687013:140,Safety,abort,abort,140,"But someone is one of our most important stakeholders! üòõ. Totally agree with closing this, we can work out the desired behavior when we fix abort in Cromwell 50.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332687013
https://github.com/broadinstitute/cromwell/issues/2418#issuecomment-336300094:141,Availability,error,errors,141,"It still exists and no one ever determined if it was appropriate, so it is still potentially relevant. It's just not how we typically handle errors that occur in API calls so it looked weird. May or may not actually be an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2418#issuecomment-336300094
https://github.com/broadinstitute/cromwell/issues/2420#issuecomment-333186989:170,Safety,Risk,Risk,170,"As a **user running workflows**, I want to **know how to `expandSubworkflows` by reading the documentation**, so that **I know when to use this feature**. Effort: Small; Risk: Extra-Small; Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2420#issuecomment-333186989
https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313568784:97,Modifiability,enhance,enhanced,97,"Hi @patmagee - I started poking at this but then just reread your ticket. Have you looked at the enhanced label functionality that was just pushed out in 28? It sounded like that might get you what you want here. . Let me know if not or if you'd still rather use the meta fields, it's easy enough to put in, just wanted to make sure it was worthwhile.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313568784
https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313569884:126,Testability,test,tested,126,"@geoffjentry prior to today I wasn't aware of the new labels functionality. I believe this will be the solution, but have not tested it! I will let you know once I've seen the output",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313569884
https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313570149:118,Integrability,rout,route,118,"@patmagee Cool, no rush. If you decide you still want the meta stuff just let me know, or if you want to go the label route just close the issue once you're confident it's the right path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313570149
https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313702938:91,Security,expose,expose,91,@geoffjentry exposing the labels in the metadata looks like it will work great. No need to expose the workflow meta object at this point,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313702938
https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-313387737:12,Testability,test,tests,12,oh and some tests wouldn't hurt üòõ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-313387737
https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451:122,Modifiability,variab,variables,122,Added some tests. @geoffjentry I think these unit tests are comprehensive enough without (re-)testing that Cromwell wires variables into engine functions correctly in centaur. Hope you agree?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451
https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451:11,Testability,test,tests,11,Added some tests. @geoffjentry I think these unit tests are comprehensive enough without (re-)testing that Cromwell wires variables into engine functions correctly in centaur. Hope you agree?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451
https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451:50,Testability,test,tests,50,Added some tests. @geoffjentry I think these unit tests are comprehensive enough without (re-)testing that Cromwell wires variables into engine functions correctly in centaur. Hope you agree?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451
https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451:94,Testability,test,testing,94,Added some tests. @geoffjentry I think these unit tests are comprehensive enough without (re-)testing that Cromwell wires variables into engine functions correctly in centaur. Hope you agree?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451
https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314422336:62,Testability,test,testing,62,"@cjllanwarne My centaur comment was more that ideally we'd be testing every function in centaur. I'm sure this wouldn't be the first one we're slacking on, don't let me hold you up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314422336
https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314423467:120,Testability,test,test,120,"@geoffjentry I'll rephrase:; - I agree that the `size` function's interaction with the outside world deserves a centaur test (and I believe it has one).; - Since this PR only adds functionality that's entirely internal to Cromwell, I don't think (even in a perfect world) that we need another `empty_optionals_size` test in centaur to make sure that empty optionals give a size of 0?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314423467
https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314423467:316,Testability,test,test,316,"@geoffjentry I'll rephrase:; - I agree that the `size` function's interaction with the outside world deserves a centaur test (and I believe it has one).; - Since this PR only adds functionality that's entirely internal to Cromwell, I don't think (even in a perfect world) that we need another `empty_optionals_size` test in centaur to make sure that empty optionals give a size of 0?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314423467
https://github.com/broadinstitute/cromwell/issues/2423#issuecomment-345259634:428,Usability,guid,guide,428,"Related to switching to circe, and printing JSON in general, pretty-vs-compact printing has been up for debate. Here is an example of how to pretty print in circe: https://github.com/broadinstitute/clio/commit/4b78f9a5d2f5a0aea389dc404859d5f91e07321c#diff-4543387a548c3521400bdaa006943757. The above always pretty-prints. It may be the case that one wants the pretty-vs-compact printing to be an [option](https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html). Because this particular solution uses implicits, one _could_ turn this. ```scala; implicit val akkaHttpJsonPrinter: Printer; ```. into. ```scala; implicit def akkaHttpJsonPrinter()(implicit cromwellPrintType: CromwellPrintType): Printer; ```. and then implicitly provide an enum set by a query string parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2423#issuecomment-345259634
https://github.com/broadinstitute/cromwell/issues/2424#issuecomment-313498705:43,Modifiability,config,config,43,@geoffjentry Is this per-job or an overall config setting?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2424#issuecomment-313498705
https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325807471:55,Deployability,rolling,rolling,55,"@patmagee not to my knowledge but I can start the ball rolling. ; @geoffjentry & @mcovarr, any opinions on supporting enums?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325807471
https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325851732:246,Availability,Ping,Pinging,246,"I'm leery but not necessarily against it. I'm also generally the one with the most conservative opinion in terms of adding WDL syntax, so view that take as a lower bound of acceptance :). @patmagee what were you thinking in terms of the syntax?. Pinging @vdauwera so she's abreast of this convo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325851732
https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325877126:196,Performance,Queue,Queue,196,"FWIW Enum support would definitely be very valuable to us in GATK-world, and are likely to be useful in general. . My one caveat would be that they should be easier to work with than they were in Queue (friendly elbow poke at @kshakir).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325877126
https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-326603028:579,Security,validat,validated,579,"@geoffjentry I share your concern with adding any sort of syntax to the spec, and my first inclination is to fake support enumeration. Im not exactly sure how to do that though, nor am I sure of whether its possible. The cleanest way I can think of implementing would be maybe something like a Java style enum:. ```; enum MyEnum {; ""A"",""B"",""C""; }. workflow wf {; MyEnum thisIsMyEnum; }; ```. Another way that we would be able to do it would be define an Enum type in a workflow like so:. ```; workflow wf {; #This would get overridden at run time, but the value would need to be validated; Enum greeting = [""HELLO"",""GOODBYE""]; or; #Done override anything but validate it; Enum[""HELLO"",""GOODBYE""] greeting; ; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-326603028
https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-326603028:659,Security,validat,validate,659,"@geoffjentry I share your concern with adding any sort of syntax to the spec, and my first inclination is to fake support enumeration. Im not exactly sure how to do that though, nor am I sure of whether its possible. The cleanest way I can think of implementing would be maybe something like a Java style enum:. ```; enum MyEnum {; ""A"",""B"",""C""; }. workflow wf {; MyEnum thisIsMyEnum; }; ```. Another way that we would be able to do it would be define an Enum type in a workflow like so:. ```; workflow wf {; #This would get overridden at run time, but the value would need to be validated; Enum greeting = [""HELLO"",""GOODBYE""]; or; #Done override anything but validate it; Enum[""HELLO"",""GOODBYE""] greeting; ; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-326603028
https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095:86,Deployability,Pipeline,Pipelines,86,"@danbills does the following sound accurate to you?. As a **user running workflows on Pipelines API**, I want **Cromwell to notify me when it hits the limit for retries**, so that **I know why my workflow failed**.; * Effort: Small; * Risk: Small; * Business value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095
https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095:235,Safety,Risk,Risk,235,"@danbills does the following sound accurate to you?. As a **user running workflows on Pipelines API**, I want **Cromwell to notify me when it hits the limit for retries**, so that **I know why my workflow failed**.; * Effort: Small; * Risk: Small; * Business value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095
https://github.com/broadinstitute/cromwell/issues/2429#issuecomment-333185907:49,Deployability,update,updated,49,@geoffjentry Is the use case that a Cromwell dev updated liquidbase and a user needs to migrate? How much effort would this take?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2429#issuecomment-333185907
https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314234253:76,Energy Efficiency,energy,energy,76,üññ üëç . I thought there were more than these but I guess not. Didn't have the energy to go back and do them again once I abandoned my first branch :). [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2433/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314234253
https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314243598:74,Availability,error,errors,74,Oh yeah. IIRC the irritation I had here was that it'd not give me all the errors at once so i'd think i was done only to get hit by yet another block :). I mentioned the unused vars in slack. There are some *definite* unused ones in there. Most of them were due to our (over?)reliance on overloaded function signatures where we may or may not use all of the args. Some of them were due to our pattern (particularly in the backend) of supplying a default no-op implementation of things. . There were a handful though that were really bizarre. As in I couldn't figure out why the compiler thought it was unused.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314243598
https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333185621:36,Performance,perform,performance,36,@geoffjentry what was the effect on performance with `mapValues`? What would users (FireCloud or individual Cromwell users) notice?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333185621
https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333308532:116,Performance,perform,performance,116,"mapValues recomputes itself every time it is accessed. As stated in the original ticket, we have had fairly serious performance problems which traced back to the usage of mapValues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333308532
https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333308532:45,Security,access,accessed,45,"mapValues recomputes itself every time it is accessed. As stated in the original ticket, we have had fairly serious performance problems which traced back to the usage of mapValues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333308532
https://github.com/broadinstitute/cromwell/issues/2438#issuecomment-315120152:530,Deployability,hotfix,hotfix,530,"Options to fix it:; - Migration that changes all `executionStatus: Preempted` metadata entries into `executionStatus: RetryableFailure`.; - Migration that changes all `executionStatus: Preempted` metadata entries into `executionStatus: RetryableFailure` and also add `backendStatus: Preempted`.; - Unlike option 1, I think this will require a custom scala migration so might be a fair bit slower; - Change the CRDT to just be fine with `Preempted` as an `ExecutionStatus` again.; - Not intellectually honest, but easy to add as a hotfix and no migration necessary",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2438#issuecomment-315120152
https://github.com/broadinstitute/cromwell/issues/2440#issuecomment-336169425:178,Safety,Risk,Risk,178,"As a **Cromwell dev** I want **to have uniquely named Metadata services**, so that **when there are multiple services I can specify which metadata service**.; - Effort: Small; - Risk: Would this break WDLs? APIs?; - Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2440#issuecomment-336169425
https://github.com/broadinstitute/cromwell/pull/2443#issuecomment-315447431:16,Testability,test,test,16,üëç . How did you test this?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2443/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2443#issuecomment-315447431
https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-332979103:89,Integrability,depend,depends,89,"This might not be an issue for the wdl focus group (in favor of upcoming situations). It depends on if this starts becoming more of a Cromwell implementation thing or a WDL thing. I think it is purely the latter, and if that's the case it should be moved to the wdl repo and left for that triage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-332979103
https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-333223653:69,Deployability,update,update,69,@geoffjentry it was partial thought :) My mind can be scattered. Ive update the comment,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-333223653
https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-335540877:315,Safety,Risk,Risk,315,"As a **user running workflows**, I want **Cromwell to follow this order for looking at inputs: Inputs provided by the inputs JSON, Inputs specified explicitly by call, and then the default value in the task**, so that **Cromwell is processing the inputs that I want it to**. - Effort: @geoffjentry any thoughts?; - Risk: Small; - Business value: Small to Medium; - @cjllanwarne have any users asked or complained about this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-335540877
https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315446494:84,Performance,queue,queue,84,"@francares are you seeing a stack overflow exception or a Slick ""task rejected from queue"" exception?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315446494
https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315450361:14,Availability,error,error,14,"```; Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-57] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.StackOverflowError; 	at scala.collection.immutable.Set$EmptySet$.seq(Set.scala:68); 	at scala.collection.SetLike$class.$plus$plus(SetLike.scala:141); 	at scala.collection.AbstractSet.$plus$plus(Set.scala:47); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:27); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	...; ```; It's easy to reproduce, just run +100 hello world workflows and then query for all those workflows ids using query POST API.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315450361
https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315450361:94,Availability,down,down,94,"```; Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-57] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.StackOverflowError; 	at scala.collection.immutable.Set$EmptySet$.seq(Set.scala:68); 	at scala.collection.SetLike$class.$plus$plus(SetLike.scala:141); 	at scala.collection.AbstractSet.$plus$plus(Set.scala:47); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:27); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	...; ```; It's easy to reproduce, just run +100 hello world workflows and then query for all those workflows ids using query POST API.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315450361
https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315450361:133,Availability,error,error,133,"```; Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-57] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.StackOverflowError; 	at scala.collection.immutable.Set$EmptySet$.seq(Set.scala:68); 	at scala.collection.SetLike$class.$plus$plus(SetLike.scala:141); 	at scala.collection.AbstractSet.$plus$plus(Set.scala:47); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:27); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	...; ```; It's easy to reproduce, just run +100 hello world workflows and then query for all those workflows ids using query POST API.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315450361
https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451631:172,Availability,down,down,172,"@francares Cool. My main concern there was that when i did the akka http conversion that I ""fixed"" it by giving bad results, so as long as the results look good I'll stand down my fretting :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451631
https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894:69,Availability,error,errors,69,"Hmm that is definitely different from the ""task rejected from queue"" errors. And anyway 28 has the larger default metadata batch size changes, so if this really was a different symptom of that problem it shouldn't be happening on 28. . I don't see much different between develop and 28_hotfix that could legitimately explain fixes in the vicinity of Slick. üòï",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894
https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894:62,Performance,queue,queue,62,"Hmm that is definitely different from the ""task rejected from queue"" errors. And anyway 28 has the larger default metadata batch size changes, so if this really was a different symptom of that problem it shouldn't be happening on 28. . I don't see much different between develop and 28_hotfix that could legitimately explain fixes in the vicinity of Slick. üòï",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-417701911:0,Availability,Ping,Pinging,0,"Pinging @ruchim and @danbills, the current PO and TL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-417701911
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:227,Availability,error,error,227,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:958,Availability,error,error,958,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:984,Availability,error,error,984,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:1071,Availability,down,down,1071,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:1110,Availability,error,error,1110,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:922,Integrability,message,message,922,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:964,Integrability,message,message,964,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:763,Usability,resume,resume,763,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617:88,Deployability,configurat,configuration,88,"@ernoc Can you provide some more info on your setup? What backend you're using, what DB configuration, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617:88,Modifiability,config,configuration,88,"@ernoc Can you provide some more info on your setup? What backend you're using, what DB configuration, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570:241,Modifiability,config,config,241,"@geoffjentry ; I'm using SGE as backend and a MariaDB database. Running cromwell inside a docker container.; Call-caching is ON, I've got a concurrent job limit of 100, and a slightly customised job script epilogue. The rest of the cromwell config is standard as in the docs.; Anything other relevant info that I could provide?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570:140,Performance,concurren,concurrent,140,"@geoffjentry ; I'm using SGE as backend and a MariaDB database. Running cromwell inside a docker container.; Call-caching is ON, I've got a concurrent job limit of 100, and a slightly customised job script epilogue. The rest of the cromwell config is standard as in the docs.; Anything other relevant info that I could provide?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423505570
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423597827:603,Availability,down,down,603,"Hi @ernoc - this appears to be caused by the number of constraints you're specifying in your workflow query. - Short term, could you try making fewer constraints in your queries? We believe this is happening because slick is recursing per constraint, and thus hitting stack overflow when you have too many? Alternatively you could increase the JVM stack-overflow limit when running Cromwell to accommodate the number of constraints you need.; - Medium term, we should catch this (either in advance or literally catch the exception) and return ""unsupported operation"", rather than allowing this to bring down the entire Cromwell server.; - Longer term, we could try to restructure the query to support this level of querying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423597827
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423617226:250,Availability,error,error,250,"Hi @cjllanwarne - I'm now chunking my queries, sending small batches of workflow IDs instead of querying for the status of 8k workflow IDs in the same query and I confirm that this resolves the issue. Thanks!; I'd still be in favour of catching this error or even before trying to interpret the query, counting how many terms are there in the json structure and reject it if too large. That way it wouldn't be so easy to bring down the server either intentionally or by mistake :); Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423617226
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423617226:427,Availability,down,down,427,"Hi @cjllanwarne - I'm now chunking my queries, sending small batches of workflow IDs instead of querying for the status of 8k workflow IDs in the same query and I confirm that this resolves the issue. Thanks!; I'd still be in favour of catching this error or even before trying to interpret the query, counting how many terms are there in the json structure and reject it if too large. That way it wouldn't be so easy to bring down the server either intentionally or by mistake :); Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423617226
https://github.com/broadinstitute/cromwell/issues/2451#issuecomment-315459247:52,Testability,test,test,52,Phew Cromwell gets this right. üòÑ I'll add a centaur test to make sure it stays right.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2451#issuecomment-315459247
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-315474312:232,Performance,scalab,scalable,232,"@francares Can you provide more details on what you're actually doing? Cromwell as-is isn't intended to be scaled like that, at least not at the moment. Also the current metadata implementation isn't expected to be a heavy hitting, scalable solution (e.g. we'll be providing at least one alternate implementation over the next couple of months for a more scalable use case)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-315474312
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-315474312:355,Performance,scalab,scalable,355,"@francares Can you provide more details on what you're actually doing? Cromwell as-is isn't intended to be scaled like that, at least not at the moment. Also the current metadata implementation isn't expected to be a heavy hitting, scalable solution (e.g. we'll be providing at least one alternate implementation over the next couple of months for a more scalable use case)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-315474312
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377:426,Deployability,configurat,configuration,426,"The only way they appear is similar at all is that they involve the database. FWIW this sort of topic falls under what I see as a third tier of Cromwell which we're not currently considering, but IMO will need to over the next year. There is/will be:. - Cromwell as we know it now. Not intended for horizontal scaling situations. Expected that it can meet the needs of most typical users needs out of the box without a lot of configuration and that it can run anywhere.; - Cromwell with cool implementations of subsystems that are optimized to take advantage of GCP for scaling purposes (i.e. CaaS, but such that a savvy user could set up their own if they wanted to); - An in between phase for people who have scaling needs beyond vanilla cromwell but aren't on GCP, and most likely are on more traditional setups (on prem, hpc, etc). To some extent we might be able to rely on outside contributions here once we've made supporting both the first two (as that'd imply taking advantage of things like the service registry for pluggable implementations) but we'll likely need to prime that pump.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377:426,Modifiability,config,configuration,426,"The only way they appear is similar at all is that they involve the database. FWIW this sort of topic falls under what I see as a third tier of Cromwell which we're not currently considering, but IMO will need to over the next year. There is/will be:. - Cromwell as we know it now. Not intended for horizontal scaling situations. Expected that it can meet the needs of most typical users needs out of the box without a lot of configuration and that it can run anywhere.; - Cromwell with cool implementations of subsystems that are optimized to take advantage of GCP for scaling purposes (i.e. CaaS, but such that a savvy user could set up their own if they wanted to); - An in between phase for people who have scaling needs beyond vanilla cromwell but aren't on GCP, and most likely are on more traditional setups (on prem, hpc, etc). To some extent we might be able to rely on outside contributions here once we've made supporting both the first two (as that'd imply taking advantage of things like the service registry for pluggable implementations) but we'll likely need to prime that pump.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377:531,Performance,optimiz,optimized,531,"The only way they appear is similar at all is that they involve the database. FWIW this sort of topic falls under what I see as a third tier of Cromwell which we're not currently considering, but IMO will need to over the next year. There is/will be:. - Cromwell as we know it now. Not intended for horizontal scaling situations. Expected that it can meet the needs of most typical users needs out of the box without a lot of configuration and that it can run anywhere.; - Cromwell with cool implementations of subsystems that are optimized to take advantage of GCP for scaling purposes (i.e. CaaS, but such that a savvy user could set up their own if they wanted to); - An in between phase for people who have scaling needs beyond vanilla cromwell but aren't on GCP, and most likely are on more traditional setups (on prem, hpc, etc). To some extent we might be able to rely on outside contributions here once we've made supporting both the first two (as that'd imply taking advantage of things like the service registry for pluggable implementations) but we'll likely need to prime that pump.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631:47,Deployability,toggle,toggled,47,"@geoffjentry, is the MetadataSummarizer config toggled with the `metadata-summary-refresh-interval` in the reference configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631:117,Deployability,configurat,configuration,117,"@geoffjentry, is the MetadataSummarizer config toggled with the `metadata-summary-refresh-interval` in the reference configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631:40,Modifiability,config,config,40,"@geoffjentry, is the MetadataSummarizer config toggled with the `metadata-summary-refresh-interval` in the reference configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631
https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631:117,Modifiability,config,configuration,117,"@geoffjentry, is the MetadataSummarizer config toggled with the `metadata-summary-refresh-interval` in the reference configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631
https://github.com/broadinstitute/cromwell/pull/2456#issuecomment-315820103:43,Deployability,hotfix,hotfix,43,"I think what what we should do is once the hotfix is in place to replace the jar. Can have a note here that makes a mention of it, but honestly no one is going to come back here to read the changelog after they've already pulled the release, and as @mcovarr points out it's not even possible if they're using a MySQL based database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2456#issuecomment-315820103
https://github.com/broadinstitute/cromwell/pull/2456#issuecomment-315820103:233,Deployability,release,release,233,"I think what what we should do is once the hotfix is in place to replace the jar. Can have a note here that makes a mention of it, but honestly no one is going to come back here to read the changelog after they've already pulled the release, and as @mcovarr points out it's not even possible if they're using a MySQL based database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2456#issuecomment-315820103
https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315833698:30,Deployability,hotfix,hotfix,30,"Since this is the same as the hotfix, I think this is pre-thumbed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315833698
https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315835050:32,Deployability,hotfix,hotfix,32,I'd buy that logic assuming the hotfix had been thumbed. üòõ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315835050
https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315835050:13,Testability,log,logic,13,I'd buy that logic assuming the hotfix had been thumbed. üòõ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315835050
https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026:462,Availability,failure,failures,462,"JES/PAPI only has [two](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#DockerExecutor) current settings for running a docker container:; - `imageName`: Image name from either Docker Hub or Google Container Registry.; - `cmd`: The command or newline delimited script to run. The command string will be executed within a bash shell. This particular ticket may need to be escalated. The entrypoints present in the docker image do seem to cause failures with JES/PAPI. This could be mitigated by PAPI using [`docker run --entrypoint="""" ‚Ä¶`](https://docs.docker.com/engine/reference/run/#entrypoint-default-command-to-execute-at-runtime). Also note: PAPI using `bash` effectively makes #1384 moot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026
https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026:82,Deployability,pipeline,pipelines,82,"JES/PAPI only has [two](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#DockerExecutor) current settings for running a docker container:; - `imageName`: Image name from either Docker Hub or Google Container Registry.; - `cmd`: The command or newline delimited script to run. The command string will be executed within a bash shell. This particular ticket may need to be escalated. The entrypoints present in the docker image do seem to cause failures with JES/PAPI. This could be mitigated by PAPI using [`docker run --entrypoint="""" ‚Ä¶`](https://docs.docker.com/engine/reference/run/#entrypoint-default-command-to-execute-at-runtime). Also note: PAPI using `bash` effectively makes #1384 moot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026
https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-438104955:70,Availability,error,errors,70,"I think this should be fairly high priority. It causes very confusing errors with the PAPI backend, made even more confusing by the fact that the same code will work on the local executor. Are there any workarounds at the moment?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-438104955
https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505704953:122,Deployability,Pipeline,Pipeline,122,"I'm definitely having this problem with AWS Backend. Not sure how newest, but I believe I had this problem during the HCA Pipeline Surges as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505704953
https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505867640:421,Deployability,Pipeline,Pipeline,421,"Hey Jaeyoung--is there a public version of such an image I can test with on; Google? As for AWS, I think there's a real feature request to make Cromwell; compatible with docker images with an entrypoint on AWS. On Tue, Jun 25, 2019 at 11:39 PM Jaeyoung Chun <notifications@github.com>; wrote:. > I'm definitely having this problem with AWS Backend. Not sure how newest,; > but I believe I had this problem during the HCA Pipeline Surges as well.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2461?email_source=notifications&email_token=ADR7XTPGWVEOCY34LELUVMDP4LQHRA5CNFSM4DTKAP42YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYSHD6I#issuecomment-505704953>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADR7XTLSYROEYXWXSLAFQ2LP4LQHRANCNFSM4DTKAP4Q>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505867640
https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505867640:63,Testability,test,test,63,"Hey Jaeyoung--is there a public version of such an image I can test with on; Google? As for AWS, I think there's a real feature request to make Cromwell; compatible with docker images with an entrypoint on AWS. On Tue, Jun 25, 2019 at 11:39 PM Jaeyoung Chun <notifications@github.com>; wrote:. > I'm definitely having this problem with AWS Backend. Not sure how newest,; > but I believe I had this problem during the HCA Pipeline Surges as well.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2461?email_source=notifications&email_token=ADR7XTPGWVEOCY34LELUVMDP4LQHRA5CNFSM4DTKAP42YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYSHD6I#issuecomment-505704953>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADR7XTLSYROEYXWXSLAFQ2LP4LQHRANCNFSM4DTKAP4Q>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505867640
https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-538183152:12,Availability,error,error,12,"This is the error that I'm getting:. ```; 2019/10/03 19:00:01 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= hisplan/samtools@sha256:12d34a022f43e87e6c51f4be29705ccb70d2562a2046f3746f04cca88674a2e9 /bin/bash /cromwell_root/script; [main] unrecognized command '/bin/bash'; ```. Even though the container does have `/bin/bash`, this error occurs if the container image is built with `ENTRYPOINT`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-538183152
https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-538183152:364,Availability,error,error,364,"This is the error that I'm getting:. ```; 2019/10/03 19:00:01 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= hisplan/samtools@sha256:12d34a022f43e87e6c51f4be29705ccb70d2562a2046f3746f04cca88674a2e9 /bin/bash /cromwell_root/script; [main] unrecognized command '/bin/bash'; ```. Even though the container does have `/bin/bash`, this error occurs if the container image is built with `ENTRYPOINT`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-538183152
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656:97,Deployability,release,released,97,Thanks for letting us know about this. We discovered a critical bug in Cromwell 28 yesterday and released a patched version of the jar. I believe @geoffjentry prepared an updated version of the Homebrew formula yesterday as well. The formula from June 30 is certainly referencing the older jar and will not match the current checksum.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656:108,Deployability,patch,patched,108,Thanks for letting us know about this. We discovered a critical bug in Cromwell 28 yesterday and released a patched version of the jar. I believe @geoffjentry prepared an updated version of the Homebrew formula yesterday as well. The formula from June 30 is certainly referencing the older jar and will not match the current checksum.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656:171,Deployability,update,updated,171,Thanks for letting us know about this. We discovered a critical bug in Cromwell 28 yesterday and released a patched version of the jar. I believe @geoffjentry prepared an updated version of the Homebrew formula yesterday as well. The formula from June 30 is certainly referencing the older jar and will not match the current checksum.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656:325,Security,checksum,checksum,325,Thanks for letting us know about this. We discovered a critical bug in Cromwell 28 yesterday and released a patched version of the jar. I believe @geoffjentry prepared an updated version of the Homebrew formula yesterday as well. The formula from June 30 is certainly referencing the older jar and will not match the current checksum.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:115,Availability,error,error,115,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:37,Deployability,release,released,37,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:152,Performance,cache,cache,152,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:97,Security,checksum,checksum,97,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639:4,Availability,down,downloadable,4,"The downloadable jar on Github was intentionally kept the same, and that's the URL used in the Homebrew formula. This whole incident has illustrated the need for better release & version management going forward but that doesn't exist right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639:169,Deployability,release,release,169,"The downloadable jar on Github was intentionally kept the same, and that's the URL used in the Homebrew formula. This whole incident has illustrated the need for better release & version management going forward but that doesn't exist right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289:127,Availability,error,error,127,"OK. Let's hard code a `version ""28.1""` in the formula even though it won't match the URL, since that will prevent the checksum error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289:118,Security,checksum,checksum,118,"OK. Let's hard code a `version ""28.1""` in the formula even though it won't match the URL, since that will prevent the checksum error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067822:19,Deployability,update,update,19,@ilovezfs Ok. Will update the PR in a few.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067822
https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316312024:129,Deployability,release,release,129,">Sorry for the confusion on this, hopefully it doesn't happen again. In the future if the jar needs to be modified after initial release, please bump the version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316312024
https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657:212,Availability,reliab,reliable,212,"> * cromwell v27; > * SGE backend; > * server mode; > ; > Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing. @LeeTL1220 hiÔºåHave you solved this problem Ôºü",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657
https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657:95,Performance,queue,queued,95,"> * cromwell v27; > * SGE backend; > * server mode; > ; > Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing. @LeeTL1220 hiÔºåHave you solved this problem Ôºü",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:257,Deployability,configurat,configuration,257,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:298,Deployability,configurat,configuration,298,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:257,Modifiability,config,configuration,257,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:298,Modifiability,config,configuration,298,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:711,Testability,log,logically,711,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316438740:39,Performance,Queue,Queue,39,Let's stash all the data in a GCP Task Queue and pull it in to the DB with a separate service.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316438740
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697:40,Deployability,configurat,configuration,40,"@geoffjentry do you mean from a default configuration perspective? We could have a pluggable ""metadata service"" in Cromwell (which I think we already have) with two implementations (direct DB write vs JMS emitter) could go into mainstream cromwell. Of course the listener for that would be a separate service (and easy to scale). Maybe that's what you mean though?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697:40,Modifiability,config,configuration,40,"@geoffjentry do you mean from a default configuration perspective? We could have a pluggable ""metadata service"" in Cromwell (which I think we already have) with two implementations (direct DB write vs JMS emitter) could go into mainstream cromwell. Of course the listener for that would be a separate service (and easy to scale). Maybe that's what you mean though?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697
https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320251484:219,Integrability,rout,route,219,"@kcibul Right now there are two problems:. - We're overloading the DB (largely metadata) in FC; - Nothing good happens when the DB is overloaded. The first one is solvable by a new metadata impl and will be the sort of route we go in CaaS. The second one should be fixed no matter what one is doing, so I'm specifically talking about all slick interaction which includes the stock metadata impl. The solution shouldn't be ""turn your buffers up"", but rather a more appropriate scheme instead of just dropping stuff to the floor. Related - as part of the CaaS milestone there are tickets (#1349 in particular) to make sure the standard impl isn't too tightly coupled with the resto f our DB stuff.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320251484
https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933:134,Availability,down,down,134,"Can-of-wormsy ToL: if `reference.conf` doubles as our config documentation, could we include this there? Otherwise, could we write it down *somewhere*?. I like not cluttering the `reference.conf` but I also don't want to have to rummage through some (I've-already-forgotten-which) class file to find how to change this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933
https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933:54,Modifiability,config,config,54,"Can-of-wormsy ToL: if `reference.conf` doubles as our config documentation, could we include this there? Otherwise, could we write it down *somewhere*?. I like not cluttering the `reference.conf` but I also don't want to have to rummage through some (I've-already-forgotten-which) class file to find how to change this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933
https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316727394:59,Usability,clear,clear,59,"@cjllanwarne Totally agree. One of the things which became clear to me toot sweet was that was that we have no docs around the service registry in general, e.g. how to make a new service. All that sorta stuff is on my radar atm.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316727394
https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639:371,Security,hash,hashDifferential,371,"I haven't used this endpoint and I'm not entirely sure what the use case is. However, this new version seems less usable to me. I can no longer look up the differential by a key I'm interested in - I have to iterate over all elements looking for the one I want. This seems worse -- why use a map?. Also I don't really know why it's in an array. Why isn't it just:. ```; ""hashDifferential"": {; ""output expression:String hi‚Äù: ; {; ""callA"": ""935C6E7EB2068B83C40B788575747EFB‚Äù, ; ""callB"": ‚Äú0183144CF6617D5341681C6B2F756046""; },; ""output thing:blah blah"": { ... },; ...; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639
https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639:114,Usability,usab,usable,114,"I haven't used this endpoint and I'm not entirely sure what the use case is. However, this new version seems less usable to me. I can no longer look up the differential by a key I'm interested in - I have to iterate over all elements looking for the one I want. This seems worse -- why use a map?. Also I don't really know why it's in an array. Why isn't it just:. ```; ""hashDifferential"": {; ""output expression:String hi‚Äù: ; {; ""callA"": ""935C6E7EB2068B83C40B788575747EFB‚Äù, ; ""callB"": ‚Äú0183144CF6617D5341681C6B2F756046""; },; ""output thing:blah blah"": { ... },; ...; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639
https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316788346:121,Security,hash,hashKey,121,"@helgridly ; - Conceptually, this is a list of differences, not a map from known keys to values of interest. Rather, the hashKey is just another piece of information you might be interested in alongside the two calls' hash values.; - I'm keeping the schema well-defined. IMO this will make parsing easier and more precise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316788346
https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316788346:218,Security,hash,hash,218,"@helgridly ; - Conceptually, this is a list of differences, not a map from known keys to values of interest. Rather, the hashKey is just another piece of information you might be interested in alongside the two calls' hash values.; - I'm keeping the schema well-defined. IMO this will make parsing easier and more precise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316788346
https://github.com/broadinstitute/cromwell/pull/2474#issuecomment-317097312:176,Integrability,wrap,wrapper,176,File this under TOL but I'll channel my inner @kcibul and point out that at some point we should look and see if swagger actually *can* generate a java client for us. making a wrapper to de-suck the java would be a lot less work than building this out to completeness.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2474#issuecomment-317097312
https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-317758745:8,Deployability,install,install,8,* `brew install git-secrets`; * for each repo; `git-secrets --install`; `git-secrets --add 'private_key'`; `git-secrets --add 'private_key_id'`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-317758745
https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-317758745:62,Deployability,install,install,62,* `brew install git-secrets`; * for each repo; `git-secrets --install`; `git-secrets --add 'private_key'`; `git-secrets --add 'private_key_id'`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-317758745
https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599:884,Availability,echo,echo,884,"To run on each repo, cd to whatever directory you have your git repositories checked out to, say `~/src`, and then paste:. ```bash; (; gitprefix=""git@github.com:broadinstitute/""; ; update-git-secrets() {; grep -q ""git secrets"" .git/hooks/commit-msg || git-secrets --install; git-secrets --add 'private_key'; git-secrets --add 'private_key_id'; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; git-secrets --add --allowed '.Data.service_account.private_key'; }; ; for dir in */; do; (; cd $dir; if [[ -d .git ]] && \; [[ $(git remote get-url origin 2> /dev/null) == ${gitprefix}* ]]; then; echo updating $dir; update-git-secrets; fi; ); done; ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599
https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599:181,Deployability,update,update-git-secrets,181,"To run on each repo, cd to whatever directory you have your git repositories checked out to, say `~/src`, and then paste:. ```bash; (; gitprefix=""git@github.com:broadinstitute/""; ; update-git-secrets() {; grep -q ""git secrets"" .git/hooks/commit-msg || git-secrets --install; git-secrets --add 'private_key'; git-secrets --add 'private_key_id'; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; git-secrets --add --allowed '.Data.service_account.private_key'; }; ; for dir in */; do; (; cd $dir; if [[ -d .git ]] && \; [[ $(git remote get-url origin 2> /dev/null) == ${gitprefix}* ]]; then; echo updating $dir; update-git-secrets; fi; ); done; ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599
https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599:266,Deployability,install,install,266,"To run on each repo, cd to whatever directory you have your git repositories checked out to, say `~/src`, and then paste:. ```bash; (; gitprefix=""git@github.com:broadinstitute/""; ; update-git-secrets() {; grep -q ""git secrets"" .git/hooks/commit-msg || git-secrets --install; git-secrets --add 'private_key'; git-secrets --add 'private_key_id'; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; git-secrets --add --allowed '.Data.service_account.private_key'; }; ; for dir in */; do; (; cd $dir; if [[ -d .git ]] && \; [[ $(git remote get-url origin 2> /dev/null) == ${gitprefix}* ]]; then; echo updating $dir; update-git-secrets; fi; ); done; ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599
https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599:904,Deployability,update,update-git-secrets,904,"To run on each repo, cd to whatever directory you have your git repositories checked out to, say `~/src`, and then paste:. ```bash; (; gitprefix=""git@github.com:broadinstitute/""; ; update-git-secrets() {; grep -q ""git secrets"" .git/hooks/commit-msg || git-secrets --install; git-secrets --add 'private_key'; git-secrets --add 'private_key_id'; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; git-secrets --add --allowed '.Data.service_account.private_key'; }; ; for dir in */; do; (; cd $dir; if [[ -d .git ]] && \; [[ $(git remote get-url origin 2> /dev/null) == ${gitprefix}* ]]; then; echo updating $dir; update-git-secrets; fi; ); done; ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599
https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-319088844:98,Modifiability,config,config,98,"To whitelist what we have in README copy this in the `[secrets]` section of your `<git_repo>/.git/config` file. ```; allowed = \""private_key_id\"": \""OMITTED\""; allowed = \""private_key\"": \""-----BEGIN PRIVATE KEY-----\\\\nBASE64 ENCODED KEY WITH \\\\n TO REPRESENT NEWLINES\\\\n-----END PRIVATE KEY-----\\\\n\""; allowed = \""client_id\"": \""22377410244549202395\""; allowed = The `private_key` portion needs; ```. or run . ```; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-319088844
https://github.com/broadinstitute/cromwell/issues/2480#issuecomment-317223254:211,Deployability,release,release,211,"Hi @tseemann - . Thanks, we're aware. We followed the instructions on brew's site and used the revision flag, but eventually worked it out with @ilovezfs to use `28.1` in this casee. We don't, at least for now, release our software to bioconda. We've since decided to switch to an x.y system as our x versions are meaningful internally and shouldn't be changing for things like critical bug fixes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2480#issuecomment-317223254
https://github.com/broadinstitute/cromwell/pull/2482#issuecomment-317560800:59,Testability,test,test,59,"Yeah I agree it's not great that I didn't / wasn't able to test this, but I will say it's almost certainly more correct than what was there before given the wdl4s subprojecting. So it's got that going for it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2482#issuecomment-317560800
https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317799694:26,Deployability,hotfix,hotfix,26,"üëç ; I feel that we should hotfix this in to the next Firecloud release, given the improvement we saw in Alpha. The problem was probably exacerbated recently with the increase to the metadata batch size (bigger batches == more slowdown in List.append).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317799694
https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317799694:63,Deployability,release,release,63,"üëç ; I feel that we should hotfix this in to the next Firecloud release, given the improvement we saw in Alpha. The problem was probably exacerbated recently with the increase to the metadata batch size (bigger batches == more slowdown in List.append).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317799694
https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184:97,Deployability,release,release,97,@geoffjentry I totally agree with @rtitle that this is something that should be added to next FC release as the performance improvement is significant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184
https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184:112,Performance,perform,performance,112,@geoffjentry I totally agree with @rtitle that this is something that should be added to next FC release as the performance improvement is significant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184
https://github.com/broadinstitute/cromwell/pull/2486#issuecomment-317813217:60,Performance,perform,performance,60,"@Horneth From what @ruchim and @rtitle said, it made a huge performance difference on FC alpha. Which makes sense as that list was getting appended to and we do a lot of appending there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2486#issuecomment-317813217
https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750:27,Availability,down,downloaded,27,I'm using v28. The jar was downloaded from this page; https://github.com/broadinstitute/cromwell/releases/tag/28,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750
https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750:97,Deployability,release,releases,97,I'm using v28. The jar was downloaded from this page; https://github.com/broadinstitute/cromwell/releases/tag/28,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750
https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:89,Availability,failure,failure,89,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447
https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:620,Availability,failure,failure,620,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447
https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:838,Safety,avoid,avoids,838,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447
https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:290,Testability,log,log,290,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447
https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265:86,Deployability,configurat,configuration,86,"As a short-term workaround you might define different config backends for each Docker configuration you want to support, but that wouldn't scale well if you have a lot of different configurations. Also that requires changes to your conf file to match your WDL which is kind of gross. Do you have any specific suggestions how you'd want this to work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265
https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265:181,Deployability,configurat,configurations,181,"As a short-term workaround you might define different config backends for each Docker configuration you want to support, but that wouldn't scale well if you have a lot of different configurations. Also that requires changes to your conf file to match your WDL which is kind of gross. Do you have any specific suggestions how you'd want this to work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265
https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265:54,Modifiability,config,config,54,"As a short-term workaround you might define different config backends for each Docker configuration you want to support, but that wouldn't scale well if you have a lot of different configurations. Also that requires changes to your conf file to match your WDL which is kind of gross. Do you have any specific suggestions how you'd want this to work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265
https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265:86,Modifiability,config,configuration,86,"As a short-term workaround you might define different config backends for each Docker configuration you want to support, but that wouldn't scale well if you have a lot of different configurations. Also that requires changes to your conf file to match your WDL which is kind of gross. Do you have any specific suggestions how you'd want this to work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265
https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265:181,Modifiability,config,configurations,181,"As a short-term workaround you might define different config backends for each Docker configuration you want to support, but that wouldn't scale well if you have a lot of different configurations. Also that requires changes to your conf file to match your WDL which is kind of gross. Do you have any specific suggestions how you'd want this to work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265
https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531:122,Availability,error,errors,122,"@droazen not that i'm aware of. If you're referring to what I think you're referring to, @leetl1220 is experiencing these errors as part of the Pipelines API process which isn't code we control.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531
https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531:144,Deployability,Pipeline,Pipelines,144,"@droazen not that i'm aware of. If you're referring to what I think you're referring to, @leetl1220 is experiencing these errors as part of the Pipelines API process which isn't code we control.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531
https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318399842:53,Deployability,Pipeline,Pipelines,53,"@geoffjentry Ah, thanks for the clarification -- by ""Pipelines API process"", do you mean JES? Are you in touch with the people who maintain that (or are we able to submit PRs to that project)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318399842
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002:70,Availability,error,error,70,"Thanks @pshapiro4broad . Yes, I agree that this should be fixed. That error is being caught by our ""oops we have no idea what happened"" when clearly there's some repeatable code path causing an error here where we *should* know what's going on. It'd be good to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002:194,Availability,error,error,194,"Thanks @pshapiro4broad . Yes, I agree that this should be fixed. That error is being caught by our ""oops we have no idea what happened"" when clearly there's some repeatable code path causing an error here where we *should* know what's going on. It'd be good to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002:141,Usability,clear,clearly,141,"Thanks @pshapiro4broad . Yes, I agree that this should be fixed. That error is being caught by our ""oops we have no idea what happened"" when clearly there's some repeatable code path causing an error here where we *should* know what's going on. It'd be good to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414:359,Availability,error,error,359,"+1. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences Platform; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jul 27, 2017 at 1:09 PM, Jeff Gentry <notifications@github.com>; wrote:. > Thanks @pshapiro4broad <https://github.com/pshapiro4broad>; >; > Yes, I agree that this should be fixed. That error is being caught by our; > ""oops we have no idea what happened"" when clearly there's some repeatable; > code path causing an error here where we *should* know what's going on.; > It'd be good to fix this.; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g_YmtNolUq9osOY-f_PNa3wEoPHrks5sSMQzgaJpZM4OlcEp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414:489,Availability,error,error,489,"+1. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences Platform; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jul 27, 2017 at 1:09 PM, Jeff Gentry <notifications@github.com>; wrote:. > Thanks @pshapiro4broad <https://github.com/pshapiro4broad>; >; > Yes, I agree that this should be fixed. That error is being caught by our; > ""oops we have no idea what happened"" when clearly there's some repeatable; > code path causing an error here where we *should* know what's going on.; > It'd be good to fix this.; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g_YmtNolUq9osOY-f_PNa3wEoPHrks5sSMQzgaJpZM4OlcEp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414:433,Usability,clear,clearly,433,"+1. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences Platform; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jul 27, 2017 at 1:09 PM, Jeff Gentry <notifications@github.com>; wrote:. > Thanks @pshapiro4broad <https://github.com/pshapiro4broad>; >; > Yes, I agree that this should be fixed. That error is being caught by our; > ""oops we have no idea what happened"" when clearly there's some repeatable; > code path causing an error here where we *should* know what's going on.; > It'd be good to fix this.; >; > ‚Äî; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g_YmtNolUq9osOY-f_PNa3wEoPHrks5sSMQzgaJpZM4OlcEp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:24,Availability,failure,failure,24,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:126,Availability,error,error,126,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:347,Availability,error,error,347,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:32,Integrability,message,message,32,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:166,Integrability,message,message,166,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:353,Integrability,message,message,353,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:53,Safety,abort,aborted,53,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:65,Availability,error,error-message-the-job-was-aborted-from-outside-cromwell,65,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:301,Availability,failure,failures,301,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:332,Availability,error,error,332,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:937,Availability,error,error,937,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:1214,Availability,failure,failure,1214,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:621,Energy Efficiency,reduce,reduce,621,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:71,Integrability,message,message-the-job-was-aborted-from-outside-cromwell,71,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:130,Integrability,message,message,130,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:338,Integrability,message,message,338,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:459,Integrability,message,message,459,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:1075,Integrability,message,message,1075,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:91,Safety,abort,aborted-from-outside-cromwell,91,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:147,Safety,abort,aborted,147,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:411,Safety,timeout,timeout,411,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:1108,Usability,simpl,simply,1108,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318449775:16,Deployability,patch,patch,16,:+1: I like dis patch. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2499/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318449775
https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624:131,Deployability,configurat,configuration,131,yeah I hesitated to argue because I know these are very stable and basically will never be used differently. I just kinda like the configuration all in one place personally. The one place also makes the scanning effort easier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624
https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624:131,Modifiability,config,configuration,131,yeah I hesitated to argue because I know these are very stable and basically will never be used differently. I just kinda like the configuration all in one place personally. The one place also makes the scanning effort easier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:51,Deployability,integrat,integration,51,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:51,Integrability,integrat,integration,51,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:222,Integrability,Rout,Route,222,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:160,Modifiability,refactor,refactoring,160,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:266,Modifiability,layers,layers,266,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:63,Testability,test,test,63,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:103,Testability,mock,mocks,103,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:121,Testability,test,tests,121,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:155,Testability,test,test,155,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318488237:75,Availability,error,errors,75,How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318488237
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318488237:18,Testability,test,tests,18,How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318488237
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402:77,Availability,error,errors,77,"> How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?. I wish! Centaur only handles `200 OK` responses. This fix returns a `400 Bad Request`, quickly, instead of a timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402:226,Safety,timeout,timeout,226,"> How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?. I wish! Centaur only handles `200 OK` responses. This fix returns a `400 Bad Request`, quickly, instead of a timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402:20,Testability,test,tests,20,"> How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?. I wish! Centaur only handles `200 OK` responses. This fix returns a `400 Bad Request`, quickly, instead of a timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402
https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390:125,Security,validat,validate,125,"If it's causing users real grief when they accidentally omit a curly brace and Cromwell hangs forever then I think we should validate that Cromwell now does the Right Thing in that circumstance, and continues to do the Right Thing going forward. Maybe building out that Centaur infrastructure isn't part of this particular ticket, but it seems like something worth doing (especially since I doubt it would be that much effort). ; ; People who actually submit pictures of Gumby deserve whatever they get.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390
https://github.com/broadinstitute/cromwell/pull/2501#issuecomment-318490274:36,Security,access,access,36,"@Horneth in CromIAM we already have access to the entity bytes, so the plan (at least initially) is to read that in as a Query result, find out which IDs can be seen by the requester, and then make a new entity out of the filtered results",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2501#issuecomment-318490274
https://github.com/broadinstitute/cromwell/issues/2504#issuecomment-326060034:105,Modifiability,enhance,enhancement,105,"I don't believe WDL ever allowed workflow declarations to be visible in tasks, but I'll label this as an enhancement request for our PO to review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2504#issuecomment-326060034
https://github.com/broadinstitute/cromwell/issues/2504#issuecomment-391424743:65,Modifiability,variab,variableName,65,"I'm a fan of this request. Perhaps something like ${workflowName.variableName} could be used within any task since it should be unique across the wdl file. In the initial example (just to be explicit), the usage could look like: ${indexes.genome}",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2504#issuecomment-391424743
https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555:136,Availability,echo,echo,136,"Running a slightly cleaned up version of your example:. ```; task my_task {; String a; String b = a + ""/"" + ""annotation.fa"". command {; echo ${b}; }; }. workflow my_workflow {; call my_task { input: a = ""my_path"" }; }; ```. I see this command go by in the logs:. ```; ... INFO - BackgroundConfigAsyncJobExecutionActor [...]: `echo my_path/annotation.fa`. ```. So this appears to be working as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555
https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555:326,Availability,echo,echo,326,"Running a slightly cleaned up version of your example:. ```; task my_task {; String a; String b = a + ""/"" + ""annotation.fa"". command {; echo ${b}; }; }. workflow my_workflow {; call my_task { input: a = ""my_path"" }; }; ```. I see this command go by in the logs:. ```; ... INFO - BackgroundConfigAsyncJobExecutionActor [...]: `echo my_path/annotation.fa`. ```. So this appears to be working as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555
https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555:256,Testability,log,logs,256,"Running a slightly cleaned up version of your example:. ```; task my_task {; String a; String b = a + ""/"" + ""annotation.fa"". command {; echo ${b}; }; }. workflow my_workflow {; call my_task { input: a = ""my_path"" }; }; ```. I see this command go by in the logs:. ```; ... INFO - BackgroundConfigAsyncJobExecutionActor [...]: `echo my_path/annotation.fa`. ```. So this appears to be working as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-319378573:345,Deployability,Pipeline,Pipelines,345,"Not sure how hard it would be to do this, but another useful metric would be network usage (as represented by bytes going in/out at a moment in time). I haven't looked at this in cromwell workflow but at my last job we had a number of workflows that were bandwidth constrained (mainly due to network drive mounts) until we moved them to to SSD. Pipelines issue: https://broadinstitute.atlassian.net/browse/DSDEGP-1360",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-319378573
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2528,Availability,echo,echo,2528,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2855,Availability,echo,echo,2855,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2871,Availability,echo,echo,2871,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2906,Availability,echo,echo,2906,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2950,Availability,echo,echo,2950,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3048,Availability,echo,echo,3048,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3089,Availability,echo,echo,3089,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3130,Availability,echo,echo,3130,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3171,Availability,echo,echo,3171,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3205,Availability,echo,echo,3205,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3227,Availability,echo,echo,3227,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3266,Availability,echo,echo,3266,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3327,Availability,echo,echo,3327,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3334,Availability,echo,echo,3334,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3106,Energy Efficiency,MONITOR,MONITORING,3106,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:473,Performance,cache,caches,473,"As an alternative I'd suggest something running *outside* the docker container (e.g. polling ""docker stats"" outside the container). I don't now enough to know if it's technically feasible, but I think it would be ideal as an always-on stats provider. There are two main reasons why:; 1) Linux memory usage is complex enough that it's easy to get this wrong. I started with the script provided above, but on many systems it gives the wrong answer because linux aggressively caches things in memory, and you therefore ""used"" - ""buffers"" - ""cached"" is a much better approximation of the memory that's tied up. Here's a plot showing these two estimates vs output from docker stats:; ![image](https://user-images.githubusercontent.com/6463752/48026456-228c0f80-e114-11e8-9240-e5e7b1c3cb23.png). 2) Seemingly every new linux distro changes the output format and calculations of free and df. The number of rows and columns change, requiring complex gymnastics with awk to extract the required values and do the math. And some docker images don't come with free or df at all. This is a problem for me because I'm optimizing WDLs but it's not always easy to change the dockers they're running in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:538,Performance,cache,cached,538,"As an alternative I'd suggest something running *outside* the docker container (e.g. polling ""docker stats"" outside the container). I don't now enough to know if it's technically feasible, but I think it would be ideal as an always-on stats provider. There are two main reasons why:; 1) Linux memory usage is complex enough that it's easy to get this wrong. I started with the script provided above, but on many systems it gives the wrong answer because linux aggressively caches things in memory, and you therefore ""used"" - ""buffers"" - ""cached"" is a much better approximation of the memory that's tied up. Here's a plot showing these two estimates vs output from docker stats:; ![image](https://user-images.githubusercontent.com/6463752/48026456-228c0f80-e114-11e8-9240-e5e7b1c3cb23.png). 2) Seemingly every new linux distro changes the output format and calculations of free and df. The number of rows and columns change, requiring complex gymnastics with awk to extract the required values and do the math. And some docker images don't come with free or df at all. This is a problem for me because I'm optimizing WDLs but it's not always easy to change the dockers they're running in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:1105,Performance,optimiz,optimizing,1105," to know if it's technically feasible, but I think it would be ideal as an always-on stats provider. There are two main reasons why:; 1) Linux memory usage is complex enough that it's easy to get this wrong. I started with the script provided above, but on many systems it gives the wrong answer because linux aggressively caches things in memory, and you therefore ""used"" - ""buffers"" - ""cached"" is a much better approximation of the memory that's tied up. Here's a plot showing these two estimates vs output from docker stats:; ![image](https://user-images.githubusercontent.com/6463752/48026456-228c0f80-e114-11e8-9240-e5e7b1c3cb23.png). 2) Seemingly every new linux distro changes the output format and calculations of free and df. The number of rows and columns change, requiring complex gymnastics with awk to extract the required values and do the math. And some docker images don't come with free or df at all. This is a problem for me because I'm optimizing WDLs but it's not always easy to change the dockers they're running in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2289,Performance,Cache,Cached,2289,"ning in (I'm guessing I'm not totally unique in this). Difficulties like this led me to develop a variant of the script that uses /proc for cpu and memory info. I've inlined it below:; ```; #!/bin/bash; set -Eeuo pipefail. MONITOR_MOUNT_POINT=${MONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027
https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436049650:246,Modifiability,flexible,flexible,246,@TedBrookings this seems like a really good idea. `docker stats` would of course only work for tasks running in a docker container but that's hopefully the majority of them. It would not have been possible to do it in PAPIv1 but PAPIv2 should be flexible enough to allow for something like that.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436049650
https://github.com/broadinstitute/cromwell/pull/2508#issuecomment-319217009:154,Availability,avail,available,154,~closing & will reopen later - going to do the same for the PAPI codes~. Should have actually done it before making such a bold claim. Doesn't seem to be available in the jar,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2508#issuecomment-319217009
https://github.com/broadinstitute/cromwell/pull/2508#issuecomment-319440432:502,Testability,test,tested,502,"@mcovarr When we handle responses from Google they're using numeric codes, and what we did the first time around was parse their documentation and set up constants to match their values. As it turns out (thanks @helgridly) at least for one of the classes of RPC codes we use they have them in a handy enum in the library I'm including so I'm switching to using that. . On the off chance that they change them, we'll pick up on that. That said, since preemption & PAPI edge cases are some of our weaker tested yet most important areas I'm also happy to just continue using our hardcoded values.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2508#issuecomment-319440432
https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778:120,Deployability,integrat,integration,120,"I'm _still_ working on implementing [this](https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390) integration test. At the moment, the develop cromwell client won't let centaur pass bad JSON for the test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778
https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778:120,Integrability,integrat,integration,120,"I'm _still_ working on implementing [this](https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390) integration test. At the moment, the develop cromwell client won't let centaur pass bad JSON for the test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778
https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778:132,Testability,test,test,132,"I'm _still_ working on implementing [this](https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390) integration test. At the moment, the develop cromwell client won't let centaur pass bad JSON for the test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778
https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778:221,Testability,test,test,221,"I'm _still_ working on implementing [this](https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390) integration test. At the moment, the develop cromwell client won't let centaur pass bad JSON for the test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778
https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-320040669:27,Security,encrypt,encrypted-,27,![verbal reviewer](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ43keR0LMoYCkj3rvBJ-psREke9Dupmo0JiZjTZ0hBTZQlpW99JQ),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-320040669
https://github.com/broadinstitute/cromwell/issues/2513#issuecomment-331905720:198,Modifiability,config,configs,198,"Talked to Doug. ""SAM"" itself doesn't serve HTTPS. The proxies do. The names are set there:; - [SAM](https://github.com/broadinstitute/firecloud-develop/blob/41b78578eaadb140ccdcf86d6d44de822bf9410c/configs/sam/proxy-compose.yaml.ctmpl#L17); - [Cromwell](https://github.com/broadinstitute/cromwell-develop/blob/d37f37cc8fd472b844a0b004178a1bfc458a3796/configs/caas/docker-compose.yaml.ctmpl#L32) (actually CaaS)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2513#issuecomment-331905720
https://github.com/broadinstitute/cromwell/issues/2513#issuecomment-331905720:351,Modifiability,config,configs,351,"Talked to Doug. ""SAM"" itself doesn't serve HTTPS. The proxies do. The names are set there:; - [SAM](https://github.com/broadinstitute/firecloud-develop/blob/41b78578eaadb140ccdcf86d6d44de822bf9410c/configs/sam/proxy-compose.yaml.ctmpl#L17); - [Cromwell](https://github.com/broadinstitute/cromwell-develop/blob/d37f37cc8fd472b844a0b004178a1bfc458a3796/configs/caas/docker-compose.yaml.ctmpl#L32) (actually CaaS)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2513#issuecomment-331905720
https://github.com/broadinstitute/cromwell/pull/2516#issuecomment-320308619:198,Testability,test,tests,198,üëç ; ToL: I was thinking it'd be nice if the client could use a `case class` version of `WorkflowOptions` before using a JSON formatter to write the actual bytes onto the wire. It'd make the centaur tests more fiddly to create bad JSON but make everyone else using the client have a much nicer time!. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2516/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2516#issuecomment-320308619
https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013:53,Safety,abort,abort,53,"It's working as expected at least, Cromwell does not abort running jobs when a workflow fails. It just stops tracking them and fails the workflow without starting new jobs.; `ContinueWhilePossible` makes Cromwell continue to start new jobs ""while possible"" even if some jobs have failed.; In neither case does Cromwell abort running jobs when the workflow fails though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013
https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013:319,Safety,abort,abort,319,"It's working as expected at least, Cromwell does not abort running jobs when a workflow fails. It just stops tracking them and fails the workflow without starting new jobs.; `ContinueWhilePossible` makes Cromwell continue to start new jobs ""while possible"" even if some jobs have failed.; In neither case does Cromwell abort running jobs when the workflow fails though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013
https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320301060:246,Safety,abort,abort,246,"Sorry, that was one run. I have another one where ContinueWhilePossible was the default value and the; same issue happened. On Aug 4, 2017 12:50 PM, ""Thib"" <notifications@github.com> wrote:. > It's working as expected at least, Cromwell does not abort running jobs; > when a workflow fails. It just stops tracking them and fails the workflow; > without starting new jobs.; > ContinueWhilePossible makes Cromwell continue to start new jobs ""while; > possible"" even if some jobs have failed.; > In neither case does Cromwell abort running jobs when the workflow fails; > though.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk9wqG1Sd-sxUhIChB1UaFrpU1rQZks5sU0urgaJpZM4Ot1MJ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320301060
https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320301060:523,Safety,abort,abort,523,"Sorry, that was one run. I have another one where ContinueWhilePossible was the default value and the; same issue happened. On Aug 4, 2017 12:50 PM, ""Thib"" <notifications@github.com> wrote:. > It's working as expected at least, Cromwell does not abort running jobs; > when a workflow fails. It just stops tracking them and fails the workflow; > without starting new jobs.; > ContinueWhilePossible makes Cromwell continue to start new jobs ""while; > possible"" even if some jobs have failed.; > In neither case does Cromwell abort running jobs when the workflow fails; > though.; >; > ‚Äî; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk9wqG1Sd-sxUhIChB1UaFrpU1rQZks5sU0urgaJpZM4Ot1MJ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320301060
https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-342588430:122,Safety,Abort,AbortJobsOnFailure,122,"This looks like a feature request along the line to what @cjllanwarne suggested. It could be a third execution mode like ""AbortJobsOnFailure"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-342588430
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502:159,Availability,error,error,159,"In this situation, metadata requests (via, eg., `curl` or similar) either hang (seemingly) indefinitely or time out with a message like these:. ```; status"": ""error"",; ""message"": ""Communications link failure\n\nThe last packet successfully received from the server was 3 milliseconds ago. The last packet sent successfully to the server was 173,470 milliseconds ago.""; }; ```. ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502:200,Availability,failure,failure,200,"In this situation, metadata requests (via, eg., `curl` or similar) either hang (seemingly) indefinitely or time out with a message like these:. ```; status"": ""error"",; ""message"": ""Communications link failure\n\nThe last packet successfully received from the server was 3 milliseconds ago. The last packet sent successfully to the server was 173,470 milliseconds ago.""; }; ```. ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502:123,Integrability,message,message,123,"In this situation, metadata requests (via, eg., `curl` or similar) either hang (seemingly) indefinitely or time out with a message like these:. ```; status"": ""error"",; ""message"": ""Communications link failure\n\nThe last packet successfully received from the server was 3 milliseconds ago. The last packet sent successfully to the server was 173,470 milliseconds ago.""; }; ```. ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502:169,Integrability,message,message,169,"In this situation, metadata requests (via, eg., `curl` or similar) either hang (seemingly) indefinitely or time out with a message like these:. ```; status"": ""error"",; ""message"": ""Communications link failure\n\nThe last packet successfully received from the server was 3 milliseconds ago. The last packet sent successfully to the server was 173,470 milliseconds ago.""; }; ```. ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321421217:27,Availability,failure,failures,27,üëç üëç . This makes debugging failures so difficult. Even with includeKey and excludeKey.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321421217
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962:180,Availability,error,errors,180,The slick exceptions could technically cause this but no this is more generally due to the metadata building process being very expensive therefore causing all kinds of timeouts / errors.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962:169,Safety,timeout,timeouts,169,The slick exceptions could technically cause this but no this is more generally due to the metadata building process being very expensive therefore causing all kinds of timeouts / errors.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694:227,Integrability,rout,routinely,227,"There's a good forum post on this topic here, including not only `includeKey` and `excludeKey` but also increasing Akka HTTP timeouts. We might want to change those timeout defaults in `reference.conf` if users are seeing this routinely even with those filters. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694:125,Safety,timeout,timeouts,125,"There's a good forum post on this topic here, including not only `includeKey` and `excludeKey` but also increasing Akka HTTP timeouts. We might want to change those timeout defaults in `reference.conf` if users are seeing this routinely even with those filters. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694:165,Safety,timeout,timeout,165,"There's a good forum post on this topic here, including not only `includeKey` and `excludeKey` but also increasing Akka HTTP timeouts. We might want to change those timeout defaults in `reference.conf` if users are seeing this routinely even with those filters. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694
https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-642789149:21,Deployability,release,release,21,"Yes, the Cromwell 51 release completed the metadata archiving implementation and it is now operating in production at Broad Institute. Archiving is targeted at our internal production instance only so it's not an officially documented or supported feature, but an enterprising Cromwell admin could surely get it working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-642789149
https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837:161,Deployability,configurat,configuration,161,"Content seems good üëç . ToL: If you don't want to in this PR, I might reorder this so that it goes from least-scary to most-scary, eg:. 1. How do I create my own configuration file for Cromwell; 1. A ""hello world"" example; 1. Description of all the options I can put in my configuration file; 1. What is reference.conf for and where is it?; 1. What is application.conf for and where is it?. Maybe with 2/3 swapped?. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837
https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837:272,Deployability,configurat,configuration,272,"Content seems good üëç . ToL: If you don't want to in this PR, I might reorder this so that it goes from least-scary to most-scary, eg:. 1. How do I create my own configuration file for Cromwell; 1. A ""hello world"" example; 1. Description of all the options I can put in my configuration file; 1. What is reference.conf for and where is it?; 1. What is application.conf for and where is it?. Maybe with 2/3 swapped?. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837
https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837:161,Modifiability,config,configuration,161,"Content seems good üëç . ToL: If you don't want to in this PR, I might reorder this so that it goes from least-scary to most-scary, eg:. 1. How do I create my own configuration file for Cromwell; 1. A ""hello world"" example; 1. Description of all the options I can put in my configuration file; 1. What is reference.conf for and where is it?; 1. What is application.conf for and where is it?. Maybe with 2/3 swapped?. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837
https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837:272,Modifiability,config,configuration,272,"Content seems good üëç . ToL: If you don't want to in this PR, I might reorder this so that it goes from least-scary to most-scary, eg:. 1. How do I create my own configuration file for Cromwell; 1. A ""hello world"" example; 1. Description of all the options I can put in my configuration file; 1. What is reference.conf for and where is it?; 1. What is application.conf for and where is it?. Maybe with 2/3 swapped?. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837
https://github.com/broadinstitute/cromwell/issues/2521#issuecomment-326039066:157,Integrability,wrap,wrap,157,"I think this [happened](https://github.com/broadinstitute/wdl4s/blob/develop/wom/src/main/scala/wdl4s/wom/expression/WomExpression.scala#L23) as part of my ""wrap WDL expressions in WOM"" work, so this might be closeable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2521#issuecomment-326039066
https://github.com/broadinstitute/cromwell/issues/2523#issuecomment-326039856:12,Integrability,interface,interface,12,"I think the interface part of this is done, but the implementation is definitely not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2523#issuecomment-326039856
https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321086048:17,Deployability,configurat,configuration,17,@LeeTL1220 ; The configuration path seems to be different from the one you have (don't know if / when / why it changed).; Here's the current location for default runtime attributes: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf#L531. I did check and this is honored,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321086048
https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321086048:17,Modifiability,config,configuration,17,@LeeTL1220 ; The configuration path seems to be different from the one you have (don't know if / when / why it changed).; Here's the current location for default runtime attributes: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf#L531. I did check and this is honored,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321086048
https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321121155:11,Deployability,update,update,11,"Okay, I'll update and see if it works. Regardless, you can close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321121155
https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321550028:36,Deployability,update,update,36,"@LeeTL1220 thx for the info, I will update the reference.conf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321550028
https://github.com/broadinstitute/cromwell/issues/2530#issuecomment-321527666:100,Deployability,release,released-making-way-for-big-changes,100,Draft of blog post is here: https://gatkforums.broadinstitute.org/dsde/discussion/10143/cromwell-29-released-making-way-for-big-changes/p1?new=1. Branch of changelog in github is here: https://github.com/broadinstitute/cromwell/tree/kv_crom29_relnotes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2530#issuecomment-321527666
https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565:83,Deployability,update,update,83,"@cjllanwarne Yes, my money is that you have pegged this exactly. I would love this update. I prefer the ``Int? mem=4`` for specifying default values. Otherwise, we have to pepper our command and runtime blocks with ``${default=4 mem}`` or, even worse, something with ``select_first``. We often have inputs that are derived (e.g. ``e``) and we do not want these exposed in ``wdltool inputs ...``. I do not have a good idea for how to handle ``f``. I'm assuming you do not have access to the raw expression when rendering ``wdltool inputs ...``, so can you just say that it has a complex default expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565
https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565:361,Security,expose,exposed,361,"@cjllanwarne Yes, my money is that you have pegged this exactly. I would love this update. I prefer the ``Int? mem=4`` for specifying default values. Otherwise, we have to pepper our command and runtime blocks with ``${default=4 mem}`` or, even worse, something with ``select_first``. We often have inputs that are derived (e.g. ``e``) and we do not want these exposed in ``wdltool inputs ...``. I do not have a good idea for how to handle ``f``. I'm assuming you do not have access to the raw expression when rendering ``wdltool inputs ...``, so can you just say that it has a complex default expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565
https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565:476,Security,access,access,476,"@cjllanwarne Yes, my money is that you have pegged this exactly. I would love this update. I prefer the ``Int? mem=4`` for specifying default values. Otherwise, we have to pepper our command and runtime blocks with ``${default=4 mem}`` or, even worse, something with ``select_first``. We often have inputs that are derived (e.g. ``e``) and we do not want these exposed in ``wdltool inputs ...``. I do not have a good idea for how to handle ``f``. I'm assuming you do not have access to the raw expression when rendering ``wdltool inputs ...``, so can you just say that it has a complex default expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565
https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-332137189:211,Usability,Simpl,Simply,211,"Not to hijack this issue, but would it be possible to put the actual default values in the .json file? Right now, users can change an optional value, but it is very hard to figure out what the default value is. Simply putting it in the json file would save users a lot of hunting through .wdl files to figure out what a sensible value is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-332137189
https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:285,Availability,failure,failures,285,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228
https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:304,Availability,failure,failures,304,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228
https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:361,Availability,FAILURE,FAILURES,361,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228
https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:402,Availability,FAILURE,FAILURES,402,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228
https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:213,Deployability,PATCH,PATCH,213,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228
https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:248,Deployability,update,updated,248,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228
https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:379,Integrability,message,message,379,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228
https://github.com/broadinstitute/cromwell/issues/2535#issuecomment-321847390:2025,Availability,ERROR,ERROR,2025,"vedTerminated(ActorCell.scala:374); #011at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:46); #011at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.aroundReceive(JesApiQueryManager.scala:26); #011at akka.actor.Actor$class.aroundReceive(Actor.scala:496); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$receive$1.applyOrElse(JesApiQueryManager.scala:51); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated(JesApiQueryManager.scala:101); #011at scala.collection.immutable.List.foreach(List.scala:381); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated$1.apply(JesApiQueryManager.scala:101); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated$1.apply(JesApiQueryManager.scala:103); cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$JesApiException: Unable to complete JES Api Request; 2017-08-10 08:29:38,408 cromwell-system-akka.dispatchers.engine-dispatcher-64 ERROR - WorkflowManagerActor Workflow e3f4d391-a0bc-480f-9203-d0ef4ee5b876 failed (during ExecutingWorkflowState): Unable to complete JES Api Request; 2017-08-10 08:29:37,713 cromwell-system-akka.dispatchers.io-dispatcher-38 INFO - $e [UUID(fb9254a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.fb9254a8-00ec-48bc-b8c8-2f0e5e74fa81.log to gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/9ef84046-7047-423b-88e8-bb138181f5a8/workflow.logs/workflow.fb9254a8-00ec-48bc-b8c8-2f0e5e74fa81.log; 2017-08-10 08:29:37,713 cromwell-system-akka.dispatchers.engine-dispatcher-6 INFO - WorkflowManagerActor WorkflowActor-fb9254a8-00ec-48bc-b8c8-2f0e5e74fa81 is in a terminal state: WorkflowFailedState```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2535#issuecomment-321847390
