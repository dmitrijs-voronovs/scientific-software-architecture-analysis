id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://hail.is/docs/0.2/change_log.html:87177,Performance,perform,performance,87177," annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88098,Performance,perform,performance,88098,"95) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88146,Performance,perform,performance,88146,"count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88339,Performance,optimiz,optimizations,88339,"tl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88668,Performance,perform,performance,88668,"was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88735,Performance,perform,performance,88735,"-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relate",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88869,Performance,perform,performance,88869," will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. Ne",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88920,Performance,perform,performance,88920,"te NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:89214,Performance,perform,performance,89214,"when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:89286,Performance,perform,performance,89286," Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic c",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:89645,Performance,perform,performance,89645,"ts. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expr",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:89690,Performance,perform,performance,89690,"rove; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:90133,Performance,load,load,90133,"e UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantile",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:90205,Performance,perform,performance,90205," Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:90370,Performance,perform,performance,90370,"ls. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for no",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:90498,Performance,perform,performance,90498,"hose versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; iden",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:90750,Performance,optimiz,optimization,90750,"tion methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Rel",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:90875,Performance,optimiz,optimization,90875," compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#637",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:93363,Performance,optimiz,optimized,93363,"32)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the en",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:93519,Performance,optimiz,optimizer,93519,"og; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broke",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:95220,Performance,perform,performance,95220,"related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886);",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:95754,Performance,perform,performance,95754,"cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail n",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:97100,Performance,optimiz,optimize,97100," samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail now uses; the correct unicode string encoding which resolves a number of issues; when a Table or MatrixTable has a key field containing unicode; characters.; (#5692) When; keyed is True, hl.maximal_independent_set now does not; produce duplicates.; (#5725) Docs now; consistently refer to hl.agg not agg.; (#5730)(#5782); Taught import_bgen to optimize its variants argument. Experimental. (#5732) The; hl.agg.approx_quantiles aggregate computes an approximation of; the quantiles of an expression.; (#5693)(#5396); Table._multi_way_zip_join now correctly handles keys that have; been truncated. Version 0.2.12; Released 2019-03-28. New features. (#5614) Add support; for multiple missing values in hl.import_table.; (#5666) Produce HTML; table output for Table.show() when running in Jupyter notebook. Bug fixes. (#5603)(#5697); Fixed issue where min_partitions on hl.import_table was; non-functional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:100776,Performance,perform,performance,100776,"xporting missing genotypes without trailing; fields. Bug fixes. (#5306) Fix; ReferenceGenome.add_sequence causing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; opti",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:100848,Performance,perform,performance,100848,"sing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:100937,Performance,perform,performance,100937,"ent; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_tabl",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:101742,Performance,optimiz,optimization,101742,"ch should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig re",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:101798,Performance,perform,performance,101798,"many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:101956,Performance,optimiz,optimizer,101956,"30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_indepen",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104041,Performance,optimiz,optimizer,104041,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104356,Performance,optimiz,optimizer,104356,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:16012,Safety,timeout,timeout,16012,"ies; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output pa",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:16093,Safety,timeout,timeout,16093,"ional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remo",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:16425,Safety,avoid,avoid,16425,"regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#142",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:18530,Safety,detect,detected,18530,"lease; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#1381",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22233,Safety,detect,detected,22233," partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:27494,Safety,avoid,avoids,27494,"relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:40796,Safety,unsafe,unsafe,40796,"message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now con",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:49458,Safety,timeout,timeout,49458,"notate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:10582,Security,access,access,10582,"ary version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some da",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:13792,Security,validat,validate,13792,"321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previously internal _num_allele_type as numeric_allele_type; and deprecated it. Add new AlleleType enumeration for users to be; able to easily use the values returned by numeric_allele_type.; (#14297); vds.sample_gc now uses independent aggregators. Users may now; import these functions and use them directly.; (#14405); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning metadata allows rows with; the same key to be split across neighboring partitions, and 2); reading it with a different partitioning than it was written. 1); would likely only happen by reading data keyed by locus and alleles,; and rekeying it to only locus before writing. 2) would likely only; happen by using the _intervals or _n_partitions arguments to; read_(matrix)_table, or possibly repartition. Please reach; out to us if you’re concerned you may have been affected by this.; (#14330) Fixes; erroneous error in export_vcf with unphased haploid Calls.; (#14303) Fix; missingness error when sampling entries from a MatrixTable.; (#14288) Contigs may; now be comp",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:25349,Security,access,access,25349,"uired when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:30515,Security,access,accessing,30515,"Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. B",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:31472,Security,access,access,31472,"ions can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:41847,Security,access,access,41847,"y leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but de",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:51377,Security,access,accessible,51377,"th the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.8",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:70611,Security,access,accessing,70611,"lease 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict asser",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:8727,Testability,log,log,8727,"project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot r",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12007,Testability,test,tests,12007,"initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service acc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12356,Testability,test,test,12356,".131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:12619,Testability,log,log,12619,"ct that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previously internal _num_allele_type as numeric_allele_type; and deprecated it. Add new AlleleType enumeration for users to be; able to easily us",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:13217,Testability,test,test,13217,"g Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previously internal _num_allele_type as numeric_allele_type; and deprecated it. Add new AlleleType enumeration for users to be; able to easily use the values returned by numeric_allele_type.; (#14297); vds.sample_gc now uses independent aggregators. Users may now; import these functions and use them directly.; (#14405); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning meta",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22299,Testability,log,logic,22299,"mat datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:22311,Testability,log,logic,22311,"lines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; -",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:30888,Testability,log,logic,30888,"comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentati",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:32703,Testability,log,logs,32703,"for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:32828,Testability,log,log,32828,"for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results a",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:33472,Testability,assert,assertion,33472,"t; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referenc",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:36757,Testability,log,logs,36757,"ue for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Qu",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:37658,Testability,log,logistic,37658,"f partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:39130,Testability,log,logistic,39130,"_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:40487,Testability,assert,assertion,40487,"ow throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:47675,Testability,log,logged,47675,"2062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching header line computation. Bug fixes. (#12115) When using; use_new_shuffle=True, fix a bug when there are more than 2^31; rows; (#12074) Fix bug; where hl.init could silently overwrite the global random seed.; (#12079) Fix bug in; handling of missing (aka NA) fields in grouped aggregation and; distinct by key.; (#12056) Fix; hl.export_vcf to actually create tabix files when requested.; (#12020) Fix bug in; hl.experimental.densify which manifested as an AssertionError; about dtypes. Version 0.2.97; Released 2022-06-30. New Features. (#11756); hb.BatchPoolExecutor and Python jobs both now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#118",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52220,Testability,assert,assertion,52220,"also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix erro",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:52922,Testability,assert,assertion,52922,"from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:53192,Testability,assert,assertion,53192,6) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:54081,Testability,log,log,54081,"icate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ndarrays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:54135,Testability,log,log,54135,".2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ndarrays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:58678,Testability,log,logic,58678," operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /h",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:59639,Testability,log,logs,59639,") Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /home/hail by default. Version 0.2.68; Released 2021-05-27. Version 0.2.67. Critical performance fix; Released 2021-05-06. (#10451) Fixed a; memory leak / performance bug triggered by; hl.literal(...).contains(...). Version 0.2.66; Released 2021-05-03. New features. (#10398) Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Rele",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:67119,Testability,log,logic,67119,"e Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, Matr",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71149,Testability,assert,assertion,71149,"ling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which con",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71217,Testability,assert,assertion,71217,"ClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Incre",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:71619,Testability,assert,assertion,71619,"nt of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify t",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:72005,Testability,log,logging,72005,"aproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:79392,Testability,assert,assertion,79392,"n to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Gene",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:85581,Testability,log,log,85581,"lled packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl datap",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:88512,Testability,assert,assertion,88512,"on 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:92578,Testability,log,log,92578," native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as inte",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:97875,Testability,log,log,97875,"ld containing unicode; characters.; (#5692) When; keyed is True, hl.maximal_independent_set now does not; produce duplicates.; (#5725) Docs now; consistently refer to hl.agg not agg.; (#5730)(#5782); Taught import_bgen to optimize its variants argument. Experimental. (#5732) The; hl.agg.approx_quantiles aggregate computes an approximation of; the quantiles of an expression.; (#5693)(#5396); Table._multi_way_zip_join now correctly handles keys that have; been truncated. Version 0.2.12; Released 2019-03-28. New features. (#5614) Add support; for multiple missing values in hl.import_table.; (#5666) Produce HTML; table output for Table.show() when running in Jupyter notebook. Bug fixes. (#5603)(#5697); Fixed issue where min_partitions on hl.import_table was; non-functional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer joins on; MatrixTables. Version 0.2.11; Released 2019-03-06. New features. (#5374) Add default; arguments to hl.add_sequence for running on GCP.; (#5481) Added; sample_cols method to MatrixTable.; (#5501) Exposed; MatrixTable.unfilter_entries. See filter_entries; documentation for more information.; (#5480) Added; n_cols argument to MatrixTable.head.; (#5529) Added; Table.{semi_join, anti_join} and; MatrixTable.{semi_join_rows, semi_join_cols, anti_join_rows, anti_join_cols}.; (#5528) Added; {MatrixTable, Table}.checkpoint methods as wrappers around; write / read_{matrix_table, table}. Bug fixes. (#5416) Resolved; issue wherein VEP and certain regressions were recomputed on each; use, rather than once.; (#5419) Resolved; issue with import_vcf force_bgz and file size che",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103425,Testability,assert,assertions,103425,"ion_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:103878,Testability,log,logging,103878,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104051,Testability,assert,assertion,104051,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104144,Testability,assert,assertion,104144,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:104366,Testability,assert,assertion,104366,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:20889,Usability,clear,clearly,20889,".; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:36636,Usability,simpl,simple,36636," bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logi",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:43159,Usability,progress bar,progress bar,43159," Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:43972,Usability,progress bar,progress bars,43972,"m and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integer or floating point type.; (#12277) Fixed bug; in reading tables/matrixtables with partition intervals that led to; error or segfault. Version 0.2.101; Released 2022-10-04. New Features. (#12218) Support; missing values in pr",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:50081,Usability,progress bar,progress bar,50081,"rance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/change_log.html:84997,Usability,progress bar,progress bar,84997,"ead tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
https://hail.is/docs/0.2/cheatsheets.html:790,Deployability,update,updated,790,"﻿. Hail | ; Cheat Sheets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Cheat Sheets. View page source. Cheat Sheets. Note; Hail’s cheat sheets are relatively new. We welcome suggestions; for additional cheatsheets, as well as feedback about our documentation. If; you’d like to add a cheatsheet to the documentation, make a pull request!. Hail Tables Cheat Sheet; Hail MatrixTables Cheat Sheet. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cheatsheets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cheatsheets.html
https://hail.is/docs/0.2/cheatsheets.html:564,Usability,feedback,feedback,564,"﻿. Hail | ; Cheat Sheets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Cheat Sheets. View page source. Cheat Sheets. Note; Hail’s cheat sheets are relatively new. We welcome suggestions; for additional cheatsheets, as well as feedback about our documentation. If; you’d like to add a cheatsheet to the documentation, make a pull request!. Hail Tables Cheat Sheet; Hail MatrixTables Cheat Sheet. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cheatsheets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cheatsheets.html
https://hail.is/docs/0.2/configuration_reference.html:2154,Availability,error,erroring,2154,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:953,Deployability,configurat,configuration,953,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1071,Deployability,configurat,configuration,1071," Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Pre",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1485,Deployability,configurat,configuration,1485,"ce. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not ha",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:2803,Deployability,update,updated,2803,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:537,Modifiability,variab,variables,537,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:662,Modifiability,config,config,662,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:767,Modifiability,variab,variables,767,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:885,Modifiability,variab,variable,885,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:953,Modifiability,config,configuration,953,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:967,Modifiability,variab,variables,967,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1000,Modifiability,variab,variables,1000,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1071,Modifiability,config,configuration,1071," Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Pre",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1085,Modifiability,variab,variable,1085," Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Pre",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1156,Modifiability,variab,variable,1156," Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Pre",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1207,Modifiability,variab,variables,1207," Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Pre",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1244,Modifiability,variab,variables,1244,"orted Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only t",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1321,Modifiability,variab,variables,1321,"e; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be config",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1368,Modifiability,variab,variables,1368,"e; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be config",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:1485,Modifiability,config,configuration,1485,"ce. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not ha",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:2349,Modifiability,config,configured,2349,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/configuration_reference.html:2430,Security,access,access,2430,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
https://hail.is/docs/0.2/datasets.html:737,Availability,avail,available,737,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
https://hail.is/docs/0.2/datasets.html:983,Deployability,pipeline,pipeline,983,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
https://hail.is/docs/0.2/datasets.html:1412,Deployability,update,updated,1412,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
https://hail.is/docs/0.2/datasets.html:1170,Energy Efficiency,charge,charges,1170,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
https://hail.is/docs/0.2/datasets.html:943,Performance,load,load,943,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
https://hail.is/docs/0.2/expressions.html:2327,Deployability,update,updated,2327," experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions. View page source. Expressions. eval(expression); Evaluate a Hail expression, returning the result. Expression; Base class for Hail expressions. ArrayExpression; Expression of type tarray. ArrayNumericExpression; Expression of type tarray with a numeric type. BooleanExpression; Expression of type tbool. CallExpression; Expression of type tcall. CollectionExpression; Expression of type tarray or tset. DictExpression; Expression of type tdict. IntervalExpression; Expression of type tinterval. LocusExpression; Expression of type tlocus. NumericExpression; Expression of numeric type. Int32Expression; Expression of type tint32. Int64Expression; Expression of type tint64. Float32Expression; Expression of type tfloat32. Float64Expression; Expression of type tfloat64. SetExpression; Expression of type tset. StringExpression; Expression of type tstr. StructExpression; Expression of type tstruct. TupleExpression; Expression of type ttuple. NDArrayExpression; Expression of type tndarray. NDArrayNumericExpression; Expression of type tndarray with a numeric element type. hail.expr.eval(expression)[source]; Evaluate a Hail expression, returning the result.; This method is extremely useful for learning about Hail expressions and; understanding how to compose them.; The expression must have no indices, but can refer to the globals; of a Table or MatrixTable.; Examples; Evaluate a conditional:; >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters:; expression (Expression) – Any expression, or a Python value that can be implicitly interpreted as an expression. Returns:; Any. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/expressions.html
https://hail.is/docs/0.2/expressions.html:1855,Usability,learn,learning,1855," experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions. View page source. Expressions. eval(expression); Evaluate a Hail expression, returning the result. Expression; Base class for Hail expressions. ArrayExpression; Expression of type tarray. ArrayNumericExpression; Expression of type tarray with a numeric type. BooleanExpression; Expression of type tbool. CallExpression; Expression of type tcall. CollectionExpression; Expression of type tarray or tset. DictExpression; Expression of type tdict. IntervalExpression; Expression of type tinterval. LocusExpression; Expression of type tlocus. NumericExpression; Expression of numeric type. Int32Expression; Expression of type tint32. Int64Expression; Expression of type tint64. Float32Expression; Expression of type tfloat32. Float64Expression; Expression of type tfloat64. SetExpression; Expression of type tset. StringExpression; Expression of type tstr. StructExpression; Expression of type tstruct. TupleExpression; Expression of type ttuple. NDArrayExpression; Expression of type tndarray. NDArrayNumericExpression; Expression of type tndarray with a numeric element type. hail.expr.eval(expression)[source]; Evaluate a Hail expression, returning the result.; This method is extremely useful for learning about Hail expressions and; understanding how to compose them.; The expression must have no indices, but can refer to the globals; of a Table or MatrixTable.; Examples; Evaluate a conditional:; >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters:; expression (Expression) – Any expression, or a Python value that can be implicitly interpreted as an expression. Returns:; Any. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/expressions.html
https://hail.is/docs/0.2/fs_api.html:3005,Availability,error,error,3005,"ata/LCR.interval_list') . Notes; If you are copying a file just to then load it into Python, you can use; open() instead. For example:; >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: ; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hailtop.fs.exists(path, *, requester_pays_config=None)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hailtop.fs.is_dir(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hailtop.fs.is_file(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hailtop.fs.ls(path, *, requester_pays_config=None)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hailtop.fs.mkdir(path, *, requester_pays_config=None)[source]; Ensure files can be created whose dirname is path. Warning; On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. hailtop.fs.open(path, mode='r', buffer_size=8192, *, requester_pays_config=None)[source]; Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS.; Examp",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/fs_api.html:5464,Availability,error,error,5464,"uester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:; >>> with hfs.open( ; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 ",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/fs_api.html:5679,Availability,error,error,5679,"bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/fs_api.html:6806,Availability,error,error,6806,"bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_pays_config=None)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/fs_api.html:7204,Deployability,update,updated,7204,"bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_pays_config=None)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/fs_api.html:2082,Performance,load,load,2082,"uester_pays_config]); Returns True if path both exists and is a file. ls(path, *[, requester_pays_config]); Returns information about files at path. mkdir(path, *[, requester_pays_config]); Ensure files can be created whose dirname is path. open(path[, mode, buffer_size, ...]); Open a file from the local filesystem of from blob storage. remove(path, *[, requester_pays_config]); Removes the file at path. rmtree(path, *[, requester_pays_config]); Recursively remove all files under the given path. stat(path, *[, requester_pays_config]); Returns information about the file or directory at a given path. hailtop.fs.copy(src, dest, *, requester_pays_config=None)[source]; Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; If you are copying a file just to then load it into Python, you can use; open() instead. For example:; >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: ; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hailtop.fs.exists(path, *, requester_pays_config=None)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hailtop.fs.is_dir(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hailtop.fs.is_file(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hailtop.fs.ls(path, *, requester_pays_config=None)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one elem",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/fs_api.html:769,Security,access,access,769,"﻿. Hail | ; hailtop.fs Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; Top-Level Functions; copy(); exists(); is_dir(); is_file(); ls(); mkdir(); open(); remove(); rmtree(); stat(). hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.fs Python API. View page source. hailtop.fs Python API; This is the API documentation for Hail’s cloud-agnostic file system implementation in hailtop.fs.; Use import hailtop.fs as hfs to access this functionality. Top-Level Functions. copy(src, dest, *[, requester_pays_config]); Copy a file between filesystems. exists(path, *[, requester_pays_config]); Returns True if path exists. is_dir(path, *[, requester_pays_config]); Returns True if path both exists and is a directory. is_file(path, *[, requester_pays_config]); Returns True if path both exists and is a file. ls(path, *[, requester_pays_config]); Returns information about files at path. mkdir(path, *[, requester_pays_config]); Ensure files can be created whose dirname is path. open(path[, mode, buffer_size, ...]); Open a file from the local filesystem of from blob storage. remove(path, *[, requester_pays_config]); Removes the file at path. rmtree(path, *[, requester_pays_config]); Recursively remove all files under the given path. stat(path, *[, requester_pays_config]); Returns information about the file or directory at a given path. hailtop.fs.copy(src, dest, *, requester_pays_config=None)[source]; Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/fs_api.html:4651,Security,access,access,4651,"se dirname is path. Warning; On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. hailtop.fs.open(path, mode='r', buffer_size=8192, *, requester_pays_config=None)[source]; Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS.; Examples; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: ; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt') as f: ; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:; >>> with hfs.open( ; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:; >>> with hfs.open( ; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclu",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/fs_api.html:5897,Security,access,access,5897,"torage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_pays_config=None)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. i",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
https://hail.is/docs/0.2/genindex.html:13252,Availability,checkpoint,checkpoint,13252, hail.expr.functions). bit_and() (in module hail.expr.functions). bit_count() (in module hail.expr.functions). bit_lshift() (in module hail.expr.functions). bit_not() (in module hail.expr.functions). bit_or() (in module hail.expr.functions). bit_rshift() (in module hail.expr.functions). bit_xor() (in module hail.expr.functions). block_size (hail.linalg.BlockMatrix property). BlockMatrix (class in hail.linalg). bool() (in module hail.expr.functions). BooleanExpression (class in hail.expr). C. cache() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). calculate_phenotypes() (in module hail.experimental.ldscsim). Call (class in hail.genetics). call() (in module hail.expr.functions). call_stats() (in module hail.expr.aggregators). CallExpression (class in hail.expr). case() (in module hail.expr.functions). CaseBuilder (class in hail.expr.builders). cdf() (in module hail.plot). ceil() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). checkpoint() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). chi_squared_test() (in module hail.expr.functions). choose_cols() (hail.MatrixTable method). citation() (in module hail). coalesce() (in module hail.expr.functions). cochran_mantel_haenszel_test() (in module hail.expr.functions). col (hail.MatrixTable property). col_key (hail.MatrixTable property). col_value (hail.MatrixTable property). collect() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.BooleanExpression method). (hail.expr.CallExpression method). (hail.expr.CollectionExpression method). (hail.expr.DictExpression method). (hail.expr.Expression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:18156,Availability,down,downcode,18156,ail.expr.DictExpression method). (hail.expr.Expression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.NumericExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). (hail.expr.StructExpression method). (hail.expr.TupleExpression method). (hail.GroupedMatrixTable method). (hail.MatrixTable method). (hail.Table method). diagonal() (hail.linalg.BlockMatrix method). (in module hail.nd). dict() (in module hail.expr.functions). DictExpression (class in hail.expr). difference() (hail.expr.SetExpression method). distinct() (hail.Table method). distinct_by_col() (hail.MatrixTable method). distinct_by_row() (hail.MatrixTable method). dnorm() (in module hail.expr.functions). downcode() (in module hail.expr.functions). downsample() (in module hail.expr.aggregators). dpois() (in module hail.expr.functions). drop() (hail.expr.StructExpression method). (hail.MatrixTable method). (hail.Table method). dtype (hail.expr.ArrayExpression property). (hail.expr.ArrayNumericExpression property). (hail.expr.BooleanExpression property). (hail.expr.CallExpression property). (hail.expr.CollectionExpression property). (hail.expr.DictExpression property). (hail.expr.Expression property). (hail.expr.Float32Expression property). (hail.expr.Float64Expression property). (hail.expr.Int32Expression property). (hail.expr.Int64Expression property). (hail.expr.IntervalExpression property). (hail.expr.LocusExpression property). (hail.expr.NDArrayExpression property). (hail.expr.NDArrayNumericExpression property). (hail.expr.NumericExpression property). (hail.expr.SetExpression property). (hail.expr.StringExpression property). (hail.expr.StructExpression property). (hail.expr.TupleExpression proper,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:18200,Availability,down,downsample,18200,xpression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.NumericExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). (hail.expr.StructExpression method). (hail.expr.TupleExpression method). (hail.GroupedMatrixTable method). (hail.MatrixTable method). (hail.Table method). diagonal() (hail.linalg.BlockMatrix method). (in module hail.nd). dict() (in module hail.expr.functions). DictExpression (class in hail.expr). difference() (hail.expr.SetExpression method). distinct() (hail.Table method). distinct_by_col() (hail.MatrixTable method). distinct_by_row() (hail.MatrixTable method). dnorm() (in module hail.expr.functions). downcode() (in module hail.expr.functions). downsample() (in module hail.expr.aggregators). dpois() (in module hail.expr.functions). drop() (hail.expr.StructExpression method). (hail.MatrixTable method). (hail.Table method). dtype (hail.expr.ArrayExpression property). (hail.expr.ArrayNumericExpression property). (hail.expr.BooleanExpression property). (hail.expr.CallExpression property). (hail.expr.CollectionExpression property). (hail.expr.DictExpression property). (hail.expr.Expression property). (hail.expr.Float32Expression property). (hail.expr.Float64Expression property). (hail.expr.Int32Expression property). (hail.expr.Int64Expression property). (hail.expr.IntervalExpression property). (hail.expr.LocusExpression property). (hail.expr.NDArrayExpression property). (hail.expr.NDArrayNumericExpression property). (hail.expr.NumericExpression property). (hail.expr.SetExpression property). (hail.expr.StringExpression property). (hail.expr.StructExpression property). (hail.expr.TupleExpression property). dtype() (in module hail.expr.types). E. ,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:52812,Deployability,update,updated,52812,".functions). UNKNOWN (hail.genetics.AlleleType attribute). unpersist() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). unphase() (hail.expr.CallExpression method). unphased_diploid_gt_index() (hail.expr.CallExpression method). (hail.genetics.Call method). unphased_diploid_gt_index_call() (in module hail.expr.functions). upper() (hail.expr.StringExpression method). V. validate() (hail.vds.VariantDataset method). values() (hail.expr.DictExpression method). (hail.expr.StructExpression method). variant_qc() (in module hail.methods). variant_str() (in module hail.expr.functions). VariantDataset (class in hail.vds). VariantDatasetCombiner (class in hail.vds.combiner). vars() (in module hail.ggplot). VDSMetadata (class in hail.vds.combiner). vep() (in module hail.methods). VEPConfig (class in hail.methods). VEPConfigGRCh37Version85 (class in hail.methods). VEPConfigGRCh38Version95 (class in hail.methods). version() (in module hail). visualize_missingness() (in module hail.plot). vstack() (in module hail.nd). W. when() (hail.expr.builders.CaseBuilder method). (hail.expr.builders.SwitchBuilder method). when_missing() (hail.expr.builders.SwitchBuilder method). window() (hail.expr.LocusExpression method). write() (hail.genetics.Pedigree method). (hail.genetics.ReferenceGenome method). (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). write_expression() (in module hail.experimental). write_from_entry_expr() (hail.linalg.BlockMatrix static method). write_image() (hail.ggplot.GGPlot method). write_many() (hail.Table method). X. x_contigs (hail.genetics.ReferenceGenome property). xlab() (in module hail.ggplot). Y. y_contigs (hail.genetics.ReferenceGenome property). ylab() (in module hail.ggplot). Z. zeros() (in module hail.nd). zip() (in module hail.expr.functions). zip_with_index() (in module hail.expr.functions). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:21288,Modifiability,extend,extend,21288,on method). (hail.expr.Expression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.NumericExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). (hail.expr.StructExpression method). (hail.expr.TupleExpression method). (hail.linalg.BlockMatrix static method). (hail.Table method). export_bgen() (in module hail.methods). export_blocks() (hail.linalg.BlockMatrix method). export_elasticsearch() (in module hail.methods). export_entries_by_col() (in module hail.experimental). export_gen() (in module hail.methods). export_plink() (in module hail.methods). export_rectangles() (hail.linalg.BlockMatrix method). export_vcf() (in module hail.methods). Expression (class in hail.expr). extend() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). eye() (in module hail.nd). F. facet_wrap() (in module hail.ggplot). fam_id (hail.genetics.Trio property). FigureAttribute (class in hail.ggplot). fill() (hail.linalg.BlockMatrix class method). filter() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.SetExpression method). (hail.linalg.BlockMatrix method). (hail.Table method). (in module hail.expr.aggregators). (in module hail.expr.functions). filter_alleles() (in module hail.methods). filter_alleles_hts() (in module hail.methods). filter_chromosomes() (in module hail.vds). filter_cols() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). filter_entries() (hail.MatrixTable method). filter_intervals() (in module hail.methods). (in module hail.vds). filter_rows() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). filter_samples() (in module hail.vds). filter_,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:12758,Performance,cache,cache,12758,r.aggregators). array_windows() (in module hail.linalg.utils). ArrayExpression (class in hail.expr). ArrayNumericExpression (class in hail.expr). asc() (in module hail). ascertainment_bias() (in module hail.experimental.ldscsim). available_datasets (hail.experimental.DB property). B. balding_nichols_model() (in module hail.methods). binarize() (in module hail.experimental.ldscsim). binary_search() (in module hail.expr.functions). bind() (in module hail.expr.functions). binom_test() (in module hail.expr.functions). bit_and() (in module hail.expr.functions). bit_count() (in module hail.expr.functions). bit_lshift() (in module hail.expr.functions). bit_not() (in module hail.expr.functions). bit_or() (in module hail.expr.functions). bit_rshift() (in module hail.expr.functions). bit_xor() (in module hail.expr.functions). block_size (hail.linalg.BlockMatrix property). BlockMatrix (class in hail.linalg). bool() (in module hail.expr.functions). BooleanExpression (class in hail.expr). C. cache() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). calculate_phenotypes() (in module hail.experimental.ldscsim). Call (class in hail.genetics). call() (in module hail.expr.functions). call_stats() (in module hail.expr.aggregators). CallExpression (class in hail.expr). case() (in module hail.expr.functions). CaseBuilder (class in hail.expr.builders). cdf() (in module hail.plot). ceil() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). checkpoint() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). chi_squared_test() (in module hail.expr.functions). choose_cols() (hail.MatrixTable method). citation() (in module hail). coalesce() (in module hail.expr.functions). cochran_mantel_haenszel_test() (in module hail.expr.functions). col (hail.MatrixTable property). col_key (hail.MatrixTable property). col_value (hail.MatrixTable property). collect() (hail.expr.ArrayExpression method).,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:32872,Performance,load,load,32872,ssion method). king() (in module hail.methods). L. lambda_gc() (in module hail.methods). last() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). ld_matrix() (in module hail.methods). ld_prune() (in module hail.methods). ld_score() (in module hail.experimental). ld_score_regression() (in module hail.experimental). len() (in module hail.expr.functions). length() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). lengths (hail.genetics.ReferenceGenome property). lgt_to_gt() (in module hail.vds). liftover() (in module hail.expr.functions). linear_mixed_model() (in module hail.methods). linear_mixed_regression_rows() (in module hail.methods). linear_regression_rows() (in module hail.methods). LinearMixedModel (class in hail.stats). linreg() (in module hail.expr.aggregators). literal() (in module hail.expr.functions). load() (hail.vds.combiner.VariantDatasetCombiner static method). load_combiner() (in module hail.vds.combiner). load_dataset() (in module hail.experimental). local_to_global() (in module hail.vds). localize_entries() (hail.MatrixTable method). Locus (class in hail.genetics). locus() (in module hail.expr.functions). locus_from_global_position() (hail.genetics.ReferenceGenome method). (in module hail.expr.functions). locus_interval() (in module hail.expr.functions). locus_windows() (in module hail.linalg.utils). LocusExpression (class in hail.expr). log() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). log10() (in module hail.expr.functions). logistic_regression_rows() (in module hail.methods). logit() (in module hail.expr.functions). loop() (in module hail.experimental). lower() (hail.expr.StringExpression method). ls() (in module hailtop.fs). M. make_betas() (in module hail.experimental.ldscsim). make_table() (hail.MatrixTable method). manhattan() (in module hail.plot). ma,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:51244,Security,validat,validate,51244,n module hail.methods). trios (hail.genetics.Pedigree property). truncate_reference_blocks() (in module hail.vds). tset (class in hail.expr.types). tstr (in module hail.expr.types). tstruct (class in hail.expr.types). ttuple (class in hail.expr.types). tuple() (in module hail.expr.functions). TupleExpression (class in hail.expr). U. unfilter_entries() (hail.MatrixTable method). union() (hail.expr.SetExpression method). (hail.Table method). union_cols() (hail.MatrixTable method). union_rows() (hail.MatrixTable method). (hail.vds.VariantDataset method). uniroot() (in module hail.expr.functions). UNKNOWN (hail.genetics.AlleleType attribute). unpersist() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). unphase() (hail.expr.CallExpression method). unphased_diploid_gt_index() (hail.expr.CallExpression method). (hail.genetics.Call method). unphased_diploid_gt_index_call() (in module hail.expr.functions). upper() (hail.expr.StringExpression method). V. validate() (hail.vds.VariantDataset method). values() (hail.expr.DictExpression method). (hail.expr.StructExpression method). variant_qc() (in module hail.methods). variant_str() (in module hail.expr.functions). VariantDataset (class in hail.vds). VariantDatasetCombiner (class in hail.vds.combiner). vars() (in module hail.ggplot). VDSMetadata (class in hail.vds.combiner). vep() (in module hail.methods). VEPConfig (class in hail.methods). VEPConfigGRCh37Version85 (class in hail.methods). VEPConfigGRCh38Version95 (class in hail.methods). version() (in module hail). visualize_missingness() (in module hail.plot). vstack() (in module hail.nd). W. when() (hail.expr.builders.CaseBuilder method). (hail.expr.builders.SwitchBuilder method). when_missing() (hail.expr.builders.SwitchBuilder method). window() (hail.expr.LocusExpression method). write() (hail.genetics.Pedigree method). (hail.genetics.ReferenceGenome method). (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:33426,Testability,log,log,33426,expr.StringExpression method). lengths (hail.genetics.ReferenceGenome property). lgt_to_gt() (in module hail.vds). liftover() (in module hail.expr.functions). linear_mixed_model() (in module hail.methods). linear_mixed_regression_rows() (in module hail.methods). linear_regression_rows() (in module hail.methods). LinearMixedModel (class in hail.stats). linreg() (in module hail.expr.aggregators). literal() (in module hail.expr.functions). load() (hail.vds.combiner.VariantDatasetCombiner static method). load_combiner() (in module hail.vds.combiner). load_dataset() (in module hail.experimental). local_to_global() (in module hail.vds). localize_entries() (hail.MatrixTable method). Locus (class in hail.genetics). locus() (in module hail.expr.functions). locus_from_global_position() (hail.genetics.ReferenceGenome method). (in module hail.expr.functions). locus_interval() (in module hail.expr.functions). locus_windows() (in module hail.linalg.utils). LocusExpression (class in hail.expr). log() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). log10() (in module hail.expr.functions). logistic_regression_rows() (in module hail.methods). logit() (in module hail.expr.functions). loop() (in module hail.experimental). lower() (hail.expr.StringExpression method). ls() (in module hailtop.fs). M. make_betas() (in module hail.experimental.ldscsim). make_table() (hail.MatrixTable method). manhattan() (in module hail.plot). map() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.SetExpression method). (in module hail.expr.functions). map2() (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). map_values() (hail.expr.DictExpression method). mat_id (hail.genetics.Trio property). matches() (hail.expr.StringExpression method). MatrixTable (class in hail). max() (in module hail.expr.a,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/genindex.html:33593,Testability,log,logit,33593,odel() (in module hail.methods). linear_mixed_regression_rows() (in module hail.methods). linear_regression_rows() (in module hail.methods). LinearMixedModel (class in hail.stats). linreg() (in module hail.expr.aggregators). literal() (in module hail.expr.functions). load() (hail.vds.combiner.VariantDatasetCombiner static method). load_combiner() (in module hail.vds.combiner). load_dataset() (in module hail.experimental). local_to_global() (in module hail.vds). localize_entries() (hail.MatrixTable method). Locus (class in hail.genetics). locus() (in module hail.expr.functions). locus_from_global_position() (hail.genetics.ReferenceGenome method). (in module hail.expr.functions). locus_interval() (in module hail.expr.functions). locus_windows() (in module hail.linalg.utils). LocusExpression (class in hail.expr). log() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). log10() (in module hail.expr.functions). logistic_regression_rows() (in module hail.methods). logit() (in module hail.expr.functions). loop() (in module hail.experimental). lower() (hail.expr.StringExpression method). ls() (in module hailtop.fs). M. make_betas() (in module hail.experimental.ldscsim). make_table() (hail.MatrixTable method). manhattan() (in module hail.plot). map() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.SetExpression method). (in module hail.expr.functions). map2() (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). map_values() (hail.expr.DictExpression method). mat_id (hail.genetics.Trio property). matches() (hail.expr.StringExpression method). MatrixTable (class in hail). max() (in module hail.expr.aggregators). (in module hail.expr.functions). maximal_independent_set() (in module hail.methods). maximum() (in module hail.nd). mean() (in module hail.expr.aggregators). (i,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
https://hail.is/docs/0.2/getting_started.html:246,Deployability,install,installation,246,"﻿. Hail | ; Installing Hail. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail. View page source. Installing Hail. Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started.html
https://hail.is/docs/0.2/getting_started.html:663,Deployability,install,installation,663,"﻿. Hail | ; Installing Hail. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail. View page source. Installing Hail. Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started.html
https://hail.is/docs/0.2/getting_started.html:761,Deployability,update,updated,761,"﻿. Hail | ; Installing Hail. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail. View page source. Installing Hail. Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started.html
https://hail.is/docs/0.2/getting_started_developing.html:806,Deployability,install,installation,806,"﻿. Hail | ; For Software Developers. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Bu",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:878,Deployability,install,install,878,"﻿. Hail | ; For Software Developers. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Bu",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:950,Deployability,install,installed,950,"﻿. Hail | ; For Software Developers. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Bu",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1116,Deployability,install,install,1116,". Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; ",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1361,Deployability,release,releases,1361," Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusi",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1569,Deployability,install,install,1569,"t. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Ke",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1633,Deployability,install,install,1633,"t. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Ke",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1737,Deployability,install,install,1737,"patible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multi",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:2212,Deployability,install,install-dev-requirements,2212,"hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, e",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:3369,Deployability,release,released,3369,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:3436,Deployability,update,updated,3436,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1857,Integrability,depend,dependencies,1857,"te: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass a",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:2186,Integrability,depend,dependencies,2186,"osted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are maki",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:3078,Integrability,message,message,3078,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:3263,Integrability,message,message,3263,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1441,Modifiability,variab,variable,1441,"the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Ch",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1491,Modifiability,variab,variable,1491,"ware Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a con",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:431,Testability,test,tests,431,"﻿. Hail | ; For Software Developers. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Bu",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:2158,Testability,test,tests,2158,"osted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are maki",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:2252,Testability,test,tests,2252,"y default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the c",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:2355,Testability,test,test,2355,"f you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is rel",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:2404,Testability,test,test,2404,"f you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is rel",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:2859,Testability,test,tests,2859,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:2923,Testability,test,tests,2923,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:3343,Testability,log,log,3343,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/getting_started_developing.html:1895,Usability,guid,guide,1895,"te: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass a",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
https://hail.is/docs/0.2/guides.html:2049,Deployability,update,updated,2049,"Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides.html
https://hail.is/docs/0.2/guides.html:602,Usability,guid,guides,602,"﻿. Hail | ; How-To Guides. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous;",MatchSource.WIKI,docs/0.2/guides.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides.html
https://hail.is/docs/0.2/guides.html:621,Usability,feedback,feedback,621,"﻿. Hail | ; How-To Guides. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous;",MatchSource.WIKI,docs/0.2/guides.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides.html
https://hail.is/docs/0.2/guides.html:679,Usability,guid,guide,679,"﻿. Hail | ; How-To Guides. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous;",MatchSource.WIKI,docs/0.2/guides.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides.html
https://hail.is/docs/0.2/guides.html:735,Usability,guid,guides,735,"﻿. Hail | ; How-To Guides. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous;",MatchSource.WIKI,docs/0.2/guides.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides.html
https://hail.is/docs/0.2/hadoop_glob_patterns.html:1232,Deployability,update,updated,1232,"﻿. Hail | ; Hadoop Glob Patterns. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Hadoop Glob Patterns. Change Log And Version Policy. menu; Hail. Other Resources; Hadoop Glob Patterns. View page source. Hadoop Glob Patterns. Pattern; Description. ?. Matches any single character. *. Matches zero or more characters. [abc]. Matches a single character from character set {a,b,c}. [a-b]. Matches a single character from the character range {a…b}. Note that the “^”; character must occur immediately to the right of the opening bracket. [^a]. Matches a single character that is not from character set or range {a}. Note that; the “^”character must occur immediately to the right of the opening bracket. \c. Removes (escapes) any special meaning of character c. {ab,cd}. Matches a string from the string set {ab, cd}. {ab,c{de, fh}}. Matches a string from the string set {ab, cde, cfh}. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hadoop_glob_patterns.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hadoop_glob_patterns.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:1802,Availability,error,error,1802,"e', 'Bob', 'Charlie']). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. aggregate; Uses the aggregator library to compute a summary from an array. append; Append an element to the array and return the result. contains; Returns a boolean indicating whether item is found in the array. extend; Concatenate two arrays and return the result. first; Returns the first element of the array, or missing if empty. grouped; Partition an array into fixed size subarrays. head; Deprecated in favor of first(). index; Returns the first index of x, or missing. last; Returns the last element of the array, or missing if empty. scan; Map each element of the array to cumulative value of function f, with initial value zero. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; ot",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:2738,Availability,error,error,2738,"(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. aggregate(f)[source]; Uses the aggregator library to compute a summary from an array.; This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, call_stats(). Parameters:; f – Aggregation function. Returns:; Expression. all(f); Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lam",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5111,Deployability,pipeline,pipeline,5111,"d return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:15408,Deployability,update,updated,15408,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5186,Energy Efficiency,efficient,efficient,5186,"d return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5439,Integrability,depend,dependencies,5439," Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:15075,Integrability,interface,interface,15075,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:1141,Modifiability,extend,extend,1141,"Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; ArrayExpression. View page source. ArrayExpression. class hail.expr.ArrayExpression[source]; Expression of type tarray.; >>> names = hl.literal(['Alice', 'Bob', 'Charlie']). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. aggregate; Uses the aggregator library to compute a summary from an array. append; Append an element to the array and return the result. contains; Returns a boolean indicating whether item is found in the array. extend; Concatenate two arrays and return the result. first; Returns the first element of the array, or missing if empty. grouped; Partition an array into fixed size subarrays. head; Deprecated in favor of first(). index; Returns the first index of x, or missing. last; Returns the last element of the array, or missing if empty. scan; Map each element of the array to cumulative value of function f, with initial value zero. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:7825,Modifiability,extend,extend,7825,"1,1,1],""fst"":[0.1,0.1,0.1],""mixture"":false}. Notes; For entry-indexed expressions, if there is one column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a)[source]; Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Not",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:7920,Modifiability,extend,extend,7920," column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a)[source]; Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; If f returns False for every element, then the result is missing. Parameters:; f (function ( (a",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:3056,Security,access,accessing,3056,"Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. aggregate(f)[source]; Uses the aggregator library to compute a summary from an array.; This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, call_stats(). Parameters:; f – Aggregation function. Returns:; Expression. all(f); Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any elemen",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5026,Testability,test,test,5026,". Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. append(item)[source]; Append an element to the array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:14006,Testability,log,logging,14006," i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f. Returns:; ArrayExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:15028,Testability,test,tested,15028,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:1959,Availability,error,error,1959,"h of each array; is identical, and will apply the operation positionally (a1 * a2 will; multiply the first element of a1 by the first element of a2, the; second element of a1 by the second element of a2, and so on).; Arithmetic with a scalar will apply the operation to each element of the; array.; >>> a1 = hl.literal([0, 1, 2, 3, 4, 5]). >>> a2 = hl.literal([1, -1, 1, -1, 1, -1]). Attributes. dtype; The data type of the expression. Methods. __add__(other)[source]; Positionally add an array or a scalar.; Examples; >>> hl.eval(a1 + 5); [5, 6, 7, 8, 9, 10]. >>> hl.eval(a1 + a2); [1, 0, 3, 2, 5, 4]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to add. Returns:; ArrayNumericExpression – Array of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by an array or a scalar using floor division.; Examples; >>> hl.eval(a1 // 2); [0, 0, 1, 1, 2, 2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __ge__(other); Return self>=value. __getitem__(item); Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __mod__(other)[source]; Positionally compute the left modulo the right.",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:3696,Availability,error,error,3696,"ion of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __mod__(other)[source]; Positionally compute the left modulo the right.; Examples; >>> hl.eval(a1 % 2); [0, 1, 0, 1, 0, 1]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __mul__(other)[source]; Positionally multiply by an array or a scalar.; Examples; >>> hl.eval(a2 * 5); [5, -5, 5, -5, 5, -5]. >>> hl.eval(a1 * a2); [0, -1, 2, -3, 4, -5]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to multiply by. Returns:; ArrayNumericExpression – Array of positional products. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the array.; Examples; >>> hl.eval(-a1); [0, -1, -2, -3, -4, -5]. Returns:; ArrayNumericExpression – Array expression of the same type. __pow__(other)[source]; Positionally raise to the power of an array or a scalar.; Examples; >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positio",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7244,Deployability,pipeline,pipeline,7244,"array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:17485,Deployability,update,updated,17485,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:4125,Energy Efficiency,power,power,4125,"ession. __mul__(other)[source]; Positionally multiply by an array or a scalar.; Examples; >>> hl.eval(a2 * 5); [5, -5, 5, -5, 5, -5]. >>> hl.eval(a1 * a2); [0, -1, 2, -3, 4, -5]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to multiply by. Returns:; ArrayNumericExpression – Array of positional products. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the array.; Examples; >>> hl.eval(-a1); [0, -1, -2, -3, -4, -5]. Returns:; ArrayNumericExpression – Array expression of the same type. __pow__(other)[source]; Positionally raise to the power of an array or a scalar.; Examples; >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positional differences. __truediv__(other)[source]; Positionally divide by an array or a scalar.; Examples; >>> hl.eval(a1 / 10) ; [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. >>> hl.eval(a2 / a1) ; [inf, -1.0, 0.5, -0.3333333333333333, 0.25, -0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to divide by. Returns:; ArrayNumericExpression – Array of positional quotients. aggregate(f); ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7319,Energy Efficiency,efficient,efficient,7319,"array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7572,Integrability,depend,dependencies,7572,"ng item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:17152,Integrability,interface,interface,17152,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:9958,Modifiability,extend,extend,9958,"t"":[1,1,1],""fst"":[0.1,0.1,0.1],""mixture"":false}. Notes; For entry-indexed expressions, if there is one column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a); Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:10045,Modifiability,extend,extend,10045,"e is one column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a); Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; If f returns False for every element, then the result is missing. Parameters:; f (function ( (a",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:5205,Security,access,accessing,5205,", 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positional differences. __truediv__(other)[source]; Positionally divide by an array or a scalar.; Examples; >>> hl.eval(a1 / 10) ; [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. >>> hl.eval(a2 / a1) ; [inf, -1.0, 0.5, -0.3333333333333333, 0.25, -0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to divide by. Returns:; ArrayNumericExpression – Array of positional quotients. aggregate(f); Uses the aggregator library to compute a summary from an array.; This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, call_stats(). Parameters:; f – Aggregation function. Returns:; Expression. all(f); Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any elemen",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7159,Testability,test,test,7159,"f the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. append(item); Append an element to the array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:16083,Testability,log,logging,16083," i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f. Returns:; ArrayExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:17105,Testability,test,tested,17105,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:1927,Availability,error,error,1927,"one. Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __and__(other)[source]; Return True if the left and right arguments are True.; Examples; >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval(f & na); False. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) & (x > 2)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if both left and right are True. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __invert__()[source]; Return the boolean negation.; Examples; >>> hl.eval",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:4227,Availability,error,error,4227,"the left side is smaller than or equal to the right side. __lt__(other); Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other); Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:10176,Deployability,update,updated,10176,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:5064,Energy Efficiency,power,power,5064,"; >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:5114,Energy Efficiency,power,power,5114,"; >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:5255,Energy Efficiency,power,power,5255,"s. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:;",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:5389,Energy Efficiency,power,power,5389,"son. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Ex",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:6210,Integrability,depend,dependencies,6210,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:9843,Integrability,interface,interface,9843,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:9318,Testability,log,logging,9318,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:9796,Testability,test,tested,9796,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
https://hail.is/docs/0.2/hail.expr.CallExpression.html:2180,Availability,error,error,2180,"e call includes two different alleles. is_het_non_ref; Evaluate whether the call includes two different alleles, neither of which is reference. is_het_ref; Evaluate whether the call includes two different alleles, one of which is reference. is_hom_ref; Evaluate whether the call includes two reference alleles. is_hom_var; Evaluate whether the call includes two identical alternate alleles. is_non_ref; Evaluate whether the call includes one or more non-reference alleles. n_alt_alleles; Returns the number of non-reference alleles. one_hot_alleles; Returns an array containing the summed one-hot encoding of the alleles. unphase; Returns an unphased version of this call. unphased_diploid_gt_index; Return the genotype index for unphased, diploid calls. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the i*th* allele.; Examples; Index with a single integer:; >>> hl.eval(call[0]); 0. >>> hl.eval(call[1]); 1. Parameters:; item (int or Expression of type tint32) – Allele index. Returns:; Expression of type tint32. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two express",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
https://hail.is/docs/0.2/hail.expr.CallExpression.html:3006,Availability,error,error,3006,"s; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the i*th* allele.; Examples; Index with a single integer:; >>> hl.eval(call[0]); 0. >>> hl.eval(call[1]); 1. Parameters:; item (int or Expression of type tint32) – Allele index. Returns:; Expression of type tint32. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains_allele(allele)[source]; Returns true if the call has one or more called alleles of the given index.; >>> c = hl.call(0, 3). >>> hl.eval(c.contains_allele(3)); True. >>> hl.eval(c.contains_allele(1)); False. Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('o",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
https://hail.is/docs/0.2/hail.expr.CallExpression.html:11778,Deployability,update,updated,11778,"l.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; >>> hl.eval(call.unphased_diploid_gt_index()); 1. Returns:; Expression of type tint32. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
https://hail.is/docs/0.2/hail.expr.CallExpression.html:3807,Integrability,depend,dependencies,3807,"ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains_allele(allele)[source]; Returns true if the call has one or more called alleles of the given index.; >>> c = hl.call(0, 3). >>> hl.eval(c.contains_allele(3)); True. >>> hl.eval(c.contains_allele(1)); False. Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
https://hail.is/docs/0.2/hail.expr.CallExpression.html:11166,Integrability,interface,interface,11166,"l.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; >>> hl.eval(call.unphased_diploid_gt_index()); 1. Returns:; Expression of type tint32. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
https://hail.is/docs/0.2/hail.expr.CallExpression.html:10641,Testability,log,logging,10641,"if the call is phased.; Examples; >>> hl.eval(call.phased); False. Returns:; BooleanExpression. property ploidy; Return the number of alleles of this call.; Examples; >>> hl.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; ",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
https://hail.is/docs/0.2/hail.expr.CallExpression.html:11119,Testability,test,tested,11119,"l.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; >>> hl.eval(call.unphased_diploid_gt_index()); 1. Returns:; Expression of type tint32. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:1864,Availability,error,error,1864,"', 'Bob', 'Charlie'}). Attributes. dtype; The data type of the expression. Methods. all; Returns True if f returns True for every element. any; Returns True if f returns True for any element. filter; Returns a new collection containing elements where f returns True. find; Returns the first element where f returns True. flatmap; Map each element of the collection to a new collection, and flatten the results. fold; Reduces the collection with the given function f, provided the initial value zero. group_by; Group elements into a dict according to a lambda function. length; Returns the size of a collection. map; Transform each element of a collection. size; Returns the size of a collection. starmap; Transform each element of a collection of tuples. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. all(f)[source]; Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanEx",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:2443,Availability,error,error,2443,"turns the size of a collection. map; Transform each element of a collection. size; Returns the size of a collection. starmap; Transform each element of a collection of tuples. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. all(f)[source]; Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f)[source]; Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:11624,Deployability,update,updated,11624,"ession refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:3894,Integrability,depend,dependencies,3894,"(arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f)[source]; Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:11291,Integrability,interface,interface,11291,"ession refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:10206,Testability,log,logging,10206," 8.0, 27.0, 64.0, 125.0]. >>> hl.eval(s3.map(lambda x: x.length())); {3, 5, 7}. Parameters:; f (function ( (arg) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This ",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:11244,Testability,test,tested,11244,"ession refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:1657,Availability,error,error,1657,"ession. View page source. DictExpression. class hail.expr.DictExpression[source]; Expression of type tdict.; >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}). Attributes. dtype; The data type of the expression. Methods. contains; Returns whether a given key is present in the dictionary. get; Returns the value associated with key k or a default value if that key is not present. items; Returns an array of tuples containing key/value pairs in the dictionary. key_set; Returns the set of keys in the dictionary. keys; Returns an array with all keys in the dictionary. map_values; Transform values of the dictionary according to a function. size; Returns the size of the dictionary. values; Returns an array with all values in the dictionary. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) ",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:2016,Availability,error,error,2016,"ue if that key is not present. items; Returns an array of tuples containing key/value pairs in the dictionary. key_set; Returns the set of keys in the dictionary. keys; Returns an array with all keys in the dictionary. map_values; Transform values of the dictionary according to a function. size; Returns the size of the dictionary. values; Returns an array with all values in the dictionary. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Re",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:2120,Availability,error,error,2120," key/value pairs in the dictionary. key_set; Returns the set of keys in the dictionary. keys; Returns an array with all keys in the dictionary. map_values; Transform values of the dictionary according to a function. size; Returns the size of the dictionary. values; Returns an array with all values in the dictionary. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is pres",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:2576,Availability,error,error,2576,"True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is present in the dictionary.; Examples; >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters:; item (Expression) – Key to test for inclusion. Returns:; BooleanExpression – True if item is a key of the dictionary, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:9095,Deployability,update,updated,9095,"urns:; DictExpression – Dictionary with transformed values. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of the dictionary.; Examples; >>> hl.eval(d.size()); 3. Returns:; Expression of type tint32 – Size of the dictionary. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; Returns an array with all values in the dictionary.; Examples; >>> hl.eval(d.values()) ; [33, 44, 43]. Returns:; ArrayExpression – All values in the dictionary. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:3453,Integrability,depend,dependencies,3453,">>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is present in the dictionary.; Examples; >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters:; item (Expression) – Key to test for inclusion. Returns:; BooleanExpression – True if item is a key of the dictionary, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:8582,Integrability,interface,interface,8582,"urns:; DictExpression – Dictionary with transformed values. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of the dictionary.; Examples; >>> hl.eval(d.size()); 3. Returns:; Expression of type tint32 – Size of the dictionary. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; Returns an array with all values in the dictionary.; Examples; >>> hl.eval(d.values()) ; [33, 44, 43]. Returns:; ArrayExpression – All values in the dictionary. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:3258,Testability,test,test,3258,"Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is present in the dictionary.; Examples; >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters:; item (Expression) – Key to test for inclusion. Returns:; BooleanExpression – True if item is a key of the dictionary, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>>",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:7905,Testability,log,logging,7905,"e dictionary according to a function.; Examples; >>> hl.eval(d.map_values(lambda x: x * 10)) ; {'Alice': 430, 'Bob': 330, 'Charles': 440}. Parameters:; f (function ( (arg) -> Expression)) – Function to apply to each value. Returns:; DictExpression – Dictionary with transformed values. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of the dictionary.; Examples; >>> hl.eval(d.size()); 3. Returns:; Expression of type tint32 – Size of the dictionary. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; Returns an array with",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.DictExpression.html:8535,Testability,test,tested,8535,"urns:; DictExpression – Dictionary with transformed values. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of the dictionary.; Examples; >>> hl.eval(d.size()); 3. Returns:; Expression of type tint32 – Size of the dictionary. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; Returns an array with all values in the dictionary.; Examples; >>> hl.eval(d.values()) ; [33, 44, 43]. Returns:; ArrayExpression – All values in the dictionary. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
https://hail.is/docs/0.2/hail.expr.Expression-1.html:1433,Availability,error,error,1433,"verview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Ext",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
https://hail.is/docs/0.2/hail.expr.Expression-1.html:2052,Availability,error,error,2052,"ole. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>)[source]; Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True)[source]; Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
https://hail.is/docs/0.2/hail.expr.Expression-1.html:6621,Deployability,update,updated,6621,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
https://hail.is/docs/0.2/hail.expr.Expression-1.html:946,Integrability,depend,dependencies,946,"﻿. Hail | ; Expression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
https://hail.is/docs/0.2/hail.expr.Expression-1.html:2623,Integrability,depend,dependencies,2623,"pression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>)[source]; Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True)[source]; Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
https://hail.is/docs/0.2/hail.expr.Expression-1.html:6280,Integrability,interface,interface,6280,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
https://hail.is/docs/0.2/hail.expr.Expression-1.html:5747,Testability,log,logging,5747,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
https://hail.is/docs/0.2/hail.expr.Expression-1.html:6233,Testability,test,tested,6233,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
https://hail.is/docs/0.2/hail.expr.Expression.html:1433,Availability,error,error,1433,"verview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Ext",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
https://hail.is/docs/0.2/hail.expr.Expression.html:2052,Availability,error,error,2052,"ole. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>)[source]; Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True)[source]; Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
https://hail.is/docs/0.2/hail.expr.Expression.html:6621,Deployability,update,updated,6621,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
https://hail.is/docs/0.2/hail.expr.Expression.html:946,Integrability,depend,dependencies,946,"﻿. Hail | ; Expression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
https://hail.is/docs/0.2/hail.expr.Expression.html:2623,Integrability,depend,dependencies,2623,"pression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>)[source]; Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True)[source]; Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
https://hail.is/docs/0.2/hail.expr.Expression.html:6280,Integrability,interface,interface,6280,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
https://hail.is/docs/0.2/hail.expr.Expression.html:5747,Testability,log,logging,5747,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
https://hail.is/docs/0.2/hail.expr.Expression.html:6233,Testability,test,tested,6233,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:1292,Availability,error,error,1292,ods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Float32Expression. View page source. Float32Expression. class hail.expr.Float32Expression[source]; Expression of type tfloat32.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:3410,Availability,error,error,3410,"the left side is smaller than or equal to the right side. __lt__(other); Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other); Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 4",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:8860,Deployability,update,updated,8860,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:3748,Energy Efficiency,power,power,3748,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:3798,Energy Efficiency,power,power,3798,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:3939,Energy Efficiency,power,power,3939,"; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:;",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:4073,Energy Efficiency,power,power,4073,"(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Ex",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:4894,Integrability,depend,dependencies,4894,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:8527,Integrability,interface,interface,8527,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:8002,Testability,log,logging,8002,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float32Expression.html:8480,Testability,test,tested,8480,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:1292,Availability,error,error,1292,ods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Float64Expression. View page source. Float64Expression. class hail.expr.Float64Expression[source]; Expression of type tfloat64.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:3410,Availability,error,error,3410,"the left side is smaller than or equal to the right side. __lt__(other); Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other); Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 4",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:8860,Deployability,update,updated,8860,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:3748,Energy Efficiency,power,power,3748,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:3798,Energy Efficiency,power,power,3798,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:3939,Energy Efficiency,power,power,3939,"; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:;",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:4073,Energy Efficiency,power,power,4073,"(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Ex",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:4894,Integrability,depend,dependencies,4894,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:8527,Integrability,interface,interface,8527,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:8002,Testability,log,logging,8002,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Float64Expression.html:8480,Testability,test,tested,8480,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:1282,Availability,error,error,1282,ns; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Int32Expression. View page source. Int32Expression. class hail.expr.Int32Expression[source]; Expression of type tint32.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:3408,Availability,error,error,3408," side is smaller than or equal to the right side. __lt__(other); Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other); Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 4",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:8858,Deployability,update,updated,8858,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:3746,Energy Efficiency,power,power,3746,"hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:3796,Energy Efficiency,power,power,3796,"hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:3937,Energy Efficiency,power,power,3937,"; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:;",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:4071,Energy Efficiency,power,power,4071,"(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Ex",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:4892,Integrability,depend,dependencies,4892,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:8525,Integrability,interface,interface,8525,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:8000,Testability,log,logging,8000,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int32Expression.html:8478,Testability,test,tested,8478,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:1282,Availability,error,error,1282,ns; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Int64Expression. View page source. Int64Expression. class hail.expr.Int64Expression[source]; Expression of type tint64.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:3400,Availability,error,error,3400,"the left side is smaller than or equal to the right side. __lt__(other); Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other); Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 4",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:8850,Deployability,update,updated,8850,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:3738,Energy Efficiency,power,power,3738,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:3788,Energy Efficiency,power,power,3788,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:3929,Energy Efficiency,power,power,3929,"; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:;",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:4063,Energy Efficiency,power,power,4063,"(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Ex",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:4884,Integrability,depend,dependencies,4884,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:8517,Integrability,interface,interface,8517,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:7992,Testability,log,logging,7992,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.Int64Expression.html:8470,Testability,test,tested,8470,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:1524,Availability,error,error,1524,"Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; IntervalExpression. View page source. IntervalExpression. class hail.expr.IntervalExpression[source]; Expression of type tinterval.; >>> interval = hl.interval(3, 11); >>> locus_interval = hl.parse_locus_interval(""1:53242-90543""). Attributes. dtype; The data type of the expression. end; Returns the end point. includes_end; True if the interval includes the end point. includes_start; True if the interval includes the start point. start; Returns the start point. Methods. contains; Tests whether a value is contained in the interval. overlaps; True if the the supplied interval contains any value in common with this one. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:2103,Availability,error,error,2103,"the start point. Methods. contains; Tests whether a value is contained in the interval. overlaps; True if the the supplied interval contains any value in common with this one. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(value)[source]; Tests whether a value is contained in the interval.; Examples; >>> hl.eval(interval.contains(3)); True. >>> hl.eval(interval.contains(11)); False. Parameters:; value – Object with type matching the interval point type. Returns:; BooleanExpression – True if value is contained in the interval, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. property end; Returns the end poin",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:7841,Deployability,update,updated,7841,"lue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:2994,Integrability,depend,dependencies,2994,"eral(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(value)[source]; Tests whether a value is contained in the interval.; Examples; >>> hl.eval(interval.contains(3)); True. >>> hl.eval(interval.contains(11)); False. Parameters:; value – Object with type matching the interval point type. Returns:; BooleanExpression – True if value is contained in the interval, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. property end; Returns the end point.; Examples; >>> hl.eval(interval.end); 11. Returns:; Expression. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... p",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:7508,Integrability,interface,interface,7508,"lue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:6876,Testability,log,logging,6876,"alue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:7461,Testability,test,tested,7461,"lue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
https://hail.is/docs/0.2/hail.expr.LocusExpression.html:2050,Availability,error,error,2050,"long the reference genome. in_autosome; Returns True if the locus is on an autosome. in_autosome_or_par; Returns True if the locus is on an autosome or a pseudoautosomal region of chromosome X or Y. in_mito; Returns True if the locus is on mitochondrial DNA. in_x_nonpar; Returns True if the locus is in a non-pseudoautosomal region of chromosome X. in_x_par; Returns True if the locus is in a pseudoautosomal region of chromosome X. in_y_nonpar; Returns True if the locus is in a non-pseudoautosomal region of chromosome Y. in_y_par; Returns True if the locus is in a pseudoautosomal region of chromosome Y. sequence_context; Return the reference genome sequence at the locus. window; Returns an interval of a specified number of bases around the locus. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
https://hail.is/docs/0.2/hail.expr.LocusExpression.html:2629,Availability,error,error,2629,"somal region of chromosome Y. sequence_context; Return the reference genome sequence at the locus. window; Returns an interval of a specified number of bases around the locus. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. property contig; Returns the chromosome.; Examples; >>> hl.eval(locus.contig); '1'. Returns:; StringExpression – The chromosome for this locus. property contig_idx; Returns the chromosome.; Examples; >>> hl.eval(locus.contig_idx); 0. Returns:; StringExpression – The index of the chromosome for this locus. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Expo",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
https://hail.is/docs/0.2/hail.expr.LocusExpression.html:11883,Deployability,update,updated,11883,"; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; The returned interval is inclusive of both the start and end; endpoints. Parameters:. before (Expression of type tint32) – Number of bases to include before the locus. Truncates at 1.; after (Expression of type tint32) – Number of bases to include after the locus. Truncates at; contig length. Returns:; IntervalExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
https://hail.is/docs/0.2/hail.expr.LocusExpression.html:3493,Integrability,depend,dependencies,3493,".; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. property contig; Returns the chromosome.; Examples; >>> hl.eval(locus.contig); '1'. Returns:; StringExpression – The chromosome for this locus. property contig_idx; Returns the chromosome.; Examples; >>> hl.eval(locus.contig_idx); 0. Returns:; StringExpression – The index of the chromosome for this locus. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
https://hail.is/docs/0.2/hail.expr.LocusExpression.html:10758,Integrability,interface,interface,10758,"able, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; The returned interval is inclusive of both the start and end; endpoints. Parameters:. before (Expression of type tint32) – Number of bases to include before the locus. Truncates at 1.; after (Expression of type tint32) – Number of bases to i",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
https://hail.is/docs/0.2/hail.expr.LocusExpression.html:9165,Performance,load,load,9165,"; Returns True if the locus is in a pseudoautosomal region; of chromosome Y.; Examples; >>> hl.eval(locus.in_y_par()); False. Note; Many variant callers only generate variants on chromosome X for the; pseudoautosomal region. In this case, all loci mapped to chromosome; Y are non-pseudoautosomal. Returns:; BooleanExpression. property position; Returns the position along the chromosome.; Examples; >>> hl.eval(locus.position); 1034245. Returns:; Expression of type tint32 – This locus’s position along its chromosome. sequence_context(before=0, after=0)[source]; Return the reference genome sequence at the locus.; Examples; Get the reference allele at a locus:; >>> hl.eval(locus.sequence_context()) ; ""G"". Get the reference sequence at a locus including the previous 5 bases:; >>> hl.eval(locus.sequence_context(before=5)) ; ""ACTCGG"". Notes; This function requires that this locus’ reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome. Parameters:. before (Expression of type tint32, optional) – Number of bases to include before the locus. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus. Truncates at; contig boundary. Returns:; StringExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handl",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
https://hail.is/docs/0.2/hail.expr.LocusExpression.html:10233,Testability,log,logging,10233,"Parameters:. before (Expression of type tint32, optional) – Number of bases to include before the locus. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus. Truncates at; contig boundary. Returns:; StringExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16'",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
https://hail.is/docs/0.2/hail.expr.LocusExpression.html:10711,Testability,test,tested,10711,"able, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; The returned interval is inclusive of both the start and end; endpoints. Parameters:. before (Expression of type tint32) – Number of bases to include before the locus. Truncates at 1.; after (Expression of type tint32) – Number of bases to i",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:1715,Availability,error,error,1715,"hail.expr.NDArrayExpression[source]; Expression of type tndarray.; >>> nd = hl.nd.array([[1, 2], [3, 4]]). Attributes. T; Reverse the dimensions of this ndarray. dtype; The data type of the expression. ndim; The number of dimensions of this ndarray. shape; The shape of this ndarray. Methods. map; Applies an element-wise operation on an NDArray. map2; Applies an element-wise binary operation on two NDArrays. reshape; Reshape this ndarray to a new shape. transpose; Permute the dimensions of this ndarray according to the ordering of axes. property T; Reverse the dimensions of this ndarray. For an n-dimensional array a,; a[i_0, …, i_n-1, i_n] = a.T[i_n, i_n-1, …, i_0].; Same as self.transpose().; See also transpose(). Returns:; NDArrayExpression. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:2294,Availability,error,error,2294,"of this ndarray. For an n-dimensional array a,; a[i_0, …, i_n-1, i_n] = a.T[i_n, i_n-1, …, i_0].; Same as self.transpose().; See also transpose(). Returns:; NDArrayExpression. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:8410,Deployability,update,updated,8410,"=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:2849,Integrability,depend,dependencies,2849,"ression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:7679,Integrability,interface,interface,7679,"=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:7154,Testability,log,logging,7154,"ession of type tint64 or) – :obj: tuple of Expression of type tint64. Examples; >>> v = hl.nd.array([1, 2, 3, 4]) ; >>> m = v.reshape((2, 2)) . Returns:; NDArrayExpression. property shape; The shape of this ndarray.; Examples; >>> hl.eval(nd.shape); (2, 2). Returns:; TupleExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimen",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:7632,Testability,test,tested,7632,"=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:2233,Availability,error,error,2233,"on). Arithmetic with a scalar will; apply the operation to each element of the ndarray.; Attributes. T; Reverse the dimensions of this ndarray. dtype; The data type of the expression. ndim; The number of dimensions of this ndarray. shape; The shape of this ndarray. Methods. sum; Sum out one or more axes of an ndarray. property T; Reverse the dimensions of this ndarray. For an n-dimensional array a,; a[i_0, …, i_n-1, i_n] = a.T[i_n, i_n-1, …, i_0].; Same as self.transpose().; See also transpose(). Returns:; NDArrayExpression. __add__(other)[source]; Positionally add an array or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to add. Returns:; NDArrayNumericExpression – NDArray of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:4400,Availability,error,error,4400,"). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __mul__(other)[source]; Positionally multiply by a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to multiply by. Returns:; NDArrayNumericExpression – NDArray of positional products. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the ndarray. Returns:; NDArrayNumericExpression – Array expression of the same type. __sub__(other)[source]; Positionally subtract a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to subtract. Returns:; NDArrayNumericExpression – NDArray of positional differences. __truediv__(other)[source]; Positionally divide by a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to divide by. Returns:; NDArrayNumericExpression – NDArray of positional quotients. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experiment",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:11277,Deployability,update,updated,11277," to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None); Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:5565,Integrability,depend,dependencies,5565,"on. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the ndarray. Returns:; NDArrayNumericExpression – Array expression of the same type. __sub__(other)[source]; Positionally subtract a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to subtract. Returns:; NDArrayNumericExpression – NDArray of positional differences. __truediv__(other)[source]; Positionally divide by a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to divide by. Returns:; NDArrayNumericExpression – NDArray of positional quotients. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:10554,Integrability,interface,interface,10554," to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None); Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:2924,Performance,perform,performed,2924,"darray to add. Returns:; NDArrayNumericExpression – NDArray of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if a has shape (3, 4, 5) and b has shape (3, 5, 6), a is treated; as a stack of three matrices of shape (4, 5) and b as a stack of three matrices of shape (5, 6). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:3016,Performance,perform,performs,3016,"r); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if a has shape (3, 4, 5) and b has shape (3, 5, 6), a is treated; as a stack of three matrices of shape (4, 5) and b as a stack of three matrices of shape (5, 6). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __mul__(other)[source]; Positionally multiply by a ndarray or a scalar. Parameters:; other (",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:9846,Testability,log,logging,9846,"ession of type tint64 or) – :obj: tuple of Expression of type tint64. Examples; >>> v = hl.nd.array([1, 2, 3, 4]) ; >>> m = v.reshape((2, 2)) . Returns:; NDArrayExpression. property shape; The shape of this ndarray.; Examples; >>> hl.eval(nd.shape); (2, 2). Returns:; TupleExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:10507,Testability,test,tested,10507," to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None); Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:1347,Availability,error,error,1347,vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; NumericExpression. View page source. NumericExpression. class hail.expr.NumericExpression[source]; Expression of numeric type.; >>> x = hl.literal(3). >>> y = hl.literal(4.5). Attributes. dtype; The data type of the expression. Methods. __add__(other)[source]; Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other)[source]; Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other)[source]; Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other)[source]; Less-than-or-equals compar,MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:3521,Availability,error,error,3521," than or equal to the right side. __lt__(other)[source]; Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other)[source]; Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:9003,Deployability,update,updated,9003,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:3867,Energy Efficiency,power,power,3867," x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:3925,Energy Efficiency,power,power,3925," x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:4066,Energy Efficiency,power,power,4066,"numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expre",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:4200,Energy Efficiency,power,power,4200,"arameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', h",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:5037,Integrability,depend,dependencies,5037,"1111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:8670,Integrability,interface,interface,8670,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:8145,Testability,log,logging,8145,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.NumericExpression.html:8623,Testability,test,tested,8623,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
https://hail.is/docs/0.2/hail.expr.SetExpression.html:1802,Availability,error,error,1802,"= hl.literal({1, 3, 5}). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. add; Returns a new set including item. contains; Returns True if item is in the set. difference; Return the set of elements in the set that are not present in set s. intersection; Return the intersection of the set and set s. is_subset; Returns True if every element is contained in set s. remove; Returns a new set excluding item. union; Return the union of the set and set s. __and__(other)[source]; Return the intersection of the set and other.; Examples; >>> hl.eval(s1 & s2); {1, 3}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in both the set and other. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Test whether every element in other is in the set. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if every element in other is in the set. False otherwise. __gt__(other)[source]; Test whether other is a proper subset of the set (other <= set and other != set). Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if other is a proper subset of the set. False otherwise. __le__(other)[source]; Test whether every element in the set is in other. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if every element in the set is in other. False otherwise. __lt__(other)[source]; Test whether the set is a proper subset of o",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
https://hail.is/docs/0.2/hail.expr.SetExpression.html:3251,Availability,error,error,3251," Test whether other is a proper subset of the set (other <= set and other != set). Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if other is a proper subset of the set. False otherwise. __le__(other)[source]; Test whether every element in the set is in other. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if every element in the set is in other. False otherwise. __lt__(other)[source]; Test whether the set is a proper subset of other (set <= other and set != other). Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if the set is a proper subset of other. False otherwise. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __or__(other)[source]; Return the union of the set and other.; Examples; >>> hl.eval(s1 | s2); {1, 2, 3, 5}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. __sub__(other)[source]; Return the difference of the set and other.; Examples; >>> hl.eval(s1 - s2); {2}. >>> hl.eval(s2 - s1); {5}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements in the set that are not in other. __xor__(other)[source]; Return the symmetric difference of the set and other.; Examples; >>> hl.eval(s1 ^ s2); {2, 5}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either the set or other but not both. ",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
https://hail.is/docs/0.2/hail.expr.SetExpression.html:14910,Deployability,update,updated,14910,"| str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. union(s)[source]; Return the union of the set and set s.; Examples; >>> hl.eval(s1.union(s2)); {1, 2, 3, 5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
https://hail.is/docs/0.2/hail.expr.SetExpression.html:5953,Integrability,depend,dependencies,5953,"or any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns True if item is in the set.; Examples; >>> hl.eval(s1.contains(1)); True. >>> hl.eval(s1.contains(10)); False. Parameters:; item (Expression) – Value for inclusion test. Returns:; BooleanExpression – True if item is in the set. describe(handler=<built-in function print>); Print information about type, index, and dependencies. difference(s)[source]; Return the set of elements in the set that are not present in set s.; Examples; >>> hl.eval(s1.difference(s2)); {2}. >>> hl.eval(s2.difference(s1)); {5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements not in s. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""]",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
https://hail.is/docs/0.2/hail.expr.SetExpression.html:14336,Integrability,interface,interface,14336,"| str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. union(s)[source]; Return the union of the set and set s.; Examples; >>> hl.eval(s1.union(s2)); {1, 2, 3, 5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
https://hail.is/docs/0.2/hail.expr.SetExpression.html:5802,Testability,test,test,5802,"ssion. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns True if item is in the set.; Examples; >>> hl.eval(s1.contains(1)); True. >>> hl.eval(s1.contains(10)); False. Parameters:; item (Expression) – Value for inclusion test. Returns:; BooleanExpression – True if item is in the set. describe(handler=<built-in function print>); Print information about type, index, and dependencies. difference(s)[source]; Return the set of elements in the set that are not present in set s.; Examples; >>> hl.eval(s1.difference(s2)); {2}. >>> hl.eval(s2.difference(s1)); {5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements not in s. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> ",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
https://hail.is/docs/0.2/hail.expr.SetExpression.html:13267,Testability,log,logging,13267,"ionExpression. – Collection where each element has been transformed according to f. remove(item)[source]; Returns a new set excluding item.; Examples; >>> hl.eval(s1.remove(1)); {2, 3}. Parameters:; item (Expression) – Value to remove. Returns:; SetExpression – Set with item removed. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
https://hail.is/docs/0.2/hail.expr.SetExpression.html:14289,Testability,test,tested,14289,"| str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. union(s)[source]; Return the union of the set and set s.; Examples; >>> hl.eval(s1.union(s2)); {1, 2, 3, 5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
https://hail.is/docs/0.2/hail.expr.StringExpression.html:2581,Availability,error,error,2581,"se. replace; Replace substrings matching pattern1 with pattern2 using regex. reverse; Returns the reversed value. split; Returns an array of strings generated by splitting the string at delim. startswith; Returns whether substr is a prefix of the string. strip; Returns a copy of the string with whitespace removed from the start and end. translate; Translates characters of the string using mapping. upper; Returns a copy of the string, but with lower case letters converted to upper case. __add__(other)[source]; Concatenate strings.; Examples; >>> hl.eval(s + ' jumped over the lazy dog'); 'The quick brown fox jumped over the lazy dog'. Parameters:; other (StringExpression) – String to concatenate. Returns:; StringExpression – Concatenated string. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Slice or index into the string.; Examples; >>> hl.eval(s[:15]); 'The quick brown'. >>> hl.eval(s[0]); 'T'. Parameters:; item (slice or Expression of type tint32) – Slice or character index. Returns:; StringExpression – Substring or character at index item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; Boole",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
https://hail.is/docs/0.2/hail.expr.StringExpression.html:3445,Availability,error,error,3445,"iteral(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Slice or index into the string.; Examples; >>> hl.eval(s[:15]); 'The quick brown'. >>> hl.eval(s[0]); 'T'. Parameters:; item (slice or Expression of type tint32) – Slice or character index. Returns:; StringExpression – Substring or character at index item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(substr)[source]; Returns whether substr is contained in the string.; Examples; >>> hl.eval(s.contains('fox')); True. >>> hl.eval(s.contains('dog')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. endswith(substr)[source]; Returns whether substr is a suffix of the string.; Examples; >>> hl.eval",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
https://hail.is/docs/0.2/hail.expr.StringExpression.html:14632,Deployability,update,updated,14632,"val(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >>> hl.eval(s.startswith('The')); True. >>> hl.eval(s.startswith('the')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. strip()[source]; Returns a copy of the string with whitespace removed from the start; and end.; Examples; >>> s2 = hl.str(' once upon a time\n'); >>> hl.eval(s2.strip()); 'once upon a time'. Returns:; StringExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. translate(mapping)[source]; Translates characters of the string using mapping.; Examples; >>> string = hl.literal('ATTTGCA'); >>> hl.eval(string.translate({'T': 'U'})); 'AUUUGCA'. Parameters:; mapping (DictExpression) – Dictionary of character-character translations. Returns:; StringExpression. See also; replace(). upper()[source]; Returns a copy of the string, but with lower case letters converted; to upper case.; Examples; >>> hl.eval(s.upper()); 'THE QUICK BROWN FOX'. Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
https://hail.is/docs/0.2/hail.expr.StringExpression.html:4272,Integrability,depend,dependencies,4272," if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(substr)[source]; Returns whether substr is contained in the string.; Examples; >>> hl.eval(s.contains('fox')); True. >>> hl.eval(s.contains('dog')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. endswith(substr)[source]; Returns whether substr is a suffix of the string.; Examples; >>> hl.eval(s.endswith('fox')); True. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
https://hail.is/docs/0.2/hail.expr.StringExpression.html:13793,Integrability,interface,interface,13793,"val(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >>> hl.eval(s.startswith('The')); True. >>> hl.eval(s.startswith('the')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. strip()[source]; Returns a copy of the string with whitespace removed from the start; and end.; Examples; >>> s2 = hl.str(' once upon a time\n'); >>> hl.eval(s2.strip()); 'once upon a time'. Returns:; StringExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. translate(mapping)[source]; Translates characters of the string using mapping.; Examples; >>> string = hl.literal('ATTTGCA'); >>> hl.eval(string.translate({'T': 'U'})); 'AUUUGCA'. Parameters:; mapping (DictExpression) – Dictionary of character-character translations. Returns:; StringExpression. See also; replace(). upper()[source]; Returns a copy of the string, but with lower case letters converted; to upper case.; Examples; >>> hl.eval(s.upper()); 'THE QUICK BROWN FOX'. Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
https://hail.is/docs/0.2/hail.expr.StringExpression.html:12204,Testability,log,logging,12204,"roup, not the canonical \1. Parameters:. pattern1 (str or StringExpression); pattern2 (str or StringExpression). reverse()[source]; Returns the reversed value.; .. rubric:: Examples; >>> string = hl.literal('ATGCC'); >>> hl.eval(string.reverse()); 'CCGTA'. Returns:; StringExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. split(delim, n=None)[source]; Returns an array of strings generated by splitting the string at delim.; Examples; >>> hl.eval(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
https://hail.is/docs/0.2/hail.expr.StringExpression.html:13746,Testability,test,tested,13746,"val(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >>> hl.eval(s.startswith('The')); True. >>> hl.eval(s.startswith('the')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. strip()[source]; Returns a copy of the string with whitespace removed from the start; and end.; Examples; >>> s2 = hl.str(' once upon a time\n'); >>> hl.eval(s2.strip()); 'once upon a time'. Returns:; StringExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. translate(mapping)[source]; Translates characters of the string using mapping.; Examples; >>> string = hl.literal('ATTTGCA'); >>> hl.eval(string.translate({'T': 'U'})); 'AUUUGCA'. Parameters:; mapping (DictExpression) – Dictionary of character-character translations. Returns:; StringExpression. See also; replace(). upper()[source]; Returns a copy of the string, but with lower case letters converted; to upper case.; Examples; >>> hl.eval(s.upper()); 'THE QUICK BROWN FOX'. Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:3071,Deployability,update,updated,3071,"__eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expression of the same type. __ge__(other); Return self>=value. __getitem__(item)[source]; Access a field of the struct by name or index.; Examples; >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters:; item (str) – Field name. Returns:; Expression – Struct field. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other)[source]; Return self!=value. annotate(**named_exprs)[source]; Add new fields or recompute existing fields.; Examples; >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; If an expression in named_exprs shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters:; named_exprs (keyword args of Expression) – Fields to add. Returns:; StructExpression – Struct with new or updated fields. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. drop(*fields)[source]; Drop fields from the struct.; Examples; >>> hl.eval(struct.drop('b')); Struct(a=5). Parameters:; fields (varargs of str) – Fields to drop. Returns:; StructExpression – Struct without certain fields. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 ",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:9200,Deployability,update,updated,9200,"n the resulting struct in the order they appear in; fields.; The named_exprs arguments are new field expressions. Parameters:. fields (varargs of str) – Field names to keep.; named_exprs (keyword args of Expression) – New field expressions. Returns:; StructExpression – Struct containing specified existing fields and computed fields. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; A list of expressions for each field. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:3438,Integrability,depend,dependencies,3438,"ns:; Expression – Struct field. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other)[source]; Return self!=value. annotate(**named_exprs)[source]; Add new fields or recompute existing fields.; Examples; >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; If an expression in named_exprs shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters:; named_exprs (keyword args of Expression) – Fields to add. Returns:; StructExpression – Struct with new or updated fields. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. drop(*fields)[source]; Drop fields from the struct.; Examples; >>> hl.eval(struct.drop('b')); Struct(a=5). Parameters:; fields (varargs of str) – Fields to drop. Returns:; StructExpression – Struct without certain fields. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:8810,Integrability,interface,interface,8810,"n the resulting struct in the order they appear in; fields.; The named_exprs arguments are new field expressions. Parameters:. fields (varargs of str) – Field names to keep.; named_exprs (keyword args of Expression) – New field expressions. Returns:; StructExpression – Struct containing specified existing fields and computed fields. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; A list of expressions for each field. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:1098,Safety,safe,safer,1098," . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.StructExpression'>>. __eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expre",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:844,Security,access,accessible,844,"﻿. Hail | ; StructExpression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:908,Security,access,access,908,"﻿. Hail | ; StructExpression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:1241,Security,access,accessible,1241,"ions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.StructExpression'>>. __eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expression of the same type. __ge__(other); Return self>=value. __getitem__(item)[source]; Access a field",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:1332,Security,access,access,1332,"experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.StructExpression'>>. __eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expression of the same type. __ge__(other); Return self>=value. __getitem__(item)[source]; Access a field of the struct by name or index.; Examples; >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Para",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:8285,Testability,log,logging,8285,"n the resulting struct in the order they appear in; fields.; The named_exprs arguments are new field expressions. Parameters:. fields (varargs of str) – Field names to keep.; named_exprs (keyword args of Expression) – New field expressions. Returns:; StructExpression – Struct containing specified existing fields and computed fields. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; A list of expressions for each field. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.StructExpression.html:8763,Testability,test,tested,8763,"n the resulting struct in the order they appear in; fields.; The named_exprs arguments are new field expressions. Parameters:. fields (varargs of str) – Field names to keep.; named_exprs (keyword args of Expression) – New field expressions. Returns:; StructExpression – Struct containing specified existing fields and computed fields. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; A list of expressions for each field. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
https://hail.is/docs/0.2/hail.expr.TupleExpression.html:1303,Availability,error,error,1303,"s; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; TupleExpression. View page source. TupleExpression. class hail.expr.TupleExpression[source]; Expression of type ttuple.; >>> tup = hl.literal((""a"", 1, [1, 2, 3])). Attributes. dtype; The data type of the expression. Methods. count; Do not use this method. index; Do not use this method. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.TupleExpression'>>. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into the tuple.; Examples; >>> hl.eval(tup[1]); 1. Parameters:; item (int) – Element index. Returns:; Expression. __gt__(other); Return self>value. __le__(other); Return self<=value. __len__()[source]; Returns the length of the tuple.; Examples; >>> len(tup); 3. Returns:; int. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expression",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
https://hail.is/docs/0.2/hail.expr.TupleExpression.html:2126,Availability,error,error,2126,"ples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into the tuple.; Examples; >>> hl.eval(tup[1]); 1. Parameters:; item (int) – Element index. Returns:; Expression. __gt__(other); Return self>value. __le__(other); Return self<=value. __len__()[source]; Returns the length of the tuple.; Examples; >>> len(tup); 3. Returns:; int. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. count(value)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus allele",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
https://hail.is/docs/0.2/hail.expr.TupleExpression.html:6927,Deployability,update,updated,6927,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
https://hail.is/docs/0.2/hail.expr.TupleExpression.html:2811,Integrability,depend,dependencies,2811,"; Returns the length of the tuple.; Examples; >>> len(tup); 3. Returns:; int. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. count(value)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
https://hail.is/docs/0.2/hail.expr.TupleExpression.html:6594,Integrability,interface,interface,6594,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
https://hail.is/docs/0.2/hail.expr.TupleExpression.html:6069,Testability,log,logging,6069,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
https://hail.is/docs/0.2/hail.expr.TupleExpression.html:6547,Testability,test,tested,6547,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5573,Availability,down,downstream,5573,"passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean)",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:4978,Deployability,pipeline,pipeline,4978,"rs (keyword args of Expression) – Column-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix, can be used to call GroupedMatrixTable.aggregate(). group_rows_by(*exprs, **named_exprs)[source]; Group rows.; Examples; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5242,Deployability,pipeline,pipeline,5242,"s; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix w",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:6747,Deployability,update,updated,6747,"MatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See also; aggregate(). Returns:; MatrixTable – Aggregated matrix table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5471,Energy Efficiency,reduce,reduces,5471,"passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean)",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5172,Performance,optimiz,optimizer,5172,"s; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix w",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
https://hail.is/docs/0.2/hail.GroupedTable.html:2486,Availability,down,downstream,2486,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
https://hail.is/docs/0.2/hail.GroupedTable.html:1909,Deployability,pipeline,pipeline,1909,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
https://hail.is/docs/0.2/hail.GroupedTable.html:2165,Deployability,pipeline,pipeline,2165,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
https://hail.is/docs/0.2/hail.GroupedTable.html:2678,Deployability,update,updated,2678,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
https://hail.is/docs/0.2/hail.GroupedTable.html:2386,Energy Efficiency,reduce,reduces,2386,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
https://hail.is/docs/0.2/hail.GroupedTable.html:2095,Performance,optimiz,optimizer,2095,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:3468,Availability,checkpoint,checkpoint,3468," fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Expl",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:18164,Availability,checkpoint,checkpoint,18164,"le must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:18750,Availability,checkpoint,checkpoint,18750,"s follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column order:; >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:; >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters:; indices (list of int) – List of old column indices. Returns:; MatrixTable. property col; Returns a struct expression of all column-indexed fields, including keys.; Examples; Get all column field names:",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:19136,Availability,checkpoint,checkpoint,19136,"dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column order:; >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:; >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters:; indices (list of int) – List of old column indices. Returns:; MatrixTable. property col; Returns a struct expression of all column-indexed fields, including keys.; Examples; Get all column field names:; >>> list(dataset.col) ; ['s', 'sample_qc', 'is_case', 'pheno', 'cov', 'cov1', 'cov2', 'cohorts', 'pop']. Returns:; StructExpression – Struct of all column fields. property col_key; Column key struct.; Examples; Get the column key field names:; >>> list(dataset.col_key); ['s']. Returns:; StructExpression. property col_value; Returns a struct expression including all non-key colu",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:33239,Availability,down,downstream,33239,"urns:; MatrixTable – Filtered matrix table. filter_entries(expr, keep=True)[source]; Filter entries of the matrix. Parameters:. expr (bool or BooleanExpression) – Filter expression.; keep (bool) – Keep entries where expr is true. Returns:; MatrixTable – Filtered matrix table. Examples; Keep entries where the sum of AD is greater than 10 and GQ is greater than 20:; >>> dataset_result = dataset.filter_entries((hl.sum(dataset.AD) > 10) & (dataset.GQ > 20)). Warning; When expr evaluates to missing, the entry will be removed regardless of; keep. Note; This method does not support aggregation. Notes; The expression expr will be evaluated for every entry of the table.; If keep is True, then entries where expr evaluates to True; will be kept (the filter removes the entries where the predicate; evaluates to False). If keep is False, then entries where; expr evaluates to True will be removed (the filter keeps the; entries where the predicate evaluates to False).; Filtered entries are removed entirely from downstream operations. This; means that the resulting matrix table has sparsity – that is, that the; number of entries is smaller than the product of count_rows(); and count_cols(). To re-densify a filtered matrix table, use the; unfilter_entries() method to restore filtered entries, populated; all fields with missing values. Below are some properties of an; entry-filtered matrix table. Filtered entries are not included in the entries() table. >>> mt_range = hl.utils.range_matrix_table(10, 10); >>> mt_range = mt_range.annotate_entries(x = mt_range.row_idx + mt_range.col_idx); >>> mt_range.count(); (10, 10). >>> mt_range.entries().count(); 100. >>> mt_filt = mt_range.filter_entries(mt_range.x % 2 == 0); >>> mt_filt.count(); (10, 10). >>> mt_filt.count_rows() * mt_filt.count_cols(); 100. >>> mt_filt.entries().count(); 50. Filtered entries are not included in aggregation. >>> mt_filt.aggregate_entries(hl.agg.count()); 50. >>> mt_filt = mt_filt.annotate_cols(col_n = hl.agg.count",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:48612,Availability,avail,available,48612," separator. If the; entry field name is empty, the separator is omitted.; The table inherits the globals from the matrix table.; Examples; Consider a matrix table with the following schema:; Global fields:; 'batch': str; Column fields:; 's': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; Entry fields:; 'GT': call; 'GQ': int32; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>. and three sample IDs: A, B and C. Then the result of; make_table():; >>> ht = mt.make_table() . has the original row fields along with 6 additional fields,; one for each sample and entry field:; Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>. n_partitions()[source]; Number of partitions.; Notes; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see here; for details. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_D",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:49809,Availability,redundant,redundant,49809,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:51426,Availability,avail,available,51426,"_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result = dataset.rename({'s': 'info', 'info': 'vcf_info'}). Parameters:; fields (dict from str to str) – Mapping from old field names to new field names. Returns:; MatrixTable – Matrix table with renamed fields. repartition(n_partitions, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> dataset_result = dataset.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:64784,Availability,down,downstream,64784,"ethods work. See also; Table.transmute(), MatrixTable.select_globals(), MatrixTable.annotate_globals(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of row",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:68744,Availability,checkpoint,checkpoint,68744,"datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be identical, both in type, value, and ordering.; The row key schemas and row schemas must match.; The entry schemas must match. The column fields in the resulting dataset are the column fields from; the first dataset; the column schemas do not need to match.; This method does not deduplicate; if a row exists identically in two; datasets, then it will be duplicated in the result. Warning; This method can trigger a shuffle, if partitions from two datasets; overlap. Parameters:; datasets (varargs of MatrixTable) – Datasets to combine. Returns:; MatrixTable – Dataset with rows from each member of datasets. unpersist()[source]; Unpersists this dataset from memory/disk.; Notes; This function will have no effect on a dataset that was not previously; persisted. Returns:; MatrixTable – Unpersisted dataset. write(output, overwrite=False, stage_locally=False, _codec_spec=None, _partitions=None)[source]; Write to disk.; Examples; >>> dataset.write('output/dataset.mt'). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_matrix_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:28666,Deployability,pipeline,pipeline,28666,"regate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection – the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using key_cols_by() with no arguments. Warning; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns:; Table – Table with all non-global fields from the matrix, with one row per entry of the matrix. property entry; Returns a struct expression including all row-and-column-indexed fields.; Examples; Get all entry field names:; >>> list(dataset.entry); ['GT', 'AD', 'DP', 'GQ', 'PL']. Returns:; StructExpression – Struct of all entry fields. explode_cols(field_expr)[source]; Explodes a column field of type array or set, copying the entire column for each element.; Examples; Explode columns by annotated cohorts:; >>> dataset_result = dataset.explode_cols(dataset.cohorts). Notes; The new matrix table will have N copies of each column, where N is the; number of elements that column contains for the field denoted by field_expr.; The field referenced in field_expr is replaced in the sequence of duplicated; columns by the sequence of elements in the array or set. All other fields remain; the same, includ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:49868,Deployability,pipeline,pipelines,49868,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:64775,Deployability,pipeline,pipeline,64775,"ethods work. See also; Table.transmute(), MatrixTable.select_globals(), MatrixTable.annotate_globals(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of row",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:64919,Deployability,pipeline,pipeline,64919,"of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows,",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:64942,Deployability,pipeline,pipeline,64942,"of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows,",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:69174,Deployability,update,updated,69174,"datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be identical, both in type, value, and ordering.; The row key schemas and row schemas must match.; The entry schemas must match. The column fields in the resulting dataset are the column fields from; the first dataset; the column schemas do not need to match.; This method does not deduplicate; if a row exists identically in two; datasets, then it will be duplicated in the result. Warning; This method can trigger a shuffle, if partitions from two datasets; overlap. Parameters:; datasets (varargs of MatrixTable) – Datasets to combine. Returns:; MatrixTable – Dataset with rows from each member of datasets. unpersist()[source]; Unpersists this dataset from memory/disk.; Notes; This function will have no effect on a dataset that was not previously; persisted. Returns:; MatrixTable – Unpersisted dataset. write(output, overwrite=False, stage_locally=False, _codec_spec=None, _partitions=None)[source]; Write to disk.; Examples; >>> dataset.write('output/dataset.mt'). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_matrix_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:3568,Energy Efficiency,efficient,efficient,3568," fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Expl",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:18438,Energy Efficiency,efficient,efficient,18438,"le must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:19003,Energy Efficiency,efficient,efficient,19003,"et = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column order:; >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:; >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters:; indices (list of int) – List of old column indices. Returns:; MatrixTable. property col; Returns a struct expression of all column-indexed fields, including keys.; Examples; Get all column field names:; >>> list(dataset.col) ; ['s', 'sample_qc', 'is_case', 'pheno', 'cov', 'cov1', 'cov2', 'cohorts', 'pop']. Returns:; StructExpression – Struct of all column fields. property col_key; Column key struct.; Examples; Get the column key field names:; >>> list(dataset.col_key)",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:9328,Integrability,depend,dependent,9328," row index field. Returns:; MatrixTable – Dataset with new field. aggregate_cols(expr, _localize=True)[source]; Aggregate over columns to a local value.; Examples; Aggregate over columns:; >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; Unlike most MatrixTable methods, this method does not support; meaningful references to fields that are not global or indexed by column.; This method should be thought of as a more convenient alternative to; the following:; >>> cols_table = dataset.cols(); >>> cols_table.aggregate(; ... hl.struct(fraction_female=hl.agg.fraction(cols_table.pheno.is_female),; ... case_ratio=hl.agg.count_where(cols_table.is_case) / hl.agg.count())). Note; This method supports (and expects!) aggregation over columns. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. aggregate_entries(expr, _localize=True)[source]; Aggregate over entries to a local value.; Examples; Aggregate over entries:; >>> dataset.aggregate_entries(hl.struct(global_gq_mean=hl.agg.mean(dataset.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(dataset.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). Notes; This method should be thought of as a more convenient alternative to; the following:; >>> entries_table = dataset.entries(); >>> entries_table.aggregate(hl.struct(global_gq_mean=hl.agg.mean(entries_table.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(entries_table.GT)))). Note; This method supports (and expects!) aggregation over entries. Parameters:; expr (Expression) – Aggregation expressions. Returns:; any – Aggregated value dependent on expr. aggregate_rows(expr, _localize=True)[source]; Aggregate over rows to a local value.; Examples; Aggregate over rows:; >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:10129,Integrability,depend,dependent,10129,"nt_where(cols_table.is_case) / hl.agg.count())). Note; This method supports (and expects!) aggregation over columns. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. aggregate_entries(expr, _localize=True)[source]; Aggregate over entries to a local value.; Examples; Aggregate over entries:; >>> dataset.aggregate_entries(hl.struct(global_gq_mean=hl.agg.mean(dataset.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(dataset.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). Notes; This method should be thought of as a more convenient alternative to; the following:; >>> entries_table = dataset.entries(); >>> entries_table.aggregate(hl.struct(global_gq_mean=hl.agg.mean(entries_table.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(entries_table.GT)))). Note; This method supports (and expects!) aggregation over entries. Parameters:; expr (Expression) – Aggregation expressions. Returns:; any – Aggregated value dependent on expr. aggregate_rows(expr, _localize=True)[source]; Aggregate over rows to a local value.; Examples; Aggregate over rows:; >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; Unlike most MatrixTable methods, this method does not support; meaningful references to fields that are not global or indexed by row.; This method should be thought of as a more convenient alternative to; the following:; >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. annotate_cols(**named_exprs)[source]; Create new column-indexed fields by name.; Examples",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:11012,Integrability,depend,dependent,11012,"pects!) aggregation over entries. Parameters:; expr (Expression) – Aggregation expressions. Returns:; any – Aggregated value dependent on expr. aggregate_rows(expr, _localize=True)[source]; Aggregate over rows to a local value.; Examples; Aggregate over rows:; >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; Unlike most MatrixTable methods, this method does not support; meaningful references to fields that are not global or indexed by row.; This method should be thought of as a more convenient alternative to; the following:; >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. annotate_cols(**named_exprs)[source]; Create new column-indexed fields by name.; Examples; Compute statistics about the GQ distribution per sample:; >>> dataset_result = dataset.annotate_cols(sample_gq_stats = hl.agg.stats(dataset.GQ)). Add sample metadata from a hail.Table.; >>> dataset_result = dataset.annotate_cols(population = s_metadata[dataset.s].pop). Note; This method supports aggregation over rows. For instance, the usage:; >>> dataset_result = dataset.annotate_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Notes; This method creates new column fields, but can also overwrite existing fields. Only; same-scope fields can be overwritten: for example, it is not possible to annotate a; global field foo and later create an column field foo. However, it would be possible; to create an column field foo and later create another column field foo, overwriting; the first.; The arguments to the method should either be Expression; obje",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:23647,Integrability,interface,interface,23647,"in their corresponding arrays. Note; The order of the columns is not guaranteed. Returns:; MatrixTable. cols()[source]; Returns a table with all column fields in the matrix.; Examples; Extract the column table:; >>> cols_table = dataset.cols(). Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the column key (which becomes the table key). To preserve the original; column order as the table row order, first unkey the columns using; key_cols_by() with no arguments. Returns:; Table – Table with all column fields from the matrix, with one row per column of the matrix. compute_entry_filter_stats(row_field='entry_stats_row', col_field='entry_stats_col')[source]; Compute statistics about the number and fraction of filtered entries. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. row_field (str) – Name for computed row field (default: entry_stats_row.; col_field (str) – Name for computed column field (default: entry_stats_col. Returns:; MatrixTable. Notes; Adds a new row field, row_field, and a new column field, col_field,; each of which are structs with the following fields:. n_filtered (tint64) - Number of filtered entries per row; or column.; n_remaining (tint64) - Number of entries not filtered per; row or column.; fraction_filtered (tfloat32) - Number of filtered entries; divided by the total number of filtered and remaining entries. See also; filter_entries(), unfilter_entries(). count()[source]; Count the number of rows and columns in the matrix.; Examples; >>> dataset.count(). Returns:; int, int – Number of rows, number of cols. count_cols(_localize=True)[source]; Count the number of columns in the matrix.; Examples; Count the number of columns:; >>> n_cols = dataset.count_cols(). Returns:; ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:28648,Integrability,depend,depending,28648,"regate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection – the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using key_cols_by() with no arguments. Warning; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns:; Table – Table with all non-global fields from the matrix, with one row per entry of the matrix. property entry; Returns a struct expression including all row-and-column-indexed fields.; Examples; Get all entry field names:; >>> list(dataset.entry); ['GT', 'AD', 'DP', 'GQ', 'PL']. Returns:; StructExpression – Struct of all entry fields. explode_cols(field_expr)[source]; Explodes a column field of type array or set, copying the entire column for each element.; Examples; Explode columns by annotated cohorts:; >>> dataset_result = dataset.explode_cols(dataset.cohorts). Notes; The new matrix table will have N copies of each column, where N is the; number of elements that column contains for the field denoted by field_expr.; The field referenced in field_expr is replaced in the sequence of duplicated; columns by the sequence of elements in the array or set. All other fields remain; the same, includ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:38181,Integrability,interface,interface,38181,"------+-------+; | 0 | 1 | 2 |; | 1 | 3 | 4 |; +---------+-------+-------+. Notes. Matrix dimensions are inferred from input data.; You must provide row and column dimensions by specifying rows or; entries (inclusive) and cols or entries (inclusive).; The respective dimensions of rows, cols and entries must match should; you provide rows and entries or cols and entries (inclusive). Parameters:. globals (dict from str to any) – Global fields by name.; rows (dict from str to list of any) – Row fields by name.; cols (dict from str to list of any) – Column fields by name.; entries (dict from str to list of list of any) – Matrix entries by name in the form entry[row_idx][col_idx]. Returns:; MatrixTable – A MatrixTable assembled from inputs whose rows are keyed by row_idx; and columns are keyed by col_idx. classmethod from_rows_table(table)[source]; Construct matrix table with no columns from a table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Examples; Import a text table and construct a rows-only matrix table:; >>> table = hl.import_table('data/variant-lof.tsv'); >>> table = table.transmute(**hl.parse_variant(table['v'])).key_by('locus', 'alleles'); >>> sites_mt = hl.MatrixTable.from_rows_table(table). Notes; All fields in the table become row-indexed fields in the; result. Parameters:; table (Table) – The table to be converted. Returns:; MatrixTable. property globals; Returns a struct expression including all global fields. Returns:; StructExpression. globals_table()[source]; Returns a table with a single row with the globals of the matrix table.; Examples; Extract the globals table:; >>> globals_table = dataset.globals_table(). Returns:; Table – Table with the globals from the matrix, with a single row. group_cols_by(*exprs, **named_exprs)[source]; Group columns, used with GroupedMatrixTable.aggregate().; Examples; Aggregate to a matrix with cohort as column keys, comput",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:60220,Integrability,interface,interface,60220,"her (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the matrix table; it is; filtering the matrix table to row keys present in another table.; To discard rows whose key is present in other, use; anti_join_rows().; Examples; >>> ds_result = ds.semi_join_rows(rows_to_keep). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement a semi-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_defined(rows_to_keep.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), semi_join_cols(). show(n_rows=None, n_cols=None, include_row_fields=False, width=None, truncate=None, types=True, handler=None)[source]; Print the first few rows of the matrix table to the console. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The output can be passed piped to another output source using the handler argument:; >>> mt.show(handler=lambda x: logging.info(x)) . Parameters:. n_rows (int) – Maximum number of rows to show.; n_cols (int) – Maximum number of columns to show.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(*, rows=True, cols=True, entries=True, handler=None)[source]; Compute and print summary information about the fields in the matrix table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. rows (bool) – Compute summary for the row fields.; cols (bool) – Compute ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:61086,Integrability,interface,interface,61086,"ew rows of the matrix table to the console. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The output can be passed piped to another output source using the handler argument:; >>> mt.show(handler=lambda x: logging.info(x)) . Parameters:. n_rows (int) – Maximum number of rows to show.; n_cols (int) – Maximum number of columns to show.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(*, rows=True, cols=True, entries=True, handler=None)[source]; Compute and print summary information about the fields in the matrix table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. rows (bool) – Compute summary for the row fields.; cols (bool) – Compute summary for the column fields.; entries (bool) – Compute summary for the entry fields. tail(n_rows, n_cols=None, *, n=None)[source]; Subset matrix to last n rows.; Examples; >>> mt_range = hl.utils.range_matrix_table(100, 100). Passing only one argument will take the last n rows:; >>> mt_range.tail(10).count(); (10, 100). Passing two arguments refers to rows and columns, respectively:; >>> mt_range.tail(10, 20).count(); (10, 20). Either argument may be None to indicate no filter.; Last 10 rows, all columns:; >>> mt_range.tail(10, None).count(); (10, 100). All rows, last 10 columns:; >>> mt_range.tail(None, 10).count(); (100, 10). Notes; For backwards compatibility, the n parameter is not named n_rows,; but the parameter refers to the number of rows to keep.; The number of partitions in the new matrix is equal to the number of; partitions containing the ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:5378,Modifiability,variab,variables,5378," in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Explodes a row field of type array or set, copying the entire row for each element. filter_cols; Filter columns of the matrix. filter_entries; Filter entries of the matrix. filter_rows; Filter rows of the matrix. from_parts; Create a MatrixTable from its component parts. from_rows_table; Construct matrix table with no columns from a table. globals_table; Returns a table with a single row with the globals of the matrix table. group_cols_by; Group columns, used with GroupedMatrixTable.aggregate(). group_rows_by; Group rows, used with GroupedMatrixTable.aggregate(). head; Subset matrix to first n_rows rows and n_cols cols. index_cols; Expose the column values as if looked up in a dictionary, indexing with exprs. index_entries; Expose the entries as if looked up in a dictionary, indexing with exprs. index_globals; Return this matrix table's global variables for use in another expression context. index_rows; Expose the row values as if looked up in a dictionary, indexing with exprs. key_cols_by; Key columns by a new set of fields. key_rows_by; Key rows by a new set of fields. localize_entries; Convert the matrix table to a table with entries localized as an array of structs. make_table; Make a table from a matrix table with one field per sample. n_partitions; Number of partitions. naive_coalesce; Naively decrease the number of partitions. persist; Persist this table in memory or on disk. rename; Rename fields of a matrix table. repartition; Change the number of partitions. rows; Returns a table with all row fields in the matrix. sample_cols; Downsample the matrix table by keeping each column with probability p. sample_rows; Downsample the matrix table by keeping each row with probability p. select_cols; Select existing column fields or create new fields by name, dropping the rest. select_entries; Select existing entry fields or ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:41702,Modifiability,variab,variable-length,41702,"argument may be None to indicate no filter.; First 10 rows, all columns:; >>> mt_range.head(10, None).count(); (10, 100). All rows, first 10 columns:; >>> mt_range.head(None, 10).count(); (100, 10). Notes; The number of partitions in the new matrix is equal to the number of; partitions containing the first n_rows rows. Parameters:. n_rows (int) – Number of rows to include (all rows included if None).; n_cols (int, optional) – Number of cols to include (all cols included if None).; n (int) – Deprecated in favor of n_rows. Returns:; MatrixTable – Matrix including the first n_rows rows and first n_cols cols. index_cols(*exprs, all_matches=False)[source]; Expose the column values as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.s).pheno). Or equivalently:; >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.col_key).pheno). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Notes; index_cols(cols) is equivalent to cols().index(exprs); or cols()[exprs].; The type of the resulting struct is the same as the type of; col_value(). Returns:; Expression. index_entries(row_exprs, col_exprs)[source]; Expose the entries as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2.index_entries(dataset.row_key, dataset.col_key).GQ). Or equivalently:; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2[dataset.row_key, dataset.col_key].GQ). Parameters:. row_exprs (tuple of Expression) – Row index expressions.; col_exprs (tuple of Expression) – Column index expressions. Notes; The type of the resulting struct is the same as the type of; entry(). Note; There is a shorthand syntax for MatrixTable.index_entries() using; square brackets (the Python __getitem__ syntax). This sy",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:42935,Modifiability,variab,variables,42935," The type of the resulting struct is the same as the type of; col_value(). Returns:; Expression. index_entries(row_exprs, col_exprs)[source]; Expose the entries as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2.index_entries(dataset.row_key, dataset.col_key).GQ). Or equivalently:; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2[dataset.row_key, dataset.col_key].GQ). Parameters:. row_exprs (tuple of Expression) – Row index expressions.; col_exprs (tuple of Expression) – Column index expressions. Notes; The type of the resulting struct is the same as the type of; entry(). Note; There is a shorthand syntax for MatrixTable.index_entries() using; square brackets (the Python __getitem__ syntax). This syntax is; preferred.; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2[dataset.row_key, dataset.col_key].GQ). Returns:; StructExpression. index_globals()[source]; Return this matrix table’s global variables for use in another; expression context.; Examples; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns:; StructExpression. index_rows(*exprs, all_matches=False)[source]; Expose the row values as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Notes; index_rows(exprs) is equivalent to rows().index(exprs); or rows()[exprs].; The type of the resulting struct is the same as the type of; row_value().",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:43627,Modifiability,variab,variable-length,43627,"x_entries() using; square brackets (the Python __getitem__ syntax). This syntax is; preferred.; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2[dataset.row_key, dataset.col_key].GQ). Returns:; StructExpression. index_globals()[source]; Return this matrix table’s global variables for use in another; expression context.; Examples; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns:; StructExpression. index_rows(*exprs, all_matches=False)[source]; Expose the row values as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Notes; index_rows(exprs) is equivalent to rows().index(exprs); or rows()[exprs].; The type of the resulting struct is the same as the type of; row_value(). Returns:; Expression. key_cols_by(*keys, **named_keys)[source]; Key columns by a new set of fields.; See Table.key_by() for more information on defining a key. Parameters:. keys (varargs of str or Expression.) – Column fields to key by.; named_keys (keyword args of Expression.) – Column fields to key by. Returns:; MatrixTable. key_rows_by(*keys, **named_keys)[source]; Key rows by a new set of fields.; Examples; >>> dataset_result = dataset.key_rows_by('locus'); >>> dataset_result = dataset.key_rows_by(dataset['locus']); >>> dataset_result = dataset.key_rows_by(**dataset.row_key.drop('alleles')). All of these expressions key the dataset by the ‘locus’ field, dropping; the ‘alleles’ field from the row key.; >>> ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:47609,Modifiability,inherit,inherits,47609,"ng the array of entry structs; for the given row.; columns_array_field_name (str) – The name of the global field containing the array of column; structs. Returns:; Table – A table whose fields are the row fields of this matrix table plus; one field named entries_array_field_name. The global fields of; this table are the global fields of this matrix table plus one field; named columns_array_field_name. make_table(separator='.')[source]; Make a table from a matrix table with one field per sample. Deprecated since version 0.2.129: use localize_entries() instead because it supports more; columns. Parameters:; separator (str) – Separator between sample IDs and entry field names. Returns:; Table. See also; localize_entries(). Notes; The table has one row for each row of the input matrix. The; per sample and entry fields are formed by concatenating the; sample ID with the entry field name using separator. If the; entry field name is empty, the separator is omitted.; The table inherits the globals from the matrix table.; Examples; Consider a matrix table with the following schema:; Global fields:; 'batch': str; Column fields:; 's': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; Entry fields:; 'GT': call; 'GQ': int32; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>. and three sample IDs: A, B and C. Then the result of; make_table():; >>> ht = mt.make_table() . has the original row fields along with 6 additional fields,; one for each sample and entry field:; Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>. n_partitions()[source]; Number of partitions.; Notes; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cor",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:55111,Modifiability,variab,variable-length,55111,"s of the sampled object. Parameters:. p (float) – Probability of keeping each row.; seed (int) – Random seed. Returns:; MatrixTable – Matrix table with approximately p * n_rows rows. select_cols(*exprs, **named_exprs)[source]; Select existing column fields or create new fields by name, dropping the rest.; Examples; Select existing fields and compute a new one:; >>> dataset_result = dataset.select_cols(; ... dataset.sample_qc,; ... dataset.pheno.age,; ... isCohort1 = dataset.pheno.cohort_name == 'Cohort1'). Notes; This method creates new column fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method supports aggregation over rows. For instance, the usage:; >>> dataset_result = dataset.select_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified column fields. select_entries(*exprs, **named_exprs)[source]; Select existing entry fields or create new fields by name, dropping the rest.; Examples; Drop all entry fields aside from GT:; >>> dataset_result = dataset.select_entries(dataset.GT). Notes; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:55913,Modifiability,variab,variable-length,55913,"the usage:; >>> dataset_result = dataset.select_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified column fields. select_entries(*exprs, **named_exprs)[source]; Select existing entry fields or create new fields by name, dropping the rest.; Examples; Drop all entry fields aside from GT:; >>> dataset_result = dataset.select_entries(dataset.GT). Notes; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified entry fields. select_globals(*exprs, **named_exprs)[source]; Select existing global fields or create new fields by name, dropping the rest.; Examples; Select one existing field and compute a new one:; >>> dataset_result = dataset.select_globals(dataset.global_field_1,; ... another_global=['AFR', 'EUR', 'EAS', 'AMR', 'SAS']). Notes; This method creates new global fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field na",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:56797,Modifiability,variab,variable-length,56797,"his method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified entry fields. select_globals(*exprs, **named_exprs)[source]; Select existing global fields or create new fields by name, dropping the rest.; Examples; Select one existing field and compute a new one:; >>> dataset_result = dataset.select_globals(dataset.global_field_1,; ... another_global=['AFR', 'EUR', 'EAS', 'AMR', 'SAS']). Notes; This method creates new global fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified global fields. select_rows(*exprs, **named_exprs)[source]; Select existing row fields or create new fields by name, dropping all; other non-key fields.; Examples; Select existing fields and compute a new one:; >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; This method creates new row fields. If a created field shares its name; with a differently-indexed field of the table, or with a row key, the; method will fail.; Row keys are preserved. To drop or change a row key field, use; MatrixTable.key_rows_by(). Note; See Table.select() for more information about using select methods. Note; This method supports aggregation over columns. For instance, the usage:",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:57982,Modifiability,variab,variable-length,57982,"ixTable with specified global fields. select_rows(*exprs, **named_exprs)[source]; Select existing row fields or create new fields by name, dropping all; other non-key fields.; Examples; Select existing fields and compute a new one:; >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; This method creates new row fields. If a created field shares its name; with a differently-indexed field of the table, or with a row key, the; method will fail.; Row keys are preserved. To drop or change a row key field, use; MatrixTable.key_rows_by(). Note; See Table.select() for more information about using select methods. Note; This method supports aggregation over columns. For instance, the usage:; >>> dataset_result = dataset.select_rows(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per row. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified row fields. semi_join_cols(other)[source]; Filters the matrix table to columns whose key appears in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The column key type of the matrix table must match the key type of other.; This method does not change the schema of the matrix table; it is a; filtering the matrix table to column keys not present in another table.; To discard collumns whose key is present in other, use; anti_join_cols().; Examples; >>> ds_result = ds.semi_join_cols(cols_to_keep). It may be inconvenient to key the matrix table by the right-side key.; In this case, it is possible to implement a semi-join using a non-key; field as follows:; >>> ds_result = ds.filter_cols(hl.is_defined(cols_to_keep.index(ds['s']))). S",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:3430,Performance,cache,cache,3430,"umn-indexed fields. globals; Returns a struct expression including all global fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of typ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:17941,Performance,cache,cache,17941,".filter_cols(hl.is_missing(cols_to_remove.index(ds['s']))). See also; semi_join_cols(), filter_cols(), anti_join_rows(). anti_join_rows(other)[source]; Filters the table to rows whose key does not appear in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; re",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:18055,Performance,cache,cache,18055,"cols(), anti_join_rows(). anti_join_rows(other)[source]; Filters the table to rows whose key does not appear in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:27685,Performance,perform,performance,27685,"xprs (varargs of str or Expression) – Names of fields to drop or field reference expressions. Returns:; MatrixTable – Matrix table without specified fields. entries()[source]; Returns a matrix in coordinate table form.; Examples; Extract the entry table:; >>> entries_table = dataset.entries(). Notes; The coordinate table representation of the source matrix table contains; one row for each non-filtered entry of the matrix – if a matrix table; has no filtered entries and contains N rows and M columns, the table will contain; M * N rows, which can be a very large number.; This representation can be useful for aggregating over both axes of a matrix table; at the same time – it is not possible to aggregate over a matrix table using; group_rows_by() and group_cols_by() at the same time (aggregating; by population and chromosome from a variant-by-sample genetics representation,; for instance). After moving to the coordinate representation with entries(),; it is possible to group and aggregate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection – the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using key_cols_by() with no arguments. Warning; If the matrix table has no row key, but has a column key, this operation; may",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:49723,Performance,cache,cache,49723,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:49847,Performance,perform,performance,49847,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:51667,Performance,perform,performance,51667,"ts, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result = dataset.rename({'s': 'info', 'info': 'vcf_info'}). Parameters:; fields (dict from str to str) – Mapping from old field names to new field names. Returns:; MatrixTable – Matrix table with renamed fields. repartition(n_partitions, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> dataset_result = dataset.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n_partitions (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; MatrixTable – Repartitioned dataset. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; Get the first five row field names:; >",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:65925,Performance,perform,performed,65925,"line can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; With row_join_type set to 'outer', an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling distinct_by_row() on each dataset first).; This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters:. other (MatrixTable) – Dataset to concatenate.; row_join_type (str) – If outer, perform an outer join on rows; if ‘inner’, perform an; inner join. Default inner.; drop_right_row_fields (bool) – If true, non-key row fields of other are dropped. Otherwise,; non-key row fields in the two datase",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:66735,Performance,perform,perform,66735," columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; With row_join_type set to 'outer', an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling distinct_by_row() on each dataset first).; This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters:. other (MatrixTable) – Dataset to concatenate.; row_join_type (str) – If outer, perform an outer join on rows; if ‘inner’, perform an; inner join. Default inner.; drop_right_row_fields (bool) – If true, non-key row fields of other are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns:; MatrixTable – Dataset with columns from both datasets. union_rows(*, _check_cols=True)[source]; Take the union of dataset rows.; Examples; Union the rows of two datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be ide",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:66778,Performance,perform,perform,66778," columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; With row_join_type set to 'outer', an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling distinct_by_row() on each dataset first).; This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters:. other (MatrixTable) – Dataset to concatenate.; row_join_type (str) – If outer, perform an outer join on rows; if ‘inner’, perform an; inner join. Default inner.; drop_right_row_fields (bool) – If true, non-key row fields of other are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns:; MatrixTable – Dataset with columns from both datasets. union_rows(*, _check_cols=True)[source]; Take the union of dataset rows.; Examples; Union the rows of two datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be ide",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:49803,Safety,avoid,avoid,49803,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:49809,Safety,redund,redundant,49809,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:52118,Safety,avoid,avoid,52118,"Repartition to 500 partitions:; >>> dataset_result = dataset.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n_partitions (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; MatrixTable – Repartitioned dataset. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; Get the first five row field names:; >>> list(dataset.row)[:5]; ['locus', 'alleles', 'rsid', 'qual', 'filters']. Returns:; StructExpression – Struct of all row fields. property row_key; Row key struct.; Examples; Get the row key field names:; >>> list(dataset.row_key); ['locus', 'alleles']. Returns:; StructExpression. property row_value; Returns a struct expression including all non-key row-indexed fields.; Examples; Get the first five non-key row field names:; >>> list(dataset.row_value)[:5]; ['",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:23600,Testability,test,tested,23600,"in their corresponding arrays. Note; The order of the columns is not guaranteed. Returns:; MatrixTable. cols()[source]; Returns a table with all column fields in the matrix.; Examples; Extract the column table:; >>> cols_table = dataset.cols(). Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the column key (which becomes the table key). To preserve the original; column order as the table row order, first unkey the columns using; key_cols_by() with no arguments. Returns:; Table – Table with all column fields from the matrix, with one row per column of the matrix. compute_entry_filter_stats(row_field='entry_stats_row', col_field='entry_stats_col')[source]; Compute statistics about the number and fraction of filtered entries. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. row_field (str) – Name for computed row field (default: entry_stats_row.; col_field (str) – Name for computed column field (default: entry_stats_col. Returns:; MatrixTable. Notes; Adds a new row field, row_field, and a new column field, col_field,; each of which are structs with the following fields:. n_filtered (tint64) - Number of filtered entries per row; or column.; n_remaining (tint64) - Number of entries not filtered per; row or column.; fraction_filtered (tfloat32) - Number of filtered entries; divided by the total number of filtered and remaining entries. See also; filter_entries(), unfilter_entries(). count()[source]; Count the number of rows and columns in the matrix.; Examples; >>> dataset.count(). Returns:; int, int – Number of rows, number of cols. count_cols(_localize=True)[source]; Count the number of columns in the matrix.; Examples; Count the number of columns:; >>> n_cols = dataset.count_cols(). Returns:; ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:38134,Testability,test,tested,38134,"------+-------+; | 0 | 1 | 2 |; | 1 | 3 | 4 |; +---------+-------+-------+. Notes. Matrix dimensions are inferred from input data.; You must provide row and column dimensions by specifying rows or; entries (inclusive) and cols or entries (inclusive).; The respective dimensions of rows, cols and entries must match should; you provide rows and entries or cols and entries (inclusive). Parameters:. globals (dict from str to any) – Global fields by name.; rows (dict from str to list of any) – Row fields by name.; cols (dict from str to list of any) – Column fields by name.; entries (dict from str to list of list of any) – Matrix entries by name in the form entry[row_idx][col_idx]. Returns:; MatrixTable – A MatrixTable assembled from inputs whose rows are keyed by row_idx; and columns are keyed by col_idx. classmethod from_rows_table(table)[source]; Construct matrix table with no columns from a table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Examples; Import a text table and construct a rows-only matrix table:; >>> table = hl.import_table('data/variant-lof.tsv'); >>> table = table.transmute(**hl.parse_variant(table['v'])).key_by('locus', 'alleles'); >>> sites_mt = hl.MatrixTable.from_rows_table(table). Notes; All fields in the table become row-indexed fields in the; result. Parameters:; table (Table) – The table to be converted. Returns:; MatrixTable. property globals; Returns a struct expression including all global fields. Returns:; StructExpression. globals_table()[source]; Returns a table with a single row with the globals of the matrix table.; Examples; Extract the globals table:; >>> globals_table = dataset.globals_table(). Returns:; Table – Table with the globals from the matrix, with a single row. group_cols_by(*exprs, **named_exprs)[source]; Group columns, used with GroupedMatrixTable.aggregate().; Examples; Aggregate to a matrix with cohort as column keys, comput",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:60173,Testability,test,tested,60173,"her (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the matrix table; it is; filtering the matrix table to row keys present in another table.; To discard rows whose key is present in other, use; anti_join_rows().; Examples; >>> ds_result = ds.semi_join_rows(rows_to_keep). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement a semi-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_defined(rows_to_keep.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), semi_join_cols(). show(n_rows=None, n_cols=None, include_row_fields=False, width=None, truncate=None, types=True, handler=None)[source]; Print the first few rows of the matrix table to the console. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The output can be passed piped to another output source using the handler argument:; >>> mt.show(handler=lambda x: logging.info(x)) . Parameters:. n_rows (int) – Maximum number of rows to show.; n_cols (int) – Maximum number of columns to show.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(*, rows=True, cols=True, entries=True, handler=None)[source]; Compute and print summary information about the fields in the matrix table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. rows (bool) – Compute summary for the row fields.; cols (bool) – Compute ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:60375,Testability,log,logging,60375,"d does not change the schema of the matrix table; it is; filtering the matrix table to row keys present in another table.; To discard rows whose key is present in other, use; anti_join_rows().; Examples; >>> ds_result = ds.semi_join_rows(rows_to_keep). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement a semi-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_defined(rows_to_keep.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), semi_join_cols(). show(n_rows=None, n_cols=None, include_row_fields=False, width=None, truncate=None, types=True, handler=None)[source]; Print the first few rows of the matrix table to the console. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The output can be passed piped to another output source using the handler argument:; >>> mt.show(handler=lambda x: logging.info(x)) . Parameters:. n_rows (int) – Maximum number of rows to show.; n_cols (int) – Maximum number of columns to show.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(*, rows=True, cols=True, entries=True, handler=None)[source]; Compute and print summary information about the fields in the matrix table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. rows (bool) – Compute summary for the row fields.; cols (bool) – Compute summary for the column fields.; entries (bool) – Compute summary for the entry fields. tail(n_rows, n_cols=None, *, n=None)[source]; Subset matrix to last n rows.",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:61039,Testability,test,tested,61039,"ew rows of the matrix table to the console. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The output can be passed piped to another output source using the handler argument:; >>> mt.show(handler=lambda x: logging.info(x)) . Parameters:. n_rows (int) – Maximum number of rows to show.; n_cols (int) – Maximum number of columns to show.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(*, rows=True, cols=True, entries=True, handler=None)[source]; Compute and print summary information about the fields in the matrix table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. rows (bool) – Compute summary for the row fields.; cols (bool) – Compute summary for the column fields.; entries (bool) – Compute summary for the entry fields. tail(n_rows, n_cols=None, *, n=None)[source]; Subset matrix to last n rows.; Examples; >>> mt_range = hl.utils.range_matrix_table(100, 100). Passing only one argument will take the last n rows:; >>> mt_range.tail(10).count(); (10, 100). Passing two arguments refers to rows and columns, respectively:; >>> mt_range.tail(10, 20).count(); (10, 20). Either argument may be None to indicate no filter.; Last 10 rows, all columns:; >>> mt_range.tail(10, None).count(); (10, 100). All rows, last 10 columns:; >>> mt_range.tail(None, 10).count(); (100, 10). Notes; For backwards compatibility, the n parameter is not named n_rows,; but the parameter refers to the number of rows to keep.; The number of partitions in the new matrix is equal to the number of; partitions containing the ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.MatrixTable.html:48966,Usability,simpl,simply,48966,"Then the result of; make_table():; >>> ht = mt.make_table() . has the original row fields along with 6 additional fields,; one for each sample and entry field:; Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>. n_partitions()[source]; Number of partitions.; Notes; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see here; for details. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
https://hail.is/docs/0.2/hail.Table.html:3676,Availability,checkpoint,checkpoint,3676," (table1.group_by(table1.SEX); ... .aggregate(mean_height_data = hl.agg.mean(table1.HT))); >>> table3.show(). Join tables together inside an annotation expression:; >>> table2 = table2.key_by('ID'); >>> table1 = table1.annotate(B = table2[table1.ID].B); >>> table1.show(). Attributes. globals; Returns a struct expression including all global fields. key; Row key struct. row; Returns a struct expression of all row-indexed fields, including keys. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_index; Add the integer index of each row as a new row field. aggregate; Aggregate over rows into a local value. all; Evaluate whether a boolean expression is true for all rows. annotate; Add new fields. annotate_globals; Add new global fields. anti_join; Filters the table to rows whose key does not appear in other. any; Evaluate whether a Boolean expression is true for at least one row. cache; Persist this table in memory. checkpoint; Checkpoint the table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. index_globals; Return this table's global variables",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:15272,Availability,checkpoint,checkpoint,15272,"table1.anti_join(table2). It may be expensive to key the left-side table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> table_result = table1.filter(hl.is_missing(table2.index(table1['ID']))). See also; semi_join(), filter(). any(expr)[source]; Evaluate whether a Boolean expression is true for at least one row.; Examples; Test whether C1 is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:15764,Availability,checkpoint,checkpoint,15764,"is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:16014,Availability,checkpoint,checkpoint,16014,"e for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+---",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:16548,Availability,error,errors,16548,".; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+-------+-----+; | t | x | y |; +--------+-------+-----+; | str | int32 | str |; +--------+-------+-----+; | ""bar"" | 2 | ""B"" |; | ""bar"" | -3 | ""C"" |; | ""foo"" | 4 | ""A"" |; | ""quam"" | 0 | ""D"" |; +--------+-------+-----+. >>> t1.collect_by_key().show(); +--------+---------------------------------+; | t | values |; +--------+---------------------------------+; | str | array<struct{x: int32, y: str}> |; +--------+---------------------------------+; | ""bar"" | [(2,""B""),(-3,""C"")] |; | ""foo"" | [(4,""A"")",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:36052,Availability,avail,available,36052,"; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using key as the sole index expression is equivalent to passing all; key fields individually:; >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:; >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; Join table1 to table2 to produce table_joined:; >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; Tables are joined at rows whose key fields have equal",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:47323,Availability,down,down,47323,"ne entry in some data_field_name array for every row in; the inputs.; The multi_way_zip_join() method assumes that inputs have distinct; keys. If any input has duplicate keys, the row value that is included; in the result array for that key is undefined. Parameters:. tables (list of Table) – A list of tables to combine; data_field_name (str) – The name of the resulting data field; global_field_name (str) – The name of the resulting global field. n_partitions()[source]; Returns the number of partitions in the table.; Examples; Range tables can be constructed with an explicit number of partitions:; >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The min_partitions argument to import_table() forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline.; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> table_result = table1.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; Table – Table with at most max_partitions partitions. order_by(*exprs)[source]; Sort by the specified fields, defaulting",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:54156,Availability,redundant,redundant,54156,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:55521,Availability,avail,available,55521,"sisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> table_result = table1.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:78417,Availability,error,error,78417,"-------------+------------------+; | <expr>.density | <expr>.next_year |; +----------------+------------------+; | float64 | int32 |; +----------------+------------------+; | 2.00e+03 | 2021 |; +----------------+------------------+. See also; Table.transmute(), Table.select_globals(), Table.annotate_globals(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; Table. union(*tables, unify=False)[source]; Union the rows of multiple tables.; Examples; Take the union of rows from two tables:; >>> union_table = table1.union(other_table). Notes; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the unify parameter is; True, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters:. tables (varargs of Table) – Tables to union.; unify (bool) – Attempt to unify table field. Returns:; Table – Table with all rows from each component table. unpersist()[source]; Unpersists this table from memory/disk.; Notes; This function will have no effect on a table that was not previously; persisted. Returns:; Table – Unpersisted table. write(output, overwrite=False, stage_locally=False, _codec_spec=None)[source]; Write to disk.; Examples; >>> table1.write('output/table1.ht', overwrite=True). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. write_many(output, fields, *, ov",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:78980,Availability,checkpoint,checkpoint,78980,"le identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the unify parameter is; True, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters:. tables (varargs of Table) – Tables to union.; unify (bool) – Attempt to unify table field. Returns:; Table – Table with all rows from each component table. unpersist()[source]; Unpersists this table from memory/disk.; Notes; This function will have no effect on a table that was not previously; persisted. Returns:; Table – Unpersisted table. write(output, overwrite=False, stage_locally=False, _codec_spec=None)[source]; Write to disk.; Examples; >>> table1.write('output/table1.ht', overwrite=True). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. write_many(output, fields, *, overwrite=False, stage_locally=False, _codec_spec=None)[source]; Write fields to distinct tables.; Examples; >>> t = hl.utils.range_table(10); >>> t = t.annotate(a = t.idx, b = t.idx * t.idx, c = hl.str(t.idx)); >>> t.write_many('output-many', fields=('a', 'b', 'c'), overwrite=True); >>> hl.read_table('output-many/a').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'a': int32; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/a').show(); +----",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:81354,Availability,checkpoint,checkpoint,81354,"32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/a').show(); +-------+-------+; | a | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 2 | 2 |; | 3 | 3 |; | 4 | 4 |; | 5 | 5 |; | 6 | 6 |; | 7 | 7 |; | 8 | 8 |; | 9 | 9 |; +-------+-------+; >>> hl.read_table('output-many/b').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'b': int32; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/b').show(); +-------+-------+; | b | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 4 | 2 |; | 9 | 3 |; | 16 | 4 |; | 25 | 5 |; | 36 | 6 |; | 49 | 7 |; | 64 | 8 |; | 81 | 9 |; +-------+-------+; >>> hl.read_table('output-many/c').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'c': str; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/c').show(); +-----+-------+; | c | idx |; +-----+-------+; | str | int32 |; +-----+-------+; | ""0"" | 0 |; | ""1"" | 1 |; | ""2"" | 2 |; | ""3"" | 3 |; | ""4"" | 4 |; | ""5"" | 5 |; | ""6"" | 6 |; | ""7"" | 7 |; | ""8"" | 8 |; | ""9"" | 9 |; +-----+-------+. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; fields (list of str) – The fields to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:23036,Deployability,pipeline,pipeline,23036,"xploded table.; Missing arrays or sets are treated as empty.; Currently, the name argument may not be used if field is not a; top-level field of the table (e.g. name may be used with ht.foo; but not ht.foo.bar). Parameters:. field (str or Expression) – Top-level field name or expression.; name (str or None) – If not None, rename the exploded field to name. Returns:; Table. export(output, types_file=None, header=True, parallel=None, delimiter='\t')[source]; Export to a text file.; Examples; Export to a tab-separated file:; >>> table1.export('output/table1.tsv.bgz'). Note; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with any Hail method, as well as with Python’s gzip.open; and R’s read.table.; Nested structures will be exported as JSON. In order to export nested struct; fields as separate fields in the resulting table, use flatten() first. Warning; Do not export to a path that is being read from in the same pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is p",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:23944,Deployability,pipeline,pipeline,23944," table, use flatten() first. Warning; Do not export to a path that is being read from in the same pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:24302,Deployability,pipeline,pipeline,24302,"e file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where Z is 3:; >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). ID; HT; SEX; X; Z. int32; int32; str; int32; int32. 2; 3; 72; 70; “M”; “F”; 6; 7; 3; 3. Remove rows where Z is 3:; >>> filtered_ht = ht.filter(ht.Z == 3, keep=False); >>> filtered_ht.show(); +-------+-------+-----+-------+-------+; |",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:47332,Deployability,pipeline,pipeline,47332,"ne entry in some data_field_name array for every row in; the inputs.; The multi_way_zip_join() method assumes that inputs have distinct; keys. If any input has duplicate keys, the row value that is included; in the result array for that key is undefined. Parameters:. tables (list of Table) – A list of tables to combine; data_field_name (str) – The name of the resulting data field; global_field_name (str) – The name of the resulting global field. n_partitions()[source]; Returns the number of partitions in the table.; Examples; Range tables can be constructed with an explicit number of partitions:; >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The min_partitions argument to import_table() forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline.; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> table_result = table1.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; Table – Table with at most max_partitions partitions. order_by(*exprs)[source]; Sort by the specified fields, defaulting",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:54215,Deployability,pipeline,pipelines,54215,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:81823,Deployability,update,updated,81823,"32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/a').show(); +-------+-------+; | a | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 2 | 2 |; | 3 | 3 |; | 4 | 4 |; | 5 | 5 |; | 6 | 6 |; | 7 | 7 |; | 8 | 8 |; | 9 | 9 |; +-------+-------+; >>> hl.read_table('output-many/b').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'b': int32; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/b').show(); +-------+-------+; | b | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 4 | 2 |; | 9 | 3 |; | 16 | 4 |; | 25 | 5 |; | 36 | 6 |; | 49 | 7 |; | 64 | 8 |; | 81 | 9 |; +-------+-------+; >>> hl.read_table('output-many/c').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'c': str; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/c').show(); +-----+-------+; | c | idx |; +-----+-------+; | str | int32 |; +-----+-------+; | ""0"" | 0 |; | ""1"" | 1 |; | ""2"" | 2 |; | ""3"" | 3 |; | ""4"" | 4 |; | ""5"" | 5 |; | ""6"" | 6 |; | ""7"" | 7 |; | ""8"" | 8 |; | ""9"" | 9 |; +-----+-------+. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; fields (list of str) – The fields to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:8133,Integrability,depend,dependent,8133,"-------+-------+-------+. Notes; This method returns a table with a new field whose name is given by; the name parameter, with type tint64. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like Table.take() or Table.export()) will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields may be defined in several ways:. In terms of constant values. Every row will have the same value.; In terms of other fields in the table.; In terms of fields in other tables, this is called “joining”. Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:37111,Integrability,depend,depends,37111,"sions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; Join table1 to table2 to produce table_joined:; >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. inner – Only rows with a matching key in the opposite table are included; in the resulting table.; left – All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; right – All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; outer – All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing. Both tables must have the same number of keys and the corresponding; types of each key must be the same (order matters), but the key names; can be different. Fo",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:45529,Integrability,interface,interface,45529,"32 | str | int32 | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | 9 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | 9 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | 10 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | 10 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; This method is used to specify all the fields of a new row key. The old; key fields may be overwritten by newly-assigned fields, as described in; Table.annotate(). If not overwritten, they are preserved as non-key; fields.; See Table.select() for more information about how to define new; key fields. Parameters:; keys (varargs of type str) – Field(s) to key by. Returns:; Table – Table with a new key. static multi_way_zip_join(tables, data_field_name, global_field_name)[source]; Combine many tables in a zip join. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The row type of the returned table is a struct with the key fields, and; one extra field, data_field_name, which is an array of structs with; the non key fields, one per input. The array elements are missing if; their corresponding input had no row with that key or possibly if there; is another input with more rows with that key than the corresponding; input.; The global type of the returned table is an array of structs of the; global type of all of the inputs.; The types for every input must be identical, not merely compatible,; including the keys.; A zip join is similar to an outer join however rows are not duplicated; to create the full Cartesian product of duplicate keys. Instead, there; is exactly one entry in some data_field_name array for every row in; the inputs.; The multi_way_zip_join() method assumes that inputs have distinct; keys. If any input has duplicate keys, the row value that is included; in the result array for ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:67438,Integrability,interface,interface,67438,"-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.show(handler=lambda x: logging.info(x)) . Parameters:. n or n_rows (int) – Maximum number of rows to show, or negative to show all rows.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(handler=None)[source]; Compute and print summary information about the fields in the table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. tail(n)[source]; Subset table to last n rows.; Examples; Subset to the last three rows:; >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; The number of partitions in the new table is equal to the number of; partitions containing the last n rows. Parameters:; n (int) – Number of rows to include. Returns:; Table – Table including the last n rows. take(n, _localize=True)[source]; Collect the first n rows of the table into a local list.; Examples; Take the first three rows:; >>> first3 = table1.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Notes; This method does not need to look at all the data in the table, and; allows for fast queries of the start of the table.; This method is equivalent to Table.head() followed by; Table.collect(). Parameters:; n (int) – Numbe",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:4699,Modifiability,variab,variables,4699,"e table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. index_globals; Return this table's global variables for use in another expression context. join; Join two tables together. key_by; Key table by a new set of fields. multi_way_zip_join; Combine many tables in a zip join. n_partitions; Returns the number of partitions in the table. naive_coalesce; Naively decrease the number of partitions. order_by; Sort by the specified fields, defaulting to ascending order. parallelize; Parallelize a local array of structs into a distributed table. persist; Persist this table in memory or on disk. rename; Rename fields of the table. repartition; Change the number of partitions. sample; Downsample the table by keeping each row with probability p. select; Select existing fields or create new fields by name, dropping the rest. select_globals; Select existing global fields or create new fields by name, dropping the rest. semi_join; Filters the table to rows whose key appears in other. show; Print the first few rows of the table to the console. summarize; Compute and print summary information about th",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:31655,Modifiability,variab,variable-length,31655,"e of the globals struct:; >>> table1.globals.dtype; dtype('struct{global_field_1: int32, global_field_2: int32}'). The number of global fields:; >>> len(table1.globals); 2. Returns:; StructExpression – Struct of all global fields. group_by(*exprs, **named_exprs)[source]; Group by a new key for use with GroupedTable.aggregate().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; This function is always followed by GroupedTable.aggregate(). Follow the; link for documentation on the aggregation step. Note; Using group_by; group_by and its sibling methods (MatrixTable.group_rows_by() and; MatrixTable.group_cols_by()) accept both variable-length (f(x, y, z)); and keyword (f(a=x, b=y, c=z)) arguments.; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a; GroupedTable grouped by fields C1 and C2 of table1.; First, variable-length string arguments:; >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:; >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:; >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two u",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:32017,Modifiability,variab,variable-length,32017," X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; This function is always followed by GroupedTable.aggregate(). Follow the; link for documentation on the aggregation step. Note; Using group_by; group_by and its sibling methods (MatrixTable.group_rows_by() and; MatrixTable.group_cols_by()) accept both variable-length (f(x, y, z)); and keyword (f(a=x, b=y, c=z)) arguments.; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a; GroupedTable grouped by fields C1 and C2 of table1.; First, variable-length string arguments:; >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:; >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:; >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, x:; >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:; >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); .",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:32173,Modifiability,variab,variable-length,32173,"um(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; This function is always followed by GroupedTable.aggregate(). Follow the; link for documentation on the aggregation step. Note; Using group_by; group_by and its sibling methods (MatrixTable.group_rows_by() and; MatrixTable.group_cols_by()) accept both variable-length (f(x, y, z)); and keyword (f(a=x, b=y, c=z)) arguments.; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a; GroupedTable grouped by fields C1 and C2 of table1.; First, variable-length string arguments:; >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:; >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:; >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, x:; >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:; >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:32480,Modifiability,variab,variable-length,32480,"e aggregation step. Note; Using group_by; group_by and its sibling methods (MatrixTable.group_rows_by() and; MatrixTable.group_cols_by()) accept both variable-length (f(x, y, z)); and keyword (f(a=x, b=y, c=z)) arguments.; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a; GroupedTable grouped by fields C1 and C2 of table1.; First, variable-length string arguments:; >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:; >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:; >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, x:; >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:; >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = (table1.group_by(table1.C1, 'C2', height_bin = table1.HT // 20); ... .aggregate(meanX = hl.agg.mean(table1.X))). Note; This method does not support aggregation in key expressions. Parameters:. exprs (varargs of type str or Expression) – Field names o",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:36316,Modifiability,variab,variable-length,36316,"ssions:; >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; Join table1 to table2 to produce table_joined:; >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. inner – Only rows with a matching key in the opposite table are included; in the resulting table.; left – All rows from the left table are included in the resulting table.; If a row in the left table h",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:36538,Modifiability,variab,variables,36538,"| 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; Join table1 to table2 to produce table_joined:; >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. inner – Only rows with a matching key in the opposite table are included; in the resulting table.; left – All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; right – All rows from the right table are included in the resulting table.; If a row in the right ta",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:59815,Modifiability,variab,variable-length,59815,"-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Parameters:. p (float) – Probability of keeping each row.; seed (int) – Random seed. Returns:; Table – Table with approximately p * n_rows rows. select(*exprs, **named_exprs)[source]; Select existing fields or create new fields by name, dropping the rest.; Examples; Select a few old fields and compute a new one:; >>> table_result = table1.select(table1.C1, Y=table1.Z - table1.X). Notes; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; Using select; Select and its sibling methods (Table.select_globals(),; MatrixTable.select_globals(), MatrixTable.select_rows(),; MatrixTable.select_cols(), and MatrixTable.select_entries()) accept; both variable-length (f(x, y, z)) and keyword (f(a=x, b=y, c=z)); arguments.; Select methods will always preserve the key along that axis; e.g. for; Table.select(), the table key will aways be kept. To modify the; key, use key_by().; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a new table with; fields C1 and C2 of table1, and the table key ID.; First, variable-length string arguments:; >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). Th",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:60346,Modifiability,variab,variable-length,60346,"nd compute a new one:; >>> table_result = table1.select(table1.C1, Y=table1.Z - table1.X). Notes; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; Using select; Select and its sibling methods (Table.select_globals(),; MatrixTable.select_globals(), MatrixTable.select_rows(),; MatrixTable.select_cols(), and MatrixTable.select_entries()) accept; both variable-length (f(x, y, z)) and keyword (f(a=x, b=y, c=z)); arguments.; Select methods will always preserve the key along that axis; e.g. for; Table.select(), the table key will aways be kept. To modify the; key, use key_by().; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a new table with; fields C1 and C2 of table1, and the table key ID.; First, variable-length string arguments:; >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, x.:; >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:; >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; This method d",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:60451,Modifiability,variab,variable-length,60451,"tes; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; Using select; Select and its sibling methods (Table.select_globals(),; MatrixTable.select_globals(), MatrixTable.select_rows(),; MatrixTable.select_cols(), and MatrixTable.select_entries()) accept; both variable-length (f(x, y, z)) and keyword (f(a=x, b=y, c=z)); arguments.; Select methods will always preserve the key along that axis; e.g. for; Table.select(), the table key will aways be kept. To modify the; key, use key_by().; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a new table with; fields C1 and C2 of table1, and the table key ID.; First, variable-length string arguments:; >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, x.:; >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:; >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:60656,Modifiability,variab,variable-length,60656,"; MatrixTable.select_globals(), MatrixTable.select_rows(),; MatrixTable.select_cols(), and MatrixTable.select_entries()) accept; both variable-length (f(x, y, z)) and keyword (f(a=x, b=y, c=z)); arguments.; Select methods will always preserve the key along that axis; e.g. for; Table.select(), the table key will aways be kept. To modify the; key, use key_by().; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a new table with; fields C1 and C2 of table1, and the table key ID.; First, variable-length string arguments:; >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, x.:; >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:; >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; Table – Table with specified fields. se",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:61421,Modifiability,variab,variable-length,61421,":; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, x.:; >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:; >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; Table – Table with specified fields. select_globals(*exprs, **named_exprs)[source]; Select existing global fields or create new fields by name, dropping the rest.; Examples; Selecting two global fields, one by name and one new one, replacing any previously annotated; global fields.; >>> ht = hl.utils.range_table(1); >>> ht = ht.annotate_globals(pops = ['EUR', 'AFR', 'EAS', 'SAS']); >>> ht = ht.annotate_globals(study_name = 'HGDP+1kg'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'study_name': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ---------------------------",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:63256,Modifiability,variab,variable-length,63256,"Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'target_date': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------. Fields may also be selected by their name:; >>> ht = ht.select_globals('target_date'); >>> ht.globals.show(); +--------------------+; | <expr>.target_date |; +--------------------+; | str |; +--------------------+; | ""2025-01-01"" |; +--------------------+. Notes; This method creates new global fields. If a created field shares its name; with a row-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; Table – Table with specified global fields. semi_join(other)[source]; Filters the table to rows whose key appears in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; Table. Notes; The key type of the table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the table to keys present in another table.; To discard keys present in other, use anti_join().; Examples; >>> table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:3639,Performance,cache,cache,3639,"and aggregate to produce a new table:; >>> table3 = (table1.group_by(table1.SEX); ... .aggregate(mean_height_data = hl.agg.mean(table1.HT))); >>> table3.show(). Join tables together inside an annotation expression:; >>> table2 = table2.key_by('ID'); >>> table1 = table1.annotate(B = table2[table1.ID].B); >>> table1.show(). Attributes. globals; Returns a struct expression including all global fields. key; Row key struct. row; Returns a struct expression of all row-indexed fields, including keys. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_index; Add the integer index of each row as a new row field. aggregate; Aggregate over rows into a local value. all; Evaluate whether a boolean expression is true for all rows. annotate; Add new fields. annotate_globals; Add new global fields. anti_join; Filters the table to rows whose key does not appear in other. any; Evaluate whether a Boolean expression is true for at least one row. cache; Persist this table in memory. checkpoint; Checkpoint the table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:7590,Performance,perform,performance,7590,"table1.add_index(); >>> table_result.show() ; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; This method returns a table with a new field whose name is given by; the name parameter, with type tint64. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like Table.take() or Table.export()) will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:15064,Performance,cache,cache,15064," Notes; The key type of the table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the table to keys not present in another table.; To restrict to keys present in other, use semi_join().; Examples; >>> table_result = table1.anti_join(table2). It may be expensive to key the left-side table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> table_result = table1.filter(hl.is_missing(table2.index(table1['ID']))). See also; semi_join(), filter(). any(expr)[source]; Evaluate whether a Boolean expression is true for at least one row.; Examples; Test whether C1 is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_locali",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:15171,Performance,cache,cache,15171,"t change the schema of the table; it is a method of; filtering the table to keys not present in another table.; To restrict to keys present in other, use semi_join().; Examples; >>> table_result = table1.anti_join(table2). It may be expensive to key the left-side table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> table_result = table1.filter(hl.is_missing(table2.index(table1['ID']))). See also; semi_join(), filter(). any(expr)[source]; Evaluate whether a Boolean expression is true for at least one row.; Examples; Test whether C1 is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Exa",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:17839,Performance,load,loaded,17839,"t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+-------+-----+; | t | x | y |; +--------+-------+-----+; | str | int32 | str |; +--------+-------+-----+; | ""bar"" | 2 | ""B"" |; | ""bar"" | -3 | ""C"" |; | ""foo"" | 4 | ""A"" |; | ""quam"" | 0 | ""D"" |; +--------+-------+-----+. >>> t1.collect_by_key().show(); +--------+---------------------------------+; | t | values |; +--------+---------------------------------+; | str | array<struct{x: int32, y: str}> |; +--------+---------------------------------+; | ""bar"" | [(2,""B""),(-3,""C"")] |; | ""foo"" | [(4,""A"")] |; | ""quam"" | [(0,""D"")] |; +--------+---------------------------------+. Notes; The order of the values array is not guaranteed. Parameters:; name (str) – Field name for all values per key. Returns:; Table. count()[source]; Count the number of rows in the table.; Examples; Count the number of rows in a table loaded from ‘data/kt_example1.tsv’. Each line of the TSV; becomes one row in the Hail Table.; >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns:; int – The number of rows in the table. describe(handler=<built-in function print>, *, widget=False)[source]; Print information about the fields in the table. Note; The widget argument is experimental. Parameters:. handler (Callable[[str], None]) – Handler function for returned string.; widget (bool) – Create an interactive IPython widget. distinct()[source]; Deduplicate keys, keeping exactly one row for each unique key. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'a': 'foo', 'b': 1},; ... {'a': 'bar', 'b': 5},; ... {'a': 'bar', 'b': 2}],; ... hl.tstruct(a=hl.tstr, b=hl.tint32),; ... key='a'). >>> t1.show(); +-------+-------+; | a | b |; +-------+-------+; | str | int32 |; +-------+-------+; | ""bar"" | 5 |; | ""bar"" | 2 |; | ""foo"" | 1 |; +-------+-------+. >>> t",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:23989,Performance,optimiz,optimization,23989,"e pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where Z is 3:; >>> fil",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:54073,Performance,cache,cache,54073,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:54195,Performance,perform,performance,54195,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:55761,Performance,perform,performance,55761,"le. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> table_result = table1.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; Table – Repartitioned table. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; The data type of the row struct:; >>> table1.row.dtype; d",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:54150,Safety,avoid,avoid,54150,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:54156,Safety,redund,redundant,54156,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:56212,Safety,avoid,avoid,56212,"es; Repartition to 500 partitions:; >>> table_result = table1.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; Table – Repartitioned table. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; The data type of the row struct:; >>> table1.row.dtype; dtype('struct{ID: int32, HT: int32, SEX: str, X: int32, Z: int32, C1: int32, C2: int32, C3: int32}'). The number of row fields:; >>> len(table1.row); 8. Returns:; StructExpression – Struct of all row fields, including key fields. property row_value; Returns a struct expression including all non-key row-indexed fields.; Examples; The data type of the row struct:; >>> table1.row_value.dtype; dtype('struct{HT: int32, SEX: str, X: int32, Z: int32, C1: int32, C2: i",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:16388,Security,access,accessed,16388,"False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+-------+-----+; | t | x | y |; +--------+-------+-----+; | str | int32 | str |; +--------+-------+-----+; | ""bar"" | 2 | ""B"" |; | ""bar"" | -3 | ""C"" |; | ""foo"" | 4 | ""A"" |; | ""quam"" | 0 | ""D"" |; +--------+-------+-----+. >>> t1.collect_by_key().show(); +--------+---------------------------------+; | t | values |; +--------+---------------------------------+; | str | array<struc",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:35650,Security,expose,expose,35650," in a dictionary, indexing; with exprs.; Examples; In the example below, both table1 and table2 are keyed by one; field ID of type int.; >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using key as the sole index expression is equivalent to passing all; key fields individually:; >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:; >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Ret",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:8434,Testability,test,test,8434," will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields may be defined in several ways:. In terms of constant values. Every row will have the same value.; In terms of other fields in the table.; In terms of fields in other tables, this is called “joining”. Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Add field Y containing the square of field X; >>> ht = ht.annotate(Y = ht.X ** 2); >>> ht.show(); +-------+-------+-----+-------+-------+----------+; | ID | HT | SEX | X | Z | Y |; +-------+-------+-----+-------+-------+----------+; | int32 | int32 | str | int32 | ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:45482,Testability,test,tested,45482,"32 | str | int32 | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | 9 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | 9 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | 10 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | 10 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; This method is used to specify all the fields of a new row key. The old; key fields may be overwritten by newly-assigned fields, as described in; Table.annotate(). If not overwritten, they are preserved as non-key; fields.; See Table.select() for more information about how to define new; key fields. Parameters:; keys (varargs of type str) – Field(s) to key by. Returns:; Table – Table with a new key. static multi_way_zip_join(tables, data_field_name, global_field_name)[source]; Combine many tables in a zip join. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The row type of the returned table is a struct with the key fields, and; one extra field, data_field_name, which is an array of structs with; the non key fields, one per input. The array elements are missing if; their corresponding input had no row with that key or possibly if there; is another input with more rows with that key than the corresponding; input.; The global type of the returned table is an array of structs of the; global type of all of the inputs.; The types for every input must be identical, not merely compatible,; including the keys.; A zip join is similar to an outer join however rows are not duplicated; to create the full Cartesian product of duplicate keys. Instead, there; is exactly one entry in some data_field_name array for every row in; the inputs.; The multi_way_zip_join() method assumes that inputs have distinct; keys. If any input has duplicate keys, the row value that is included; in the result array for ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:51146,Testability,log,login,51146,"lelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... schema=hl.tstruct(a=hl.tint, b=hl.tint),; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail Struct or dict s.; >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in partial_type. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly.; >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+------------",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:51275,Testability,log,login,51275,"; +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail Struct or dict s.; >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in partial_type. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly.; >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:51411,Testability,log,login,51411,"+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail Struct or dict s.; >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in partial_type. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly.; >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:51536,Testability,log,login,51536,"> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in partial_type. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly.; >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:51664,Testability,log,login,51664,"| a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in partial_type. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly.; >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:51992,Testability,log,login,51992,"(the element type is unspecified), so we specify the type of labels; explicitly.; >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; +-----------+------------+. Parallelizing with a specified number of partitions:; >>> rows = [ {'a': i} for i in range(100) ]; >>> ht = hl.Table.parallelize(rows, n_partitions=10); >>> ht.n_partitions(); 10; >>> ht.count(); 100. Parallelizing with some global",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:66789,Testability,log,logging,66789,"5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; +-------+-------+-----+-------+-------+-------+-------+-------+. See also; anti_join(). show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None)[source]; Print the first few rows of the table to the console.; Examples; Show the first lines of the table:; >>> table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.show(handler=lambda x: logging.info(x)) . Parameters:. n or n_rows (int) – Maximum number of rows to show, or negative to show all rows.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(handler=None)[source]; Compute and print summary information about the fields in the table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. tail(n)[source]; Subset table to last n rows.; Examples; Subset to the last three rows:; >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; The number of partitions in the new table is equal to the number of; partitions containing the last n rows. Parameters:; n (int) – Number of rows to i",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:67391,Testability,test,tested,67391,"-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.show(handler=lambda x: logging.info(x)) . Parameters:. n or n_rows (int) – Maximum number of rows to show, or negative to show all rows.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(handler=None)[source]; Compute and print summary information about the fields in the table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. tail(n)[source]; Subset table to last n rows.; Examples; Subset to the last three rows:; >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; The number of partitions in the new table is equal to the number of; partitions containing the last n rows. Parameters:; n (int) – Number of rows to include. Returns:; Table – Table including the last n rows. take(n, _localize=True)[source]; Collect the first n rows of the table into a local list.; Examples; Take the first three rows:; >>> first3 = table1.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Notes; This method does not need to look at all the data in the table, and; allows for fast queries of the start of the table.; This method is equivalent to Table.head() followed by; Table.collect(). Parameters:; n (int) – Numbe",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:846,Usability,learn,learning,846,"﻿. Hail | ; Table. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Table; GroupedTable; MatrixTable; GroupedMatrixTable. Modules; Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Table. View page source. Table. class hail.Table[source]; Hail’s distributed implementation of a dataframe or SQL table.; Use read_table() to read a table that was written with; Table.write(). Use to_spark() and Table.from_spark(); to inter-operate with PySpark’s; SQL and; machine learning; functionality.; Examples; The examples below use table1 and table2, which are imported; from text files using import_table().; >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). +-------+-------+--------+; | ID | A | B |; +-------+-------+--------+; | int32 | int32 | str |; +-------+-------+--------+; | 1 | 65 | cat |; | 2 | 72 | dog |; | 3 | 70 | mouse |; | 4 | 60 | rabbit |; +-------+-------+--------+. Define new annotations:; >>> height_mean_m = 68; >>> height_sd_m = 3; >>> he",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:7571,Usability,simpl,simple,7571,"table1.add_index(); >>> table_result.show() ; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; This method returns a table with a new field whose name is given by; the name parameter, with type tint64. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like Table.take() or Table.export()) will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:35816,Usability,usab,usable,35816,"esult = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using key as the sole index expression is equivalent to passing all; key fields individually:; >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:; >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:39256,Usability,simpl,simple,39256,"e joined (their; rows will be joined where table1.a == table2.c and table1.b ==; table2.d).; The key fields and order from the left table are preserved,; while the key fields from the right table are not present in; the result. Note; These join methods implement a traditional Cartesian product join, and; the number of records in the resulting table can be larger than; the number of records on the left or right if duplicate keys are; present. Parameters:. right (Table) – Table to join.; how (str) – Join type. One of “inner”, “left”, “right”, “outer”. Returns:; Table – Joined table. property key; Row key struct.; Examples; List of key field names:; >>> list(table1.key); ['ID']. Number of key fields:; >>> len(table1.key); 1. Returns:; StructExpression. key_by(*keys, **named_keys)[source]; Key table by a new set of fields.; Table keys control both the order of the rows in the table and the ability to join or; annotate one table with the information in another table.; Examples; Consider a simple unkeyed table. Its rows appear are guaranteed to appear in the same order; as they were in the source text file.; >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Changing the key forces the rows to appear in ascending order. For this reason,; key_by() is a relatively expensive operation. It must sort the entire dataset.; >>> ht = ht.key_by('HT'); >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail.Table.html:47703,Usability,simpl,simply,47703,"l field. n_partitions()[source]; Returns the number of partitions in the table.; Examples; Range tables can be constructed with an explicit number of partitions:; >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The min_partitions argument to import_table() forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline.; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> table_result = table1.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; Table – Table with at most max_partitions partitions. order_by(*exprs)[source]; Sort by the specified fields, defaulting to ascending order. Will unkey the table if it is keyed.; Examples; Let’s assume we have a field called HT in our table.; By default, ascending order is used:; >>> sorted_table = table1.order_by(table1.HT). >>> sorted_table = table1.order_by('HT'). You can sort in ascending order explicitly:; >>> sorted_table = table1.order_by(hl.asc(table1.HT)). >>> sorted_table = table1.order_by(hl.asc('HT')). Tables can be sorted by field descending ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
https://hail.is/docs/0.2/hail_on_the_cloud.html:1421,Deployability,update,updated,1421,"﻿. Hail | ; Hail on the Cloud. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud. View page source. Hail on the Cloud; Public clouds are a natural place to run Hail, offering the ability to run on-demand workloads with; high elasticity. Microsoft Azure, Google Cloud Platform, Databricks and Amazon Web Services make it; possible to rent Spark clusters with thousands of cores on-demand, providing for the elastic compute; requirements of scientific research without an up-front capital investment in hardware. General Advice; Start Small; Estimating time; Estimating cost. Query-on-Batch; Getting Started; Variant Effect Predictor (VEP). Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail_on_the_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail_on_the_cloud.html
https://hail.is/docs/0.2/index.html:717,Deployability,install,installation,717,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
https://hail.is/docs/0.2/index.html:867,Deployability,install,installation,867,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
https://hail.is/docs/0.2/index.html:4127,Deployability,update,updated,4127,"ersion 0.2.114; Version 0.2.113; Version 0.2.112; Version 0.2.111; Version 0.2.110; Version 0.2.109; Version 0.2.108; Version 0.2.107; Version 0.2.106; Version 0.2.105; Version 0.2.104; Version 0.2.103; Version 0.2.102; Version 0.2.101; Version 0.2.100; Version 0.2.99; Version 0.2.98; Version 0.2.97; Version 0.2.96; Version 0.2.95; Version 0.2.94; Version 0.2.93; Version 0.2.92; Version 0.2.91; Version 0.2.90; Version 0.2.89; Version 0.2.88; Version 0.2.87; Version 0.2.86; Version 0.2.85; Version 0.2.84; Version 0.2.83; Version 0.2.82; Version 0.2.81; Version 0.2.80; Version 0.2.79; Version 0.2.78; Version 0.2.77; Version 0.2.76; Version 0.2.75; Version 0.2.74; Version 0.2.73; Version 0.2.72; Version 0.2.71; Version 0.2.70; Version 0.2.69; Version 0.2.68; Version 0.2.67; Version 0.2.66; Version 0.2.65; Version 0.2.64; Version 0.2.63; Version 0.2.62; Version 0.2.61; Version 0.2.60; Version 0.2.59; Version 0.2.58; Version 0.2.57; Version 0.2.56; Version 0.2.55; Version 0.2.54; Version 0.2.53; Version 0.2.52; Version 0.2.51; Version 0.2.50; Version 0.2.49; Version 0.2.48; Version 0.2.47; Version 0.2.46; Version 0.2.45; Version 0.2.44; Version 0.2.43; Version 0.2.42; Version 0.2.41; Version 0.2.40; Version 0.2.39; Version 0.2.38; Version 0.2.37; Version 0.2.36; Version 0.2.35; Version 0.2.34; Version 0.2.33; Version 0.2.32; Version 0.2.31; Version 0.2.30; Version 0.2.29; Version 0.2.28; Version 0.2.27; Version 0.2.26; Version 0.2.25; Version 0.2.24; Version 0.2.23; Version 0.2.22; Version 0.2.21; Version 0.2.20; Version 0.2.19; Version 0.2.18; Version 0.2.17; Version 0.2.16; Version 0.2.15; Version 0.2.14; Version 0.2.13; Version 0.2.12; Version 0.2.11; Version 0.2.10; Version 0.2.9; Version 0.2.8; Version 0.2.7; Version 0.2.6; Version 0.2.5; Version 0.2.4: Beginning of history!. Indices and tables. Index. If you would like to refer to our Hail v0.1 (deprecated) docs, please view Hail 0.1 docs. Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
https://hail.is/docs/0.2/index.html:478,Performance,scalab,scalable,478,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
https://hail.is/docs/0.2/index.html:1675,Testability,test,tests,1675,"ial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; Version 0.2.123; Version 0.2.122; Version 0.2.121; Version 0.2.120; Version 0.2.119; Version 0.2.118; Version 0.2.117; Version 0.2.116; Version 0.2.115; Version 0.2.114; Version 0.2.113; Version 0.2.112; Version 0.2.111; Version 0.2.110; Version 0.2.109; Version 0.2.108; Version 0.2.107; Version 0.2.106; Version 0.2.105; Version 0.2.104; Version 0.2.103; Version 0.2.102; Version 0.2.101; Version 0.2.100; Version 0.2.99; Version 0.2.98; Version 0.2.97; Version 0.2.96; Version 0.2.95; Version 0.2.94; Version 0.2.93; Version 0.2.92; Version 0.2.91; Version 0.2.90; Version 0.2.89; Version 0.2.88; Version 0.2.87; Version 0.2.86; Ver",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
https://hail.is/docs/0.2/index.html:646,Usability,simpl,simple,646,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
https://hail.is/docs/0.2/libraries.html:922,Deployability,install,install,922,"﻿. Hail | ; Libraries. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Libraries. View page source. Libraries; This pages lists any external libraries we are aware of that are built on top of Hail. These libraries are not developed by the Hail team so we cannot necessarily answer; questions about them, but they may provide useful functions not included in base Hail. gnomad (Hail Utilities for gnomAD); This repo contains a number of Hail utility functions and scripts for the gnomAD project and the Translational Genomics Group.; Install with pip install gnomad.; More info can be found in the documentation or on the PyPI project page. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/libraries.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/libraries.html
https://hail.is/docs/0.2/libraries.html:1069,Deployability,update,updated,1069,"﻿. Hail | ; Libraries. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Libraries. View page source. Libraries; This pages lists any external libraries we are aware of that are built on top of Hail. These libraries are not developed by the Hail team so we cannot necessarily answer; questions about them, but they may provide useful functions not included in base Hail. gnomad (Hail Utilities for gnomAD); This repo contains a number of Hail utility functions and scripts for the gnomAD project and the Translational Genomics Group.; Install with pip install gnomad.; More info can be found in the documentation or on the PyPI project page. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/libraries.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/libraries.html
https://hail.is/docs/0.2/other_resources.html:565,Deployability,update,updated,565,"﻿. Hail | ; Other Resources. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Hadoop Glob Patterns. Change Log And Version Policy. menu; Hail. Other Resources. View page source. Other Resources. Hadoop Glob Patterns. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/other_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/other_resources.html
https://hail.is/docs/0.2/plot.html:5899,Availability,avail,available,5899,"ins=10, range=((0, 1), None)). Parameters:. x (NumericExpression) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range (None or ((float, float), (float, float))) – The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width (int) – Plot width (default 600px).; height (int) – Plot height (default 600px).; title (str) – Title of the plot.; colors (Sequence[str]) – List of colors (hex codes, or strings as described; here). Compatible with one of the many; built-in palettes available here.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.scatter(x, y, label=None, title=None, xlabel=None, ylabel=None, size=4, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create an interactive scatter plot.; x and y must both be either:; - a NumericExpression from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (othe",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:7356,Availability,down,down,7356,"n from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the result",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:9018,Availability,down,downsample,9018,"th categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool, optional) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; missing_label (str) – Label to use when a point is missing data for a categorical label. Returns:; bokeh.models.Plot if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.qq(pvals, label=None, title='Q-Q plot', xlabel='Expected -log10(p)', ylabel='Observed -log10(p)', size=6, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot); If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive qq plot; Points will be colored by one of the labels defined in the label using t",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:10563,Availability,down,down,10563," height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot); If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive qq plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. pvals (NumericExpression) – List of x-values to be plotted.; label (Expression or Dict[str, Expression]]) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point o",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:12096,Availability,down,downsample,12096,"be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; missing_label (str) – Label to use when a point is missing data for a categorical label. Returns:; bokeh.plotting.figure if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.manhattan(pvals, locus=None, title=None, size=4, hover_fields=None, collect_all=None, n_divisions=500, significance_line=5e-08)[source]; Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters:. pvals (Float64Expression) – P-values to be plotted.; locus (LocusExpression, optional) – Locus values to be plotted.; title (str, optional) – Title of the plot.; size (int) – Size of markers in screen space units.; hover_fields (Dict[str, Expression], optional) – Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all (bool, optional) – Deprecated - use n_divisions instead.; n_divisions (int, optio",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:13114,Availability,down,downsample,13114,"ue = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; missing_label (str) – Label to use when a point is missing data for a categorical label. Returns:; bokeh.plotting.figure if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.manhattan(pvals, locus=None, title=None, size=4, hover_fields=None, collect_all=None, n_divisions=500, significance_line=5e-08)[source]; Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters:. pvals (Float64Expression) – P-values to be plotted.; locus (LocusExpression, optional) – Locus values to be plotted.; title (str, optional) – Title of the plot.; size (int) – Size of markers in screen space units.; hover_fields (Dict[str, Expression], optional) – Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all (bool, optional) – Deprecated - use n_divisions instead.; n_divisions (int, optional.) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; significance_line (float, optional) – p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If None, no line is added. Returns:; bokeh.models.Plot. hail.plot.output_notebook()[source]; Configure the Bokeh output state to generate output in notebook; cells when bokeh.io.show() is called. Calls; bokeh.io.output_notebook(). hail.plot.visualize_missingness(entry_field, row_field=None, column_field=None, window=6000000, plot_width=1800, plot_height=900)[source]; Visualize missingness in a MatrixTable.; Inspired by naniar.; Row field is windowed by default, and missingness is aggregated over this window to generate a proportion defined.; This windowing is set to 6,000,000 by default, so that the human genome is divided into ~500 rows.; With ~2,000 columns, this function returns a sensibly-sized plot with this windowing. Warning; Generating ",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:7072,Deployability,continuous,continuous,7072,"plot.scatter(x, y, label=None, title=None, xlabel=None, ylabel=None, size=4, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create an interactive scatter plot.; x and y must both be either:; - a NumericExpression from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be ",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:8027,Deployability,continuous,continuous,8027," and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool, optional) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to down",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:10279,Deployability,continuous,continuous,10279,"ategorical label. Returns:; bokeh.models.Plot if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.qq(pvals, label=None, title='Q-Q plot', xlabel='Expected -log10(p)', ylabel='Observed -log10(p)', size=6, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot); If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive qq plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. pvals (NumericExpression) – List of x-values to be plotted.; label (Expression or Dict[str, Expression]]) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optio",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:11115,Deployability,continuous,continuous,11115,"e is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. pvals (NumericExpression) – List of x-values to be plotted.; label (Expression or Dict[str, Expression]]) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to downsample (de",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:14844,Deployability,update,updated,14844,"ields (Dict[str, Expression], optional) – Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all (bool, optional) – Deprecated - use n_divisions instead.; n_divisions (int, optional.) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; significance_line (float, optional) – p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If None, no line is added. Returns:; bokeh.models.Plot. hail.plot.output_notebook()[source]; Configure the Bokeh output state to generate output in notebook; cells when bokeh.io.show() is called. Calls; bokeh.io.output_notebook(). hail.plot.visualize_missingness(entry_field, row_field=None, column_field=None, window=6000000, plot_width=1800, plot_height=900)[source]; Visualize missingness in a MatrixTable.; Inspired by naniar.; Row field is windowed by default, and missingness is aggregated over this window to generate a proportion defined.; This windowing is set to 6,000,000 by default, so that the human genome is divided into ~500 rows.; With ~2,000 columns, this function returns a sensibly-sized plot with this windowing. Warning; Generating a plot with more than ~1M points takes a long time for Bokeh to render. Consider windowing carefully. Parameters:. entry_field (Expression) – Field for which to check missingness.; row_field (NumericExpression or LocusExpression) – Row field to use for y-axis (can be windowed). If not provided, the row key will be used.; column_field (StringExpression) – Column field to use for x-axis. If not provided, the column key will be used.; window (int, optional) – Size of window to summarize by row_field. If set to None, each field will be used individually.; plot_width (int) – Plot width in px.; plot_height (int) – Plot height in px. Returns:; bokeh.plotting.figure. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:1018,Modifiability,extend,extend,1018,"﻿. Hail | ; Plot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plot. View page source. Plot. Warning; Plotting functionality is in early stages and is experimental. Interfaces will change regularly. Plotting in Hail is easy. Hail’s plot functions utilize Bokeh plotting libraries to create attractive,; interactive figures. Plotting functions in this module return a Bokeh Figure, so you can call; a method to plot your data and then choose to extend the plot however you like by interacting; directly with Bokeh. See the GWAS tutorial for examples.; Plot functions in Hail accept data in the form of either Python objects or Table and MatrixTable fields. cdf; Create a cumulative density plot. pdf. smoothed_pdf; Create a density plot. histogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; leg",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:4587,Modifiability,variab,variable,4587,"og10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.histogram2d(x, y, bins=40, range=None, title=None, width=600, height=600, colors=('#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'), log=False)[source]; Plot a two-dimensional histogram.; x and y must both be a NumericExpression from the same Table.; If x_range or y_range are not provided, the function will do a pass through the data to determine; min and max of each variable.; Examples; >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters:. x (NumericExpression) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range (None or ((float, float), (float, float))) – The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histo",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:1810,Testability,log,log,1810,"ot functions utilize Bokeh plotting libraries to create attractive,; interactive figures. Plotting functions in this module return a Bokeh Figure, so you can call; a method to plot your data and then choose to extend the plot however you like by interacting; directly with Bokeh. See the GWAS tutorial for examples.; Plot functions in Hail accept data in the form of either Python objects or Table and MatrixTable fields. cdf; Create a cumulative density plot. pdf. smoothed_pdf; Create a density plot. histogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:2159,Testability,log,log,2159," objects or Table and MatrixTable fields. cdf; Create a cumulative density plot. pdf. smoothed_pdf; Create a density plot. histogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the ",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:2216,Testability,log,log,2216," objects or Table and MatrixTable fields. cdf; Create a cumulative density plot. pdf. smoothed_pdf; Create a density plot. histogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the ",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:2321,Testability,log,log,2321,"istogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to pl",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:2437,Testability,log,log,2437," interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:2747,Testability,log,log,2747,"legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or F",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:3081,Testability,log,log,3081,".; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalize",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:3508,Testability,log,log,3508," (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.histogram2d(x, y, bins=40, range=None, title=None, width=600, height=600, colors=('#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'), log=False)[source]; Plot a two-dimensional histogram.; x and y must both be a NumericExpression from the same Table.; If x_range or y_range are not provided, the function will do a",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:3687,Testability,log,log,3687,"d (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.histogram2d(x, y, bins=40, range=None, title=None, width=600, height=600, colors=('#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'), log=False)[source]; Plot a two-dimensional histogram.; x and y must both be a NumericExpression from the same Table.; If x_range or y_range are not provided, the function will do a pass through the data to determine; min and max of each variable.; Examples; >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_n",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:4084,Testability,log,log,4084,"urce]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.histogram2d(x, y, bins=40, range=None, title=None, width=600, height=600, colors=('#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'), log=False)[source]; Plot a two-dimensional histogram.; x and y must both be a NumericExpression from the same Table.; If x_range or y_range are not provided, the function will do a pass through the data to determine; min and max of each variable.; Examples; >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters:. x (NumericExpression) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specifi",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:4141,Testability,log,log,4141,"urce]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.histogram2d(x, y, bins=40, range=None, title=None, width=600, height=600, colors=('#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'), log=False)[source]; Plot a two-dimensional histogram.; x and y must both be a NumericExpression from the same Table.; If x_range or y_range are not provided, the function will do a pass through the data to determine; min and max of each variable.; Examples; >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters:. x (NumericExpression) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specifi",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:4349,Testability,log,log,4349,"ce of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.histogram2d(x, y, bins=40, range=None, title=None, width=600, height=600, colors=('#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'), log=False)[source]; Plot a two-dimensional histogram.; x and y must both be a NumericExpression from the same Table.; If x_range or y_range are not provided, the function will do a pass through the data to determine; min and max of each variable.; Examples; >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters:. x (NumericExpression) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; rang",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/plot.html:5916,Testability,log,log,5916,"ession) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range (None or ((float, float), (float, float))) – The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width (int) – Plot width (default 600px).; height (int) – Plot height (default 600px).; title (str) – Title of the plot.; colors (Sequence[str]) – List of colors (hex codes, or strings as described; here). Compatible with one of the many; built-in palettes available here.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.scatter(x, y, label=None, title=None, xlabel=None, ylabel=None, size=4, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create an interactive scatter plot.; x and y must both be either:; - a NumericExpression from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; m",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
https://hail.is/docs/0.2/root_api.html:749,Deployability,update,updated,749,"﻿. Hail | ; Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API. View page source. Python API; This is the Python API documentation for all Hail Python libraries including Query (hail), a cloud-agnostic; file system implementation (hailtop.fs), and Batch (hailtop.batch). hail; hailtop.fs; hailtop.batch. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/root_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/root_api.html
https://hail.is/docs/0.2/scans.html:765,Deployability,rolling,rolling,765,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,docs/0.2/scans.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/scans.html
https://hail.is/docs/0.2/scans.html:3063,Deployability,update,updated,3063,"ator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_non_ref)); >>> ds_scan.rows().show(); +---------------+------------+-----------+---------------+; | locus | alleles | n_non_ref | cum_n_non_ref |; +---------------+------------+-----------+---------------+; | locus<GRCh37> | array<str> | int64 | int64 |; +---------------+------------+-----------+---------------+; | 20:10579373 | [""C"",""T""] | 1 | 0 |; | 20:10579398 | [""C"",""T""] | 1 | 1 |; | 20:10627772 | [""C"",""T""] | 2 | 2 |; | 20:10633237 | [""G"",""A""] | 69 | 4 |; | 20:10636995 | [""C"",""T""] | 2 | 73 |; | 20:10639222 | [""G"",""A""] | 22 | 75 |; | 20:13763601 | [""A"",""G""] | 2 | 97 |; | 20:16223922 | [""T"",""C""] | 66 | 99 |; | 20:17479617 | [""G"",""A""] | 9 | 165 |; +---------------+------------+-----------+---------------+. Scans over column fields can be done in a similar manner. Danger; Computing the result of certain aggregators, such as; hardy_weinberg_test(), can be very expensive when done; for every row in a scan.”. See the Aggregators module for documentation on the behavior; of specific aggregators. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/scans.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/scans.html
https://hail.is/docs/0.2/scans.html:757,Performance,perform,perform,757,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,docs/0.2/scans.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/scans.html
https://hail.is/docs/0.2/scans.html:689,Security,expose,exposed,689,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,docs/0.2/scans.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/scans.html
https://hail.is/docs/0.2/search.html:516,Deployability,update,updated,516,"﻿. Hail | ; Search. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Search. Please activate JavaScript to enable the search functionality.; . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/search.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/search.html
https://hail.is/docs/0.2/tutorials-landing.html:835,Availability,down,downloading,835,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html
https://hail.is/docs/0.2/tutorials-landing.html:900,Deployability,install,install,900,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html
https://hail.is/docs/0.2/tutorials-landing.html:1228,Deployability,update,updated,1228,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html
https://hail.is/docs/0.2/tutorials-landing.html:680,Testability,test,test,680,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html
https://hail.is/docs/0.2/types.html:11849,Deployability,update,updated,11849,"ct type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique.; Structs are very common in Hail. Each component of a Table and MatrixTable; is a struct:. Table.row(); Table.globals(); MatrixTable.row(); MatrixTable.col(); MatrixTable.entry(); MatrixTable.globals(). Structs appear below the top-level component types as well. Consider the following join:; >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to table1 called table2_fields. In the new table,; table2_fields will be a struct containing all the non-key fields from table2. Parameters:; field_types (keyword args of HailType) – Fields. See also; StructExpression, Struct. class hail.expr.types.ttuple(*types)[source]; Hail type for tuples.; In Python, these are represented as tuple. Parameters:; types (varargs of HailType) – Element types. See also; TupleExpression. hail.expr.types.tcall = dtype('call'); Hail type for a diploid genotype.; In Python, these are represented by Call. See also; CallExpression, Call, call(), parse_call(), unphased_diploid_gt_index_call(). class hail.expr.types.tlocus(reference_genome='default')[source]; Hail type for a genomic coordinate with a contig and a position.; In Python, these are represented by Locus. Parameters:; reference_genome (ReferenceGenome or str) – Reference genome to use. See also; LocusExpression, locus(), parse_locus(), Locus. reference_genome; Reference genome. Returns:; ReferenceGenome – Reference genome. class hail.expr.types.tinterval(point_type)[source]; Hail type for intervals of ordered values.; In Python, these are represented by Interval. Parameters:; point_type (HailType) – Interval point type. See also; IntervalExpression, Interval, interval(), parse_locus_interval(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:8651,Integrability,interface,interface,8651,"bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType)",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:2225,Modifiability,variab,variable-length,2225,"e numbers, strings, or; numpy arrays.; Types are very important in Hail, because the fields of Table and; MatrixTable objects have data types. Primitive types; Hail’s primitive data types for boolean, numeric and string objects are:. tint; Alias for tint32. tint32; Hail type for signed 32-bit integers. tint64; Hail type for signed 64-bit integers. tfloat; Alias for tfloat64. tfloat32; Hail type for 32-bit floating point numbers. tfloat64; Hail type for 64-bit floating point numbers. tstr; Hail type for text strings. tbool; Hail type for Boolean (True or False) values. Container types; Hail’s container types are:. tarray - Ordered collection of homogenous objects.; tndarray - Ordered n-dimensional arrays of homogenous objects.; tset - Unordered collection of distinct homogenous objects.; tdict - Key-value map. Keys and values are both homogenous.; ttuple - Tuple of heterogeneous values.; tstruct - Structure containing named fields, each with its own; type. tarray; Hail type for variable-length arrays of elements. tndarray; Hail type for n-dimensional arrays. tset; Hail type for collections of distinct elements. tdict; Hail type for key-value maps. ttuple; Hail type for tuples. tinterval; Hail type for intervals of ordered values. tstruct; Hail type for structured groups of heterogeneous fields. Genetics types; Hail has two genetics-specific types:. tlocus; Hail type for a genomic coordinate with a contig and a position. tcall; Hail type for a diploid genotype. When to work with types; In general, you won’t need to mention types explicitly.; There are a few situations where you may want to specify types explicitly:. To specify column types in import_table() if the impute flag does not; infer the type you want.; When converting a Python value to a Hail expression with literal(),; if you don’t wish to rely on the inferred type.; With functions like missing() and empty_array(). Viewing an object’s type; Hail objects have a dtype field that will print their type.; >>",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:5552,Modifiability,variab,variable,5552,"xamples; >>> hl.dtype('int'); dtype('int32'). >>> hl.dtype('float'); dtype('float64'). >>> hl.dtype('array<int32>'); dtype('array<int32>'). >>> hl.dtype('dict<str, bool>'); dtype('dict<str, bool>'). >>> hl.dtype('struct{a: int32, `field with spaces`: int64}'); dtype('struct{a: int32, `field with spaces`: int64}'). Notes; This function is able to reverse str(t) on a HailType.; The grammar is defined as follows:; type = _ ( array / bool / call / dict / interval / int64 / int32 / float32 / float64 / locus / ndarray / rng_state / set / stream / struct / str / tuple / union / void / variable ) _; variable = ""?"" simple_identifier ("":"" simple_identifier)?; void = ""void"" / ""tvoid""; int64 = ""int64"" / ""tint64""; int32 = ""int32"" / ""tint32"" / ""int"" / ""tint""; float32 = ""float32"" / ""tfloat32""; float64 = ""float64"" / ""tfloat64"" / ""tfloat"" / ""float""; bool = ""tbool"" / ""bool""; call = ""tcall"" / ""call""; str = ""tstr"" / ""str""; locus = (""tlocus"" / ""locus"") _ ""<"" identifier "">""; array = (""tarray"" / ""array"") _ ""<"" type "">""; ndarray = (""tndarray"" / ""ndarray"") _ ""<"" type "","" nat "">""; set = (""tset"" / ""set"") _ ""<"" type "">""; stream = (""tstream"" / ""stream"") _ ""<"" type "">""; dict = (""tdict"" / ""dict"") _ ""<"" type "","" type "">""; struct = (""tstruct"" / ""struct"") _ ""{"" (fields / _) ""}""; union = (""tunion"" / ""union"") _ ""{"" (fields / _) ""}""; tuple = (""ttuple"" / ""tuple"") _ ""("" ((type ("","" type)*) / _) "")""; fields = field ("","" field)*; field = identifier "":"" type; interval = (""tinterval"" / ""interval"") _ ""<"" type "">""; identifier = _ (simple_identifier / escaped_identifier) _; simple_identifier = ~r""\w+""; escaped_identifier = ~""`([^`\\\\]|\\\\.)*`""; nat = _ (nat_literal / nat_variable) _; nat_literal = ~""[0-9]+""; nat_variable = ""?nat""; rng_state = ""rng_state""; _ = ~r""\s*"". Parameters:; type_str (str) – String representation of type. Returns:; HailType. hail.expr.types.tint = dtype('int32'); Alias for tint32. hail.expr.types.tint32 = dtype('int32'); Hail type for signed 32-bit integers.; Their values can range fr",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:5566,Modifiability,variab,variable,5566,"xamples; >>> hl.dtype('int'); dtype('int32'). >>> hl.dtype('float'); dtype('float64'). >>> hl.dtype('array<int32>'); dtype('array<int32>'). >>> hl.dtype('dict<str, bool>'); dtype('dict<str, bool>'). >>> hl.dtype('struct{a: int32, `field with spaces`: int64}'); dtype('struct{a: int32, `field with spaces`: int64}'). Notes; This function is able to reverse str(t) on a HailType.; The grammar is defined as follows:; type = _ ( array / bool / call / dict / interval / int64 / int32 / float32 / float64 / locus / ndarray / rng_state / set / stream / struct / str / tuple / union / void / variable ) _; variable = ""?"" simple_identifier ("":"" simple_identifier)?; void = ""void"" / ""tvoid""; int64 = ""int64"" / ""tint64""; int32 = ""int32"" / ""tint32"" / ""int"" / ""tint""; float32 = ""float32"" / ""tfloat32""; float64 = ""float64"" / ""tfloat64"" / ""tfloat"" / ""float""; bool = ""tbool"" / ""bool""; call = ""tcall"" / ""call""; str = ""tstr"" / ""str""; locus = (""tlocus"" / ""locus"") _ ""<"" identifier "">""; array = (""tarray"" / ""array"") _ ""<"" type "">""; ndarray = (""tndarray"" / ""ndarray"") _ ""<"" type "","" nat "">""; set = (""tset"" / ""set"") _ ""<"" type "">""; stream = (""tstream"" / ""stream"") _ ""<"" type "">""; dict = (""tdict"" / ""dict"") _ ""<"" type "","" type "">""; struct = (""tstruct"" / ""struct"") _ ""{"" (fields / _) ""}""; union = (""tunion"" / ""union"") _ ""{"" (fields / _) ""}""; tuple = (""ttuple"" / ""tuple"") _ ""("" ((type ("","" type)*) / _) "")""; fields = field ("","" field)*; field = identifier "":"" type; interval = (""tinterval"" / ""interval"") _ ""<"" type "">""; identifier = _ (simple_identifier / escaped_identifier) _; simple_identifier = ~r""\w+""; escaped_identifier = ~""`([^`\\\\]|\\\\.)*`""; nat = _ (nat_literal / nat_variable) _; nat_literal = ~""[0-9]+""; nat_variable = ""?nat""; rng_state = ""rng_state""; _ = ~r""\s*"". Parameters:; type_str (str) – String representation of type. Returns:; HailType. hail.expr.types.tint = dtype('int32'); Alias for tint32. hail.expr.types.tint32 = dtype('int32'); Hail type for signed 32-bit integers.; Their values can range fr",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:8137,Modifiability,variab,variable-length,8137,"tint64 = dtype('int64'); Hail type for signed 64-bit integers.; Their values can range from \(-2^{63}\) to \(2^{63} - 1\).; In Python, these are represented as int. See also; Int64Expression, int64(). hail.expr.types.tfloat = dtype('float64'); Alias for tfloat64. hail.expr.types.tfloat32 = dtype('float32'); Hail type for 32-bit floating point numbers.; In Python, these are represented as float. See also; Float32Expression, float64(). hail.expr.types.tfloat64 = dtype('float64'); Hail type for 64-bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets conta",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:8275,Modifiability,parameteriz,parameterized,8275,"Python, these are represented as int. See also; Int64Expression, int64(). hail.expr.types.tfloat = dtype('float64'); Alias for tfloat64. hail.expr.types.tfloat32 = dtype('float32'); Hail type for 32-bit floating point numbers.; In Python, these are represented as float. See also; Float32Expression, float64(). hail.expr.types.tfloat64 = dtype('float64'); Hail type for 64-bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:8802,Modifiability,parameteriz,parameterized,8802,"'str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType) – Value type. See also; DictExpression, dict(), Collection functions. class hail.expr.types.tstruct(**field_types)[source]; Hail type for structured ",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:9171,Modifiability,parameteriz,parameterized,9171,"ys of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType) – Value type. See also; DictExpression, dict(), Collection functions. class hail.expr.types.tstruct(**field_types)[source]; Hail type for structured groups of heterogeneous fields.; In Python, these are represented as Struct.; Hail’s tstruct type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique.; Structs are very common in Hail. Each component of a Table and Mat",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:9486,Modifiability,parameteriz,parameterize,9486," for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType) – Value type. See also; DictExpression, dict(), Collection functions. class hail.expr.types.tstruct(**field_types)[source]; Hail type for structured groups of heterogeneous fields.; In Python, these are represented as Struct.; Hail’s tstruct type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique.; Structs are very common in Hail. Each component of a Table and MatrixTable; is a struct:. Table.row(); Table.globals(); MatrixTable.row(); MatrixTable.col(); MatrixTable.entry(); MatrixTable.globals(). Structs appear below the top-level component types as well. Consider the following join:; >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to table1 called table2_fields. In ",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/0.2/types.html:8604,Testability,test,tested,8604,"bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType)",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
https://hail.is/docs/batch/advanced_search_help.html:2375,Availability,failure,failure,2375,"tches.; Example: name = pca_pipeline; Example: name =~ pca. Predefined Keyword Expression; The left hand side of the statement is a special Batch-specific keyword which can be one of the values; listed in the tables below. Allowed operators are dependent on the type of the value expected for each; keyword, but can be one of =, ==, !=, >, >=, <, <=, =~, !~.; The right hand side is the value to search against. Keywords. Keyword; Value Type; Allowed Operators; Extra. cost; float; =, ==, !=, >, >=, <, <=. duration; float; =, ==, !=, >, >=, <, <=; Values are rounded to the millisecond. start_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. end_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. Example: cost >= 1.00; Example: duration > 5; Example: start_time >= 2023-02-24T17:15:25Z. Keywords specific to searching for batches. Keyword; Value Type; Allowed Operators; Extra. batch_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are running, complete, success, failure, cancelled, open, closed. user; str; =, ==, !=, =~, !~. billing_project; str; =, ==, !=, =~, !~. Example: state = running; Example: user = johndoe; Example: billing_project = johndoe-trial. Keywords specific to searching for jobs in a batch. Keyword; Value Type; Allowed Operators; Extra. job_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are pending, ready, creating, running, live, cancelled, error, failed, bad, success, done. instance; str; =, ==, !=, =~, !~; use this to search for all jobs that ran on a given worker. instance_collection; str; =, ==, !=, =~, !~; use this to search for all jobs in a given pool. Example: user = johndoe; Example: billing_project = johndoe-trial; Example: instance_collection = standard. Combining Multiple Statements; Example: Searching for batches in a time window; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-07-01T12:35:00Z. Example: Searching for batches that have run since June 2023 that cos",MatchSource.WIKI,docs/batch/advanced_search_help.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/advanced_search_help.html
https://hail.is/docs/batch/advanced_search_help.html:2805,Availability,error,error,2805,"search against. Keywords. Keyword; Value Type; Allowed Operators; Extra. cost; float; =, ==, !=, >, >=, <, <=. duration; float; =, ==, !=, >, >=, <, <=; Values are rounded to the millisecond. start_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. end_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. Example: cost >= 1.00; Example: duration > 5; Example: start_time >= 2023-02-24T17:15:25Z. Keywords specific to searching for batches. Keyword; Value Type; Allowed Operators; Extra. batch_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are running, complete, success, failure, cancelled, open, closed. user; str; =, ==, !=, =~, !~. billing_project; str; =, ==, !=, =~, !~. Example: state = running; Example: user = johndoe; Example: billing_project = johndoe-trial. Keywords specific to searching for jobs in a batch. Keyword; Value Type; Allowed Operators; Extra. job_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are pending, ready, creating, running, live, cancelled, error, failed, bad, success, done. instance; str; =, ==, !=, =~, !~; use this to search for all jobs that ran on a given worker. instance_collection; str; =, ==, !=, =~, !~; use this to search for all jobs in a given pool. Example: user = johndoe; Example: billing_project = johndoe-trial; Example: instance_collection = standard. Combining Multiple Statements; Example: Searching for batches in a time window; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-07-01T12:35:00Z. Example: Searching for batches that have run since June 2023 that cost more than $5 submitted by a given user; start_time >= 2023-06-01; cost > 5.00; user = johndoe. Example: Searching for failed batches where the batch name contains pca; state = failed; name =~ pca. Example: Searching for jobs within a given range of ids; job_id >= 1000; job_id < 2000. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/advanced_search_help.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/advanced_search_help.html
https://hail.is/docs/batch/advanced_search_help.html:1602,Integrability,depend,dependent,1602,"artial match, keyword, or predefined keyword) listed below. When the query is run, each; statement will be joined to the next with the AND operator. Exact Match Expression; A single word enclosed with double quotes that is an exact match for either the name or; value of an attribute.; Example: ""pca_pipeline"". Partial Match Expression; A single word without any quotes that is a partial match for either the name or the value; of an attribute.; Example: pipe. Keyword Expression; The left hand side of the statement is the name of the attribute and the right hand side; is the value to search against. Allowed operators are =, ==, !=, =~, and; !~ where the operators with tildes are looking for partial matches.; Example: name = pca_pipeline; Example: name =~ pca. Predefined Keyword Expression; The left hand side of the statement is a special Batch-specific keyword which can be one of the values; listed in the tables below. Allowed operators are dependent on the type of the value expected for each; keyword, but can be one of =, ==, !=, >, >=, <, <=, =~, !~.; The right hand side is the value to search against. Keywords. Keyword; Value Type; Allowed Operators; Extra. cost; float; =, ==, !=, >, >=, <, <=. duration; float; =, ==, !=, >, >=, <, <=; Values are rounded to the millisecond. start_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. end_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. Example: cost >= 1.00; Example: duration > 5; Example: start_time >= 2023-02-24T17:15:25Z. Keywords specific to searching for batches. Keyword; Value Type; Allowed Operators; Extra. batch_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are running, complete, success, failure, cancelled, open, closed. user; str; =, ==, !=, =~, !~. billing_project; str; =, ==, !=, =~, !~. Example: state = running; Example: user = johndoe; Example: billing_project = johndoe-trial. Keywords specific to searching for jobs in a batch. Keyword; Value Type; ",MatchSource.WIKI,docs/batch/advanced_search_help.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/advanced_search_help.html
https://hail.is/docs/batch/api.html:3766,Deployability,install,installed,3766,"of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.build_python_image; Build a new Python image with dill and the specified pip packages installed. utils.concatenate; Concatenate files using tree aggregation. utils.plink_merge; Merge binary PLINK files using tree aggregation. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
https://hail.is/docs/batch/api.html:774,Integrability,interface,interface,774,". Python API — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob. Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture. Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend. Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API. View page source. Python API; This is the API documentation for Batch, and provides detailed information; on the Python programming interface.; Use import hailtop.batch to access this functionality. Batches; A Batch is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represent",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
https://hail.is/docs/batch/api.html:929,Integrability,depend,dependencies,929,". Python API — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob. Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture. Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend. Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API. View page source. Python API; This is the API documentation for Batch, and provides detailed information; on the Python programming interface.; Use import hailtop.batch to access this functionality. Batches; A Batch is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represent",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
https://hail.is/docs/batch/api.html:1946,Integrability,depend,dependency,1946," order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cl",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
https://hail.is/docs/batch/api.html:2687,Integrability,interface,interface,2687,"esourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
https://hail.is/docs/batch/api.html:2731,Performance,concurren,concurrent,2731,"esourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
https://hail.is/docs/batch/api.html:814,Security,access,access,814,". Python API — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob. Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture. Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend. Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API. View page source. Python API; This is the API documentation for Batch, and provides detailed information; on the Python programming interface.; Use import hailtop.batch to access this functionality. Batches; A Batch is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represent",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
https://hail.is/docs/batch/api.html:3312,Security,access,access,3312,"of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.build_python_image; Build a new Python image with dill and the specified pip packages installed. utils.concatenate; Concatenate files using tree aggregation. utils.plink_merge; Merge binary PLINK files using tree aggregation. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
https://hail.is/docs/batch/change_log.html:1849,Availability,error,error,1849,"d_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle argume",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:3124,Availability,error,error,3124,"17. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:3711,Availability,error,errors,3711,"rencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Larg",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:3795,Availability,error,erroring,3795,"rencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Larg",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:4219,Availability,avail,available,4219,"obs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the Service",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:4731,Availability,error,errors,4731," now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_f",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:5382,Availability,error,error,5382,"nd has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBack",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:5451,Availability,error,error,5451,"nd has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBack",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:6728,Availability,error,error,6728," the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fixed the documentation for job memory and storage requests to have default units in bytes. Previous. © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:6768,Availability,error,errors,6768," the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fixed the documentation for job memory and storage requests to have default units in bytes. Previous. © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:504,Deployability,release,released,504,". Python Version Compatibility Policy — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:621,Deployability,release,released,621,". Python Version Compatibility Policy — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:1539,Deployability,configurat,configuration,1539,"n released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:1871,Deployability,pipeline,pipeline,1871,"d_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle argume",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:2826,Deployability,release,release,2826,"ide a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called functio",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:3465,Deployability,update,update,3465,"fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where larg",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:6224,Energy Efficiency,allocate,allocated,6224," Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fi",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:6734,Integrability,message,message,6734," the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fixed the documentation for job memory and storage requests to have default units in bytes. Previous. © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:1539,Modifiability,config,configuration,1539,"n released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:4082,Modifiability,config,configurable,4082,"iately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:4626,Modifiability,variab,variables,4626,"d Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the proje",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:1173,Performance,queue,queued,1173,"on Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memo",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:4800,Safety,avoid,avoid,4800,"g only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:5012,Safety,timeout,timeout,5012,"ersion 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:4272,Security,authoriz,authorization,4272,"king directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the Lo",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/change_log.html:1494,Testability,log,login,1494,"n released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
https://hail.is/docs/batch/docker_resources.html:1809,Availability,down,download,1809," focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Docke",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/docker_resources.html:1893,Availability,down,download,1893,"following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current w",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/docker_resources.html:889,Deployability,install,install,889,". Docker Resources — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Docker Resources. View page source. Docker Resources. What is Docker?; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker i",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/docker_resources.html:1325,Deployability,install,installed,1325,"on Compatibility Policy; Change Log. Batch. Docker Resources. View page source. Docker Resources. What is Docker?; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/docker_resources.html:1503,Deployability,install,installed,1503,"to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For mor",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/docker_resources.html:1708,Deployability,install,installing,1708,"nd a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/docker_resources.html:1906,Deployability,install,install,1906,"following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current w",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/docker_resources.html:2066,Deployability,update,update,2066," of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current working directory,; * `-t <name>` specifies the image name, and; * `-f <dockerfile>` specifies the Dockerfile file.; * A more complete description may be found `here: <https://docs.docker.com/engine/reference/com",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/docker_resources.html:2084,Deployability,install,install,2084," of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current working directory,; * `-t <name>` specifies the image name, and; * `-f <dockerfile>` specifies the Dockerfile file.; * A more complete description may be found `here: <https://docs.docker.com/engine/reference/com",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
https://hail.is/docs/batch/genindex.html:4391,Safety,timeout,timeout,4391,"iltop.batch.job). JobResourceFile (class in hailtop.batch.resource). L. LocalBackend (class in hailtop.batch.backend). M. map() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). memory() (hailtop.batch.job.Job method). N. new_bash_job() (hailtop.batch.batch.Batch method). new_job() (hailtop.batch.batch.Batch method). new_python_job() (hailtop.batch.batch.Batch method). P. plink_merge() (in module hailtop.batch.utils). PythonJob (class in hailtop.batch.job). PythonResult (class in hailtop.batch.resource). R. read_input() (hailtop.batch.batch.Batch method). read_input_group() (hailtop.batch.batch.Batch method). regions() (hailtop.batch.job.Job method). requester_pays_fs() (hailtop.batch.backend.Backend method). Resource (class in hailtop.batch.resource). ResourceFile (class in hailtop.batch.resource). ResourceGroup (class in hailtop.batch.resource). result() (hailtop.batch.batch_pool_executor.BatchPoolFuture method). run() (hailtop.batch.batch.Batch method). running() (hailtop.batch.batch_pool_executor.BatchPoolFuture method). RunningBatchType (class in hailtop.batch.backend). S. select_jobs() (hailtop.batch.batch.Batch method). ServiceBackend (class in hailtop.batch.backend). shutdown() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). source() (hailtop.batch.resource.InputResourceFile method). (hailtop.batch.resource.JobResourceFile method). (hailtop.batch.resource.PythonResult method). (hailtop.batch.resource.Resource method). (hailtop.batch.resource.ResourceGroup method). spot() (hailtop.batch.job.Job method). storage() (hailtop.batch.job.Job method). submit() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). supported_regions() (hailtop.batch.backend.ServiceBackend static method). T. timeout() (hailtop.batch.job.Job method). V. validate_file() (hailtop.batch.backend.Backend method). W. write_output() (hailtop.batch.batch.Batch method). © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/genindex.html
https://hail.is/docs/batch/getting_started.html:456,Availability,avail,available,456,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
https://hail.is/docs/batch/getting_started.html:1218,Availability,echo,echo,1218,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
https://hail.is/docs/batch/getting_started.html:683,Deployability,install,install,683,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
https://hail.is/docs/batch/getting_started.html:861,Deployability,install,install,861,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
https://hail.is/docs/batch/getting_started.html:1011,Deployability,install,install,1011,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
https://hail.is/docs/batch/getting_started.html:555,Integrability,depend,depends,555,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
https://hail.is/docs/batch/index.html:505,Deployability,pipeline,pipelines,505,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch. View page source. Batch; Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the Batch Service. Contents. Getting Started; Installation. Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Clumping GWAS Results; Random Forest. Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Exact Match Expression; Partial Match Expression; Keyword Expression; Predefined Keyword Expression; Combining Multiple Statements. Python Version Compatibility Policy; Change Log. Indices and tables. Index; Search Page. Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/index.html
https://hail.is/docs/batch/index.html:544,Integrability,depend,dependencies,544,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch. View page source. Batch; Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the Batch Service. Contents. Getting Started; Installation. Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Clumping GWAS Results; Random Forest. Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Exact Match Expression; Partial Match Expression; Keyword Expression; Predefined Keyword Expression; Combining Multiple Statements. Python Version Compatibility Policy; Change Log. Indices and tables. Index; Search Page. Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/index.html
https://hail.is/docs/batch/service.html:509,Availability,avail,available,509,"﻿. Batch Service — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Loca",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:1241,Availability,avail,available,1241,"he UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have b",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:2109,Availability,down,downloads,2109,"obs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and B",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:2288,Availability,down,downloaded,2288,"; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:2625,Availability,down,downstream,2625,"ect is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:2757,Availability,down,download,2757,"create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Do",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:3589,Availability,avail,available,3589,"either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. Compute cost. Caution; The prices shown below are approximate prices based on us-central1. Actual prices are; based on the curren",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8483,Availability,error,error,8483,"the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tm",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:9393,Availability,echo,echo,9393,"ld see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:12302,Availability,error,error,12302,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:12621,Availability,error,error,12621,"ur images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:13156,Availability,failure,failure,13156,"code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Bat",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:13389,Availability,failure,failure,13389,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:13428,Availability,failure,failure,13428,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:676,Deployability,install,installed,676,"﻿. Batch Service — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Loca",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:6890,Deployability,configurat,configuration,6890,"ore/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:7893,Deployability,install,installed,7893," two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor;",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:10884,Deployability,configurat,configuration,10884,"bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possi",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:1048,Energy Efficiency,schedul,scheduler,1048,"tch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:6250,Energy Efficiency,schedul,scheduled,6250,"Local SSD; Average number of days per month = 365.25 / 12 = 30.4375. Cost per GB per month = $0.048. Cost per core per hour = $0.048 * 375 / 30.4375 / 24 / 16. = $0.001685 per core per hour. Storage; Average number of days per month = 365.25 / 12 = 30.4375. Cost per GB per month = $0.17. Cost per GB per hour = $0.17 / 30.4375 / 24. IP network cost= $0.0003125 per core per hour for nonpreemptible worker types; = $0.00015625 per core per hour for spot worker types. Service cost= $0.01 per core per hour. Logs, Specs, and Firewall Fee= $0.005 per core per hour. The sum of these costs is $0.02684125 per core/hour for standard spot workers, $0.02929425 per core/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with th",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:6949,Energy Efficiency,power,power,6949,"ore/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:7801,Energy Efficiency,schedul,scheduled,7801,"se, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object wit",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:10151,Energy Efficiency,charge,charges,10151,"n open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Pla",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:10178,Energy Efficiency,charge,charges,10178,"ackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:11322,Energy Efficiency,charge,charges,11322,"ning example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker contain",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:11613,Energy Efficiency,charge,charges,11613,"parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where t",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:12094,Energy Efficiency,schedul,scheduled,12094,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:12137,Energy Efficiency,schedul,scheduled,12137,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:14084,Energy Efficiency,charge,charges,14084,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:2269,Integrability,depend,dependent,2269," users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gclo",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:2608,Integrability,depend,dependencies,2608,"ect is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:4136,Integrability,depend,depends,4136," more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. Compute cost. Caution; The prices shown below are approximate prices based on us-central1. Actual prices are; based on the current spot prices for a given worker type and the region in which the worker is running in.; You can use Job.regions() to specify which regions to run a job in. = $0.01 per core per hour for spot standard worker types; = $0.012453 per core per hour for spot highmem worker types; = $0.0074578 per core per hour for spot highcpu worker types; = $0.04749975 per core per hour for nonpreemptible standard worker types; = $0.0591515 per core per hour for nonpreemptible highmem worker types; = $0.0354243 per core per hour for nonpreemptible highcpu worker types. Disk cost; Boot Disk; Average number of days per month = 365.25 / 12 = 30.4",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8412,Integrability,message,message,8412,"dditional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8489,Integrability,message,messages,8489,"the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tm",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:11991,Integrability,depend,dependencies,11991,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:12040,Integrability,depend,dependencies,12040,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:12627,Integrability,message,messages,12627,"ur images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:13461,Integrability,depend,depend,13461,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:6890,Modifiability,config,configuration,6890,"ore/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:9558,Modifiability,config,config,9558,"ice. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:9619,Modifiability,config,config,9619,"ice. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:10789,Modifiability,variab,variable,10789,"hysical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. T",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:10884,Modifiability,config,configuration,10884,"bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possi",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:10898,Modifiability,variab,variable,10898,"bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possi",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:10917,Modifiability,config,config,10917,"bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possi",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8570,Safety,avoid,avoid,8570,"ill be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-proj",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:10164,Safety,avoid,avoid,10164,"ackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:14061,Safety,avoid,avoid,14061,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:1320,Security,access,access,1320," API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; fil",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:1442,Security,authenticat,authenticate,1442,"he Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:2946,Security,access,access,2946,"calization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-p",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:3205,Security,access,access,3205,"either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; The cost for executing a job depends on the underlying machine type, the region in w",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:3872,Security,access,access,3872,"ta; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. Compute cost. Caution; The prices shown below are approximate prices based on us-central1. Actual prices are; based on the current spot prices for a given worker type and the region in which the worker is running in.; You can use Job.regions() to specify which regions to run a job in. = $0.01 per core per hour for spot standard worker types; = $0",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8060,Security,authenticat,authenticate,8060,"than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and exe",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8341,Security,authenticat,authenticate,8341," method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8436,Security,authenticat,authenticated,8436,"dditional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8872,Security,access,access,8872,". Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the loc",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:1327,Testability,log,logs,1327," API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; fil",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:3082,Testability,test,test,3082,"calization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-p",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8193,Testability,log,login,8193,"than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and exe",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8213,Testability,log,login,8213,"than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and exe",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:8262,Testability,log,login,8262," method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:9335,Testability,test,test,9335,"account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/service.html:12389,Testability,log,logs,12389,"ur images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
https://hail.is/docs/batch/tutorial.html:2374,Availability,echo,echo,2374,"e resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:2767,Availability,echo,echo,2767,"k out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:3449,Availability,echo,echo,3449," “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:3518,Availability,echo,echo,3518,"er in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file d",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:3832,Availability,echo,echo,3832,"h a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware tha",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:3901,Availability,echo,echo,3901," and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job object",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:4192,Availability,down,downstream,4192,"e run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:5618,Availability,echo,echo,5618,"fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (s",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:6238,Availability,echo,echo,6238,"ath into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.ne",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:6386,Availability,echo,echo,6386,"epend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:7197,Availability,echo,echo,7197,"matically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(n",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:7294,Availability,echo,echo,7294,"me in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(na",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:7752,Availability,echo,echo,7752,"e Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:8231,Availability,echo,echo,8231,"; >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch ob",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:9046,Availability,echo,echo,9046,"mmand for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:9699,Availability,echo,echo,9699,"shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='nested-scatter-3'); >>> head = b.new_job(name='head'); >>> user_sinks = []; >>> for user in ['Alice', 'Bob', 'Dan']:; ... user_sink = do_chores(b, head, user); ... user_sinks.append(user_sink); >>> final_sink = b.new_job(name='final-sink'); >>> final_sink.depend",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:10250,Availability,echo,echo,10250,"a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='nested-scatter-3'); >>> head = b.new_job(name='head'); >>> user_sinks = []; >>> for user in ['Alice', 'Bob', 'Dan']:; ... user_sink = do_chores(b, head, user); ... user_sinks.append(user_sink); >>> final_sink = b.new_job(name='final-sink'); >>> final_sink.depends_on(*user_sinks); >>> b.run(). Input Files; Previously, we discussed that JobResourceFile are temporary files and; are created from Job objects. However, in order to read a file that; was not generated by executing jobs (input file), we use the method; Batch.read_input() to create an InputResourceFile. An input; resource file can be used exactly in the same way as a; JobResourceFile. We can refer to an input resource file in a command; using an f-string. In the example below, we add the file data/hello.txt as an; input resource file called inpu",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:12401,Availability,down,downstream,12401,"b.read_input('data/hello.txt'); >>> j = b.new_job(name='hello'); >>> j.command(f'cat {input}'); >>> b.run(). Why do we need to explicitly add input files to batches rather than referring; directly to the path in the command? You could refer directly to the path when using the; LocalBackend, but only if you are not specifying a docker image to use when running; the command with BashJob.image(). This is because Batch copies any input files to a special; temporary directory which gets mounted to the Docker container. When using the ServiceBackend,; input files would be files in Google Storage. Many commands do not know how to handle file; paths in Google Storage. Therefore, we suggest explicitly adding all input files as input resource; files to the batch so to make sure the same code can run in all scenarios. Files that are already; in a Docker image do not need to be read as inputs to the batch. Output Files; All files generated by Batch are temporary files! They are copied as appropriate between jobs; for downstream jobs’ use, but will be removed when the batch has completed. In order to save; files generated by a batch for future use, you need to explicitly call Batch.write_output().; The first argument to Batch.write_output() can be any type of ResourceFile which includes input resource; files and job resource files as well as resource groups as described below. The second argument to write_output; should be either a local file path or a google storage file path when using the LocalBackend.; For the ServiceBackend, the second argument must be a google storage file path.; >>> b = hb.Batch(name='hello-input'); >>> j = b.new_job(name='hello'); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Resource Groups; Many bioinformatics tools treat files as a group with a common file; path and specific file extensions. For example, PLINK; stores genetic data in three files: *.bed has the genotype data,; *.bim has the ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:13067,Availability,echo,echo,13067,"xplicitly adding all input files as input resource; files to the batch so to make sure the same code can run in all scenarios. Files that are already; in a Docker image do not need to be read as inputs to the batch. Output Files; All files generated by Batch are temporary files! They are copied as appropriate between jobs; for downstream jobs’ use, but will be removed when the batch has completed. In order to save; files generated by a batch for future use, you need to explicitly call Batch.write_output().; The first argument to Batch.write_output() can be any type of ResourceFile which includes input resource; files and job resource files as well as resource groups as described below. The second argument to write_output; should be either a local file path or a google storage file path when using the LocalBackend.; For the ServiceBackend, the second argument must be a google storage file path.; >>> b = hb.Batch(name='hello-input'); >>> j = b.new_job(name='hello'); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Resource Groups; Many bioinformatics tools treat files as a group with a common file; path and specific file extensions. For example, PLINK; stores genetic data in three files: *.bed has the genotype data,; *.bim has the variant information, and *.fam has the sample information.; PLINK can take as an input the path to the files expecting there will be three; files with the appropriate extensions. It also writes files with a common file root and; specific file extensions including when writing out a new dataset or outputting summary statistics.; To enable Batch to work with file groups, we added a ResourceGroup object; that is essentially a dictionary from file extension name to file path. When creating; a ResourceGroup in a Job (equivalent to a JobResourceFile),; you first need to use the method BashJob.declare_resource_group() to declare the files; in the resource group explicitly before referring t",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:598,Deployability,install,installed,598,". Tutorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states th",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:4118,Deployability,pipeline,pipelines,4118,"e run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:9540,Deployability,pipeline,pipelines,9540,"tter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:1947,Integrability,depend,dependencies,1947,"rrent variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two job",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:1964,Integrability,depend,dependency,1964,"because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:2008,Integrability,depend,dependent,2008,"because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:3335,Integrability,depend,dependencies,3335,"ich we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:3567,Integrability,depend,dependency,3567,"atch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type Jo",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:3658,Integrability,depend,depends,3658,"h and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:4034,Integrability,depend,dependencies,4034,"t, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:4280,Integrability,depend,dependencies,4280,"Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.of",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:5394,Integrability,depend,depend,5394,"d” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:5435,Integrability,depend,dependency,5435,"d” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:5838,Integrability,depend,dependency,5838,"t was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:6635,Integrability,depend,depends,6635,"_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:6813,Integrability,depend,dependent,6813,"reate more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:7412,Integrability,depend,depends,7412,"ated for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:7528,Integrability,depend,dependencies,7528,"ated for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:7583,Integrability,depend,dependencies,7583,"ated for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:8811,Integrability,depend,dependencies,8811,"e all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:9471,Integrability,depend,dependency,9471,"tter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:913,Modifiability,variab,variable,913,". Tutorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states th",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:1027,Modifiability,variab,variable,1027,"utorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:6512,Modifiability,variab,variable,6512,"kend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:7953,Modifiability,inherit,inherits,7953," all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for ",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:17834,Modifiability,variab,variable,17834,"ternate type of job called a PythonJob. Unlike BashJob, PythonJob; does not have a BashJob.command() method and instead have a PythonJob.call() method; that takes a Python function to call and the positional arguments and key-word arguments to provide; to the function. The result of PythonJob.call() is a PythonResult which can be; used as either arguments to another PythonJob or to other BashJob by using one; of the methods to convert a PythonResult to a file: PythonResult.as_str(),; PythonResult.as_repr(), and PythonResult.as_json().; In the example below, we first define two Python functions: hello_world() and upper().; Next, we create a batch and then create a new PythonJob with Batch.new_python_job().; Then we use PythonJob.call() and pass the hello_world function that we want to call.; Notice we just passed the reference to the function and not hello_world(). We also add; a Python string alice as an argument to the function. The result of the j.call() is; a PythonResult which we’ve assigned to the variable hello_str.; We want to use the hello_str result and make all the letters in upper case. We call; PythonJob.call() and pass a reference to the upper function.; But now the argument is hello_str which holds the result from calling hello_world; above. We assign the new output to the variable result.; At this point, we want to write out the transformed hello world result to a text file.; However, result is a PythonResult. Therefore, we need to use the PythonResult.as_str(); to convert result to a JobResourceFile with the string output HELLO WORLD ALICE. Now; we can write the result to a file.; def hello_world(name):; return f'hello {name}'. def upper(s):; return s.upper(). b = hb.Batch(name='hello'); j = b.new_python_job(); hello_str = j.call(hello_world, 'alice'); result = j.call(upper, hello_str); b.write_output(result.as_str(), 'output/hello-alice.txt'); b.run(). Backends; There are two backends that execute batches: the LocalBackend and the; ServiceBackend. T",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:18124,Modifiability,variab,variable,18124,") is a PythonResult which can be; used as either arguments to another PythonJob or to other BashJob by using one; of the methods to convert a PythonResult to a file: PythonResult.as_str(),; PythonResult.as_repr(), and PythonResult.as_json().; In the example below, we first define two Python functions: hello_world() and upper().; Next, we create a batch and then create a new PythonJob with Batch.new_python_job().; Then we use PythonJob.call() and pass the hello_world function that we want to call.; Notice we just passed the reference to the function and not hello_world(). We also add; a Python string alice as an argument to the function. The result of the j.call() is; a PythonResult which we’ve assigned to the variable hello_str.; We want to use the hello_str result and make all the letters in upper case. We call; PythonJob.call() and pass a reference to the upper function.; But now the argument is hello_str which holds the result from calling hello_world; above. We assign the new output to the variable result.; At this point, we want to write out the transformed hello world result to a text file.; However, result is a PythonResult. Therefore, we need to use the PythonResult.as_str(); to convert result to a JobResourceFile with the string output HELLO WORLD ALICE. Now; we can write the result to a file.; def hello_world(name):; return f'hello {name}'. def upper(s):; return s.upper(). b = hb.Batch(name='hello'); j = b.new_python_job(); hello_str = j.call(hello_world, 'alice'); result = j.call(upper, hello_str); b.write_output(result.as_str(), 'output/hello-alice.txt'); b.run(). Backends; There are two backends that execute batches: the LocalBackend and the; ServiceBackend. The local backend is used by default and executes jobs; on your local computer. The service backend executes jobs in a shared compute cluster; managed by the Hail team. To use the Batch Service, follow the directions here. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; them",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:4637,Security,access,accessed,4637,"s_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo """,MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:4710,Security,access,access,4710,"atch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is impl",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/batch/tutorial.html:1744,Usability,learn,learn,1744,"‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('e",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:1736,Energy Efficiency,reduce,reduced,1736,"butes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the ",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:3413,Energy Efficiency,reduce,reduced,3413," num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion SNP.; This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. num_mismatch()[source]¶; Returns the number of mismatched bases in this alternate allele.; Fails if the ref and alt alleles are not the same length. Return type:int. ref¶; Reference allele. Return type:str. stripped_snp()[source]¶; Returns the one-character reduced SNP.; Fails if called on an alternate allele that is not a SNP. Return type:str, str. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:593,Modifiability,polymorphi,polymorphism,593,"﻿. . AltAllele — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; AltAllele. View page source. AltAllele¶. class hail.representation.AltAllele(ref, alt)[source]¶; An object that represents an allele in a polymorphism deviating from the reference allele. Parameters:; ref (str) – reference allele; alt (str) – alternate allele. Attributes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphis",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:1010,Modifiability,polymorphi,polymorphism,1010,"﻿. . AltAllele — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; AltAllele. View page source. AltAllele¶. class hail.representation.AltAllele(ref, alt)[source]¶; An object that represents an allele in a polymorphism deviating from the reference allele. Parameters:; ref (str) – reference allele; alt (str) – alternate allele. Attributes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphis",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:1091,Modifiability,polymorphi,polymorphism,1091,".1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; AltAllele. View page source. AltAllele¶. class hail.representation.AltAllele(ref, alt)[source]¶; An object that represents an allele in a polymorphism deviating from the reference allele. Parameters:; ref (str) – reference allele; alt (str) – alternate allele. Attributes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate ",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:1990,Modifiability,polymorphi,polymorphism,1990," this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion ",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:2100,Modifiability,polymorphi,polymorphism,2100,"single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion SNP.; This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). Thi",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:2841,Modifiability,polymorphi,polymorphism,2841," num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion SNP.; This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. num_mismatch()[source]¶; Returns the number of mismatched bases in this alternate allele.; Fails if the ref and alt alleles are not the same length. Return type:int. ref¶; Reference allele. Return type:str. stripped_snp()[source]¶; Returns the one-character reduced SNP.; Fails if called on an alternate allele that is not a SNP. Return type:str, str. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:3110,Modifiability,polymorphi,polymorphism,3110," num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion SNP.; This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. num_mismatch()[source]¶; Returns the number of mismatched bases in this alternate allele.; Fails if the ref and alt alleles are not the same length. Return type:int. ref¶; Reference allele. Return type:str. stripped_snp()[source]¶; Returns the one-character reduced SNP.; Fails if called on an alternate allele that is not a SNP. Return type:str, str. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
https://hail.is/docs/0.1/representation/hail.representation.Call.html:2783,Modifiability,variab,variables,2783,"; True if the call contains two different alternate alleles. Return type:bool. is_het_ref()[source]¶; True if the call contains one reference and one alternate allele. Return type:bool. is_hom_ref()[source]¶; True if the call is 0/0. Return type:bool. is_hom_var()[source]¶; True if the call contains two identical alternate alleles. Return type:bool. is_not_called()[source]¶; True if the call is missing. Return type:bool. num_alt_alleles()[source]¶; Returns the count of non-reference alleles.; This function returns None if the genotype call is missing. Return type:int or None. one_hot_alleles(num_alleles)[source]¶; Returns a list containing the one-hot encoded representation of the called alleles.; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Call.html
https://hail.is/docs/0.1/representation/hail.representation.Call.html:3536,Modifiability,variab,variables,3536,"ar()[source]¶; True if the call contains two identical alternate alleles. Return type:bool. is_not_called()[source]¶; True if the call is missing. Return type:bool. num_alt_alleles()[source]¶; Returns the count of non-reference alleles.; This function returns None if the genotype call is missing. Return type:int or None. one_hot_alleles(num_alleles)[source]¶; Returns a list containing the one-hot encoded representation of the called alleles.; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the call is missing. Parameters:num_genotypes (int) – number of possible genotypes. Return type:list of int or None. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Call.html
https://hail.is/docs/0.1/representation/hail.representation.Genotype.html:4655,Modifiability,variab,variables,4655,"ol. is_hom_ref()[source]¶; True if the genotype call is 0/0. Return type:bool. is_hom_var()[source]¶; True if the genotype call contains two identical alternate alleles. Return type:bool. is_not_called()[source]¶; True if the genotype call is missing. Return type:bool. num_alt_alleles()[source]¶; Returns the count of non-reference alleles.; This function returns None if the genotype call is missing. Return type:int or None. od()[source]¶; Returns the difference between the total depth and the allelic depth sum.; Equivalent to:; g.dp - sum(g.ad). Return type:int or None. one_hot_alleles(num_alleles)[source]¶; Returns a list containing the one-hot encoded representation of the called alleles.; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotyp",MatchSource.WIKI,docs/0.1/representation/hail.representation.Genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html
https://hail.is/docs/0.1/representation/hail.representation.Genotype.html:5429,Modifiability,variab,variables,5429,".; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. Parameters:num_genotypes (int) – number of possible genotypes. Return type:list of int or None. p_ab(theta=0.5)[source]¶; Returns the p-value associated with finding the given allele depth ratio.; This function uses a one-tailed binomial test.; This function returns None if the allelic depth (ad) is missing. Parameters:theta (float) – null reference probability for binomial model. Return type:float. pl¶; Returns the phred-scaled genotype posterior likelihoods. Return type:list of int or None. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html
https://hail.is/docs/0.1/representation/hail.representation.Genotype.html:6017,Testability,test,test,6017,".; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. Parameters:num_genotypes (int) – number of possible genotypes. Return type:list of int or None. p_ab(theta=0.5)[source]¶; Returns the p-value associated with finding the given allele depth ratio.; This function uses a one-tailed binomial test.; This function returns None if the allelic depth (ad) is missing. Parameters:theta (float) – null reference probability for binomial model. Return type:float. pl¶; Returns the phred-scaled genotype posterior likelihoods. Return type:list of int or None. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html
https://hail.is/docs/0.1/representation/hail.representation.Pedigree.html:1786,Testability,test,test,1786,"rio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. complete_trios; List of trio objects that have a defined father, mother, and sex. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]¶; List of trio objects that have a defined father, mother, and sex. Return type:list of Trio. filter_to(samples)[source]¶; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, the father is set to None.; If the mother is not in the list of samples provided, the mother is set to None. Parameters:samples (list of str) – list of sample IDs to keep. Return type:Pedigree. static read(fam_path, delimiter='\\s+')[source]¶; Read a .fam file and return a pedigree object.; Examples; >>> ped = Pedigree.read('data/test.fam'). Notes; This method reads a PLINK .fam file.; Hail expects a file in the same spec as PLINK outlines. Parameters:; fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:Pedigree. trios¶; List of trio objects in this pedigree. Return type:list of Trio. write(path)[source]¶; Write a .fam file to the given path.; Examples; >>> ped = Pedigree.read('data/test.fam'); >>> ped.write('out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data structure in Hail.; Reading and writing a PLINK .fam file will result in loss of this information.; Use the key table method import_fam() to manipulate this; information. Parameters:path (str) – output path. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Pedigree.html
https://hail.is/docs/0.1/representation/hail.representation.Pedigree.html:2179,Testability,test,test,2179,"rio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. complete_trios; List of trio objects that have a defined father, mother, and sex. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]¶; List of trio objects that have a defined father, mother, and sex. Return type:list of Trio. filter_to(samples)[source]¶; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, the father is set to None.; If the mother is not in the list of samples provided, the mother is set to None. Parameters:samples (list of str) – list of sample IDs to keep. Return type:Pedigree. static read(fam_path, delimiter='\\s+')[source]¶; Read a .fam file and return a pedigree object.; Examples; >>> ped = Pedigree.read('data/test.fam'). Notes; This method reads a PLINK .fam file.; Hail expects a file in the same spec as PLINK outlines. Parameters:; fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:Pedigree. trios¶; List of trio objects in this pedigree. Return type:list of Trio. write(path)[source]¶; Write a .fam file to the given path.; Examples; >>> ped = Pedigree.read('data/test.fam'); >>> ped.write('out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data structure in Hail.; Reading and writing a PLINK .fam file will result in loss of this information.; Use the key table method import_fam() to manipulate this; information. Parameters:path (str) – output path. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Pedigree.html
https://hail.is/docs/0.1/representation/hail.representation.Struct.html:707,Security,access,accessing,707,"﻿. . Struct — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Struct. View page source. Struct¶. class hail.representation.Struct(attributes)[source]¶; Nested annotation structure.; >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name “1kg”, for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:; >>> getattr(bar, '1kg'); >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:attributes (dict) – struct members. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. get; Get an item, or return a default value if the item is not found. get(item, default=None)[source]¶; Get an item, or return a default value if the item is not found. Parameters:; item (str) – Name of attribute.; default – Default value. Returns:Value of item if found, or default value if not. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Struct.html
https://hail.is/docs/0.1/representation/hail.representation.Struct.html:1096,Security,access,access,1096,"﻿. . Struct — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Struct. View page source. Struct¶. class hail.representation.Struct(attributes)[source]¶; Nested annotation structure.; >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name “1kg”, for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:; >>> getattr(bar, '1kg'); >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:attributes (dict) – struct members. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. get; Get an item, or return a default value if the item is not found. get(item, default=None)[source]¶; Get an item, or return a default value if the item is not found. Parameters:; item (str) – Name of attribute.; default – Default value. Returns:Value of item if found, or default value if not. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Struct.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:596,Modifiability,polymorphi,polymorphism,596,"﻿. . Variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Variant. View page source. Variant¶. class hail.representation.Variant(contig, start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial D",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:893,Modifiability,polymorphi,polymorphism,893,"﻿. . Variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Variant. View page source. Variant¶. class hail.representation.Variant(contig, start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial D",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1316,Modifiability,polymorphi,polymorphism,1316,"Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Variant. View page source. Variant¶. class hail.representation.Variant(contig, start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1412,Modifiability,polymorphi,polymorphism,1412," representation »; Variant. View page source. Variant¶. class hail.representation.Variant(contig, start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for t",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1508,Modifiability,polymorphi,polymorphism,1508,", start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1604,Modifiability,polymorphi,polymorphism,1604,"tig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1704,Modifiability,polymorphi,polymorphism,1704,"ed); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_b",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1790,Modifiability,polymorphi,polymorphism,1790," or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired a",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1915,Modifiability,polymorphi,polymorphism,1915,"this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1960,Modifiability,polymorphi,polymorphism,1960,"s locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multial",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:2046,Modifiability,polymorphi,polymorphism,2046,"; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:2117,Modifiability,polymorphi,polymorphism,2117,"rns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiall",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:2221,Modifiability,polymorphi,polymorphism,2221,"ele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in t",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3199,Modifiability,polymorphi,polymorphism,3199,"rns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this poly",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3327,Modifiability,polymorphi,polymorphism,3327,"nt object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3452,Modifiability,polymorphi,polymorphism,3452,"ncluded in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[sourc",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3577,Modifiability,polymorphi,polymorphism,3577,"ultiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the tot",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3702,Modifiability,polymorphi,polymorphism,3702,".alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3831,Modifiability,polymorphi,polymorphism,3831,"n of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3946,Modifiability,polymorphi,polymorphism,3946," called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4100,Modifiability,polymorphi,polymorphism,4100,"allelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>>",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4174,Modifiability,polymorphi,polymorphism,4174,"e allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). Re",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4289,Modifiability,polymorphi,polymorphism,4289,"ifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). Return type:Variant. ref¶; Reference allele at this locus. Return type:str. start¶; Chromosomal ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4390,Modifiability,polymorphi,polymorphism,4390,"gion of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). Return type:Variant. ref¶; Reference allele at this locus. Return type:str. start¶; Chromosomal position (1-based). Return type:int. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx usi",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4522,Modifiability,polymorphi,polymorphism,4522,"n_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). Return type:Variant. ref¶; Reference allele at this locus. Return type:str. start¶; Chromosomal position (1-based). Return type:int. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
https://hail.is/docs/0.1/representation/index.html:567,Modifiability,polymorphi,polymorphism,567,"﻿. . representation — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation. View page source. representation¶. Classes. hail.representation.Variant; An object that represents a genomic polymorphism. hail.representation.AltAllele; An object that represents an allele in a polymorphism deviating from the reference allele. hail.representation.Genotype; An object that represents an individual’s genotype at a genomic locus. hail.representation.Call; An object that represents an individual’s call at a genomic locus. hail.representation.Locus; An object that represents a location in the genome. hail.representation.Interval; A genomic interval marked by start and end loci. hail.representation.Trio; Class containing information about nuclear family relatedness and sex. hail.representation.Pedigree; Class containing a list of trios, with extra functionality. hail.representation.Struct; Nested annotation structure. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/index.html
https://hail.is/docs/0.1/representation/index.html:653,Modifiability,polymorphi,polymorphism,653,"﻿. . representation — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation. View page source. representation¶. Classes. hail.representation.Variant; An object that represents a genomic polymorphism. hail.representation.AltAllele; An object that represents an allele in a polymorphism deviating from the reference allele. hail.representation.Genotype; An object that represents an individual’s genotype at a genomic locus. hail.representation.Call; An object that represents an individual’s call at a genomic locus. hail.representation.Locus; An object that represents a location in the genome. hail.representation.Interval; A genomic interval marked by start and end loci. hail.representation.Trio; Class containing information about nuclear family relatedness and sex. hail.representation.Pedigree; Class containing a list of trios, with extra functionality. hail.representation.Struct; Nested annotation structure. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/index.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:433,Availability,down,download,433,"﻿. . Using the expression language to slice, dice, and query genetic data — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Introduction to the expression language; Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.is",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1483,Availability,avail,available,1483,"xpressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.is",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1648,Availability,error,error,1648,"on API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). Downloading data (~50M) from Google Storage...; Downl",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1832,Availability,down,download,1832," slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). Downloading data (~50M) from Google Storage...; Download finished!; Extracting...; Done!. We will read a dataset from disk, and print some summary statistics; about it to re-familiarize ourselves. In [4]:. vds = hc.read('data/1kg.vds'); vds.summa",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1866,Availability,down,downloads,1866," slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). Downloading data (~50M) from Google Storage...; Download finished!; Extracting...; Done!. We will read a dataset from disk, and print some summary statistics; about it to re-familiarize ourselves. In [4]:. vds = hc.read('data/1kg.vds'); vds.summa",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:11470,Availability,avail,available,11470,"us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 330646029.00001156}. Count¶; The count aggregator is pretty simple - it counts the number of; elements in the aggregable. In [26]:. vds.query_genotypes('gs.count()'). Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:19041,Energy Efficiency,power,powerful,19041,"',; u'HG02282',; u'HG02477']},; {u'at': u'8:95863909:A:T', u'homvars': []},; {u'at': u'8:97172671:C:T', u'homvars': []}]. takeBy¶; takeBy is an aggregator that takes elements of an aggregable ordered; by a lambda function (smallest to largest). We can easily select the; variants with the lowest p-values after regression:. In [42]:. top_5_pvals = (vds.linreg('sa.metadata.CaffeineConsumption'); .query_variants('variants.map(v => {at: str(v), pval: va.linreg.pval}).takeBy(x => x.pval, 5)')); pprint(top_5_pvals). 2018-10-18 01:26:07 Hail: INFO: Running linear regression on 1000 samples with 1 covariate including intercept... [{u'at': u'10:56025604:A:C', u'pval': 5.595049078641033e-05},; {u'at': u'20:55431571:A:C', u'pval': 0.00010899661736561121},; {u'at': u'10:91099630:T:C', u'pval': 0.00013497679316886596},; {u'at': u'4:149350527:T:C', u'pval': 0.00017786066989195366},; {u'at': u'7:152600817:G:A', u'pval': 0.0002252314501866726}]. Aggregating by key¶; The; aggregate_by_key; method is likely the most powerful piece of query functionality in Hail.; It’s a method on KeyTable.; You can produce key tables from a; VariantDataset with; three methods:. variants_table():; a key table with the variant and variant annotations as columns.; There is one row per variant.; samples_table():; a key table with the sample and sample annotations as columns. There; is one row per sample.; genotypes_table():; a key table that is the coordinate representation of the genetic; matrix. The columns are the variant, variant annotations, sample,; sample annotations, and genotype. There is one row per variant/sample; combination: (N * M) total rows!. Using; aggregate_by_key; with; genotypes_table; can produce counts of loss of function variants in cases and controls; per gene, compute the mean depth per sample per exon, and much more. You; define the aggregation keys, and you define how to combine the rows.; This method produces another; KeyTable.; We use it here to compute the mean depth and quali",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1330,Integrability,wrap,wraps,1330,"xpression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:6572,Integrability,interface,interface,6572,"n `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed). In [13]:. pprint(vds.sample_schema). Struct{; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; metadata: Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int; }; }. We can apply conditional filters on things like population with; if/else:. In [14]:. vds.filter_samples_expr('if (sa.metadata.Population == ""EAS"") sa.qc.dpMean > 8 else sa.qc.dpMean > 4').num_samples. Out[14]:. 897. Filtering variants and genotypes¶; One of the advantages of Hail’s filtering interface is that it’s equally; easy to filter samples, variants, or genotypes. If one is handed a fresh; VCF text file, it’s pretty easy to write a program to filter variants,; but much harder to filter samples or genotypes. Other data; representations may lend themselves to a different operation being easy,; and the others hard. In Hail, we’ve abstracted away all of this – it’s; easy to filter anything!. In [15]:. vds.count_variants(). Out[15]:. 10961L. In [16]:. # Filter on allele frequency; vds.filter_variants_expr('va.qc.AF > 0.1', keep=True).count_variants(). Out[16]:. 7993L. In [17]:. # Filter on allele frequency and GQ mean; vds.filter_variants_expr('va.qc.AF > 0.1 && va.qc.gqMean > 20').count_variants(). Out[17]:. 7879L. In [18]:. # Genotype call rate across the entire dataset; vds.summarize().call_rate. Out[18]:. 0.9831634887327798. As we can see in the previous cell, the overall call rate of this; dataset is 98.7%. In [19]:. vds.filter_genotypes('g.gq >= 20', keep=True).summa",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:11021,Integrability,interface,interface,11021,"mean of 5 trillion numbers?; That’s a lot of data, and turns out to be the rough number of genotypes; in the preprocessed gnomAD VCF,; which contained about 20 thousand samples and 250 million variants. Hail; is designed to handle datasets of this size and larger, and does so by; computing in parallel on many computers using Apache; Spark.; But we still want a simple programming model that allows us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 33064602",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:3967,Modifiability,variab,variable,3967,"0961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. Types in action¶; We’ll produce some sample annotations with the; sample_qc; method, then use these annotations to demonstrate some of the expression; language features. In [5]:. vds = vds.variant_qc().cache().sample_qc(). In [6]:. pprint(vds.sample_schema). Struct{; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; }; }. Filtering with expressions¶; The schema printed above is the type of the sample annotations, which; are given the variable name ‘sa’ wherever they appear. Here, we use the; filter_samples_expr method to filter samples based on these; annotations. If we want to filter on the “dpMean” above, we need to; select the ‘qc’ field from the ‘sa’ struct, then select the ‘dpMean’; field from the ‘qc’ struct. These selections are done with dots.; There are four Hail methods that use the expression language to filter a; dataset: -; filter_variants_expr; -; filter_samples_expr; -; filter_genotypes; -; filter_alleles; All these methods take a Hail expression as a string argument, and; return a filtered dataset. In [7]:. # unfiltered; vds.num_samples. Out[7]:. 1000. In [8]:. vds.filter_samples_expr('sa.qc.dpMean > 5', keep=True).num_samples. Out[8]:. 699. In [9]:. vds.filter_samples_expr('sa.qc.dpMean <= 5', keep=False).num_samples. Out[9]:. 699. In [10]:. vds.filter_samples_expr('sa.qc.callRate > 0.95', keep=True).num_samples. Out[10]:. 928. In [11]:. vds.filter_samples_expr(",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:3419,Performance,cache,cache,3419,"tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). Downloading data (~50M) from Google Storage...; Download finished!; Extracting...; Done!. We will read a dataset from disk, and print some summary statistics; about it to re-familiarize ourselves. In [4]:. vds = hc.read('data/1kg.vds'); vds.summarize().report(). Samples: 1000; Variants: 10961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. Types in action¶; We’ll produce some sample annotations with the; sample_qc; method, then use these annotations to demonstrate some of the expression; language features. In [5]:. vds = vds.variant_qc().cache().sample_qc(). In [6]:. pprint(vds.sample_schema). Struct{; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; }; }. Filtering with expressions¶; The schema printed above is the type of the sample annotations, which; are given the variable name ‘sa’ wherever they appear. Here, we use the; filter_samples_expr method to filter samples based on these; annotations. If we want to filter on the “dpMean” above, we need to; select the ‘qc’ field from the ‘sa’ struct, then select the ‘dpMean’; field from the ‘qc’ struct. These selections are done with dots.; There are four Hail methods that use the expression language to filter a; dataset: -; filter_variants_expr; -; filter_samples_expr;",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:10939,Performance,scalab,scalable,10939,"ean of an array of numbers with .mean(), find their max with; .max(), and so on.; But what if we wanted to compute the mean of 5 trillion numbers?; That’s a lot of data, and turns out to be the rough number of genotypes; in the preprocessed gnomAD VCF,; which contained about 20 thousand samples and 250 million variants. Hail; is designed to handle datasets of this size and larger, and does so by; computing in parallel on many computers using Apache; Spark.; But we still want a simple programming model that allows us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,;",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1366,Security,access,accessed,1366,"xpression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:11560,Security,expose,exposes,11560," In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 330646029.00001156}. Count¶; The count aggregator is pretty simple - it counts the number of; elements in the aggregable. In [26]:. vds.query_genotypes('gs.count()'). Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million. How can we make this more useful? With filter!. In [28]:. vds.query_genotypes('gs.filter(g => g.isHet()).c",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:13208,Security,access,access,13208,". Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million. How can we make this more useful? With filter!. In [28]:. vds.query_genotypes('gs.filter(g => g.isHet()).count()'). Out[28]:. 2583309L. Of the 11 million genotypes in the dataset, about 2.5M are heterozygous.; What about combining sample annotations with genotype information? How; many heterozygote genotypes are found in the American samples? A simple; way to implement this is by filtering to American samples first and then; running the same query. In [29]:. (vds.filter_samples_expr('sa.metadata.SuperPopulation == ""AMR""'); .query_genotypes('gs.filter(g => g.isHet()).count()')). Out[29]:. 754850L. The next cell is a bit tricky - aggregables have an extra “context” that; they carry around. We can actually access the sample, sample; annotations, variant, and variant annotations inside of operations on; gs. We don’t need to filter samples first, we can do it inside the; query:. In [30]:. vds.query_genotypes('gs.filter(g => g.isHet() && sa.metadata.SuperPopulation == ""AMR"").count()'). Out[30]:. 754850L. Here’s an example where we use the variant annotations to count the; number of heterozygous genotypes in Americans at rare loci. In [31]:. vds.query_genotypes('''gs.filter(g => g.isHet(); && sa.metadata.SuperPopulation == ""AMR""; && va.qc.AF < 0.01).count()'''). Out[31]:. 1879L. Sum¶; The sum aggregator can be used to compute useful statistics per; sample or variant. For example, we may want to count the total number of; non-reference alleles per sample:. In [32]:. (vds.annotate_samples_expr('sa.nNonRefAlleles = gs.map(g => g.nNonRefAlleles()).sum()'); .query_samples('samples.map(s => sa.nNonRefAlleles).take(10)')). Out[32]:. [6423, 6530, 6606, 6638, 6570, 6572, 6542, 6490, 6606, 6464]. Fraction¶; The fraction aggregator can actuall",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:8073,Usability,simpl,simple,8073,"ds.filter_variants_expr('va.qc.AF > 0.1', keep=True).count_variants(). Out[16]:. 7993L. In [17]:. # Filter on allele frequency and GQ mean; vds.filter_variants_expr('va.qc.AF > 0.1 && va.qc.gqMean > 20').count_variants(). Out[17]:. 7879L. In [18]:. # Genotype call rate across the entire dataset; vds.summarize().call_rate. Out[18]:. 0.9831634887327798. As we can see in the previous cell, the overall call rate of this; dataset is 98.7%. In [19]:. vds.filter_genotypes('g.gq >= 20', keep=True).summarize().call_rate. Out[19]:. 0.5495507709150625. However, 40% of those called genotypes are called with GQ 20 or less!; This corresponds to less than 99% confidence in the call. Annotating with expressions¶; It is also possible to produce new annotations with the expression; language. These take an expression of the form:; <new annotation name> = <expression>. To annotate samples, the new annotation name must also start with; sa. To annotate variants, it must always begin with va.; Here are some simple examples. In [20]:. (vds.annotate_samples_expr('sa.keepThisSample = sa.qc.callRate > 0.95 && sa.qc.dpMean > 5'); .filter_samples_expr('sa.keepThisSample', keep=True).num_samples). Out[20]:. 696. In [21]:. (vds.annotate_variants_expr('va.keepThisVariant = va.qc.AF > 0.1 && va.qc.gqMean > 20'); .filter_variants_expr('va.keepThisVariant').count_variants()). Out[21]:. 7879L. Key tables also have an; annotate; method. We can use this to produce new columns or redefine old ones:. In [22]:. kt.to_dataframe().show(5). +-------+----------+---------------+--------+----------+-------------------+; | Sample|Population|SuperPopulation|isFemale|PurpleHair|CaffeineConsumption|; +-------+----------+---------------+--------+----------+-------------------+; |NA19784| MXL| AMR| false| false| 8|; |NA19102| YRI| AFR| true| false| 6|; |HG00141| GBR| EUR| false| false| 6|; |HG01890| ACB| AFR| false| false| 8|; |HG00263| GBR| EUR| true| true| 6|; +-------+----------+---------------+--------+----------+",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:10455,Usability,simpl,simple,10455," MXL| AMR| false| false| 8| true|; |NA19102| YRI| AFR| true| false| 6| false|; |HG00141| GBR| EUR| false| false| 6| false|; |HG01890| ACB| AFR| false| false| 8| false|; |HG00263| GBR| EUR| true| true| 6| false|; +-------+----------+---------------+--------+----------+-------------------+-----------+; only showing top 5 rows. Aggregables¶; We’ve now seen how it’s possible to use the Hail expression language to; manipulate various things like numbers and arrays. We can compute the; mean of an array of numbers with .mean(), find their max with; .max(), and so on.; But what if we wanted to compute the mean of 5 trillion numbers?; That’s a lot of data, and turns out to be the rough number of genotypes; in the preprocessed gnomAD VCF,; which contained about 20 thousand samples and 250 million variants. Hail; is designed to handle datasets of this size and larger, and does so by; computing in parallel on many computers using Apache; Spark.; But we still want a simple programming model that allows us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expr",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:12145,Usability,simpl,simple,12145,"like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 330646029.00001156}. Count¶; The count aggregator is pretty simple - it counts the number of; elements in the aggregable. In [26]:. vds.query_genotypes('gs.count()'). Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million. How can we make this more useful? With filter!. In [28]:. vds.query_genotypes('gs.filter(g => g.isHet()).count()'). Out[28]:. 2583309L. Of the 11 million genotypes in the dataset, about 2.5M are heterozygous.; What about combining sample annotations with genotype information? How; many heterozygote genotypes are found in the American samples? A simple; way to implement this is by filtering to American samples first and then; running the same query. In [29]:. (vds.filter_samples_expr('sa.metadata.SuperPopulation == ""AMR""'); .query_genotypes('gs.filter(g => g.isHet()).count()')). Out[29]:. 754850L. The next cell is a bit tricky - aggregables have an extra",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:12842,Usability,simpl,simple,12842,"n. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 330646029.00001156}. Count¶; The count aggregator is pretty simple - it counts the number of; elements in the aggregable. In [26]:. vds.query_genotypes('gs.count()'). Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million. How can we make this more useful? With filter!. In [28]:. vds.query_genotypes('gs.filter(g => g.isHet()).count()'). Out[28]:. 2583309L. Of the 11 million genotypes in the dataset, about 2.5M are heterozygous.; What about combining sample annotations with genotype information? How; many heterozygote genotypes are found in the American samples? A simple; way to implement this is by filtering to American samples first and then; running the same query. In [29]:. (vds.filter_samples_expr('sa.metadata.SuperPopulation == ""AMR""'); .query_genotypes('gs.filter(g => g.isHet()).count()')). Out[29]:. 754850L. The next cell is a bit tricky - aggregables have an extra “context” that; they carry around. We can actually access the sample, sample; annotations, variant, and variant annotations inside of operations on; gs. We don’t need to filter samples first, we can do it inside the; query:. In [30]:. vds.query_genotypes('gs.filter(g => g.isHet() && sa.metadata.SuperPopulation == ""AMR"").count()'). Out[30]:. 754850L. Here’s an example where we use the variant annotations to count the; number of heterozygous genotypes in Americans at rare loci. In [31]:. vds.query_genotypes('''gs.filter(g => g.isHet(); && sa.metadata.SuperPopulation == ""AMR""; && va.qc.AF < 0.01).count()'''). Out[31]:. 1879L. Sum¶; The sum aggregator can be used ",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:219,Availability,down,download,219,"﻿. . Overview — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:1230,Availability,avail,available,1230,"sary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data ",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:1395,Availability,error,error,1395,"is; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:1893,Availability,down,download,1893,"ng caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). All files are present and accounted for!. Loading data from disk¶; Hail has its own internal data representation, called a Variant Dataset; (VDS). This is both an on-disk file format and a Python; object. See the; overview for a complete story.; ",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:1927,Availability,down,downloads,1927,"ng caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). All files are present and accounted for!. Loading data from disk¶; Hail has its own internal data representation, called a Variant Dataset; (VDS). This is both an on-disk file format and a Python; object. See the; overview for a complete story.; ",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:3007,Availability,down,downsampling,3007,"irectory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). All files are present and accounted for!. Loading data from disk¶; Hail has its own internal data representation, called a Variant Dataset; (VDS). This is both an on-disk file format and a Python; object. See the; overview for a complete story.; Here, we read a VDS from disk.; This dataset was created by downsampling a public 1000 genomes VCF to; about 50 MB. In [5]:. vds = hc.read('data/1kg.vds'). Getting to know our data¶; It’s important to have easy ways to slice, dice, query, and summarize a; dataset. Some of these methods are demonstrated below.; The; summarize; method is useful for providing a broad overview of the data contained in; a dataset. In [6]:. vds.summarize().report(). Samples: 1000; Variants: 10961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. The; query_variants; method is the first time we’ll see the Hail expression; language. The expression; language allows for a variety of incredibly expressive queries and; computations, but is probably the most complex part of Hail. See the; pair of tutorials on the expression language to learn more!; Here, we ",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:6289,Availability,error,error,6289,"type(GT=0, AD=[12, 0], DP=12, GQ=36, PL=[0, 36, 420])]. Integrate sample annotations¶; Hail treats variant and sample annotations as first-class citizens.; Annotations are usually a critical part of any genetic study. Sample; annotations are where you’ll store information about sample phenotypes,; ancestry, sex, and covariates. Variant annotations can be used to store; information like gene membership and functional impact for use in QC or; analysis.; In this tutorial, we demonstrate how to take a text file and use it to; annotate the samples in a VDS.; iPython supports various cell “magics”. The %%sh magic is one which; interprets the cell with bash, rather than Python. We can use this to; look at the first few lines of our annotation file. This file contains; the sample ID, the population and “super-population” designations, the; sample sex, and two simulated phenotypes (one binary, one discrete). In [11]:. %%sh; head data/1kg_annotations.txt | column -t. sh: 1: column: not found; head: write error: Broken pipe. This file can be imported into Hail with; HailContext.import_table.; This method produces a; KeyTable; object. Think of this as a Pandas or R dataframe that isn’t limited by; the memory on your machine – behind the scenes, it’s distributed with; Spark. In [12]:. table = hc.import_table('data/1kg_annotations.txt', impute=True)\; .key_by('Sample'). 2018-10-18 01:26:28 Hail: INFO: Reading table to impute column types; 2018-10-18 01:26:28 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed). A good way to peek at the structure of a KeyTable is to look at its; schema. In [13]:. print(table.schema). Struct{Sample:String,Population:String,SuperPopulation",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:17574,Availability,error,error,17574," the bottom left quadrant. In [34]:. plt.scatter(df[""sa.qc.dpMean""], df[""sa.qc.callRate""],; alpha=0.1); plt.xlabel('Mean DP'); plt.ylabel('Call Rate'); plt.xlim(0, 20); plt.axhline(0.97, c='k'); plt.axvline(4, c='k'); plt.show(). It’s easy to filter when we’ve got the cutoff values decided:. In [35]:. vds = vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); print('After filter, %d/1000 samples remain.' % vds.num_samples). After filter, 843/1000 samples remain. Next is genotype QC. To start, we’ll print the post-sample-QC call rate.; It’s actually gone up since the initial summary - dropping low-quality; samples disproportionally removed missing genotypes. In [36]:. call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('pre QC call rate is %.3f' % call_rate). pre QC call rate is 0.991. It’s a good idea to filter out genotypes where the reads aren’t where; they should be: if we find a genotype called homozygous reference with; >10% alternate reads, a genotype called homozygous alternate with >10%; reference reads, or a genotype called heterozygote without a ref / alt; balance near 1:1, it is likely to be an error. In [37]:. filter_condition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:18612,Availability,down,downstream,18612,"ndition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantg",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:21466,Availability,down,downsampled,21466,"t’s do a GWAS!¶; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); uncorrelated (not in linkage disequilibrium). Both of these are easy in Hail. In [43]:. common_vds = (vds; .filter_variants_expr('va.qc.AF > 0.01'); .ld_prune(memory_per_core=256, num_cores=4)). 2018-10-18 01:26:50 Hail: INFO: Running LD prune with nSamples=843, nVariants=9085, nPartitions=4, and maxQueueSize=257123.; 2018-10-18 01:26:50 Hail: INFO: LD prune step 1 of 3: nVariantsKept=8478, nPartitions=4, time=351.375ms; 2018-10-18 01:26:51 Hail: INFO: LD prune step 2 of 3: nVariantsKept=8478, nPartitions=12, time=1.184s; 2018-10-18 01:26:52 Hail: INFO: Coerced sorted dataset; 2018-10-18 01:26:52 Hail: INFO: LD prune step 3 of 3: nVariantsKept=8478, time=481.478ms. In [44]:. common_vds.count(). Out[44]:. (843L, 8555L). These filters removed about 15% of sites (we started with a bit over; 10,000). This is NOT representative of most sequencing datasets! We; have already downsampled the full thousand genomes dataset to include; more common variants than we’d expect by chance.; In Hail, the association tests accept sample annotations for the sample; phenotype and covariates. Since we’ve already got our phenotype of; interest (caffeine consumption) in the dataset, we are good to go:. In [45]:. gwas = common_vds.linreg('sa.CaffeineConsumption'); pprint(gwas.variant_schema). 2018-10-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCa",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:22931,Availability,error,error,22931,"-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: Struct{; beta: Double,; se: Double,; tstat: Double,; pval: Double; }; }. Looking at the bottom of the above printout, you can see the linear; regression adds new variant annotations for the beta, standard error,; t-statistic, and p-value. In [46]:. def qqplot(pvals, xMax, yMax):; spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals)); exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratif",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:24183,Availability,error,error,24183,"ge(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of these labels due to human error. Genomes don’t have that; problem! Instead of using reported ancestry, we will use genetic; ancestry by including computed principal components in our model.; The; pca; method produces sample PCs in sample annotations, and can also produce; variant loadings and global eigenvalues when asked. In [48]:. pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'). 2018-10-18 01:26:55 Hail: INFO: Running PCA with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSin",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:25605,Availability,recover,recover,25605," with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Double; }; }. Now that we’ve got principal components per sample, we may as well plot; them! Human history exerts a strong effect in genetic datasets. Even; with a 50MB sequencing dataset, we can recover the major human; populations. In [51]:. pca_table = pca.samples_table().to_pandas(); colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}; plt.scatter(pca_table[""sa.pca.PC1""], pca_table[""sa.pca.PC2""],; c = pca_table[""sa.SuperPopulation""].map(colors),; alpha = .5); plt.xlim(-0.6, 0.6); plt.xlabel(""PC1""); plt.ylabel(""PC2""); legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]; plt.legend(handles=legend_entries, loc=2); plt.show(). Now we can rerun our linear regression, controlling for the first few; principal components and sample sex. In [52]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including inter",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:1617,Deployability,patch,patches,1617,"w¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:12251,Deployability,release,release,12251,"ants('variants.map(v => v.altAllele()).filter(aa => aa.isSNP()).counter()'); pprint(Counter(snp_counts).most_common()). [(AltAllele(ref=C, alt=T), 2436L),; (AltAllele(ref=G, alt=A), 2387L),; (AltAllele(ref=A, alt=G), 1944L),; (AltAllele(ref=T, alt=C), 1879L),; (AltAllele(ref=C, alt=A), 496L),; (AltAllele(ref=G, alt=T), 480L),; (AltAllele(ref=T, alt=G), 468L),; (AltAllele(ref=A, alt=C), 454L),; (AltAllele(ref=C, alt=G), 150L),; (AltAllele(ref=G, alt=C), 112L),; (AltAllele(ref=T, alt=A), 79L),; (AltAllele(ref=A, alt=T), 76L)]. It’s nice to see that we can actually uncover something biological from; this small dataset: we see that these frequencies come in pairs. C/T and; G/A are actually the same mutation, just viewed from from opposite; strands. Likewise, T/A and A/T are the same mutation on opposite; strands. There’s a 30x difference between the frequency of C/T and A/T; SNPs. Why?; The same Python, R, and Unix tools could do this work as well, but we’re; starting to hit a wall - the latest gnomAD; release publishes about 250; million variants, and that won’t fit in memory on a single computer.; What about genotypes? Hail can query the collection of all genotypes in; the dataset, and this is getting large even for our tiny dataset. Our; 1,000 samples and 10,000 variants produce 10 million unique genotypes.; The gnomAD dataset has about 5 trillion unique genotypes.; Here we will use the hist aggregator to produce and plot a histogram; of DP values for genotypes in our thousand genomes dataset. In [26]:. dp_hist = vds.query_genotypes('gs.map(g => g.dp).hist(0, 30, 30)'); plt.xlim(0, 31); plt.bar(dp_hist.binEdges[1:], dp_hist.binFrequencies); plt.show(). Quality Control¶; QC is where analysts spend most of their time with sequencing datasets.; QC is an iterative process, and is different for every project: there is; no “push-button” solution for QC. Each time the Broad collects a new; group of samples, it finds new batch effects. However, by practicing; open science an",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:21734,Energy Efficiency,consumption,consumption,21734,".01'); .ld_prune(memory_per_core=256, num_cores=4)). 2018-10-18 01:26:50 Hail: INFO: Running LD prune with nSamples=843, nVariants=9085, nPartitions=4, and maxQueueSize=257123.; 2018-10-18 01:26:50 Hail: INFO: LD prune step 1 of 3: nVariantsKept=8478, nPartitions=4, time=351.375ms; 2018-10-18 01:26:51 Hail: INFO: LD prune step 2 of 3: nVariantsKept=8478, nPartitions=12, time=1.184s; 2018-10-18 01:26:52 Hail: INFO: Coerced sorted dataset; 2018-10-18 01:26:52 Hail: INFO: LD prune step 3 of 3: nVariantsKept=8478, time=481.478ms. In [44]:. common_vds.count(). Out[44]:. (843L, 8555L). These filters removed about 15% of sites (we started with a bit over; 10,000). This is NOT representative of most sequencing datasets! We; have already downsampled the full thousand genomes dataset to include; more common variants than we’d expect by chance.; In Hail, the association tests accept sample annotations for the sample; phenotype and covariates. Since we’ve already got our phenotype of; interest (caffeine consumption) in the dataset, we are good to go:. In [45]:. gwas = common_vds.linreg('sa.CaffeineConsumption'); pprint(gwas.variant_schema). 2018-10-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: S",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:23720,Energy Efficiency,consumption,consumption,23720,"e; },; linreg: Struct{; beta: Double,; se: Double,; tstat: Double,; pval: Double; }; }. Looking at the bottom of the above printout, you can see the linear; regression adds new variant annotations for the beta, standard error,; t-statistic, and p-value. In [46]:. def qqplot(pvals, xMax, yMax):; spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals)); exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of these labels due to human error. Genomes don’t have that; problem! Instead of using reported ancestry, we will use genetic; ancestry by including computed principal components in our model.; The; pca; method produces sample PCs in sample annotations, and can also produce; variant loadings and global eigenvalues when asked. In [48]:. pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'). 2018-10-18 01:26:55 Hail: INFO: Running PCA with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:25716,Energy Efficiency,green,green,25716,"01822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Double; }; }. Now that we’ve got principal components per sample, we may as well plot; them! Human history exerts a strong effect in genetic datasets. Even; with a 50MB sequencing dataset, we can recover the major human; populations. In [51]:. pca_table = pca.samples_table().to_pandas(); colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}; plt.scatter(pca_table[""sa.pca.PC1""], pca_table[""sa.pca.PC2""],; c = pca_table[""sa.SuperPopulation""].map(colors),; alpha = .5); plt.xlim(-0.6, 0.6); plt.xlabel(""PC1""); plt.ylabel(""PC2""); legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]; plt.legend(handles=legend_entries, loc=2); plt.show(). Now we can rerun our linear regression, controlling for the first few; principal components and sample sex. In [52]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including intercept... In [53]:. qqplot(pvals, 5, 6). In [54]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:4868,Integrability,interface,interface,4868,"utations, but is probably the most complex part of Hail. See the; pair of tutorials on the expression language to learn more!; Here, we can use query_variants to pull out 5 variants to see what; they look like. In [7]:. vds.query_variants('variants.take(5)'). Out[7]:. [Variant(contig=1, start=904165, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=909917, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=986963, ref=C, alts=[AltAllele(ref=C, alt=T)]),; Variant(contig=1, start=1563691, ref=T, alts=[AltAllele(ref=T, alt=G)]),; Variant(contig=1, start=1707740, ref=T, alts=[AltAllele(ref=T, alt=G)])]. There are often several ways to do something in Hail. Here are two ways; to peek at the first few sample IDs:. In [8]:. vds.query_samples('samples.take(5)'). Out[8]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. In [9]:. vds.sample_ids[:5]. Out[9]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. There’s a similar interface for looking at the genotypes in a dataset.; We use; query_genotypes; to look at the first few genotype calls. In [10]:. vds.query_genotypes('gs.take(5)'). Out[10]:. [Genotype(GT=0, AD=[4, 0], DP=4, GQ=12, PL=[0, 12, 194]),; Genotype(GT=1, AD=[4, 3], DP=7, GQ=85, PL=[85, 0, 109]),; Genotype(GT=0, AD=[1, 0], DP=1, GQ=3, PL=[0, 3, 42]),; Genotype(GT=0, AD=[14, 0], DP=14, GQ=42, PL=[0, 42, 533]),; Genotype(GT=0, AD=[12, 0], DP=12, GQ=36, PL=[0, 36, 420])]. Integrate sample annotations¶; Hail treats variant and sample annotations as first-class citizens.; Annotations are usually a critical part of any genetic study. Sample; annotations are where you’ll store information about sample phenotypes,; ancestry, sex, and covariates. Variant annotations can be used to store; information like gene membership and functional impact for use in QC or; analysis.; In this tutorial, we demonstrate how to take a text file and use it to; annotate the samples in a VDS.; iPython supports various cell “magics”. The %%",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:10837,Integrability,interface,interfaces,10837,"er, these metrics aren’t perfectly representative of the samples in; our dataset. Here’s why:. In [21]:. table.count(). Out[21]:. 3500L. In [22]:. vds.num_samples. Out[22]:. 1000. Since there are fewer samples in our dataset than in the full thousand; genomes cohort, we need to look at annotations on the dataset. We can do; this with; query_samples. In [23]:. vds.query_samples('samples.map(s => sa.SuperPopulation).counter()'). Out[23]:. {u'AFR': 101L, u'AMR': 285L, u'EAS': 308L, u'EUR': 298L, u'SAS': 8L}. In [24]:. pprint(vds.query_samples('samples.map(s => sa.CaffeineConsumption).stats()')). {u'max': 10.0,; u'mean': 6.783000000000003,; u'min': 3.0,; u'nNotMissing': 1000L,; u'stdev': 1.624780292839619,; u'sum': 6783.000000000003}. The functionality demonstrated in the last few cells isn’t anything; especially new: it’s certainly not difficult to ask these questions with; Pandas or R dataframes, or even Unix tools like awk. But Hail can; use the same interfaces and query language to analyze collections that; are much larger, like the set of variants.; Here we calculate the counts of each of the 12 possible unique SNPs (4; choices for the reference base * 3 choices for the alternate base). To; do this, we need to map the variants to their alternate allele, filter; to SNPs, and count by unique ref/alt pair:. In [25]:. snp_counts = vds.query_variants('variants.map(v => v.altAllele()).filter(aa => aa.isSNP()).counter()'); pprint(Counter(snp_counts).most_common()). [(AltAllele(ref=C, alt=T), 2436L),; (AltAllele(ref=G, alt=A), 2387L),; (AltAllele(ref=A, alt=G), 1944L),; (AltAllele(ref=T, alt=C), 1879L),; (AltAllele(ref=C, alt=A), 496L),; (AltAllele(ref=G, alt=T), 480L),; (AltAllele(ref=T, alt=G), 468L),; (AltAllele(ref=A, alt=C), 454L),; (AltAllele(ref=C, alt=G), 150L),; (AltAllele(ref=G, alt=C), 112L),; (AltAllele(ref=T, alt=A), 79L),; (AltAllele(ref=A, alt=T), 76L)]. It’s nice to see that we can actually uncover something biological from; this small dataset: we see that t",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:18573,Performance,cache,cache,18573,"ndition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantg",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:18591,Performance,optimiz,optimize,18591,"ndition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantg",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:18668,Performance,cache,cache,18668,"0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantgq_means, bins = np.arange(0, 84, 2)); plt.xlabel(""Variant Mean GQ""); pl",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:24438,Performance,load,loadings,24438,"ow(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of these labels due to human error. Genomes don’t have that; problem! Instead of using reported ancestry, we will use genetic; ancestry by including computed principal components in our model.; The; pca; method produces sample PCs in sample annotations, and can also produce; variant loadings and global eigenvalues when asked. In [48]:. pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'). 2018-10-18 01:26:55 Hail: INFO: Running PCA with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Doubl",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:25605,Safety,recover,recover,25605," with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Double; }; }. Now that we’ve got principal components per sample, we may as well plot; them! Human history exerts a strong effect in genetic datasets. Even; with a 50MB sequencing dataset, we can recover the major human; populations. In [51]:. pca_table = pca.samples_table().to_pandas(); colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}; plt.scatter(pca_table[""sa.pca.PC1""], pca_table[""sa.pca.PC2""],; c = pca_table[""sa.SuperPopulation""].map(colors),; alpha = .5); plt.xlim(-0.6, 0.6); plt.xlabel(""PC1""); plt.ylabel(""PC2""); legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]; plt.legend(handles=legend_entries, loc=2); plt.show(). Now we can rerun our linear regression, controlling for the first few; principal components and sample sex. In [52]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including inter",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:29431,Safety,detect,detect,29431,"otypes_table(); .aggregate_by_key(key_expr=['''maf_bin = if (va.qc.AF < 0.01) ""< 1%""; else if (va.qc.AF < 0.05) ""1%-5%""; else ""> 5%"" ''',; 'purple_hair = sa.PurpleHair'],; agg_expr=['mean_gq = g.map(g => g.gq).stats().mean',; 'mean_dp = g.map(g => g.dp).stats().mean'])). In [59]:. kt2.to_dataframe().show(). +-------+-----------+------------------+-----------------+; |maf_bin|purple_hair| mean_gq| mean_dp|; +-------+-----------+------------------+-----------------+; | > 5%| true| 36.09305651197578|7.407450459057423|; | < 1%| true| 22.68197887434976|7.374254453728496|; | < 1%| false|22.986128698357074|7.492131714314245|; | > 5%| false|36.341259980753755|7.533399982371768|; | 1%-5%| true|24.093123033233528|7.269552536649012|; | 1%-5%| false| 24.3519587208908|7.405582424428774|; +-------+-----------+------------------+-----------------+. We’ve shown that it’s easy to aggregate by a couple of arbitrary; statistics. This specific examples may not provide especially useful; pieces of information, but this same pattern can be used to detect; effects of rare variation:. Count the number of heterozygous genotypes per gene by functional; category (synonymous, missense, or loss-of-function) to estimate; per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in; cases and controls to detect genes involved in disease. Eplilogue¶; Congrats! If you’ve made it this far, you’re perfectly primed to read; the Overview, look through the; Hail objects representing many; core concepts in genetics, and check out the many Hail functions defined; in the Python API. If you use Hail; for your own science, we’d love to hear from you on Gitter; chat or the discussion; forum.; There’s also a lot of functionality inside Hail that we didn’t get to in; this broad overview. Things like:. Flexible import and export to a variety of data and annotation; formats (VCF, BGEN, PLINK, JSON, TSV, …); Simulation; Burden tests; Kinship and pruning (IBD, GRM, RRM); Fami",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:29724,Safety,detect,detect,29724,"',; 'mean_dp = g.map(g => g.dp).stats().mean'])). In [59]:. kt2.to_dataframe().show(). +-------+-----------+------------------+-----------------+; |maf_bin|purple_hair| mean_gq| mean_dp|; +-------+-----------+------------------+-----------------+; | > 5%| true| 36.09305651197578|7.407450459057423|; | < 1%| true| 22.68197887434976|7.374254453728496|; | < 1%| false|22.986128698357074|7.492131714314245|; | > 5%| false|36.341259980753755|7.533399982371768|; | 1%-5%| true|24.093123033233528|7.269552536649012|; | 1%-5%| false| 24.3519587208908|7.405582424428774|; +-------+-----------+------------------+-----------------+. We’ve shown that it’s easy to aggregate by a couple of arbitrary; statistics. This specific examples may not provide especially useful; pieces of information, but this same pattern can be used to detect; effects of rare variation:. Count the number of heterozygous genotypes per gene by functional; category (synonymous, missense, or loss-of-function) to estimate; per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in; cases and controls to detect genes involved in disease. Eplilogue¶; Congrats! If you’ve made it this far, you’re perfectly primed to read; the Overview, look through the; Hail objects representing many; core concepts in genetics, and check out the many Hail functions defined; in the Python API. If you use Hail; for your own science, we’d love to hear from you on Gitter; chat or the discussion; forum.; There’s also a lot of functionality inside Hail that we didn’t get to in; this broad overview. Things like:. Flexible import and export to a variety of data and annotation; formats (VCF, BGEN, PLINK, JSON, TSV, …); Simulation; Burden tests; Kinship and pruning (IBD, GRM, RRM); Family-based tests and utilities; Distributed file system utilities; Interoperability with Python and Spark machine learning libraries; More!. For reference, here’s the full workflow to all tutorial endpoints; combined into one",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:891,Testability,test,test,891,"﻿. . Overview — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:1688,Testability,log,log,1688,"w¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:21599,Testability,test,tests,21599,"ibrium). Both of these are easy in Hail. In [43]:. common_vds = (vds; .filter_variants_expr('va.qc.AF > 0.01'); .ld_prune(memory_per_core=256, num_cores=4)). 2018-10-18 01:26:50 Hail: INFO: Running LD prune with nSamples=843, nVariants=9085, nPartitions=4, and maxQueueSize=257123.; 2018-10-18 01:26:50 Hail: INFO: LD prune step 1 of 3: nVariantsKept=8478, nPartitions=4, time=351.375ms; 2018-10-18 01:26:51 Hail: INFO: LD prune step 2 of 3: nVariantsKept=8478, nPartitions=12, time=1.184s; 2018-10-18 01:26:52 Hail: INFO: Coerced sorted dataset; 2018-10-18 01:26:52 Hail: INFO: LD prune step 3 of 3: nVariantsKept=8478, time=481.478ms. In [44]:. common_vds.count(). Out[44]:. (843L, 8555L). These filters removed about 15% of sites (we started with a bit over; 10,000). This is NOT representative of most sequencing datasets! We; have already downsampled the full thousand genomes dataset to include; more common variants than we’d expect by chance.; In Hail, the association tests accept sample annotations for the sample; phenotype and covariates. Since we’ve already got our phenotype of; interest (caffeine consumption) in the dataset, we are good to go:. In [45]:. gwas = common_vds.linreg('sa.CaffeineConsumption'); pprint(gwas.variant_schema). 2018-10-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; r",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:23078,Testability,log,log,23078," pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: Struct{; beta: Double,; se: Double,; tstat: Double,; pval: Double; }; }. Looking at the bottom of the above printout, you can see the linear; regression adds new variant annotations for the beta, standard error,; t-statistic, and p-value. In [46]:. def qqplot(pvals, xMax, yMax):; spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals)); exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:23162,Testability,log,log,23162,"RankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: Struct{; beta: Double,; se: Double,; tstat: Double,; pval: Double; }; }. Looking at the bottom of the above printout, you can see the linear; regression adds new variant annotations for the beta, standard error,; t-statistic, and p-value. In [46]:. def qqplot(pvals, xMax, yMax):; spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals)); exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of th",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:27384,Testability,log,logistic,27384,"eineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including intercept... In [53]:. qqplot(pvals, 5, 6). In [54]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption',; covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale'],; use_dosages=True); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:08 Hail: INFO: Running linear regression on 843 samples with 5 covariates including intercept... In [55]:. qqplot(pvals, 5, 6). That’s more like it! We may not be publishing ten new coffee-drinking; loci in Nature, but we shouldn’t expect to find anything but the; strongest signals from a dataset of 1000 individuals anyway. Rare variant analysis¶; Hail doesn’t yet have rare variant kernel-based methods, but we have; linear; and; logistic; burden tests.; We won’t be showing those here, though. Instead, we’ll demonstrate how; one can use the expression language to group and count by any arbitrary; properties in variant or sample annotations. In [56]:. kt = (vds.genotypes_table(); .aggregate_by_key(key_expr=['pop = sa.SuperPopulation', 'chromosome = v.contig'],; agg_expr=['n_het = g.filter(g => g.isHet()).count()'])). In [57]:. kt.to_dataframe().show(). +---+----------+-----+; |pop|chromosome|n_het|; +---+----------+-----+; |EUR| 14|16380|; |SAS| 17| 511|; |EUR| 5|30717|; |AFR| 7|11889|; |EAS| 9|23951|; |AFR| 21| 3529|; |EAS| X| 7403|; |EAS| 1|49375|; |EUR| 19|13483|; |AMR| 15|18935|; |AMR| 7|31527|; |EUR| 13|17321|; |EUR| 12|26134|; |EUR| 15|15807|; |EUR| 6|33910|; |EAS| 20|17466|; |SAS| 11| 901|; |AFR| 3|15829|; |EAS| 2|45384|; |AMR| 18|18982|; +---+----------+-----+; only showing top 20 rows. What if we want to group by minor allele frequency bin and hair color,; and c",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:27401,Testability,test,tests,27401,"eineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including intercept... In [53]:. qqplot(pvals, 5, 6). In [54]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption',; covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale'],; use_dosages=True); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:08 Hail: INFO: Running linear regression on 843 samples with 5 covariates including intercept... In [55]:. qqplot(pvals, 5, 6). That’s more like it! We may not be publishing ten new coffee-drinking; loci in Nature, but we shouldn’t expect to find anything but the; strongest signals from a dataset of 1000 individuals anyway. Rare variant analysis¶; Hail doesn’t yet have rare variant kernel-based methods, but we have; linear; and; logistic; burden tests.; We won’t be showing those here, though. Instead, we’ll demonstrate how; one can use the expression language to group and count by any arbitrary; properties in variant or sample annotations. In [56]:. kt = (vds.genotypes_table(); .aggregate_by_key(key_expr=['pop = sa.SuperPopulation', 'chromosome = v.contig'],; agg_expr=['n_het = g.filter(g => g.isHet()).count()'])). In [57]:. kt.to_dataframe().show(). +---+----------+-----+; |pop|chromosome|n_het|; +---+----------+-----+; |EUR| 14|16380|; |SAS| 17| 511|; |EUR| 5|30717|; |AFR| 7|11889|; |EAS| 9|23951|; |AFR| 21| 3529|; |EAS| X| 7403|; |EAS| 1|49375|; |EUR| 19|13483|; |AMR| 15|18935|; |AMR| 7|31527|; |EUR| 13|17321|; |EUR| 12|26134|; |EUR| 15|15807|; |EUR| 6|33910|; |EAS| 20|17466|; |SAS| 11| 901|; |AFR| 3|15829|; |EAS| 2|45384|; |AMR| 18|18982|; +---+----------+-----+; only showing top 20 rows. What if we want to group by minor allele frequency bin and hair color,; and c",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:30341,Testability,test,tests,30341,"pieces of information, but this same pattern can be used to detect; effects of rare variation:. Count the number of heterozygous genotypes per gene by functional; category (synonymous, missense, or loss-of-function) to estimate; per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in; cases and controls to detect genes involved in disease. Eplilogue¶; Congrats! If you’ve made it this far, you’re perfectly primed to read; the Overview, look through the; Hail objects representing many; core concepts in genetics, and check out the many Hail functions defined; in the Python API. If you use Hail; for your own science, we’d love to hear from you on Gitter; chat or the discussion; forum.; There’s also a lot of functionality inside Hail that we didn’t get to in; this broad overview. Things like:. Flexible import and export to a variety of data and annotation; formats (VCF, BGEN, PLINK, JSON, TSV, …); Simulation; Burden tests; Kinship and pruning (IBD, GRM, RRM); Family-based tests and utilities; Distributed file system utilities; Interoperability with Python and Spark machine learning libraries; More!. For reference, here’s the full workflow to all tutorial endpoints; combined into one cell. It may take a minute! It’s doing a lot of work. In [60]:. table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); common_vds = (hc.read('data/1kg.vds'); .annotate_samples_table(table, root='sa'); .sample_qc(); .filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); .filter_genotypes('''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''); .variant_qc(); .filter_variants_expr('va.qc.AF > 0.01'); .ld_prune(memory_per_core=512, num_cores=4)). pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'); pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineCons",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:30398,Testability,test,tests,30398,"pieces of information, but this same pattern can be used to detect; effects of rare variation:. Count the number of heterozygous genotypes per gene by functional; category (synonymous, missense, or loss-of-function) to estimate; per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in; cases and controls to detect genes involved in disease. Eplilogue¶; Congrats! If you’ve made it this far, you’re perfectly primed to read; the Overview, look through the; Hail objects representing many; core concepts in genetics, and check out the many Hail functions defined; in the Python API. If you use Hail; for your own science, we’d love to hear from you on Gitter; chat or the discussion; forum.; There’s also a lot of functionality inside Hail that we didn’t get to in; this broad overview. Things like:. Flexible import and export to a variety of data and annotation; formats (VCF, BGEN, PLINK, JSON, TSV, …); Simulation; Burden tests; Kinship and pruning (IBD, GRM, RRM); Family-based tests and utilities; Distributed file system utilities; Interoperability with Python and Spark machine learning libraries; More!. For reference, here’s the full workflow to all tutorial endpoints; combined into one cell. It may take a minute! It’s doing a lot of work. In [60]:. table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); common_vds = (hc.read('data/1kg.vds'); .annotate_samples_table(table, root='sa'); .sample_qc(); .filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); .filter_genotypes('''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''); .variant_qc(); .filter_variants_expr('va.qc.AF > 0.01'); .ld_prune(memory_per_core=512, num_cores=4)). pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'); pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineCons",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:3998,Usability,learn,learn,3998,"ng a public 1000 genomes VCF to; about 50 MB. In [5]:. vds = hc.read('data/1kg.vds'). Getting to know our data¶; It’s important to have easy ways to slice, dice, query, and summarize a; dataset. Some of these methods are demonstrated below.; The; summarize; method is useful for providing a broad overview of the data contained in; a dataset. In [6]:. vds.summarize().report(). Samples: 1000; Variants: 10961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. The; query_variants; method is the first time we’ll see the Hail expression; language. The expression; language allows for a variety of incredibly expressive queries and; computations, but is probably the most complex part of Hail. See the; pair of tutorials on the expression language to learn more!; Here, we can use query_variants to pull out 5 variants to see what; they look like. In [7]:. vds.query_variants('variants.take(5)'). Out[7]:. [Variant(contig=1, start=904165, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=909917, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=986963, ref=C, alts=[AltAllele(ref=C, alt=T)]),; Variant(contig=1, start=1563691, ref=T, alts=[AltAllele(ref=T, alt=G)]),; Variant(contig=1, start=1707740, ref=T, alts=[AltAllele(ref=T, alt=G)])]. There are often several ways to do something in Hail. Here are two ways; to peek at the first few sample IDs:. In [8]:. vds.query_samples('samples.take(5)'). Out[8]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. In [9]:. vds.sample_ids[:5]. Out[9]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. There’s a similar interface for looking at the genotypes in a dataset.; We use; query_genotypes; to look at the first few genotype calls. In [10]:. vds.query_genotypes",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/hail-overview.html:30501,Usability,learn,learning,30501,"pieces of information, but this same pattern can be used to detect; effects of rare variation:. Count the number of heterozygous genotypes per gene by functional; category (synonymous, missense, or loss-of-function) to estimate; per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in; cases and controls to detect genes involved in disease. Eplilogue¶; Congrats! If you’ve made it this far, you’re perfectly primed to read; the Overview, look through the; Hail objects representing many; core concepts in genetics, and check out the many Hail functions defined; in the Python API. If you use Hail; for your own science, we’d love to hear from you on Gitter; chat or the discussion; forum.; There’s also a lot of functionality inside Hail that we didn’t get to in; this broad overview. Things like:. Flexible import and export to a variety of data and annotation; formats (VCF, BGEN, PLINK, JSON, TSV, …); Simulation; Burden tests; Kinship and pruning (IBD, GRM, RRM); Family-based tests and utilities; Distributed file system utilities; Interoperability with Python and Spark machine learning libraries; More!. For reference, here’s the full workflow to all tutorial endpoints; combined into one cell. It may take a minute! It’s doing a lot of work. In [60]:. table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); common_vds = (hc.read('data/1kg.vds'); .annotate_samples_table(table, root='sa'); .sample_qc(); .filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); .filter_genotypes('''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''); .variant_qc(); .filter_variants_expr('va.qc.AF > 0.01'); .ld_prune(memory_per_core=512, num_cores=4)). pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'); pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineCons",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:1706,Availability,avail,available,1706,"guage. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to programming are able to use it to explore; genetic data, even if this means copying motifs and expressions found on; places like Hail discussion forum.; For learning purposes, HailContext contains the method; eval_expr_ty",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:7161,Availability,error,error,7161,"only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K and V here mean any type, including other compound; types. Hail’s Array[T] objects are similar to Python’s lists, except; they must be homogenous: that is, each element must be of the same type.; Arrays are 0-indexed. Here are some examples of simple array; expressions.; Array literals are constructed with square brackets. In [28]:. hc.eval_expr_typed('[1, 2, 3, 4, 5]'). Out[28]:. ([1, 2, 3, 4, 5], Array[Int]). Arrays are indexed with square brackets and support Python’s slice; syntax. In [29]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[0]'). Out[29]:. (1, Int). In [30]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:3]'). Out[30]:. ([2, 3], Array[Int]). In [31]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:]'). Out[31]:. ([2, 3, 4, 5], Array[Int]). In [32]:. hc.eval_expr_",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:1476,Integrability,wrap,wraps,1476,"ed types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to pro",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:7167,Integrability,message,message,7167,"only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K and V here mean any type, including other compound; types. Hail’s Array[T] objects are similar to Python’s lists, except; they must be homogenous: that is, each element must be of the same type.; Arrays are 0-indexed. Here are some examples of simple array; expressions.; Array literals are constructed with square brackets. In [28]:. hc.eval_expr_typed('[1, 2, 3, 4, 5]'). Out[28]:. ([1, 2, 3, 4, 5], Array[Int]). Arrays are indexed with square brackets and support Python’s slice; syntax. In [29]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[0]'). Out[29]:. (1, Int). In [30]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:3]'). Out[30]:. ([2, 3], Array[Int]). In [31]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:]'). Out[31]:. ([2, 3, 4, 5], Array[Int]). In [32]:. hc.eval_expr_",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:460,Modifiability,variab,variables,460,"﻿. . Introduction to the Expression Language — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:5995,Modifiability,variab,variable,5995,"are converted to None in Python. In [12]:. hc.eval_expr_typed('NA: Int') # missing Int. Out[12]:. (None, Int). In [13]:. hc.eval_expr_typed('NA: Dict[String, Int]'). Out[13]:. (None, Dict[String,Int]). In [14]:. hc.eval_expr_typed('1 + NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). ",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:6124,Modifiability,variab,variable,6124,"3]:. (None, Dict[String,Int]). In [14]:. hc.eval_expr_typed('1 + NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the erro",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:6235,Modifiability,variab,variables,6235,"+ NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Comp",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:6295,Modifiability,variab,variable,6295,"ped('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K a",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:6356,Modifiability,variab,variables,6356,"ped('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K a",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:12147,Modifiability,variab,variables,12147,"h a; syntax similar to Python’s dict syntax. Struct fields are; accessed using the . syntax. In [42]:. print(hc.eval_expr_typed('{gene: ""ACBD"", function: ""LOF"", nHet: 12}')). (Struct{u'function': u'LOF', u'nHet': 12, u'gene': u'ACBD'}, Struct{gene:String,function:String,nHet:Int}). In [43]:. hc.eval_expr_typed('let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene'). Out[43]:. (u'ACBD', String). In [44]:. hc.eval_expr_typed('let s = NA: Struct { gene: String, function: String, nHet: Int} in s.gene'). Out[44]:. (None, String). Genetic Types¶; Hail contains several genetic types: -; Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call; These are designed to make it easy to manipulate genetic data. There are; many built-in functions for asking common questions about these data; types, like whether an alternate allele is a SNP, or the fraction of; reads a called genotype that belong to the reference allele. Demo variables¶; To explore these types and constructs, we have defined five; representative variables which you can access in eval_expr:. In [45]:. # 'v' is used to indicate 'Variant' in Hail; hc.eval_expr_typed('v'). Out[45]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [46]:. # 's' is used to refer to sample ID in Hail; hc.eval_expr_typed('s'). Out[46]:. (u'NA12878', String). In [47]:. # 'g' is used to refer to the genotype in Hail; hc.eval_expr_typed('g'). Out[47]:. (Genotype(GT=1, AD=[14, 0, 12], DP=26, GQ=60, PL=[60, 65, 126, 0, 67, 65]),; Genotype). In [48]:. # 'sa' is used to refer to sample annotations; hc.eval_expr_typed('sa'). Out[48]:. (Struct{u'cohort': u'1KG', u'covariates': Struct{u'PC2': -0.61512, u'PC3': 0.3166666, u'age': 34, u'PC1': 0.102312, u'isFemale': True}},; Struct{cohort:String,covariates:Struct{PC1:Double,PC2:Double,PC3:Double,age:Int,isFemale:Boolean}}). The above output is a bit wordy. Let’s try 'va':. In [49]:. # 'va' is used to refer to variant annota",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:12235,Modifiability,variab,variables,12235,"h a; syntax similar to Python’s dict syntax. Struct fields are; accessed using the . syntax. In [42]:. print(hc.eval_expr_typed('{gene: ""ACBD"", function: ""LOF"", nHet: 12}')). (Struct{u'function': u'LOF', u'nHet': 12, u'gene': u'ACBD'}, Struct{gene:String,function:String,nHet:Int}). In [43]:. hc.eval_expr_typed('let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene'). Out[43]:. (u'ACBD', String). In [44]:. hc.eval_expr_typed('let s = NA: Struct { gene: String, function: String, nHet: Int} in s.gene'). Out[44]:. (None, String). Genetic Types¶; Hail contains several genetic types: -; Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call; These are designed to make it easy to manipulate genetic data. There are; many built-in functions for asking common questions about these data; types, like whether an alternate allele is a SNP, or the fraction of; reads a called genotype that belong to the reference allele. Demo variables¶; To explore these types and constructs, we have defined five; representative variables which you can access in eval_expr:. In [45]:. # 'v' is used to indicate 'Variant' in Hail; hc.eval_expr_typed('v'). Out[45]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [46]:. # 's' is used to refer to sample ID in Hail; hc.eval_expr_typed('s'). Out[46]:. (u'NA12878', String). In [47]:. # 'g' is used to refer to the genotype in Hail; hc.eval_expr_typed('g'). Out[47]:. (Genotype(GT=1, AD=[14, 0, 12], DP=26, GQ=60, PL=[60, 65, 126, 0, 67, 65]),; Genotype). In [48]:. # 'sa' is used to refer to sample annotations; hc.eval_expr_typed('sa'). Out[48]:. (Struct{u'cohort': u'1KG', u'covariates': Struct{u'PC2': -0.61512, u'PC3': 0.3166666, u'age': 34, u'PC1': 0.102312, u'isFemale': True}},; Struct{cohort:String,covariates:Struct{PC1:Double,PC2:Double,PC3:Double,age:Int,isFemale:Boolean}}). The above output is a bit wordy. Let’s try 'va':. In [49]:. # 'va' is used to refer to variant annota",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:15560,Modifiability,variab,variables,15560,"102},; u'transcripts': [{u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE1',; u'isoform': u'GENE1.1'},; {u'canonical': True,; u'consequence': u'LOF',; u'gene': u'GENE1',; u'isoform': u'GENE1.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.1'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.3'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.1'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.2'}]},; Struct{; info: Struct{; AC: Array[Int],; AN: Int,; AF: Array[Double]; },; transcripts: Array[Struct{; gene: String,; isoform: String,; canonical: Boolean,; consequence: String; }]; }). You’ll rarely need to construct a Variant or Genotype object; inside the Hail expression language. More commonly, these objects will; be provided to you as variables. In the remainder of this notebook, we; will explore how to to manipulate the demo variables. In the next; notebook, we start using the expression langauge to annotate and filter; a dataset.; First, a short demonstration of some of the methods accessible on; Variant and Genotype objects:. In [52]:. hc.eval_expr_typed('v'). Out[52]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [53]:. hc.eval_expr_typed('v.contig'). Out[53]:. (u'16', String). In [54]:. hc.eval_expr_typed('v.start'). Out[54]:. (19200405, Int). In [55]:. hc.eval_expr_typed('v.ref'). Out[55]:. (u'C', String). In [56]:. hc.eval_expr_typed('v.altAlleles'). Out[56]:. ([AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)], Array[AltAllele]). In [57]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isSNP())'). Out[57]:. ([True, False], Array[Boolean]). In [58]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isInsertion())'). ",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:15653,Modifiability,variab,variables,15653," u'GENE1',; u'isoform': u'GENE1.1'},; {u'canonical': True,; u'consequence': u'LOF',; u'gene': u'GENE1',; u'isoform': u'GENE1.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.1'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.3'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.1'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.2'}]},; Struct{; info: Struct{; AC: Array[Int],; AN: Int,; AF: Array[Double]; },; transcripts: Array[Struct{; gene: String,; isoform: String,; canonical: Boolean,; consequence: String; }]; }). You’ll rarely need to construct a Variant or Genotype object; inside the Hail expression language. More commonly, these objects will; be provided to you as variables. In the remainder of this notebook, we; will explore how to to manipulate the demo variables. In the next; notebook, we start using the expression langauge to annotate and filter; a dataset.; First, a short demonstration of some of the methods accessible on; Variant and Genotype objects:. In [52]:. hc.eval_expr_typed('v'). Out[52]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [53]:. hc.eval_expr_typed('v.contig'). Out[53]:. (u'16', String). In [54]:. hc.eval_expr_typed('v.start'). Out[54]:. (19200405, Int). In [55]:. hc.eval_expr_typed('v.ref'). Out[55]:. (u'C', String). In [56]:. hc.eval_expr_typed('v.altAlleles'). Out[56]:. ([AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)], Array[AltAllele]). In [57]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isSNP())'). Out[57]:. ([True, False], Array[Boolean]). In [58]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isInsertion())'). Out[58]:. ([False, True], Array[Boolean]). In [59]:. hc.eval_expr_typed('g'). Out",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:4038,Safety,safe,safe,4038,"ssociated type.; Hail defines the following types:; Primitives: - Int -; Double -; Float -; Long -; Boolean -; String; Compound Types: - Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; Genetic Types: - Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call. Primitive Types¶; Let’s start with simple primitive types. Primitive types are a basic; building block for any programming language - these are things like; numbers and strings and boolean values.; Hail expressions are passed as Python strings to Hail methods. In [2]:. # the Boolean literals are 'true' and 'false'; hc.eval_expr_typed('true'). Out[2]:. (True, Boolean). The return value is True, not true. Why? When values are; returned by Hail methods, they are returned as the corresponding Python; value. In [3]:. hc.eval_expr_typed('123'). Out[3]:. (123, Int). In [4]:. hc.eval_expr_typed('123.45'). Out[4]:. (123.45, Double). String literals are denoted with double-quotes. The ‘u’ preceding the; printed result denotes a unicode string, and is safe to ignore. In [5]:. hc.eval_expr_typed('""Hello, world""'). Out[5]:. (u'Hello, world', String). Primitive types support all the usual operations you’d expect. For; details, refer to the documentation on; operators and; types. Here are some examples. In [6]:. hc.eval_expr_typed('3 + 8'). Out[6]:. (11, Int). In [7]:. hc.eval_expr_typed('3.2 * 0.5'). Out[7]:. (1.6, Double). In [8]:. hc.eval_expr_typed('3 ** 3'). Out[8]:. (27.0, Double). In [9]:. hc.eval_expr_typed('25 ** 0.5'). Out[9]:. (5.0, Double). In [10]:. hc.eval_expr_typed('true || false'). Out[10]:. (True, Boolean). In [11]:. hc.eval_expr_typed('true && false'). Out[11]:. (False, Boolean). Missingness¶; Like R, all values in Hail can be missing. Most operations, like; addition, return missing if any of their inputs is missing. There are a; few special operations for manipulating missing values. There is also a; missing literal, but you have to specify it’s type. Missing Hail values; are converted to ",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:1512,Security,access,accessed,1512,"ed types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to pro",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:11274,Security,access,accessed,11274,"ulates the sum of the squared residuals (x; - mean) of an array. In [41]:. # Uncomment the below code by deleting the triple-quotes and write an expression to calculate the residuals. """"""; result, t = hc.eval_expr_typed('''; let a = [1, -2, 11, 3, -2]; and mean = <FILL IN>; in a.map(x => <FILL IN> ).sum(); '''); """""". try:; print('Your result: %s (%s)' % (result, t)); print('Expected answer: 114.8 (Double)'); except NameError:; print('### Remove the triple quotes around the above code to start the exercise ### '). ### Remove the triple quotes around the above code to start the exercise ###. What if a contains a missing value NA: Int? Will your code still; work?. Structs¶; Structs are a collection of named values known as fields. Hail; does not have tuples like Python. Unlike arrays, the values can be; heterogenous. Unlike Dicts, the set of names are part of the type; and must be known statically. Structs are constructed with a; syntax similar to Python’s dict syntax. Struct fields are; accessed using the . syntax. In [42]:. print(hc.eval_expr_typed('{gene: ""ACBD"", function: ""LOF"", nHet: 12}')). (Struct{u'function': u'LOF', u'nHet': 12, u'gene': u'ACBD'}, Struct{gene:String,function:String,nHet:Int}). In [43]:. hc.eval_expr_typed('let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene'). Out[43]:. (u'ACBD', String). In [44]:. hc.eval_expr_typed('let s = NA: Struct { gene: String, function: String, nHet: Int} in s.gene'). Out[44]:. (None, String). Genetic Types¶; Hail contains several genetic types: -; Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call; These are designed to make it easy to manipulate genetic data. There are; many built-in functions for asking common questions about these data; types, like whether an alternate allele is a SNP, or the fraction of; reads a called genotype that belong to the reference allele. Demo variables¶; To explore these types and constructs, we have defined five; representative variables which you can access in eval_",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:12259,Security,access,access,12259,"h a; syntax similar to Python’s dict syntax. Struct fields are; accessed using the . syntax. In [42]:. print(hc.eval_expr_typed('{gene: ""ACBD"", function: ""LOF"", nHet: 12}')). (Struct{u'function': u'LOF', u'nHet': 12, u'gene': u'ACBD'}, Struct{gene:String,function:String,nHet:Int}). In [43]:. hc.eval_expr_typed('let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene'). Out[43]:. (u'ACBD', String). In [44]:. hc.eval_expr_typed('let s = NA: Struct { gene: String, function: String, nHet: Int} in s.gene'). Out[44]:. (None, String). Genetic Types¶; Hail contains several genetic types: -; Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call; These are designed to make it easy to manipulate genetic data. There are; many built-in functions for asking common questions about these data; types, like whether an alternate allele is a SNP, or the fraction of; reads a called genotype that belong to the reference allele. Demo variables¶; To explore these types and constructs, we have defined five; representative variables which you can access in eval_expr:. In [45]:. # 'v' is used to indicate 'Variant' in Hail; hc.eval_expr_typed('v'). Out[45]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [46]:. # 's' is used to refer to sample ID in Hail; hc.eval_expr_typed('s'). Out[46]:. (u'NA12878', String). In [47]:. # 'g' is used to refer to the genotype in Hail; hc.eval_expr_typed('g'). Out[47]:. (Genotype(GT=1, AD=[14, 0, 12], DP=26, GQ=60, PL=[60, 65, 126, 0, 67, 65]),; Genotype). In [48]:. # 'sa' is used to refer to sample annotations; hc.eval_expr_typed('sa'). Out[48]:. (Struct{u'cohort': u'1KG', u'covariates': Struct{u'PC2': -0.61512, u'PC3': 0.3166666, u'age': 34, u'PC1': 0.102312, u'isFemale': True}},; Struct{cohort:String,covariates:Struct{PC1:Double,PC2:Double,PC3:Double,age:Int,isFemale:Boolean}}). The above output is a bit wordy. Let’s try 'va':. In [49]:. # 'va' is used to refer to variant annota",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:15814,Security,access,accessible,15814,"E2',; u'isoform': u'GENE2.1'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.3'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.1'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.2'}]},; Struct{; info: Struct{; AC: Array[Int],; AN: Int,; AF: Array[Double]; },; transcripts: Array[Struct{; gene: String,; isoform: String,; canonical: Boolean,; consequence: String; }]; }). You’ll rarely need to construct a Variant or Genotype object; inside the Hail expression language. More commonly, these objects will; be provided to you as variables. In the remainder of this notebook, we; will explore how to to manipulate the demo variables. In the next; notebook, we start using the expression langauge to annotate and filter; a dataset.; First, a short demonstration of some of the methods accessible on; Variant and Genotype objects:. In [52]:. hc.eval_expr_typed('v'). Out[52]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [53]:. hc.eval_expr_typed('v.contig'). Out[53]:. (u'16', String). In [54]:. hc.eval_expr_typed('v.start'). Out[54]:. (19200405, Int). In [55]:. hc.eval_expr_typed('v.ref'). Out[55]:. (u'C', String). In [56]:. hc.eval_expr_typed('v.altAlleles'). Out[56]:. ([AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)], Array[AltAllele]). In [57]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isSNP())'). Out[57]:. ([True, False], Array[Boolean]). In [58]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isInsertion())'). Out[58]:. ([False, True], Array[Boolean]). In [59]:. hc.eval_expr_typed('g'). Out[59]:. (Genotype(GT=1, AD=[14, 0, 12], DP=26, GQ=60, PL=[60, 65, 126, 0, 67, 65]),; Genotype). In [60]:. hc.eval_expr_typed('g.dp'). Out[60]:. (26, Int). In [61]:. hc.eval_expr_typed('g.ad'). O",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:5271,Testability,test,test,5271,". In [6]:. hc.eval_expr_typed('3 + 8'). Out[6]:. (11, Int). In [7]:. hc.eval_expr_typed('3.2 * 0.5'). Out[7]:. (1.6, Double). In [8]:. hc.eval_expr_typed('3 ** 3'). Out[8]:. (27.0, Double). In [9]:. hc.eval_expr_typed('25 ** 0.5'). Out[9]:. (5.0, Double). In [10]:. hc.eval_expr_typed('true || false'). Out[10]:. (True, Boolean). In [11]:. hc.eval_expr_typed('true && false'). Out[11]:. (False, Boolean). Missingness¶; Like R, all values in Hail can be missing. Most operations, like; addition, return missing if any of their inputs is missing. There are a; few special operations for manipulating missing values. There is also a; missing literal, but you have to specify it’s type. Missing Hail values; are converted to None in Python. In [12]:. hc.eval_expr_typed('NA: Int') # missing Int. Out[12]:. (None, Int). In [13]:. hc.eval_expr_typed('NA: Dict[String, Int]'). Out[13]:. (None, Dict[String,Int]). In [14]:. hc.eval_expr_typed('1 + NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. ",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:2648,Usability,learn,learning,2648,"ark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to programming are able to use it to explore; genetic data, even if this means copying motifs and expressions found on; places like Hail discussion forum.; For learning purposes, HailContext contains the method; eval_expr_typed.; This method takes a Python string of Hail expr code, evaluates it, and; returns a tuple with the result and the type. We’ll be using this method; throughout the expression language tutorial. Hail Types¶; The Hail expression language is strongly typed, meaning that every; expression has an associated type.; Hail defines the following types:; Primitives: - Int -; Double -; Float -; Long -; Boolean -; String; Compound Types: - Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; Genetic Types: - Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call. Primitive Types¶; Let’s start with simple primitive types. Primitive types are a basic; building block for any programming language - these are things like; numbers and strings and boolean values.; Hail expressions are passed as Python strings to Hail methods. In [2]:. # the Boolean literals are 'true' and 'false'; hc.eval_expr_typed('true'). Out[2]:. (True, Boolean). The return value is T",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:3322,Usability,simpl,simple,3322,"n or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to programming are able to use it to explore; genetic data, even if this means copying motifs and expressions found on; places like Hail discussion forum.; For learning purposes, HailContext contains the method; eval_expr_typed.; This method takes a Python string of Hail expr code, evaluates it, and; returns a tuple with the result and the type. We’ll be using this method; throughout the expression language tutorial. Hail Types¶; The Hail expression language is strongly typed, meaning that every; expression has an associated type.; Hail defines the following types:; Primitives: - Int -; Double -; Float -; Long -; Boolean -; String; Compound Types: - Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; Genetic Types: - Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call. Primitive Types¶; Let’s start with simple primitive types. Primitive types are a basic; building block for any programming language - these are things like; numbers and strings and boolean values.; Hail expressions are passed as Python strings to Hail methods. In [2]:. # the Boolean literals are 'true' and 'false'; hc.eval_expr_typed('true'). Out[2]:. (True, Boolean). The return value is True, not true. Why? When values are; returned by Hail methods, they are returned as the corresponding Python; value. In [3]:. hc.eval_expr_typed('123'). Out[3]:. (123, Int). In [4]:. hc.eval_expr_typed('123.45'). Out[4]:. (123.45, Double). String literals are denoted with double-quotes. The ‘u’ preceding the; printed result denotes a unicode string, and is safe to ignore. In [5]:. hc.eval_expr_typed('""Hello, world""'). Out[5]:. (u'Hello, world', String). Primitive types support all the usual operations you’d expect. For; details, refer to the documentation on; operators and; types. Here are some examples. In [6]:. hc.eval_expr_typ",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:6772,Usability,simpl,simple,6772,"rElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K and V here mean any type, including other compound; types. Hail’s Array[T] objects are similar to Python’s lists, except; they must be homogenous: that is, each element must be of the same type.; Arrays are 0-indexed. Here are some examples of simple array; expressions.; Array literals are constructed with square brackets. In [28]:. hc.eval_expr_typed('[1, 2, 3, 4, 5]'). Out[28]:. ([1, 2, 3, 4, 5], Array[Int]). Arrays are inde",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:7587,Usability,simpl,simple,7587,"The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K and V here mean any type, including other compound; types. Hail’s Array[T] objects are similar to Python’s lists, except; they must be homogenous: that is, each element must be of the same type.; Arrays are 0-indexed. Here are some examples of simple array; expressions.; Array literals are constructed with square brackets. In [28]:. hc.eval_expr_typed('[1, 2, 3, 4, 5]'). Out[28]:. ([1, 2, 3, 4, 5], Array[Int]). Arrays are indexed with square brackets and support Python’s slice; syntax. In [29]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[0]'). Out[29]:. (1, Int). In [30]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:3]'). Out[30]:. ([2, 3], Array[Int]). In [31]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:]'). Out[31]:. ([2, 3, 4, 5], Array[Int]). In [32]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a.length()'). Out[32]:. (5, Int). Arrays can be transformed with functional operators filter and; map. These operations return a new array, never modify the original. In [33]:. # keep the elements that are less than 10; hc.eval_expr_typed('let a = [1, 2, 22, 7, 10, 11] in a.filter(x => x < 10)'). Out[33]:. ([1, 2, 7], Array[Int]). In [34]:. # square the elements of an array; hc.eval_expr_typed('let a = [1",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
https://hail.is/docs/0.1/_modules/index.html:318,Availability,avail,available,318,"﻿. . Overview: module code — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Overview: module code. All modules for which code is available; hail.context; hail.dataset; hail.expr; hail.keytable; hail.kinshipMatrix; hail.ldMatrix; hail.representation.annotations; hail.representation.genotype; hail.representation.interval; hail.representation.pedigree; hail.representation.variant; hail.utils. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/index.html
https://hail.is/docs/0.1/_modules/hail/context.html:1486,Availability,error,error,1486,"om pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change conf",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:3277,Availability,error,errors,3277,"=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.ver",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:3313,Availability,error,error,3313,"=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.ver",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:3500,Availability,avail,available,3500,"d(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Exam",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:3991,Availability,recover,recovery,3991,"r, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:4011,Availability,recover,recover,4011," = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regula",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:5410,Availability,toler,tolerance,5410," path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at `RegExr <http://regexr.com/>`__. :param str regex: The regular expression to match. :param path: The files to search.; :type path: str or list of str. :param int max_count: The maximum number of matches to return.; """""". self._jhc.grep(regex, jindexed_seq_args(path), max_count). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; tolerance=numeric,; sample_file=nullable(strlike),; min_partitions=nullable(integral)); def import_bgen(self, path, tolerance=0.2, sample_file=None, min_partitions=None):; """"""Import .bgen file(s) as variant dataset. **Examples**. Importing a BGEN file as a VDS (assuming it has already been indexed). >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). **Notes**. Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Pa",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:5526,Availability,toler,tolerance,5526," path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at `RegExr <http://regexr.com/>`__. :param str regex: The regular expression to match. :param path: The files to search.; :type path: str or list of str. :param int max_count: The maximum number of matches to return.; """""". self._jhc.grep(regex, jindexed_seq_args(path), max_count). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; tolerance=numeric,; sample_file=nullable(strlike),; min_partitions=nullable(integral)); def import_bgen(self, path, tolerance=0.2, sample_file=None, min_partitions=None):; """"""Import .bgen file(s) as variant dataset. **Examples**. Importing a BGEN file as a VDS (assuming it has already been indexed). >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). **Notes**. Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Pa",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:6856,Availability,toler,tolerance,6856," Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. .. _gpfilters:. **Genotype probability (``gp``) representation**:. The following modifications are made to genotype probabilities in BGEN v1.1 files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file:",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:6900,Availability,toler,tolerance,6900,"mat,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. .. _gpfilters:. **Genotype probability (``gp``) representation**:. The following modifications are made to genotype probabilities in BGEN v1.1 files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:7576,Availability,toler,tolerance,7576,"files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_fil",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:7668,Availability,toler,tolerance,7668,"lity distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load mu",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:8068,Availability,toler,tolerance,8068,"l probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with :ref:`Hadoop glob patterns <sec-hadoop-glob>`:. >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). **Notes**. For more information on the .gen file format, see `here <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. To ensure that the .gen file(s) and ",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:8250,Availability,toler,tolerance,8250," following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with :ref:`Hadoop glob patterns <sec-hadoop-glob>`:. >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). **Notes**. For more information on the .gen file format, see `here <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. To ensure that the .gen file(s) and .sample file are correctly prepared for import:. - If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome u",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:8382,Availability,toler,tolerance,8382," following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with :ref:`Hadoop glob patterns <sec-hadoop-glob>`:. >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). **Notes**. For more information on the .gen file format, see `here <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. To ensure that the .gen file(s) and .sample file are correctly prepared for import:. - If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome u",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:9984,Availability,toler,tolerance,9984,"l#mozTocId40300>`__. To ensure that the .gen file(s) and .sample file are correctly prepared for import:. - If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the ``chromosome`` parameter. - No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID ``s``. Also, see section in :py:meth:`~hail.HailContext.import_bgen` linked :ref:`here <gpfilters>` for information about Hail's genotype probability representation. **Annotations**. :py:meth:`~hail.HailContext.import_gen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .gen files to import.; :type path: str or list of str. :param str sample_file: The sample file. :param float tolerance: If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param chromosome: Chromosome if not listed in the .gen file.; :type chromosome: str or None. :return: Variant dataset imported from .gen and .sample files.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importGens(jindexed_seq_args(path), sample_file, joption(chromosome), joption(min_partitions),; tolerance); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(paths=oneof(strlike, listof(strlike)),; key=oneof(strlike, listof(strlike)),; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=nullable(strlike),; delimiter=strlike,; missing=strlike,; types=dictof(strlike, Type),; quote=nullable(char)); def import_table(self, paths, key=[], min_partitions=None, impute=False, no_header=False,; comment=None, delimiter=""\t"", missing=""NA"", types={}, quote=None):;",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:10084,Availability,toler,tolerance,10084,"rectly prepared for import:. - If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the ``chromosome`` parameter. - No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID ``s``. Also, see section in :py:meth:`~hail.HailContext.import_bgen` linked :ref:`here <gpfilters>` for information about Hail's genotype probability representation. **Annotations**. :py:meth:`~hail.HailContext.import_gen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .gen files to import.; :type path: str or list of str. :param str sample_file: The sample file. :param float tolerance: If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param chromosome: Chromosome if not listed in the .gen file.; :type chromosome: str or None. :return: Variant dataset imported from .gen and .sample files.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importGens(jindexed_seq_args(path), sample_file, joption(chromosome), joption(min_partitions),; tolerance); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(paths=oneof(strlike, listof(strlike)),; key=oneof(strlike, listof(strlike)),; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=nullable(strlike),; delimiter=strlike,; missing=strlike,; types=dictof(strlike, Type),; quote=nullable(char)); def import_table(self, paths, key=[], min_partitions=None, impute=False, no_header=False,; comment=None, delimiter=""\t"", missing=""NA"", types={}, quote=None):; """"""Import delimited text file (text table) as key table. The resulting key ",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:10515,Availability,toler,tolerance,10515,"<gpfilters>` for information about Hail's genotype probability representation. **Annotations**. :py:meth:`~hail.HailContext.import_gen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .gen files to import.; :type path: str or list of str. :param str sample_file: The sample file. :param float tolerance: If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param chromosome: Chromosome if not listed in the .gen file.; :type chromosome: str or None. :return: Variant dataset imported from .gen and .sample files.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importGens(jindexed_seq_args(path), sample_file, joption(chromosome), joption(min_partitions),; tolerance); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(paths=oneof(strlike, listof(strlike)),; key=oneof(strlike, listof(strlike)),; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=nullable(strlike),; delimiter=strlike,; missing=strlike,; types=dictof(strlike, Type),; quote=nullable(char)); def import_table(self, paths, key=[], min_partitions=None, impute=False, no_header=False,; comment=None, delimiter=""\t"", missing=""NA"", types={}, quote=None):; """"""Import delimited text file (text table) as key table. The resulting key table will have no key columns, use :py:meth:`.KeyTable.key_by`; to specify keys.; ; **Example**; ; Given this file. .. code-block:: text. $ cat data/samples1.tsv; Sample	Height	Status Age; PT-1234	154.1	ADHD	24; PT-1236	160.9	Control	19; PT-1238	NA	ADHD	89; PT-1239	170.3	Control	55. The interesting thing about this table is that column ``Height`` is a floating-point number, ; and column ``Age",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:24794,Availability,down,downstream,24794,"s(""VQSRTranche99.5..."")``. Variants that are flagged as ""PASS"" ; will have no filters applied; for these variants, ``va.filters.isEmpty()`` is true. Thus, ; filtering to PASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fi",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:33108,Availability,down,down,33108,"TruncatedBetaDist`. :param int seed: Random seed. :return: Variant dataset simulated using the Balding-Nichols model.; :rtype: :class:`.VariantDataset`; """""". if pop_dist is None:; jvm_pop_dist_opt = joption(pop_dist); else:; jvm_pop_dist_opt = joption(jarray(self._jvm.double, pop_dist)). if fst is None:; jvm_fst_opt = joption(fst); else:; jvm_fst_opt = joption(jarray(self._jvm.double, fst)). jvds = self._jhc.baldingNicholsModel(populations, samples, variants,; joption(num_partitions),; jvm_pop_dist_opt,; jvm_fst_opt,; af_dist._jrep(),; seed); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(expr=strlike); def eval_expr_typed(self, expr):; """"""Evaluate an expression and return the result as well as its type. :param str expr: Expression to evaluate. :rtype: (annotation, :class:`.Type`). """""". x = self._jhc.eval(expr); t = Type._from_java(x._2()); v = t._convert_to_py(x._1()); return (v, t). [docs] @handle_py4j; @typecheck_method(expr=strlike); def eval_expr(self, expr):; """"""Evaluate an expression. :param str expr: Expression to evaluate. :rtype: annotation; """""". r, t = self.eval_expr_typed(expr); return r. [docs] def stop(self):; """""" Shut down the Hail context. It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context.; """""". self.sc.stop(); self.sc = None; Env._jvm = None; Env._gateway = None; Env._hc = None. [docs] @handle_py4j; @typecheck_method(path=strlike); def read_table(self, path):; """"""Read a KT file as key table. :param str path: KT file to read. :return: Key table read from disk.; :rtype: :class:`.KeyTable`; """""". jkt = self._jhc.readTable(path); return KeyTable(self, jkt). [docs] @handle_py4j; def report(self):; """"""Print information and warnings about VCF + GEN import and deduplication.""""""; self._jhc.report(). © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:1052,Deployability,configurat,configuration,1052,". HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.context. Source code for hail.context; from __future__ import print_function # Python 2 and 3 print compatibility. from hail.typecheck import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=s",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:2453,Deployability,configurat,configuration,2453,"aram log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:29951,Deployability,continuous,continuous,29951,"titions defaults to one partition per million genotypes (i.e., samples * variants / 10^6) or 8, whichever is larger. The Balding-Nichols model models genotypes of individuals from a structured population comprising :math:`K` homogeneous subpopulations; that have each diverged from a single ancestral population (a `star phylogeny`). We take :math:`N` samples and :math:`M` bi-allelic variants in perfect; linkage equilibrium. The relative sizes of the subpopulations are given by a probability vector :math:`\pi`; the ancestral allele frequencies are; drawn independently from a frequency spectrum :math:`P_0`; the subpopulations have diverged with possibly different :math:`F_{ST}` parameters :math:`F_k`; (here and below, lowercase indices run over a range bounded by the corresponding uppercase parameter, e.g. :math:`k = 1, \ldots, K`).; For each variant, the subpopulation allele frequencies are drawn a `beta distribution <https://en.wikipedia.org/wiki/Beta_distribution>`__, a useful continuous approximation of; the effect of genetic drift. We denote the individual subpopulation memberships by :math:`k_n`, the ancestral allele frequences by :math:`p_{0, m}`,; the subpopulation allele frequencies by :math:`p_{k, m}`, and the genotypes by :math:`g_{n, m}`. The generative model in then given by:. .. math::; k_n \,&\sim\, \pi. p_{0,m}\,&\sim\, P_0. p_{k,m}\mid p_{0,m}\,&\sim\, \mathrm{Beta}(\mu = p_{0,m},\, \sigma^2 = F_k p_{0,m}(1 - p_{0,m})). g_{n,m}\mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}). We have parametrized the beta distribution by its mean and variance; the usual parameters are :math:`a = (1 - p)(1 - F)/F,\; b = p(1-F)/F` with :math:`F = F_k,\; p = p_{0,m}`. **Annotations**. :py:meth:`~hail.HailContext.balding_nichols_model` adds the following global, sample, and variant annotations:. - **global.nPops** (*Int*) -- Number of populations; - **global.nSamples** (*Int*) -- Number of samples; - **global.nVariants** (*Int*) -- Number of variants; - **glob",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:2798,Integrability,rout,routed,2798,"ry for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmetho",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:14843,Integrability,depend,depends,14843,"missing`` parameters are **NOT** regexes. The ``no_header`` option indicates that the file has no header line. If this option is passed, ; then the column names will be ``f0``, ``f1``, ... ``fN`` (0-indexed). ; ; The ``types`` option allows the user to pass the types of columns in the table. This is a ; dict keyed by ``str``, with :py:class:`~hail.expr.Type` values. See the examples above for; a standard usage. Additionally, this option can be used to override type imputation. For example,; if a column in a file refers to chromosome and does not contain any sex chromosomes, it will be; imputed as an integer, while most Hail methods expect chromosome to be passed as a string. Using; the ``impute=True`` mode and passing ``types={'Chromosome': TString()}`` will solve this problem.; ; The ``min_partitions`` option can be used to increase the number of partitions (level of sharding); of an imported table. The default partition size depends on file system and a number of other ; factors (including the ``min_block_size`` of the hail context), but usually is between 32M and 128M.; ; :param paths: Files to import.; :type paths: str or list of str. :param key: Key column(s).; :type key: str or list of str. :param min_partitions: Minimum number of partitions.; :type min_partitions: int or None. :param bool no_header: File has no header and the N columns are named ``f0``, ``f1``, ... ``fN`` (0-indexed); ; :param bool impute: Impute column types from the file; ; :param comment: Skip lines beginning with the given pattern; :type comment: str or None; ; :param str delimiter: Field delimiter regex; ; :param str missing: Specify identifier to be treated as missing; ; :param types: Define types of fields in annotations files ; :type types: dict with str keys and :py:class:`.Type` values; ; :return: Key table constructed from text table.; :rtype: :class:`.KeyTable`. :param quote: Quote character; :type quote: str or None; """""". key = wrap_to_list(key); paths = wrap_to_list(paths); jtype",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:1052,Modifiability,config,configuration,1052,". HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.context. Source code for hail.context; from __future__ import print_function # Python 2 and 3 print compatibility. from hail.typecheck import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=s",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:2453,Modifiability,config,configuration,2453,"aram log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:6373,Performance,load,load,6373,"ance=numeric,; sample_file=nullable(strlike),; min_partitions=nullable(integral)); def import_bgen(self, path, tolerance=0.2, sample_file=None, min_partitions=None):; """"""Import .bgen file(s) as variant dataset. **Examples**. Importing a BGEN file as a VDS (assuming it has already been indexed). >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). **Notes**. Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. .. _gpfilters:. **Genotype probability (``gp``) representation**:. The following modifications are made to genotype probabilities in BGEN v1.1 files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*St",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:19081,Performance,load,loading,19081,"g``. :param str bed: PLINK BED file. :param str bim: PLINK BIM file. :param str fam: PLINK FAM file. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param str missing: The string used to denote missing values **only** for the phenotype field. This is in addition to ""-9"", ""0"", and ""N/A"" for case-control phenotypes. :param str delimiter: FAM file field delimiter regex. :param bool quantpheno: If True, FAM phenotype is interpreted as quantitative. :return: Variant dataset imported from PLINK binary file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importPlink(bed, bim, fam, joption(min_partitions), delimiter, missing, quantpheno). return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; drop_samples=bool,; drop_variants=bool); def read(self, path, drop_samples=False, drop_variants=False):; """"""Read .vds files as variant dataset. When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. :param path: VDS files to read.; :type path: str or list of str. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations; or gneotypes. :param bool drop_variants: If True, create samples-only variant; dataset (no variants or genotypes). :return: Variant dataset read from disk.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(; self,; self._jhc.readAll(jindexed_seq_args(path), drop_samples, drop_variants)). [docs] @handle_py4j; @typecheck_method(path=strlike); def write_partitioning(self, path):; """"""Write partitioning.json.gz file for legacy VDS file. :param str path: path to VDS file.; """""". self._jhc.writePartitioning(path). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; force=bool,; force_bgz=bool,; header_file=nullable(strlike),; min_partitions=nullable(integral),; drop_samples=bool,; store_gq=bool,; pp_as_pl=bool,; skip_ba",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:19340,Performance,load,load,19340,"te missing values **only** for the phenotype field. This is in addition to ""-9"", ""0"", and ""N/A"" for case-control phenotypes. :param str delimiter: FAM file field delimiter regex. :param bool quantpheno: If True, FAM phenotype is interpreted as quantitative. :return: Variant dataset imported from PLINK binary file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importPlink(bed, bim, fam, joption(min_partitions), delimiter, missing, quantpheno). return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; drop_samples=bool,; drop_variants=bool); def read(self, path, drop_samples=False, drop_variants=False):; """"""Read .vds files as variant dataset. When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. :param path: VDS files to read.; :type path: str or list of str. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations; or gneotypes. :param bool drop_variants: If True, create samples-only variant; dataset (no variants or genotypes). :return: Variant dataset read from disk.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(; self,; self._jhc.readAll(jindexed_seq_args(path), drop_samples, drop_variants)). [docs] @handle_py4j; @typecheck_method(path=strlike); def write_partitioning(self, path):; """"""Write partitioning.json.gz file for legacy VDS file. :param str path: path to VDS file.; """""". self._jhc.writePartitioning(path). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; force=bool,; force_bgz=bool,; header_file=nullable(strlike),; min_partitions=nullable(integral),; drop_samples=bool,; store_gq=bool,; pp_as_pl=bool,; skip_bad_ad=bool,; generic=bool,; call_fields=oneof(strlike, listof(strlike))); def import_vcf(self, path, force=False, force_bgz=False, header_file=None, min_partitions=None,; drop_samples=False, store_gq=False, pp_as_pl=False, skip",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:20729,Performance,load,load,20729,"ethod(path=strlike); def write_partitioning(self, path):; """"""Write partitioning.json.gz file for legacy VDS file. :param str path: path to VDS file.; """""". self._jhc.writePartitioning(path). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; force=bool,; force_bgz=bool,; header_file=nullable(strlike),; min_partitions=nullable(integral),; drop_samples=bool,; store_gq=bool,; pp_as_pl=bool,; skip_bad_ad=bool,; generic=bool,; call_fields=oneof(strlike, listof(strlike))); def import_vcf(self, path, force=False, force_bgz=False, header_file=None, min_partitions=None,; drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False,; call_fields=[]):; """"""Import VCF file(s) as variant dataset. **Examples**. >>> vds = hc.import_vcf('data/example2.vcf.bgz'). **Notes**. Hail is designed to be maximally compatible with files in the `VCF v4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. :py:meth:`~hail.HailContext.import_vcf` takes a list of VCF files to load. All files must have the same header and the same set of samples in the same order; (e.g., a variant dataset split by chromosome). Files can be specified as :ref:`Hadoop glob patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should either be uncompressed (*.vcf*) or block compressed; (*.vcf.bgz*). If you have a large compressed VCF that ends in *.vcf.gz*, it is likely that the file is actually block-compressed,; and you should rename the file to "".vcf.bgz"" accordingly. If you actually have a standard gzipped file, it is possible to import; it to Hail using the ``force`` optional parameter. However, this is not recommended -- all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer. If ``generic`` equals False (default), Hail makes certain assumptions about the genotype fields, see :class:`Representation <hail.representation.Gen",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:22593,Performance,perform,performance,22593,"onger. If ``generic`` equals False (default), Hail makes certain assumptions about the genotype fields, see :class:`Representation <hail.representation.Genotype>`. On import, Hail filters; (sets to no-call) any genotype that violates these assumptions. Hail interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped. If ``generic`` equals True, the genotype schema is a :py:class:`~hail.type.TStruct` with field names equal to the IDs of the FORMAT fields.; The ``GT`` field is automatically read in as a :py:class:`~hail.type.TCall` type. To specify additional fields to import as a; :py:class:`~hail.type.TCall` type, use the ``call_fields`` parameter. All other fields are imported as the type specified in the FORMAT header field. An example genotype schema after importing a VCF with ``generic=True`` is. .. code-block:: text. Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. .. warning::. - The variant dataset generated with ``generic=True`` will have significantly slower performance. - Not all :py:class:`.VariantDataset` methods will work with a generic genotype schema. - The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. :py:meth:`~hail.HailContext.import_vcf` does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant. Since Hail's genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If ``generic=False``, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS ""genotype schema. Below are two example haploid genotypes and diploid equivalents that Hail sees. .. code-block:: text. Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:22887,Performance,perform,perform,22887,"type.TStruct` with field names equal to the IDs of the FORMAT fields.; The ``GT`` field is automatically read in as a :py:class:`~hail.type.TCall` type. To specify additional fields to import as a; :py:class:`~hail.type.TCall` type, use the ``call_fields`` parameter. All other fields are imported as the type specified in the FORMAT header field. An example genotype schema after importing a VCF with ``generic=True`` is. .. code-block:: text. Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. .. warning::. - The variant dataset generated with ``generic=True`` will have significantly slower performance. - Not all :py:class:`.VariantDataset` methods will work with a generic genotype schema. - The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. :py:meth:`~hail.HailContext.import_vcf` does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant. Since Hail's genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If ``generic=False``, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS ""genotype schema. Below are two example haploid genotypes and diploid equivalents that Hail sees. .. code-block:: text. Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:0,0,9:9:24:24,1000,40,1000:1000:0. .. note::; ; Using the **FILTER** field:; ; The information in the FILTER field of a VCF is contained in the ``va.filters`` annotation.; This annotation is a ``Set`` and can be queried for filter membership with expressions ; like ``va.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged as ""PASS"" ; will have no filters applied; for these variants, ``va.filters.isE",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:24750,Performance,load,load,24750,"ation is a ``Set`` and can be queried for filter membership with expressions ; like ``va.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged as ""PASS"" ; will have no filters applied; for these variants, ``va.filters.isEmpty()`` is true. Thus, ; filtering to PASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:24944,Performance,load,load,24944,"plied; for these variants, ``va.filters.isEmpty()`` is true. Thus, ; filtering to PASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:25017,Performance,load,load,25017,"ASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``.; :type call_fields: str or list of str. :return: Variant dataset imported from V",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:25280,Performance,load,load,25280,"(*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``.; :type call_fields: str or list of str. :return: Variant dataset imported from VCF file(s); :rtype: :py:class:`.VariantDataset`. """""". if generic:; jvds = self._jhc.importVCFsGeneric(jindexed_seq_args(path), force, force_bgz, joption(header_file),; joption(min_partitions), drop_samples, jset_args(call_fields)); else:; jvds = self._jhc.importVCFs(jindexed_seq_args(path",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:3991,Safety,recover,recovery,3991,"r, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:4011,Safety,recover,recover,4011," = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regula",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:1408,Testability,log,log,1408,"import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart s",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:1454,Testability,log,logging,1454,"om pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change conf",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:1524,Testability,log,log,1524,"ataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = S",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:1981,Testability,log,log,1981,"e that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc i",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:2201,Testability,log,log,2201,"e that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc i",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:2211,Testability,log,log,2211,"ontext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case some",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:2915,Testability,log,log,2915,"ke,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forg",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:16536,Testability,test,test,16536,"ssing: Specify identifier to be treated as missing; ; :param types: Define types of fields in annotations files ; :type types: dict with str keys and :py:class:`.Type` values; ; :return: Key table constructed from text table.; :rtype: :class:`.KeyTable`. :param quote: Quote character; :type quote: str or None; """""". key = wrap_to_list(key); paths = wrap_to_list(paths); jtypes = {k: v._jtype for k, v in types.items()}. jkt = self._jhc.importTable(paths, key, min_partitions, jtypes, comment, delimiter, missing,; no_header, impute, quote); return KeyTable(self, jkt). [docs] @handle_py4j; @typecheck_method(bed=strlike,; bim=strlike,; fam=strlike,; min_partitions=nullable(integral),; delimiter=strlike,; missing=strlike,; quantpheno=bool); def import_plink(self, bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False):; """"""Import PLINK binary file (BED, BIM, FAM) as variant dataset. **Examples**. Import data from a PLINK binary file:. >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). **Notes**. Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the ``--make-bed`` option. The centiMorgan position is not currently used in Hail (Column 3 in BIM file). The ID (``s``) used by Hail is the individual ID (column 2 in FAM file). .. warning::. No duplicate individual IDs are allowed. Chromosome names (Column 1) are automatically converted in the following cases:. - 23 => ""X""; - 24 => ""Y""; - 25 => ""X""; - 26 => ""MT"". **Annotations**. :py:meth:`~hail.HailContext.import_plink` adds the following annotations:. - **va.rsid** (*String*) -- Column 2 in the BIM file.; - **sa.famID** (*String*) -- Column 1 in the FAM file. Set to missing if ID equals ""0"".; - **sa.patID** (*String*) -- Column 3 in the FAM file. Set to missing if ID equals ""0"".; - **sa.matID** (*String*) -- Column 4 in the FAM file. Set",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:16562,Testability,test,test,16562,"treated as missing; ; :param types: Define types of fields in annotations files ; :type types: dict with str keys and :py:class:`.Type` values; ; :return: Key table constructed from text table.; :rtype: :class:`.KeyTable`. :param quote: Quote character; :type quote: str or None; """""". key = wrap_to_list(key); paths = wrap_to_list(paths); jtypes = {k: v._jtype for k, v in types.items()}. jkt = self._jhc.importTable(paths, key, min_partitions, jtypes, comment, delimiter, missing,; no_header, impute, quote); return KeyTable(self, jkt). [docs] @handle_py4j; @typecheck_method(bed=strlike,; bim=strlike,; fam=strlike,; min_partitions=nullable(integral),; delimiter=strlike,; missing=strlike,; quantpheno=bool); def import_plink(self, bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False):; """"""Import PLINK binary file (BED, BIM, FAM) as variant dataset. **Examples**. Import data from a PLINK binary file:. >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). **Notes**. Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the ``--make-bed`` option. The centiMorgan position is not currently used in Hail (Column 3 in BIM file). The ID (``s``) used by Hail is the individual ID (column 2 in FAM file). .. warning::. No duplicate individual IDs are allowed. Chromosome names (Column 1) are automatically converted in the following cases:. - 23 => ""X""; - 24 => ""Y""; - 25 => ""X""; - 26 => ""MT"". **Annotations**. :py:meth:`~hail.HailContext.import_plink` adds the following annotations:. - **va.rsid** (*String*) -- Column 2 in the BIM file.; - **sa.famID** (*String*) -- Column 1 in the FAM file. Set to missing if ID equals ""0"".; - **sa.patID** (*String*) -- Column 3 in the FAM file. Set to missing if ID equals ""0"".; - **sa.matID** (*String*) -- Column 4 in the FAM file. Set to missing if ID equals ""0"".; -",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/context.html:16588,Testability,test,test,16588,"am types: Define types of fields in annotations files ; :type types: dict with str keys and :py:class:`.Type` values; ; :return: Key table constructed from text table.; :rtype: :class:`.KeyTable`. :param quote: Quote character; :type quote: str or None; """""". key = wrap_to_list(key); paths = wrap_to_list(paths); jtypes = {k: v._jtype for k, v in types.items()}. jkt = self._jhc.importTable(paths, key, min_partitions, jtypes, comment, delimiter, missing,; no_header, impute, quote); return KeyTable(self, jkt). [docs] @handle_py4j; @typecheck_method(bed=strlike,; bim=strlike,; fam=strlike,; min_partitions=nullable(integral),; delimiter=strlike,; missing=strlike,; quantpheno=bool); def import_plink(self, bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False):; """"""Import PLINK binary file (BED, BIM, FAM) as variant dataset. **Examples**. Import data from a PLINK binary file:. >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). **Notes**. Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the ``--make-bed`` option. The centiMorgan position is not currently used in Hail (Column 3 in BIM file). The ID (``s``) used by Hail is the individual ID (column 2 in FAM file). .. warning::. No duplicate individual IDs are allowed. Chromosome names (Column 1) are automatically converted in the following cases:. - 23 => ""X""; - 24 => ""Y""; - 25 => ""X""; - 26 => ""MT"". **Annotations**. :py:meth:`~hail.HailContext.import_plink` adds the following annotations:. - **va.rsid** (*String*) -- Column 2 in the BIM file.; - **sa.famID** (*String*) -- Column 1 in the FAM file. Set to missing if ID equals ""0"".; - **sa.patID** (*String*) -- Column 3 in the FAM file. Set to missing if ID equals ""0"".; - **sa.matID** (*String*) -- Column 4 in the FAM file. Set to missing if ID equals ""0"".; - **sa.isFemale** (*String*",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:4858,Availability,avail,available,4858,"enotype(self):; return self._jvds.isGenericGenotype(). @property; @handle_py4j; def sample_ids(self):; """"""Return sampleIDs. :return: List of sample IDs.; :rtype: list of str; """""". if self._sample_ids is None:; self._sample_ids = jiterable_to_list(self._jvds.sampleIds()); return self._sample_ids. @property; @handle_py4j; def sample_annotations(self):; """"""Return a dict of sample annotations. The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. :return: dict; """""". if self._sample_annotations is None:; zipped_annotations = Env.jutils().iterableToArrayList(; self._jvds.sampleIdsAndAnnotations(); ); r = {}; for element in zipped_annotations:; r[element._1()] = self.sample_schema._convert_to_py(element._2()); self._sample_annotations = r; return self._sample_annotations. [docs] @handle_py4j; def num_partitions(self):; """"""Number of partitions. **Notes**. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. :rtype: int; """""". return self._jvds.nPartitions(). @property; @handle_py4j; def num_samples(self):; """"""Number of samples. :rtype: int; """""". if self._num_samples is None:; self._num_samples = self._jvds.nSamples(); return self._num_samples. [docs] @handle_py4j; def count_variants(self):; """"""Count number of variants in variant dataset. :rtype: long; """""". return self._jvds.countVariants(). [docs] @handle_py4j; def was_split(self):; """"""True if multiallelic variants have been split into multiple biallelic variants. Result is True if :py:meth:`~hail.VariantDataset.split_multi` or :py:meth:`~hail.VariantDataset.filter_multi` has been called on this variant dataset,; or if the variant dataset was imported w",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:5012,Availability,resilien,resilient-distributed-datasets-rdds,5012,"eIds()); return self._sample_ids. @property; @handle_py4j; def sample_annotations(self):; """"""Return a dict of sample annotations. The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. :return: dict; """""". if self._sample_annotations is None:; zipped_annotations = Env.jutils().iterableToArrayList(; self._jvds.sampleIdsAndAnnotations(); ); r = {}; for element in zipped_annotations:; r[element._1()] = self.sample_schema._convert_to_py(element._2()); self._sample_annotations = r; return self._sample_annotations. [docs] @handle_py4j; def num_partitions(self):; """"""Number of partitions. **Notes**. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. :rtype: int; """""". return self._jvds.nPartitions(). @property; @handle_py4j; def num_samples(self):; """"""Number of samples. :rtype: int; """""". if self._num_samples is None:; self._num_samples = self._jvds.nSamples(); return self._num_samples. [docs] @handle_py4j; def count_variants(self):; """"""Count number of variants in variant dataset. :rtype: long; """""". return self._jvds.countVariants(). [docs] @handle_py4j; def was_split(self):; """"""True if multiallelic variants have been split into multiple biallelic variants. Result is True if :py:meth:`~hail.VariantDataset.split_multi` or :py:meth:`~hail.VariantDataset.filter_multi` has been called on this variant dataset,; or if the variant dataset was imported with :py:meth:`~hail.HailContext.import_plink`, :py:meth:`~hail.HailContext.import_gen`,; or :py:meth:`~hail.HailContext.import_bgen`, or if the variant dataset was simulated with :py:meth:`~hail.HailContext.balding_nichols_model`. :rtype: bool; """""". return self._",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:8125,Availability,down,downcoded,8125,"join(agg_exprs). return KeyTable(self.hc, self._jvds.aggregateByKey(key_exprs, agg_exprs)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=oneof(strlike, listof(strlike)),; propagate_gq=bool); def annotate_alleles_expr(self, expr, propagate_gq=False):; """"""Annotate alleles with expression. .. include:: requireTGenotype.rst. **Examples**. To create a variant annotation ``va.nNonRefSamples: Array[Int]`` where the ith entry of; the array is the number of samples carrying the ith alternate allele:. >>> vds_result = vds.annotate_alleles_expr('va.nNonRefSamples = gs.filter(g => g.isCalledNonRef()).count()'). **Notes**. This method is similar to :py:meth:`.annotate_variants_expr`. :py:meth:`.annotate_alleles_expr` dynamically splits multi-allelic sites,; evaluates each expression on each split allele separately, and for each expression annotates with an array with one element per alternate allele. In the splitting, genotypes are downcoded and each alternate allele is represented; using its minimal representation (see :py:meth:`split_multi` for more details). :param expr: Annotation expression.; :type expr: str or list of str; :param bool propagate_gq: Propagate GQ instead of computing from (split) PL. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = "","".join(expr). jvds = self._jvdf.annotateAllelesExpr(expr, propagate_gq); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_genotypes_expr(self, expr):; """"""Annotate genotypes with expression. **Examples**. Convert the genotype schema to a :py:class:`~hail.expr.TStruct` with two fields ``GT`` and ``CASE_HET``:. >>> vds_result = vds.annotate_genotypes_expr('g = {GT: g.gt, CASE_HET: sa.pheno.isCase && g.isHet()}'). Assume a VCF is imported with ``generic=True`` and the resulting genotype schema; is a ``Struct`` and the field ``GTA`` is a ``Call`` type. Use the ``.toGenotype",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:12866,Availability,down,downstream,12866,"e_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. - ``global``: global annotations. :param expr: Annotation expression; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateGlobalExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(path=strlike,; annotation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotation path starting in 'global'. :param annotation: annotation to add to global. :param annotation_type: Hail type of annotation; :type annotation_type: :py:class:`.Type`. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". annotation_type._typecheck(annotation). annotated = self._jvds.annotateGlobal(annotation_type._convert_to_j(annotation), annotation_type._jtype, path); assert annotated.globalSignature().typeCheck(annotated.globalAnnotation()), 'error in java type checking'; return VariantDataset(self.hc, annotated). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_samples_expr(self, expr):; """"""Annotate samples with expression. **Examples**. Compute per-sample GQ statistics for hets:. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.i",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:13480,Availability,error,error,13480,"n_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotation path starting in 'global'. :param annotation: annotation to add to global. :param annotation_type: Hail type of annotation; :type annotation_type: :py:class:`.Type`. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". annotation_type._typecheck(annotation). annotated = self._jvds.annotateGlobal(annotation_type._convert_to_j(annotation), annotation_type._jtype, path); assert annotated.globalSignature().typeCheck(annotated.globalAnnotation()), 'error in java type checking'; return VariantDataset(self.hc, annotated). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_samples_expr(self, expr):; """"""Annotate samples with expression. **Examples**. Compute per-sample GQ statistics for hets:. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:. >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).collect()')). To create an annotation for ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:35253,Availability,error,error,35253,"d VEP annotations to your VDS, make sure to add the initialization action ; :code:`gs://hail-common/vep/vep/vep85-init.sh` when starting your cluster. :param annotations: List of annotations to import from the database.; :type annotations: str or list of str . :param gene_key: Existing variant annotation used to map variants to gene symbols if importing gene-level ; annotations. If not provided, the method will add VEP annotations and parse them as described in the ; database documentation to obtain one gene symbol per variant.; :type gene_key: str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". # import modules needed by this function; import sqlite3. # collect user-supplied annotations, converting str -> list if necessary and dropping duplicates; annotations = list(set(wrap_to_list(annotations))). # open connection to in-memory SQLite database; conn = sqlite3.connect(':memory:'). # load database with annotation metadata, print error if not on Google Cloud Platform; try:; f = hadoop_read('gs://annotationdb/ADMIN/annotationdb.sql'); except FatalError:; raise EnvironmentError('Cannot read from Google Storage. Must be running on Google Cloud Platform to use annotation database.'); else:; curs = conn.executescript(f.read()); f.close(). # parameter substitution string to put in SQL query; like = ' OR '.join('a.annotation LIKE ?' for i in xrange(2*len(annotations))). # query to extract path of all needed database files and their respective annotation exprs ; qry = """"""SELECT file_path, annotation, file_type, file_element, f.file_id; FROM files AS f INNER JOIN annotations AS a ON f.file_id = a.file_id; WHERE {}"""""".format(like). # run query and collect results in a file_path: expr dictionary; results = curs.execute(qry, [x + '.%' for x in annotations] + annotations).fetchall(). # all file_ids to be used; file_ids = list(set([x[4] for x in results])). # parameter substitution string; sub = ','.join('?' for x in file_ids). # query to fetch coun",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:58627,Availability,down,downstream,58627,"y exports the contents of ``va.info`` to the INFO field. No other annotations besides ``va.info`` are exported. The genotype schema must have the type :py:class:`~hail.expr.TGenotype` or :py:class:`~hail.expr.TStruct`. If the type is; :py:class:`~hail.expr.TGenotype`, then the FORMAT fields will be GT, AD, DP, GQ, and PL (or PP if ``export_pp`` is True).; If the type is :py:class:`~hail.expr.TStruct`, then the exported FORMAT fields will be the names of each field of the Struct.; Each field must have a type of String, Char, Int, Double, or Call. Arrays and Sets are also allowed as long as they are not nested.; For example, a field with type ``Array[Int]`` can be exported but not a field with type ``Array[Array[Int]]``.; Nested Structs are also not allowed. .. caution::. If samples or genotypes are filtered after import, the value stored in ``va.info.AC`` value may no longer reflect the number of called alternate alleles in the filtered VDS. If the filtered VDS is then exported to VCF, downstream tools may produce erroneous results. The solution is to create new annotations in ``va.info`` or overwrite existing annotations. For example, in order to produce an accurate ``AC`` field, one can run :py:meth:`~hail.VariantDataset.variant_qc` and copy the ``va.qc.AC`` field to ``va.info.AC``:. >>> (vds.filter_genotypes('g.gq >= 20'); ... .variant_qc(); ... .annotate_variants_expr('va.info.AC = va.qc.AC'); ... .export_vcf('output/example.vcf.bgz')). :param str output: Path of .vcf file to write. :param append_to_header: Path of file to append to VCF header.; :type append_to_header: str or None. :param bool export_pp: If true, export linear-scaled probabilities (Hail's `pp` field on genotype) as the VCF PP FORMAT field. :param bool parallel: If true, return a set of VCF files (one per partition) rather than serially concatenating these files.; """""". self._jvdf.exportVCF(output, joption(append_to_header), export_pp, parallel). [docs] @handle_py4j; @convertVDS; @typecheck_method(o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:61773,Availability,down,downcode,61773,"e, max_shift=100, keep_star=False):; """"""Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. ``aIndex`` will never be zero). .. include:: requireTGenotype.rst. **Examples**. To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:. >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of ``aIndices`` because; we are mapping between the old and new *allele* indices, not; the *alternate allele* indices. **Notes**. If ``filter_altered_genotypes`` is true, genotypes that contain filtered-out alleles are set to missing. :py:meth:`~hail.VariantDataset.filter_alleles` implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. **Subset algorithm**. The subset algorithm (the default, ``subset=True``) subsets the; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:63160,Availability,down,downcode,63160," to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``25,20``.; - DP: No change.; - PL: The filtered alleles' columns are eliminated and the remaining columns shifted so the minimum value is 0.; - GQ: The second-lowest PL (after shifting). **Downcode algorithm**. The downcode algorithm (``subset=False``) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to :py:meth:`~hail.VariantDataset.split_multi`. The downcoding algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: The filtered alleles' columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles t",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:63411,Availability,down,downcodeing,63411,"f; that the filtered alleles are not real so we should discard any; probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``25,20``.; - DP: No change.; - PL: The filtered alleles' columns are eliminated and the remaining columns shifted so the minimum value is 0.; - GQ: The second-lowest PL (after shifting). **Downcode algorithm**. The downcode algorithm (``subset=False``) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to :py:meth:`~hail.VariantDataset.split_multi`. The downcoding algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: The filtered alleles' columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Expression Variables**. The following symbols are in scope for ``expr``:. - ``v`` (*Variant*): :ref:`variant`; ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:63801,Availability,down,downcoding,63801,"le(s).; - AD: The filtered alleles' columns are eliminated, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``25,20``.; - DP: No change.; - PL: The filtered alleles' columns are eliminated and the remaining columns shifted so the minimum value is 0.; - GQ: The second-lowest PL (after shifting). **Downcode algorithm**. The downcode algorithm (``subset=False``) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to :py:meth:`~hail.VariantDataset.split_multi`. The downcoding algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: The filtered alleles' columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Expression Variables**. The following symbols are in scope for ``expr``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndex`` (*Int*): the index of the allele being tested. The following symbols are in scope for ``annotation``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndices`` (*Array[Int]*): the array of old indices (such that ``aIndices[newIndex] = oldIndex`` and ``aIndices[0] = 0",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:65166,Availability,down,downcodes,65166,"`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Expression Variables**. The following symbols are in scope for ``expr``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndex`` (*Int*): the index of the allele being tested. The following symbols are in scope for ``annotation``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndices`` (*Array[Int]*): the array of old indices (such that ``aIndices[newIndex] = oldIndex`` and ``aIndices[0] = 0``). :param str expr: Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index). :param str annotation: Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices). :param bool subset: If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs. :param bool keep: If true, keep variants matching expr. :param bool filter_altered_genotypes: If true, genotypes that contain filtered-out alleles are set to missing. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :param bool keepStar: If true, keep variants where the only allele left is a ``*`` allele. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.filterAlleles(expr, annotation, filter_altered_genotypes, keep, subset, max_shift,; keep_star); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(expr=strlike,; keep=bool); def filter_genotypes(self, expr, keep=True):; """"""Filter genotypes based on expression. **Examples**. Filter genotypes by allele balance dependent on genot",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:65553,Availability,error,error,65553," the allele being tested. The following symbols are in scope for ``annotation``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndices`` (*Array[Int]*): the array of old indices (such that ``aIndices[newIndex] = oldIndex`` and ``aIndices[0] = 0``). :param str expr: Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index). :param str annotation: Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices). :param bool subset: If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs. :param bool keep: If true, keep variants matching expr. :param bool filter_altered_genotypes: If true, genotypes that contain filtered-out alleles are set to missing. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :param bool keepStar: If true, keep variants where the only allele left is a ``*`` allele. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.filterAlleles(expr, annotation, filter_altered_genotypes, keep, subset, max_shift,; keep_star); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(expr=strlike,; keep=bool); def filter_genotypes(self, expr, keep=True):; """"""Filter genotypes based on expression. **Examples**. Filter genotypes by allele balance dependent on genotype call:. >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.1) || ' +; ... '(g.isHet() && ab >= 0.25 && ab <= 0.75) || ' +; ... '(g.isHomVar() && ab >= 0.9))'). **Notes**. ``expr`` is in genotype context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``v`` (*Variant*): :ref:`variant`; - ``sa``: sample annotations; - ``v",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:75968,Availability,down,down,75968,"; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Fil",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:81389,Availability,down,downstream,81389,"71/journal.pgen.0020190>`__. (The resulting amplification of signal from the low end of the allele frequency spectrum will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor :math:`1/m` gives each sample row approximately unit total variance (assuming linkage equilibrium) so that the diagonal entries of the GRM are approximately 1. Equivalently,; ; .. math::. G_{ik} = \\frac{1}{m} \\sum_{j=1}^m \\frac{(C_{ij}-2p_j)(C_{kj}-2p_j)}{2 p_j (1-p_j)} ; ; :return: Genetic Relatedness Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """""". jkm = self._jvdf.grm(); return KinshipMatrix(jkm). [docs] @handle_py4j; @requireTGenotype; def hardcalls(self):; """"""Drop all genotype fields except the GT field. .. include:: requireTGenotype.rst. A hard-called variant dataset is about two orders of magnitude; smaller than a standard sequencing dataset. Use this; method to create a smaller, faster; representation for downstream processing that only; requires the GT field. :return: Variant dataset with no genotype metadata.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.hardCalls()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(maf=nullable(strlike),; bounded=bool,; min=nullable(numeric),; max=nullable(numeric)); def ibd(self, maf=None, bounded=True, min=None, max=None):; """"""Compute matrix of identity-by-descent estimations. .. include:: requireTGenotype.rst. **Examples**. To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:. >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in [0.2, 0.9], using minor allele frequencies stored in; ``va.panel_maf``:. >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). **Notes**. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :py:m",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:85472,Availability,avail,available,85472,"a PI_HAT value greater than or equal to 0.6.; ; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:. >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). **Notes**. The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted. ; ; The tiebreaking_expr namespace has the following variables available:; ; - ``s1``: The first sample id.; - ``sa1``: The annotations associated with s1.; - ``s2``: The second sample id. ; - ``sa2``: The annotations associated with s2. ; ; The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping ``s1``. Any positive integer expresses a preference for keeping ``s2``. A zero expresses no preference. This function must induce a `preorder <https://en.wikipedia.org/wiki/Preorder>`__ on the samples, in particular:. - ``tiebreaking_expr(sample1, sample2)`` must equal ``-1 * tie breaking_expr(sample2, sample1)``, which evokes the common sense understanding that if ``x < y`` then `y > x``.; - ``tiebreaking_expr(sample1, sample1)`` must equal 0, i.e. ``x = x``; - if sample1 is preferred to sample2 and sample2 is preferred to sample3, then sample1 must also be preferred to sample3. The last requirement is only important if you have three related sample",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:95310,Availability,avail,available,95310,"f variants in a different order than PLINK. Be sure to provide enough disk space per worker because :py:meth:`.ld_prune` `persists <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters ``r2`` and ``window``. The number of bytes stored in memory per variant is about ``nSamples / 4 + 50``. .. warning::. The variants in the pruned set are not guaranteed to be identical each time :py:meth:`.ld_prune` is run. We recommend running :py:meth:`.ld_prune` once and exporting the list of LD pruned variants using; :py:meth:`.export_variants` for future use. :param float r2: Maximum :math:`R^2` threshold between two variants in the pruned set within a given window. :param int window: Width of window in base-pairs for computing pair-wise :math:`R^2` values. :param int memory_per_core: Total amount of memory available for each core in MB. If unsure, use the default value. :param int num_cores: The number of cores available. Equivalent to the total number of workers times the number of cores per worker. :return: Variant dataset filtered to those variants which remain after LD pruning.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.ldPrune(r2, window, num_cores, memory_per_core); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(force_local=bool); def ld_matrix(self, force_local=False):; """"""Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. **Examples**. >>> ld_mat = vds.ld_matrix(). **Notes**. Each entry (i, j) in the LD matrix gives the :math:`r` value between variants i and j, defined as; `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`\\rho_{x_i,x_j}` between the two genotype vectors :math:`x_i` and :math:`x_j`. .. math::",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:95417,Availability,avail,available,95417,"py:meth:`.ld_prune` `persists <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters ``r2`` and ``window``. The number of bytes stored in memory per variant is about ``nSamples / 4 + 50``. .. warning::. The variants in the pruned set are not guaranteed to be identical each time :py:meth:`.ld_prune` is run. We recommend running :py:meth:`.ld_prune` once and exporting the list of LD pruned variants using; :py:meth:`.export_variants` for future use. :param float r2: Maximum :math:`R^2` threshold between two variants in the pruned set within a given window. :param int window: Width of window in base-pairs for computing pair-wise :math:`R^2` values. :param int memory_per_core: Total amount of memory available for each core in MB. If unsure, use the default value. :param int num_cores: The number of cores available. Equivalent to the total number of workers times the number of cores per worker. :return: Variant dataset filtered to those variants which remain after LD pruning.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.ldPrune(r2, window, num_cores, memory_per_core); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(force_local=bool); def ld_matrix(self, force_local=False):; """"""Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. **Examples**. >>> ld_mat = vds.ld_matrix(). **Notes**. Each entry (i, j) in the LD matrix gives the :math:`r` value between variants i and j, defined as; `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`\\rho_{x_i,x_j}` between the two genotype vectors :math:`x_i` and :math:`x_j`. .. math::. \\rho_{x_i,x_j} = \\frac{\\mathrm{Cov}(X_i,X_j)}{\\sigma_{X_i} \\sigma_{X_j}}. Also note that varian",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:101397,Availability,error,error,101397," as:. >>> vds_result = vds.linreg('if (sa.pheno.isFemale) sa.pheno.age else (2 * sa.pheno.age + 10)'). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__. See; equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 2` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; genotype and intercept. **Annotations**. With the default root, the following four variant annotations are added. - **va.linreg.beta** (*Double*) -- fit genotype coefficient, :math:`\hat\beta_1`; - **va.linreg.se** (*Double*) -- estimated standard error, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Double*) -- :math:`t`-statistic, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Double*) -- :math:`p`-value. :param str y: Response expression. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosages genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linreg(y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac, min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; y=strlike,; covariates=listof(s",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:106187,Availability,error,error,106187,"ls are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the linear regression model using the supplied phenotype and covariates.; The model is that of :py:meth:`.linreg` with sample genotype ``gt`` replaced by the score in the sample; key table. For each key, missing scores are mean-imputed across all samples. The resulting **linear regression key table** has the following columns:. - value of ``key_name`` (*String*) -- descriptor of variant group key (key column); - **beta** (*Double*) -- fit coefficient, :math:`\hat\beta_1`; - **se** (*Double*) -- estimated standard error, :math:`\widehat{\mathrm{se}}`; - **tstat** (*Double*) -- :math:`t`-statistic, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **pval** (*Double*) -- :math:`p`-value. :py:meth:`.linreg_burden` returns both the linear regression key table and the sample key table. **Extended example**. Let's walk through these steps in the ``max()`` toy example above.; There are six samples with the following annotations:. +--------+-------+------+------+; | Sample | pheno | cov1 | cov2 |; +========+=======+======+======+; | A | 0 | 0 | -1 |; +--------+-------+------+------+; | B | 0 | 2 | 3 |; +--------+-------+------+------+; | C | 1 | 1 | 5 |; +--------+-------+------+------+; | D | 1 | -2 | 0 |; +--------+-------+------+------+; | E | 1 | -2 | -4 |; +--------+-------+------+------+; | F | 1 | 4 | 3 |; +--------+-------+------+------+. There are three variants with the following ``gt`` values:. +---------+---+---+---+---+---+---+; | Variant | A | B | C | D | E | F |; +=========+===",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:111015,Availability,error,errors,111015,"kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg_multi_pheno(self, ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes more efficiently; than looping over :py:meth:`.linreg`. .. warning::. :py:meth:`.linreg_multi_pheno` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of ``y``. - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linregMultiPheno(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac,; min_af); return VariantDataset(self.hc, jvds). [docs] ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:113368,Availability,error,errors,113368,"his method runs linear regression for multiple phenotypes; more efficiently than looping over :py:meth:`.linreg`. This; method is more efficient than :py:meth:`.linreg_multi_pheno`; but doesn't implicitly filter on allele count or allele; frequency. .. warning::. :py:meth:`.linreg3` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of ``y``. - **va.linreg.nCompleteSamples** (*Int*) -- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int variant_block_size: Number of variant regressions to perform simultaneously. Larger block size requires more memmory. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`. """""". jvds = self._jvdf.linreg3(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, variant_block_size); return VariantDataset(self.hc,",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:120369,Availability,error,error,120369,"----------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.dropped_variance_fraction`` | Double | specified value of ``dropped_variance_fraction`` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.evals`` | Array[Double] | all eigenvalues of the kinship matrix in descending order |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.seH2`` | Double | standard error of :math:`\\hat{h}^2` under asymptotic normal approximation |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.normLkhdH2`` | Array[Double] | likelihood function of :math:`h^2` normalized on the discrete grid ``0.01, 0.02, ..., 0.99``. Index ``i`` is the likelihood for percentage ``i``. |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125300,Availability,avail,available,125300,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125387,Availability,avail,available,125387,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:131502,Availability,toler,tolerance,131502,"a_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:132979,Availability,error,error,132979,"hrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:133641,Availability,error,error,133641,"1 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on :math:`h^2`. **Testing each variant for association**. Fixing a single variant, we define:. - :math:`v = n \\times 1` vector of genotypes, with missing genotypes imputed as the mean of called genotypes; - :math:`X_v = \\left[v | X \\right] = n \\times (1 + c)` matrix concatenating :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the globa",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141737,Availability,error,error,141737," the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitt",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:144449,Availability,error,errors,144449,"are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete se",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:154447,Availability,error,errors,154447," key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are mod",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:154800,Availability,error,errors,154800,"'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK f",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:154917,Availability,error,errors,154917,"variates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:155052,Availability,error,errors,155052,"yTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:155585,Availability,error,errors,155585,"les**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in thi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:155632,Availability,error,error,155632,"tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third tabl",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:155949,Availability,error,error,155949,"notated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:155996,Availability,error,error,155996,"=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:156025,Availability,error,error,156025,"errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:156081,Availability,error,error,156081,"errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:156109,Availability,error,errors,156109,"per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this in",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:156536,Availability,error,errors,156536,"n the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.f",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:156607,Availability,error,errors,156607,"ontains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:156666,Availability,error,errors,156666,"ble that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157001,Availability,error,errors,157001,"ror code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuc",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157081,Availability,error,errors,157081,"or.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extendi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157134,Availability,error,error,157134,"w per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157190,Availability,error,error,157190,"w per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157218,Availability,error,errors,157218,"ly reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157397,Availability,error,errors,157397,"mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautoso",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157439,Availability,error,error,157439," (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, m",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157486,Availability,error,errors,157486," **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157541,Availability,error,error,157541,"ly.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :m",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157612,Availability,error,error,157612,"s nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157670,Availability,error,error,157670,"s. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement in this set. +--------+------------+------------+----------+------------------+; |Code | Dad | Mom | Kid | Copy State |; +========+=======",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:157945,Availability,error,error,157945," Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement in this set. +--------+------------+------------+----------+------------------+; |Code | Dad | Mom | Kid | Copy State |; +========+============+============+==========+==================+; | 1 | HomVar | HomVar | Het | Auto |; +--------+------------+------------+----------+------------------+; | 2 | HomRef | HomRef ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:158036,Availability,error,error,158036,"r of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement in this set. +--------+------------+------------+----------+------------------+; |Code | Dad | Mom | Kid | Copy State |; +========+============+============+==========+==================+; | 1 | HomVar | HomVar | Het | Auto |; +--------+------------+------------+----------+------------------+; | 2 | HomRef | HomRef | Het | Auto |; +--------+------------+------------+----------+------------------+; | 3 | HomRef | ! HomRef | HomVar | Auto",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:160539,Availability,error,error,160539,"-------+------------+----------+------------------+; | 8 | HomVar | HomVar | HomRef | Auto |; +--------+------------+------------+----------+------------------+; | 9 | Any | HomVar | HomRef | HemiX |; +--------+------------+------------+----------+------------------+; | 10 | Any | HomRef | HomVar | HemiX |; +--------+------------+------------+----------+------------------+; | 11 | HomVar | Any | HomRef | HemiY |; +--------+------------+------------+----------+------------------+; | 12 | HomRef | Any | HomVar | HemiY |; +--------+------------+------------+----------+------------------+. This method only considers children with two parents and a defined sex. PAR is currently defined with respect to reference; `GRCh37 <http://www.ncbi.nlm.nih.gov/projects/genome/assembly/grc/human/>`__:. - X: 60001 - 2699520, 154931044 - 155260560; - Y: 10001 - 2649520, 59034050 - 59363566. :param pedigree: Sample pedigree.; :type pedigree: :class:`~hail.representation.Pedigree`. :returns: Four tables with Mendel error statistics.; :rtype: (:class:`.KeyTable`, :class:`.KeyTable`, :class:`.KeyTable`, :class:`.KeyTable`); """""". kts = self._jvdf.mendelErrors(pedigree._jrep); return KeyTable(self.hc, kts._1()), KeyTable(self.hc, kts._2()), \; KeyTable(self.hc, kts._3()), KeyTable(self.hc, kts._4()). [docs] @handle_py4j; @typecheck_method(max_shift=integral); def min_rep(self, max_shift=100):; """"""; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position. **Examples**. 1. Simple trimming of a multi-allelic site, no change in variant position; `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`. 2. Trimming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:161408,Availability,error,error,161408,"63566. :param pedigree: Sample pedigree.; :type pedigree: :class:`~hail.representation.Pedigree`. :returns: Four tables with Mendel error statistics.; :rtype: (:class:`.KeyTable`, :class:`.KeyTable`, :class:`.KeyTable`, :class:`.KeyTable`); """""". kts = self._jvdf.mendelErrors(pedigree._jrep); return KeyTable(self.hc, kts._1()), KeyTable(self.hc, kts._2()), \; KeyTable(self.hc, kts._3()), KeyTable(self.hc, kts._4()). [docs] @handle_py4j; @typecheck_method(max_shift=integral); def min_rep(self, max_shift=100):; """"""; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position. **Examples**. 1. Simple trimming of a multi-allelic site, no change in variant position; `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`. 2. Trimming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports prin",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:170380,Availability,down,down,170380,">>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of their first ``k`` principal; component coordinates. As such, the efficacy of this method rests on two; assumptions:. - an individual's first ``k`` principal component coordinates fully; describe their allele-frequency-relevant ancestry, and. - the relationship between ancestry (as de",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:174055,Availability,avail,available,174055,"r identity-by-descent two is given by:. .. math::. \\widehat{k^{(2)}_{ij}} := \\frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}\\widehat{\\sigma^2_{is}} \\widehat{\\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \\widehat{k^{(0)}_{ij}} :=; \\begin{cases}; \\frac{\\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \\widehat{\\mu_{is}}^2(1 - \\widehat{\\mu_{js}})^2 + (1 - \\widehat{\\mu_{is}})^2\\widehat{\\mu_{js}}^2}; & \\widehat{\phi_{ij}} > 2^{-5/2} \\\\; 1 - 4 \\widehat{\phi_{ij}} + k^{(2)}_{ij}; & \\widehat{\phi_{ij}} \le 2^{-5/2}; \\end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \\widehat{k^{(1)}_{ij}} := 1 - \\widehat{k^{(2)}_{ij}} - \\widehat{k^{(0)}_{ij}}. **Details**. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:175129,Availability,error,errors,175129,"ductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_size`` larger than 512 tends to cause memory exhaustion errors. The minimum allele frequency filter is applied per-pair: if either of; the two individual's individual-specific minor allele frequency is below; the threshold, then the variant's contribution to relatedness estimates; is zero. Under the PC-Relate model, kinship, \[ \phi_{ij} \], ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \[ k^{(2)}_{ij} \],; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; \[ 2^{",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:176179,Availability,reliab,reliably,176179,"ed per-pair: if either of; the two individual's individual-specific minor allele frequency is below; the threshold, then the variant's contribution to relatedness estimates; is zero. Under the PC-Relate model, kinship, \[ \phi_{ij} \], ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \[ k^{(2)}_{ij} \],; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; \[ 2^{-3} = 12.5 % \] of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. The resulting :py:class:`.KeyTable` entries have the type: *{ i: String,; j: String, kin: Double, k2: Double, k1: Double, k0: Double }*. The key; list is: *i: String, j: String*. :param int k: The number of principal components to use to distinguish; ancestries. :param float maf: The minimum individual-specific allele frequency for; an allele used to measure relatedness. :param int block_size: the side length of the blocks of the block-; distributed matrices; this should be set such; that at least three of these matrices fit in; memory (in addition to all other objects; necessary for Spark and Hail). :param float min_kinship: Pairs of samples with kinship lower than; ``min_kinship`` are excluded from the results. :param str statistics: the set of statistics to compute, 'phi' will only; compute the kinship statistic, 'phik2' will; compute the kinship and identity-by-descent two; statistics, 'phik2k0' wi",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:178141,Availability,redundant,redundant,178141,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:191822,Availability,avail,available,191822," 'id1', 'ID2': 'id2'}). Use a file with an ""old_id"" and ""new_id"" column to rename samples:. >>> mapping_table = hc.import_table('data/sample_mapping.txt'); >>> mapping_dict = {row.old_id: row.new_id for row in mapping_table.collect()}; >>> vds_result = vds.rename_samples(mapping_dict). :param dict mapping: Mapping from old to new sample IDs. :return: Dataset with remapped sample IDs.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.renameSamples(mapping); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(num_partitions=integral,; shuffle=bool); def repartition(self, num_partitions, shuffle=True):; """"""Increase or decrease the number of variant dataset partitions. **Examples**. Repartition the variant dataset to have 500 partitions:. >>> vds_result = vds.repartition(500). **Notes**. Check the current number of partitions with :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, res",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:192415,Availability,resilien,resilient-distributed-datasets-rdds,192415,"ons. **Examples**. Repartition the variant dataset to have 500 partitions:. >>> vds_result = vds.repartition(500). **Notes**. Check the current number of partitions with :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset with the number of partitions equal to at most ``num_partitions``; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.coalesce(num_partitions, shuffle); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(max_partitions=integral); def naive_coalesce(self, max_partitions):; """"""Naively descrease the number of partitions. .. warning ::. :py:meth:`~hail.Va",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:196170,Availability,toler,tolerance,196170,":math:`1/m`, which gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the :math:`n \\times n` sample correlation or realized relationship matrix (RRM) :math:`K` as simply. .. math::. K = MM^T. Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in :py:meth:`~hail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:196212,Availability,toler,tolerance,196212,":math:`1/m`, which gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the :math:`n \\times n` sample correlation or realized relationship matrix (RRM) :math:`K` as simply. .. math::. K = MM^T. Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in :py:meth:`~hail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:196420,Availability,toler,tolerance,196420," MM^T. Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in :py:meth:`~hail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:196451,Availability,toler,tolerance,196451," MM^T. Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in :py:meth:`~hail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:196594,Availability,toler,tolerance,196594,"ail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of ge",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:196732,Availability,toler,tolerance,196732,"quilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of genotypes called |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomRef`` | Int | Number o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:196758,Availability,toler,tolerance,196758,"quilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of genotypes called |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomRef`` | Int | Number o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:196837,Availability,toler,tolerance,196837,"dvanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of genotypes called |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomRef`` | Int | Number of homozygous reference genotypes |; +---------------------------+--------+---------------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:208836,Availability,down,downcoded,208836,"h starting with 'va', period-delimited. :param str attribute: The attribute to remove (key). :return: Annotated dataset with the updated variant annotation without the attribute.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(self.hc, self._jvds.deleteVaAttribute(ann_path, attribute)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(propagate_gq=bool,; keep_star_alleles=bool,; max_shift=integral); def split_multi(self, propagate_gq=False, keep_star_alleles=False, max_shift=100):; """"""Split multiallelic variants. .. include:: requireTGenotype.rst. **Examples**. >>> vds.split_multi().write('output/split.vds'). **Notes**. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries. The biallelic DP is the same as the multiallelic DP. The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:209413,Availability,down,downcode,209413,"llelic variants. .. include:: requireTGenotype.rst. **Examples**. >>> vds.split_multi().write('output/split.vds'). **Notes**. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries. The biallelic DP is the same as the multiallelic DP. The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype. By default, GQ is recomputed from PL. If ``propagate_gq=True``; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split annotations in the info field.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:209584,Availability,down,downcoding,209584,"7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries. The biallelic DP is the same as the multiallelic DP. The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype. By default, GQ is recomputed from PL. If ``propagate_gq=True``; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split annotations in the info field. This means; that if a multiallelic site with ``info.AC`` value ``[10, 2]`` is; split, each split site will contain the same array ``[10,; 2]``. The provided allele index annotation ``va.aIndex`` can be used; to select the value cor",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:212473,Availability,error,error,212473,"lowing annotations:. - **va.wasSplit** (*Boolean*) -- true if this variant was; originally multiallelic, otherwise false.; - **va.aIndex** (*Int*) -- The original index of this; alternate allele in the multiallelic representation (NB: 1; is the first alternate allele or the only alternate allele; in a biallelic variant). For example, 1:100:A:T,C splits; into two variants: 1:100:A:T with ``aIndex = 1`` and; 1:100:A:C with ``aIndex = 2``. :param bool propagate_gq: Set the GQ of output (split); genotypes to be the GQ of the input (multi-allelic) variants; instead of recompute GQ as the difference between the two; smallest PL values. Intended to be used in conjunction with; ``import_vcf(store_gq=True)``. This option will be obviated; in the future by generic genotype schemas. Experimental.; :param bool keep_star_alleles: Do not filter out * alleles.; :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :return: A biallelic variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.splitMulti(propagate_gq, keep_star_alleles, max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree,; root=strlike); def tdt(self, pedigree, root='va.tdt'):; """"""Find transmitted and untransmitted variants; count per variant and; nuclear family. .. include:: requireTGenotype.rst. **Examples**. Compute TDT association results:. >>> pedigree = Pedigree.read('data/trios.fam'); >>> (vds.tdt(pedigree); ... .export_variants(""output/tdt_results.tsv"", ""Variant = v, va.tdt.*"")). **Notes**. The transmission disequilibrium test tracks the number of times the alternate allele is transmitted (t) or not transmitted (u) from a heterozgyous parent to an affected child under the null that the rate of such transmissions is 0.5. For variants where transmission is guaranteed (i.e., the",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:45290,Deployability,pipeline,pipeline,45290,"econd index is the state on the right.; For example, ``concordance[1][4]`` is the number of ""no call"" genotypes on the left that were called ; homozygous variant on the right. ; ; :param right: right hand variant dataset for concordance; :type right: :class:`.VariantDataset`. :return: The global concordance statistics, a key table with sample concordance; statistics, and a key table with variant concordance statistics.; :rtype: (list of list of int, :py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.concordance(right._jvds); j_global_concordance = r._1(); sample_kt = KeyTable(self.hc, r._2()); variant_kt = KeyTable(self.hc, r._3()); global_concordance = [[j_global_concordance.apply(j).apply(i) for i in xrange(5)] for j in xrange(5)]. return global_concordance, sample_kt, variant_kt. [docs] @handle_py4j; def count(self):; """"""Returns number of samples and variants in the dataset.; ; **Examples**; ; >>> samples, variants = vds.count(); ; **Notes**; ; This is also the fastest way to force evaluation of a Hail pipeline.; ; :returns: The sample and variant counts.; :rtype: (int, int); """""". r = self._jvds.count(). return r._1(), r._2(). [docs] @handle_py4j; def deduplicate(self):; """"""Remove duplicate variants. :return: Deduplicated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.deduplicate()). [docs] @handle_py4j; @typecheck_method(fraction=numeric,; seed=integral); def sample_variants(self, fraction, seed=1):; """"""Downsample variants to a given fraction of the dataset.; ; **Examples**; ; >>> small_vds = vds.sample_variants(0.01); ; **Notes**; ; This method may not sample exactly ``(fraction * n_variants)``; variants from the dataset. :param float fraction: (Expected) fraction of variants to keep. :param int seed: Random seed. :return: Downsampled variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.sampleVariants(fraction, seed)). [docs] @handle_py4j; @re",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:61168,Deployability,update,update,61168,"ion. The resulting VDS will be larger and slower in Hail but the genotypes will be accessible from other tools that support Parquet. """""". if self._is_generic_genotype:; self._jvdf.write(output, overwrite); else:; self._jvdf.write(output, overwrite, parquet_genotypes). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=strlike,; annotation=strlike,; subset=bool,; keep=bool,; filter_altered_genotypes=bool,; max_shift=integral,; keep_star=bool); def filter_alleles(self, expr, annotation='va = va', subset=True, keep=True,; filter_altered_genotypes=False, max_shift=100, keep_star=False):; """"""Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. ``aIndex`` will never be zero). .. include:: requireTGenotype.rst. **Examples**. To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:. >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of ``aIndices`` because; we are mapping between the old and new *allele* indices, not; the *alternate allele* indices. **Notes**. If ``filter_altered_genotypes`` is true, genotypes that contain filtered-out alleles are set to missing. :py:meth:`~hail.VariantDataset.filter_alleles` implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. **Subset algorithm**. The subset algorithm (the default, ``subset=True``) subsets the; AD and PL",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:126441,Deployability,configurat,configuration,126441,"il/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:. - :math:`y = n \\times 1` vector of phenotypes; - :math:`X = n \\times c` matrix of sample covariates and intercept column of ones; - :math:`K = n \\times n` kinship matrix; - :math:`I = n \\times n` identity matrix; - :math:`\\beta = c \\times 1` vector of covariate coefficients; - :math:`\sigma_g^2 =` coefficient of genetic variance component :math:`K`; - :math:`\sigma_e^2 =` coefficient of environmental variance component :math:`I`; - :math:`\delta = \\frac{",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:132234,Deployability,integrat,integrate,132234,"nning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:133783,Deployability,continuous,continuous,133783,"are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on :math:`h^2`. **Testing each variant for association**. Fixing a single variant, we define:. - :math:`v = n \\times 1` vector of genotypes, with missing genotypes imputed as the mean of called genotypes; - :math:`X_v = \\left[v | X \\right] = n \\times (1 + c)` matrix concatenating :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the global REML estimate :math:`\\hat{\delta}`, we find the REML estimate :math:`(\\hat{\\beta}_v, \\hat{\si",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:174138,Deployability,release,release,174138,"j}} := \\frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}\\widehat{\\sigma^2_{is}} \\widehat{\\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \\widehat{k^{(0)}_{ij}} :=; \\begin{cases}; \\frac{\\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \\widehat{\\mu_{is}}^2(1 - \\widehat{\\mu_{js}})^2 + (1 - \\widehat{\\mu_{is}})^2\\widehat{\\mu_{js}}^2}; & \\widehat{\phi_{ij}} > 2^{-5/2} \\\\; 1 - 4 \\widehat{\phi_{ij}} + k^{(2)}_{ij}; & \\widehat{\phi_{ij}} \le 2^{-5/2}; \\end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \\widehat{k^{(1)}_{ij}} := 1 - \\widehat{k^{(2)}_{ij}} - \\widehat{k^{(0)}_{ij}}. **Details**. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_size`` larger than 512 tends to cause memory exhaustion errors. The mini",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:178201,Deployability,pipeline,pipelines,178201,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:207970,Deployability,update,updated,207970,"ader when importing a VCF and written; to the VCF header when exporting a VCF:. - INFO fields attributes (attached to (`va.info.*`)):. - 'Number': The arity of the field. Can take values. - `0` (Boolean flag),; - `1` (single value),; - `R` (one value per allele, including the reference),; - `A` (one value per non-reference allele),; - `G` (one value per genotype), and; - `.` (any number of values). - When importing: The value in read from the VCF INFO field definition; - When exporting: The default value is `0` for **Boolean**, `.` for **Arrays** and 1 for all other types. - 'Description' (default is ''). - FILTER entries in the VCF header are generated based on the attributes; of `va.filters`. Each key/value pair in the attributes will generate a; FILTER entry in the VCF with ID = key and Description = value. :param str ann_path: Variant annotation path starting with 'va', period-delimited. :param str attribute: The attribute to remove (key). :return: Annotated dataset with the updated variant annotation without the attribute.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(self.hc, self._jvds.deleteVaAttribute(ann_path, attribute)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(propagate_gq=bool,; keep_star_alleles=bool,; max_shift=integral); def split_multi(self, propagate_gq=False, keep_star_alleles=False, max_shift=100):; """"""Split multiallelic variants. .. include:: requireTGenotype.rst. **Examples**. >>> vds.split_multi().write('output/split.vds'). **Notes**. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0;",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:221769,Deployability,configurat,configuration,221769,"rt modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223269,Deployability,configurat,configuration,223269,"onment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:223451,Deployability,release,release-,223451,"ache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be queried with :py:attr:`~hail.VariantDataset.variant_schema`. .. code-block:: text. Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:227116,Deployability,configurat,configuration,227116,"_region_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. :param str config: Path to VEP configuration file. :param block_size: Number of variants to annotate per VEP invocation.; :type block_size: int. :param str root: Variant annotation path to store VEP output. :param bool csq: If ``True``, annotates VCF CSQ field as a String.; If ``False``, annotates with the full nested struct schema. :return: An annotated with variant annotations from VEP.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvds.vep(config, root, csq, block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; def variants_table(self):; """"""Convert variants and variant annotations to a KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; v: Variant; va: variant annotations; }. with a single key ``v``. :return: Key table with variants and variant annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.variantsKT()). [docs] @handle_py4j; def samples_table(self):; """"""Convert samples and sample annotations to KeyTable. The resulting",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:75142,Energy Efficiency,efficient,efficiently,75142,"iants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:84977,Energy Efficiency,efficient,efficient,84977,")). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(threshold=numeric,; tiebreaking_expr=nullable(strlike),; maf=nullable(strlike),; bounded=bool); def ibd_prune(self, threshold, tiebreaking_expr=None, maf=None, bounded=True):; """"""; Prune samples from the :py:class:`.VariantDataset` based on :py:meth:`~hail.VariantDataset.ibd` PI_HAT measures of relatedness. .. include:: requireTGenotype.rst. **Examples**; ; Prune samples so that no two have a PI_HAT value greater than or equal to 0.6.; ; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:. >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). **Notes**. The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted. ; ; The tiebreaking_expr namespace has the following variables available:; ; - ``s1``: The first sample id.; - ``sa1``: The annotations associated with s1.; - ``s2``: The second sample id. ; - ``sa2``: The annotations associated with s2. ; ; The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping ``s1``. Any positive integer expresses a preference for keeping ``s2``. A zero expresses no preference. This function must induce a `preorder <https://en.wikipedia.org/wiki/Preorder>`__ on the samples, ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:96704,Energy Efficiency,reduce,reduce,96704,"set(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(force_local=bool); def ld_matrix(self, force_local=False):; """"""Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. **Examples**. >>> ld_mat = vds.ld_matrix(). **Notes**. Each entry (i, j) in the LD matrix gives the :math:`r` value between variants i and j, defined as; `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`\\rho_{x_i,x_j}` between the two genotype vectors :math:`x_i` and :math:`x_j`. .. math::. \\rho_{x_i,x_j} = \\frac{\\mathrm{Cov}(X_i,X_j)}{\\sigma_{X_i} \\sigma_{X_j}}. Also note that variants with zero variance (:math:`\\sigma = 0`) will be dropped from the matrix. .. caution::. The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you'll want to reduce the number of variants with methods like; :py:meth:`.sample_variants`, :py:meth:`.filter_variants_expr`, or :py:meth:`.ld_prune` before; calling this unless your dataset is very small. :param bool force_local: If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds :math:`5000^2` and locally otherwise. :return: Matrix of r values between pairs of variants.; :rtype: :py:class:`LDMatrix`; """""". jldm = self._jvdf.ldMatrix(force_local); return LDMatrix(jldm). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg(self, y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association us",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:110464,Energy Efficiency,efficient,efficiently,110464,"ollection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of linear regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.linregBurden(key_name, variant_keys, single_key, agg_expr, y,; jarray(Env.jvm().java.lang.String, covariates)); linreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return linreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg_multi_pheno(self, ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes more efficiently; than looping over :py:meth:`.linreg`. .. warning::. :py:meth:`.linreg_multi_pheno` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of ``y``. - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:112428,Energy Efficiency,efficient,efficiently,112428,"r. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linregMultiPheno(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac,; min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; variant_block_size=integral); def linreg3(self, ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes; more efficiently than looping over :py:meth:`.linreg`. This; method is more efficient than :py:meth:`.linreg_multi_pheno`; but doesn't implicitly filter on allele count or allele; frequency. .. warning::. :py:meth:`.linreg3` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of ``y``. - **va.linreg.nCompleteSamples** (*Int*) -- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.li",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:112499,Energy Efficiency,efficient,efficient,112499,"am bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linregMultiPheno(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac,; min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; variant_block_size=integral); def linreg3(self, ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes; more efficiently than looping over :py:meth:`.linreg`. This; method is more efficient than :py:meth:`.linreg_multi_pheno`; but doesn't implicitly filter on allele count or allele; frequency. .. warning::. :py:meth:`.linreg3` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of ``y``. - **va.linreg.nCompleteSamples** (*Int*) -- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:130258,Energy Efficiency,efficient,efficient,130258,"order. :math:`S_{ii}` is the eigenvalue of eigenvector :math:`U_{:,i}`; - :math:`U^T = n \\times n` orthonormal matrix, the transpose (and inverse) of :math:`U`. A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^TX\\beta, \sigma_g^2 (S + \delta I)\\right). for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (:math:`y`) and covariate vectors (columns of :math:`X`) in :math:`\mathbb{R}^n` by :math:`U^T` transforms the model to one with independent residuals. For any particular value of :math:`\delta`, the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in :math:`n`. In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over :math:`\delta` to find the REML estimate :math:`(\hat{\delta}, \\hat{\\beta}, \\hat{\sigma}_g^2)` of the triple :math:`(\delta, \\beta, \sigma_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_me",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:135828,Energy Efficiency,reduce,reduces,135828,"{g,v}^2)` via rotation of the model. .. math::. y \\sim \\mathrm{N}\\left(X_v\\beta_v, \sigma_{g,v}^2 (K + \\hat{\delta} I)\\right). Note that the only new rotation to compute here is :math:`U^T v`. To test the null hypothesis that the genotype coefficient :math:`\\beta^0_v` is zero, we consider the restricted model with parameters :math:`((0, \\beta^1_v, \ldots, \\beta^c_v), \sigma_{g,v}^2)` within the full model with parameters :math:`(\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller variant dataset in the same order. **Low-rank approximation of kinship for improved performance**. :py:meth:`.lmmreg` can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter ``n_eigs`` to use only the top ``n_eigs`` eigenvectors. Alternatively, specify ``dropped_variance_fraction`` to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:145672,Energy Efficiency,reduce,reduces,145672,"t:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association. The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the `Jeffrey's invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the ``sa.lmmreg.fit`` annotations reflect the null model; otherwise, they reflect the full model. See `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__ for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert's notes `Statistical Theory <ht",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:169478,Energy Efficiency,efficient,efficient,169478,"riantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles ar",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:18975,Integrability,depend,depends,18975,"ment expects a list of Hail expressions whose types match, in order, the ; table's key types.; ; Each expression in the list ``vds_key`` has the following symbols in; scope:. - ``s`` (*String*): sample ID; - ``sa``: sample annotations; ; **The** ``root`` **and** ``expr`` **arguments**; ; .. note::; ; One of ``root`` or ``expr`` is required, but not both. ; ; The ``expr`` parameter expects an annotation expression involving ``sa`` (the existing ; sample annotations in the dataset) and ``table`` (a struct containing the columns in ; the table), like ``sa.col1 = table.col1, sa.col2 = table.col2`` or ``sa = merge(sa, table)``.; The ``root`` parameter expects an annotation path beginning in ``sa``, like ``sa.annotations``.; Passing ``root='sa.annotations'`` is exactly the same as passing ``expr='sa.annotations = table'``. ``expr`` has the following symbols in scope:. - ``sa``: sample annotations; - ``table``: See note. .. note:: ; ; The value of ``table`` inside root/expr depends on the number of values in the key table, ; as well as the ``product`` argument. There are three behaviors based on the number of values; and one branch for ``product`` being true and false, for a total of six modes:; ; +-------------------------+-------------+--------------------+-----------------------------------------------+; | Number of value columns | ``product`` | Type of ``table`` | Value of ``table`` |; +=========================+=============+====================+===============================================+; | More than 2 | False | ``Struct`` | Struct with an element for each column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 1 | False | ``T`` | The value column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 0 | False | ``Boolean`` | Existence of any matching key. |; +-------------------------+-------------+--------------------+-----------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:27037,Integrability,depend,depends,27037," order, ; the table's key types. Note that using ``vds_key`` is slower than annotation with a standard ; key type. Each expression in the list ``vds_key`` has the following symbols in; scope:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; ; **The** ``root`` **and** ``expr`` **arguments**; ; .. note::; ; One of ``root`` or ``expr`` is required, but not both. ; ; The ``expr`` parameter expects an annotation assignment involving ``va`` (the existing ; variant annotations in the dataset) and ``table`` (the values(s) in the table),; like ``va.col1 = table.col1, va.col2 = table.col2`` or ``va = merge(va, table)``.; The ``root`` parameter expects an annotation path beginning in ``va``, like ``va.annotations``.; Passing ``root='va.annotations'`` is the same as passing ``expr='va.annotations = table'``. ``expr`` has the following symbols in scope:. - ``va``: variant annotations; - ``table``: See note. .. note:: ; ; The value of ``table`` inside root/expr depends on the number of values in the key table, ; as well as the ``product`` argument. There are three behaviors based on the number of values; and one branch for ``product`` being true and false, for a total of six modes:; ; +-------------------------+-------------+--------------------+-----------------------------------------------+; | Number of value columns | ``product`` | Type of ``table`` | Value of ``table`` |; +=========================+=============+====================+===============================================+; | More than 2 | False | ``Struct`` | Struct with an element for each column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 1 | False | ``T`` | The value column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 0 | False | ``Boolean`` | Existence of any matching key. |; +-------------------------+-------------+--------------------+-----------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:66131,Integrability,depend,dependent,66131," If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs. :param bool keep: If true, keep variants matching expr. :param bool filter_altered_genotypes: If true, genotypes that contain filtered-out alleles are set to missing. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :param bool keepStar: If true, keep variants where the only allele left is a ``*`` allele. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.filterAlleles(expr, annotation, filter_altered_genotypes, keep, subset, max_shift,; keep_star); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(expr=strlike,; keep=bool); def filter_genotypes(self, expr, keep=True):; """"""Filter genotypes based on expression. **Examples**. Filter genotypes by allele balance dependent on genotype call:. >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.1) || ' +; ... '(g.isHet() && ab >= 0.25 && ab <= 0.75) || ' +; ... '(g.isHomVar() && ab >= 0.9))'). **Notes**. ``expr`` is in genotype context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``v`` (*Variant*): :ref:`variant`; - ``sa``: sample annotations; - ``va``: variant annotations; - ``global``: global annotations. For more information, see the documentation on `data representation, annotations <overview.html#>`__, and; the `expression language <exprlang.html>`__. .. caution::; When ``expr`` evaluates to missing, the genotype will be removed regardless of whether ``keep=True`` or ``keep=False``. :param str expr: Boolean filter expression.; ; :param bool keep: Keep genotypes where ``expr`` evaluates to true. :return: Filtered variant dataset.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.filterGenotypes(expr, ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:94602,Integrability,depend,depend,94602,"whether; the variants are independent (:math:`R^2` < ``r2``) where ``r2`` is the maximum :math:`R^2` allowed.; :math:`R^2` is defined as the square of `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`{\\rho}_{x,y}` between the two genotype vectors :math:`{\\mathbf{x}}` and :math:`{\\mathbf{y}}`. .. math::. {\\rho}_{x,y} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}. :py:meth:`.ld_prune` with default arguments is equivalent to ``plink --indep-pairwise 1000kb 1 0.2``.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK. Be sure to provide enough disk space per worker because :py:meth:`.ld_prune` `persists <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters ``r2`` and ``window``. The number of bytes stored in memory per variant is about ``nSamples / 4 + 50``. .. warning::. The variants in the pruned set are not guaranteed to be identical each time :py:meth:`.ld_prune` is run. We recommend running :py:meth:`.ld_prune` once and exporting the list of LD pruned variants using; :py:meth:`.export_variants` for future use. :param float r2: Maximum :math:`R^2` threshold between two variants in the pruned set within a given window. :param int window: Width of window in base-pairs for computing pair-wise :math:`R^2` values. :param int memory_per_core: Total amount of memory available for each core in MB. If unsure, use the default value. :param int num_cores: The number of cores available. Equivalent to the total number of workers times the number of cores per worker. :return: Variant dataset filtered to those variants which remain after LD pruning.; :rtype: :py:class:`.VariantDataset`; """""". jvd",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:116805,Integrability,depend,depending,116805," 2) compute the eigendecomposition :math:`K = USU^T` of the kinship matrix; 3) fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (`REML <https://en.wikipedia.org/wiki/Restricted_maximum_likelihood>`__), storing results in global annotations under ``global.lmmreg``; 4) test each variant for association, storing results under ``va.lmmreg`` in variant annotations. This plan can be modified as follows:. - Set ``run_assoc=False`` to not test any variants for association, i.e. skip Step 5.; - Set ``use_ml=True`` to use maximum likelihood instead of REML in Steps 4 and 5.; - Set the ``delta`` argument to manually set the value of :math:`\delta` rather that fitting :math:`\delta` in Step 4.; - Set the ``global_root`` argument to change the global annotation root in Step 4.; - Set the ``va_root`` argument to change the variant annotation root in Step 5. :py:meth:`.lmmreg` adds 9 or 13 global annotations in Step 4, depending on whether :math:`\delta` is set or fit. +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | Annotation | Type | Value |; +==============================================+======================+======================================================================================================================================================+; | ``global.lmmreg.useML`` | Boolean | true if fit by ML, false if fit by REML |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.beta`` | Dict[String, Double] | map from *intercept* and the given ``covariates`` expressions to the corresponding fit :math:`\\beta` coefficients ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125103,Integrability,rout,routine,125103,"hrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. **Performance**. Hail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:125310,Integrability,rout,routines,125310,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:128710,Integrability,depend,depends,128710,"rom the :math:`n`-dimensional `multivariate normal distribution <https://en.wikipedia.org/wiki/Multivariate_normal_distribution>`__ with mean :math:`X \\beta` and variance components that are scalar multiples of :math:`K` and :math:`I`:. .. math::. y \sim \mathrm{N}\\left(X\\beta, \sigma_g^2 K + \sigma_e^2 I\\right). Thus the model posits that the residuals :math:`y_i - X_{i,:}\\beta` and :math:`y_j - X_{j,:}\\beta` have covariance :math:`\sigma_g^2 K_{ij}` and approximate correlation :math:`h^2 K_{ij}`. Informally: phenotype residuals are correlated as the product of overall heritability and pairwise kinship. By contrast, standard (unmixed) linear regression is equivalent to fixing :math:`\sigma_2` (equivalently, :math:`h^2`) at 0 above, so that all phenotype residuals are independent. **Caution:** while it is tempting to interpret :math:`h^2` as the `narrow-sense heritability <https://en.wikipedia.org/wiki/Heritability#Definition>`__ of the phenotype alone, note that its value depends not only the phenotype and genetic data, but also on the choice of sample covariates. **Fitting the global model**. The core algorithm is essentially a distributed implementation of the spectral approach taken in `FastLMM <https://www.microsoft.com/en-us/research/project/fastlmm/>`__. Let :math:`K = USU^T` be the `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices>`__ of the real symmetric matrix :math:`K`. That is:. - :math:`U = n \\times n` orthonormal matrix whose columns are the eigenvectors of :math:`K`; - :math:`S = n \\times n` diagonal matrix of eigenvalues of :math:`K` in descending order. :math:`S_{ii}` is the eigenvalue of eigenvector :math:`U_{:,i}`; - :math:`U^T = n \\times n` orthonormal matrix, the transpose (and inverse) of :math:`U`. A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:132234,Integrability,integrat,integrate,132234,"nning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:141432,Integrability,depend,depend,141432,"_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each vari",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:143711,Integrability,depend,dependent,143711," summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follo",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:12021,Modifiability,variab,variable,12021,"expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = "","".join(expr). jvds = self._jvdf.annotateGenotypesExpr(expr); vds = VariantDataset(self.hc, jvds); if isinstance(vds.genotype_schema, TGenotype):; return VariantDataset(self.hc, vds._jvdf.toVDS()); else:; return vds. [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_global_expr(self, expr):; """"""Annotate global with expression. **Example**. Annotate global with an array of populations:. >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:. >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. - ``global``: global annotations. :param expr: Annotation expression; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateGlobalExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(path=strlike,; annotation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotatio",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:37354,Modifiability,config,config,37354,"S f INNER JOIN annotations AS a ON f.file_id = a.file_id; WHERE f.file_id IN ({}); GROUP BY file_path"""""".format(sub). # collect counts in file_id: count dictionary; cnts = {x[0]: x[1] for x in curs.execute(qry, file_ids).fetchall()}. # close database connection; conn.close(). # collect dictionary of file_path: expr entries; file_exprs = {}; for r in results:; expr = r[1] + '=' + 'table' if r[2] == 'table' and cnts[r[0]] < 2 else r[1] + '=' + r[2] + '.' + r[3]; try:; file_exprs[r[0]] += ',' + expr; except KeyError:; file_exprs[r[0]] = expr. # are there any gene annotations?; are_genes = 'gs://annotationdb/gene/gene.kt' in file_exprs #any([x.startswith('gs://annotationdb/gene/') for x in file_exprs]). # subset to VEP annotations; veps = any([x == 'vep' for x in file_exprs]). # if VEP annotations are selected, or if gene-level annotations are selected with no specified gene_key, annotate with VEP; if veps or (are_genes and not gene_key):. # VEP annotate the VDS; self = self.vep(config='/vep/vep-gcloud.properties', root='va.vep'). # extract 1 gene symbol per variant from VEP annotations if a gene_key parameter isn't provided; if are_genes:. # hierarchy of possible variant consequences, from most to least severe; csq_terms = [; 'transcript_ablation',; 'splice_acceptor_variant',; 'splice_donor_variant',; 'stop_gained',; 'frameshift_variant',; 'stop_lost',; 'start_lost',; 'transcript_amplification',; 'inframe_insertion',; 'inframe_deletion',; 'missense_variant',; 'protein_altering_variant',; 'incomplete_terminal_codon_variant',; 'stop_retained_variant',; 'synonymous_variant',; 'splice_region_variant',; 'coding_sequence_variant',; 'mature_miRNA_variant',; '5_prime_UTR_variant',; '3_prime_UTR_variant',; 'non_coding_transcript_exon_variant',; 'intron_variant',; 'NMD_transcript_variant',; 'non_coding_transcript_variant',; 'upstream_gene_variant',; 'downstream_gene_variant',; 'TFBS_ablation',; 'TFBS_amplification',; 'TF_binding_site_variant',; 'regulatory_region_ablation',; 're",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:68314,Modifiability,variab,variable,68314,"st. This method is much less computationally expensive than; :py:meth:`.split_multi`, and can also be used to produce; a variant dataset that can be used with methods that do not; support multiallelic variants. :return: Dataset with no multiallelic sites, which can; be used for biallelic-only methods.; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.filterMulti()). [docs] @handle_py4j; def drop_samples(self):; """"""Removes all samples from variant dataset. The variants, variant annotations, and global annnotations will remain,; producing a sites-only variant dataset. :return: Sites-only variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.dropSamples()). [docs] @handle_py4j; @typecheck_method(expr=strlike,; keep=bool); def filter_samples_expr(self, expr, keep=True):; """"""Filter samples with the expression language. **Examples**. Filter samples by phenotype (assumes sample annotation *sa.isCase* exists and is a Boolean variable):. >>> vds_result = vds.filter_samples_expr(""sa.isCase""). Remove samples with an ID that matches a regular expression:. >>> vds_result = vds.filter_samples_expr('""^NA"" ~ s' , keep=False). Filter samples from sample QC metrics and write output to a new variant dataset:. >>> (vds.sample_qc(); ... .filter_samples_expr('sa.qc.callRate >= 0.99 && sa.qc.dpMean >= 10'); ... .write(""output/filter_samples.vds"")). **Notes**. ``expr`` is in sample context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. For more information, see the documentation on `data representation, annotations <overview.html#>`__, and; the `expression language <exprlang.html>`__. .. caution::; When ``expr`` evaluates to missing, the sample will be removed regardless of whether ``keep=True`` or ``keep=False``. :param str expr: Boolean filter e",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:85462,Modifiability,variab,variables,85462,"a PI_HAT value greater than or equal to 0.6.; ; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:. >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). **Notes**. The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted. ; ; The tiebreaking_expr namespace has the following variables available:; ; - ``s1``: The first sample id.; - ``sa1``: The annotations associated with s1.; - ``s2``: The second sample id. ; - ``sa2``: The annotations associated with s2. ; ; The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping ``s1``. Any positive integer expresses a preference for keeping ``s2``. A zero expresses no preference. This function must induce a `preorder <https://en.wikipedia.org/wiki/Preorder>`__ on the samples, in particular:. - ``tiebreaking_expr(sample1, sample2)`` must equal ``-1 * tie breaking_expr(sample2, sample1)``, which evokes the common sense understanding that if ``x < y`` then `y > x``.; - ``tiebreaking_expr(sample1, sample1)`` must equal 0, i.e. ``x = x``; - if sample1 is preferred to sample2 and sample2 is preferred to sample3, then sample1 must also be preferred to sample3. The last requirement is only important if you have three related sample",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:126441,Modifiability,config,configuration,126441,"il/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:. - :math:`y = n \\times 1` vector of phenotypes; - :math:`X = n \\times c` matrix of sample covariates and intercept column of ones; - :math:`K = n \\times n` kinship matrix; - :math:`I = n \\times n` identity matrix; - :math:`\\beta = c \\times 1` vector of covariate coefficients; - :math:`\sigma_g^2 =` coefficient of genetic variance component :math:`K`; - :math:`\sigma_e^2 =` coefficient of environmental variance component :math:`I`; - :math:`\delta = \\frac{",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:132028,Modifiability,variab,variables,132028,"nning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
https://hail.is/docs/0.1/_modules/hail/dataset.html:132301,Modifiability,variab,variables,132301,"nning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of ",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
