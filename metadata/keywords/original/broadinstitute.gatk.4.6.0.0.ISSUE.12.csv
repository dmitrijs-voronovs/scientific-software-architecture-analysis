id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/pull/7571:3,Testability,test,test,3,"To test I ran this before and after the fix. Before it would sit there after the line . ```; Tool returned:; 0; ```; for ~10 minutes before it would time out and exit. After the fix it shuts down cleanly. ```; gatk --java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 \; CreateVariantIngestFiles \; -V gs://fc-e2f6ffa2-4033-4517-98fc-889bee4cc7a6/5e6b194b-5f69-40f2-a6de-f4f3f80ce05a/ReblockGVCF/702cdbf7-0666-4ee5-b889-91ba0ffa90bd/call-Reblock/HG00405.haplotypeCalls.er.raw.vcf.gz.rb.g.vcf.gz \; -L chr20:1-100000 \; -IG FORTY \; --ignore-above-gq-threshold false \; --project-id broad-dsp-spec-ops \; --dataset-name gvs_qs_v2_kc \; --output-type BQ \; --enable-reference-ranges true \; --enable-pet false \; --enable-vet true \; -SN ERS4367795 \; --gvs-sample-id 99 \; --ref-version 38; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7571
https://github.com/broadinstitute/gatk/pull/7572:0,Testability,test,tested,0,tested here https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/AoU_DRC_WGS_ah_reblock0825/workflows,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7572
https://github.com/broadinstitute/gatk/pull/7573:75,Modifiability,refactor,refactored,75,Major changes:. - remove workspace datamodel updating from GvsAssignIds; - refactored GvsImportGenomes to remove all bq load code; - added new load status table append the load status of a sample; - changed SetIsLoaded and CheckForDuplicateData to read from the partitions AND the load status table,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573
https://github.com/broadinstitute/gatk/pull/7573:120,Performance,load,load,120,Major changes:. - remove workspace datamodel updating from GvsAssignIds; - refactored GvsImportGenomes to remove all bq load code; - added new load status table append the load status of a sample; - changed SetIsLoaded and CheckForDuplicateData to read from the partitions AND the load status table,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573
https://github.com/broadinstitute/gatk/pull/7573:143,Performance,load,load,143,Major changes:. - remove workspace datamodel updating from GvsAssignIds; - refactored GvsImportGenomes to remove all bq load code; - added new load status table append the load status of a sample; - changed SetIsLoaded and CheckForDuplicateData to read from the partitions AND the load status table,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573
https://github.com/broadinstitute/gatk/pull/7573:172,Performance,load,load,172,Major changes:. - remove workspace datamodel updating from GvsAssignIds; - refactored GvsImportGenomes to remove all bq load code; - added new load status table append the load status of a sample; - changed SetIsLoaded and CheckForDuplicateData to read from the partitions AND the load status table,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573
https://github.com/broadinstitute/gatk/pull/7573:281,Performance,load,load,281,Major changes:. - remove workspace datamodel updating from GvsAssignIds; - refactored GvsImportGenomes to remove all bq load code; - added new load status table append the load status of a sample; - changed SetIsLoaded and CheckForDuplicateData to read from the partitions AND the load status table,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573
https://github.com/broadinstitute/gatk/pull/7574:128,Performance,load,load,128,- `localization_optional` for indexes localization in `CreateImportTsvs`; - check to make sure there are more than 0 samples to load before going forward in `GetSampleIds`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7574
https://github.com/broadinstitute/gatk/pull/7576:174,Security,expose,exposes,174,"This was unearthed by #7542 and is plumbed correctly in this PR. Note that we need to still address the broader issue of hooking arguments to GenomicsDB - #6456 . GenomicsDB exposes a whole set of export arguments all added in response of gatk requests, some of them are hardcoded by certain tools(e.g GenotypeGVCFs uses --max-alternate-alleles while SelectVariants does not), many are unused.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7576
https://github.com/broadinstitute/gatk/issues/7577:407,Testability,log,log,407,"## Documentation request. Incorrect statistical definition in the article [Calculation of PL and GQ by HaplotypeCaller and GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/articles/360035890451). The conditional probability in the definition of PL is backwards. ### Description . The article contains the following (between the lines):. ----. The basic formula for calculating PL is:. $$ PL = -10 * \log{P(Genotype | Data)} $$. where P(Genotype | Data) is the conditional probability of the Genotype given the sequence Data that we have observed. The process by which we determine the value of P(Genotype | Data) is described here. ----. The genotype likelihood _and, thus, PL values_ are actually P(Data|Genotype). ; P(Genotype|Data) is the _posterior_.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7577
https://github.com/broadinstitute/gatk/issues/7578:19,Testability,test,tests,19,Need to write unit tests for testing with an invalid kingdom name and for testing with an invalid species name,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7578
https://github.com/broadinstitute/gatk/issues/7578:29,Testability,test,testing,29,Need to write unit tests for testing with an invalid kingdom name and for testing with an invalid species name,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7578
https://github.com/broadinstitute/gatk/issues/7578:74,Testability,test,testing,74,Need to write unit tests for testing with an invalid kingdom name and for testing with an invalid species name,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7578
https://github.com/broadinstitute/gatk/issues/7579:135,Availability,Down,Downloaded,135,"### Affected tool; - CollectReadCounts. ### Affected version; - The Genome Analysis Toolkit (GATK) v4.2.3.0; - HTSJDK Version: 2.24.1. Downloaded from https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip. ### Description ; When calling `CollectReadCounts` tool with symlink as input BAM-file, `java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE` occurs. Symlinks seem to work fine with Picard BAM-tools as well as `HaplotypeCaller` and `Mutect2`. Probably HTSJDK level issue, but popped up exception is kind of misleading. #### Stacktrace:; ```; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:863); at htsjdk.samtools.MemoryMappedFileBuffer.<init>(MemoryMappedFileBuffer.java:23); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:931); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579
https://github.com/broadinstitute/gatk/issues/7579:199,Availability,down,download,199,"### Affected tool; - CollectReadCounts. ### Affected version; - The Genome Analysis Toolkit (GATK) v4.2.3.0; - HTSJDK Version: 2.24.1. Downloaded from https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip. ### Description ; When calling `CollectReadCounts` tool with symlink as input BAM-file, `java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE` occurs. Symlinks seem to work fine with Picard BAM-tools as well as `HaplotypeCaller` and `Mutect2`. Probably HTSJDK level issue, but popped up exception is kind of misleading. #### Stacktrace:; ```; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:863); at htsjdk.samtools.MemoryMappedFileBuffer.<init>(MemoryMappedFileBuffer.java:23); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:931); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579
https://github.com/broadinstitute/gatk/issues/7579:3179,Availability,error,error,3179,"eader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:384); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:97); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; `gatk --java-options ""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" CollectReadCounts -R [...].fasta -L [...].interval_list -I Sample.bam --interval-merging-rule OVERLAPPING_ONLY -O Sample.counts.hdf5` where `Sample.bam` is a symlink to a BAM-file from an NFS-mounted location. #### Possible workaround; You can create symlink for the parent directory of the target BAM-file instead. #### Expected behavior; I would expect a better error message that would help to indicate the cause behind ""Size exceeds Integer.MAX_VALUE"" exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579
https://github.com/broadinstitute/gatk/issues/7579:190,Deployability,release,releases,190,"### Affected tool; - CollectReadCounts. ### Affected version; - The Genome Analysis Toolkit (GATK) v4.2.3.0; - HTSJDK Version: 2.24.1. Downloaded from https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip. ### Description ; When calling `CollectReadCounts` tool with symlink as input BAM-file, `java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE` occurs. Symlinks seem to work fine with Picard BAM-tools as well as `HaplotypeCaller` and `Mutect2`. Probably HTSJDK level issue, but popped up exception is kind of misleading. #### Stacktrace:; ```; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:863); at htsjdk.samtools.MemoryMappedFileBuffer.<init>(MemoryMappedFileBuffer.java:23); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:931); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579
https://github.com/broadinstitute/gatk/issues/7579:3185,Integrability,message,message,3185,"eader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:384); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:97); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; `gatk --java-options ""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" CollectReadCounts -R [...].fasta -L [...].interval_list -I Sample.bam --interval-merging-rule OVERLAPPING_ONLY -O Sample.counts.hdf5` where `Sample.bam` is a symlink to a BAM-file from an NFS-mounted location. #### Possible workaround; You can create symlink for the parent directory of the target BAM-file instead. #### Expected behavior; I would expect a better error message that would help to indicate the cause behind ""Size exceeds Integer.MAX_VALUE"" exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579
https://github.com/broadinstitute/gatk/issues/7579:1437,Performance,load,loadNextIterator,1437," `HaplotypeCaller` and `Mutect2`. Probably HTSJDK level issue, but popped up exception is kind of misleading. #### Stacktrace:; ```; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:863); at htsjdk.samtools.MemoryMappedFileBuffer.<init>(MemoryMappedFileBuffer.java:23); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:931); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:384); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:97); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579
https://github.com/broadinstitute/gatk/issues/7581:69,Availability,error,errors,69,"## Documentation request. There are multiple statistical terminology errors, especially misuses of the term ""likelihood,"" in [Assigning per-sample genotypes (HaplotypeCaller)](https://gatk.broadinstitute.org/hc/en-us/articles/360035890511). The statistics are difficult enough without pervasive misuse of statistical terminology in the documentation of software considered a _de facto_ standard, which IMO makes this fairly serious. This is a follow-on report to #7577 . ----. 1. The heading ""Calculating genotype likelihoods using Bayes' Theorem"" should read ""Calculating genotype **posteriors** using Bayes' Theorem"" The Contents section needs corresponding correction, of course.; 2. The quote: ""...when we used the PairHMM to produce the likelihoods of each read against each haplotype, and then marginalized them to find the likelihoods of each read for each allele under consideration"" **should read** ""...when we used the PairHMM to produce **P(read|haplotype),** and then marginalized them to find the **likelihoods of each allele** under consideration.""; 3. The quote: ""...the point of calculating this likelihood is to determine..."" **should read** ""...the point of calculating this **posterior** is to determine..."". ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7581
https://github.com/broadinstitute/gatk/issues/7582:101,Availability,error,errors,101,"This user is noticing that when running BQSR with the default inflater, some blocks have compression errors that result in issues while running HaplotypeCaller. This request was created from a contribution made by Jacob Wang on November 02, 2021 08:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:368,Availability,Error,Error,368,"This user is noticing that when running BQSR with the default inflater, some blocks have compression errors that result in issues while running HaplotypeCaller. This request was created from a contribution made by Jacob Wang on November 02, 2021 08:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:479,Availability,Error,Error,479,"This user is noticing that when running BQSR with the default inflater, some blocks have compression errors that result in issues while running HaplotypeCaller. This request was created from a contribution made by Jacob Wang on November 02, 2021 08:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:702,Availability,error,error,702,"This user is noticing that when running BQSR with the default inflater, some blocks have compression errors that result in issues while running HaplotypeCaller. This request was created from a contribution made by Jacob Wang on November 02, 2021 08:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:1387,Availability,error,error,1387,".broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:14:32.573 INFO HaplotypeCaller - Executing as ngs@ngs-linux on Linux v5.8.0-59-generic amd64 ; ; 14:14:32.573 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_292-8u292-b10-0ubuntu1~20.04-b10 ; ; 14:14:32.573 INFO HaplotypeCall",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:4677,Availability,Avail,Available,4677," the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled ; ; 14:14:36.866 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output ; ; 14:14:36.866 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output ; ; 14:14:36.876 INFO NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 14:14:36.878 INFO NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 14:14:36.927 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 14:14:36.928 INFO IntelPairHmm - Available threads: 8 ; ; 14:14:36.928 INFO IntelPairHmm - Requested threads: 4 ; ; 14:14:36.928 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 14:14:37.228 INFO ProgressMeter - Starting traversal ; ; 14:14:37.228 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; ; 14:14:38.715 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:10439 and possibly subsequent; at least 10 samples must have called genotypes ; ; 14:14:47.243 INFO ProgressMeter - chr1:186172 0.2 920 5511.7 ; ; 14:14:57.278 INFO ProgressMeter - chr1:830665 0.3 3650 10922.7 ; ; 14:15:05.692 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:977935 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 14:15:05.692 WARN StrandBiasBySample - Annotation will not be calculated at position chr1:977935 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 14:15:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:7953,Availability,down,down,7953,N StrandBiasBySample - Annotation will not be calculated at position chr6:67407399 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 23:44:10.556 WARN DepthPerSampleHC - Annotation will not be calculated at position chr6:67407415 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 23:44:10.556 WARN StrandBiasBySample - Annotation will not be calculated at position chr6:67407415 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 23:44:14.224 INFO ProgressMeter - chr6:67607778 569.6 5544800 9734.3 ; ; 23:44:24.280 INFO ProgressMeter - chr6:68147283 569.8 5547230 9735.7 ; ; 23:44:30.026 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 27.307544954 ; ; 23:44:30.027 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 4768.198119518001 ; ; 23:44:30.027 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 4695.36 sec ; ; 23:44:30.027 INFO HaplotypeCaller - Shutting down engine ; ; \[2021年11月1日 下午11时44分30秒\] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 569.97 minutes. ; ; Runtime.totalMemory()=742916096 ; ; htsjdk.samtools.SAMFormatException: Did not inflate expected amount ; ; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147) ; ; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:8846,Availability,avail,available,8846,me in java Smith-Waterman : 4695.36 sec ; ; 23:44:30.027 INFO HaplotypeCaller - Shutting down engine ; ; \[2021年11月1日 下午11时44分30秒\] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 569.97 minutes. ; ; Runtime.totalMemory()=742916096 ; ; htsjdk.samtools.SAMFormatException: Did not inflate expected amount ; ; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147) ; ; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331) ; ; at java.io.DataInputStream.read(DataInputStream.java:149) ; ; at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:421) ; ; at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:394) ; ; at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380) ; ; at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:282) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:866) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:1005) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:840) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:834) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:802) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReade,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:1439,Performance,Load,Loading,1439,"29876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:14:32.573 INFO HaplotypeCaller - Executing as ngs@ngs-linux on Linux v5.8.0-59-generic amd64 ; ; 14:14:32.573 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_292-8u292-b10-0ubuntu1~20.04-b10 ; ; 14:14:32.573 INFO HaplotypeCaller - Start Date/Time: 2021年11月1日 下午02时14分31秒 ; ; 14",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:4213,Performance,Load,Loading,4213,"e ; ; 14:14:32.574 INFO HaplotypeCaller - Deflater: IntelDeflater ; ; 14:14:32.574 INFO HaplotypeCaller - Inflater: IntelInflater ; ; 14:14:32.574 INFO HaplotypeCaller - GCS max retries/reopens: 20 ; ; 14:14:32.574 INFO HaplotypeCaller - Requester pays: disabled ; ; 14:14:32.574 INFO HaplotypeCaller - Initializing engine ; ; 14:14:36.824 INFO HaplotypeCaller - Done initializing engine ; ; 14:14:36.826 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled ; ; 14:14:36.866 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output ; ; 14:14:36.866 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output ; ; 14:14:36.876 INFO NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 14:14:36.878 INFO NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 14:14:36.927 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 14:14:36.928 INFO IntelPairHmm - Available threads: 8 ; ; 14:14:36.928 INFO IntelPairHmm - Requested threads: 4 ; ; 14:14:36.928 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 14:14:37.228 INFO ProgressMeter - Starting traversal ; ; 14:14:37.228 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; ; 14:14:38.715 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:10439 and possibly subsequent; at least 10 samples must have called genotypes ; ; 14:14:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:4397,Performance,Load,Loading,4397,"opens: 20 ; ; 14:14:32.574 INFO HaplotypeCaller - Requester pays: disabled ; ; 14:14:32.574 INFO HaplotypeCaller - Initializing engine ; ; 14:14:36.824 INFO HaplotypeCaller - Done initializing engine ; ; 14:14:36.826 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled ; ; 14:14:36.866 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output ; ; 14:14:36.866 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output ; ; 14:14:36.876 INFO NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 14:14:36.878 INFO NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 14:14:36.927 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 14:14:36.928 INFO IntelPairHmm - Available threads: 8 ; ; 14:14:36.928 INFO IntelPairHmm - Requested threads: 4 ; ; 14:14:36.928 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 14:14:37.228 INFO ProgressMeter - Starting traversal ; ; 14:14:37.228 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; ; 14:14:38.715 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:10439 and possibly subsequent; at least 10 samples must have called genotypes ; ; 14:14:47.243 INFO ProgressMeter - chr1:186172 0.2 920 5511.7 ; ; 14:14:57.278 INFO ProgressMeter - chr1:830665 0.3 3650 10922.7 ; ; 14:15:05.692 WARN DepthPerSampleHC - Annotation will not be ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:4805,Performance,multi-thread,multi-threaded,4805,"isherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled ; ; 14:14:36.866 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output ; ; 14:14:36.866 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output ; ; 14:14:36.876 INFO NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 14:14:36.878 INFO NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 14:14:36.927 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 14:14:36.928 INFO IntelPairHmm - Available threads: 8 ; ; 14:14:36.928 INFO IntelPairHmm - Requested threads: 4 ; ; 14:14:36.928 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 14:14:37.228 INFO ProgressMeter - Starting traversal ; ; 14:14:37.228 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; ; 14:14:38.715 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:10439 and possibly subsequent; at least 10 samples must have called genotypes ; ; 14:14:47.243 INFO ProgressMeter - chr1:186172 0.2 920 5511.7 ; ; 14:14:57.278 INFO ProgressMeter - chr1:830665 0.3 3650 10922.7 ; ; 14:15:05.692 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:977935 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 14:15:05.692 WARN StrandBiasBySample - Annotation will not be calculated at position chr1:977935 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 14:15:07.307 INFO ProgressMeter - chr1:1001168 0.5 4640 9255.6 ; ; 14:15:17.364 INFO ProgressMeter - chr1:1225127 0.7 5890 8805.1 ; ; ..............",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:10295,Performance,load,loadNextRecord,10295,code(BAMRecordCodec.java:282) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:866) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:1005) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:840) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:834) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:802) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1058) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1048) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1012) ; ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:591) ; ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:570) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:119) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:156) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.loadNextRead(ReadFilteringIterator.java:53) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.next(ReadFilteringIterator.java:47) ; ; at org.broadinstitute.hellbender.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:11124,Performance,load,loadNextRead,11124, ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:570) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:119) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:156) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.loadNextRead(ReadFilteringIterator.java:53) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.next(ReadFilteringIterator.java:47) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.next(ReadFilteringIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.fillCache(PushToPullIterator.java:72) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.advanceToNextElement(PushToPullIterator.java:58) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.next(PushToPullIterator.java:52) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadCachingIterator.next(ReadCachingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadCachingIterator.ne,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:13376,Performance,load,loadNextAssemblyRegion,13376,ger.collectPendingReads(ReadStateManager.java:160) ; ; at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.lazyLoadNextAlignmentContext(LocusIteratorByState.java:315) ; ; at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.hasNext(LocusIteratorByState.java:252) ; ; at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContext(IntervalAlignmentContextIterator.java:104) ; ; at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContextToCurrentInterval(IntervalAlignmentContextIterator.java:99) ; ; at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.next(IntervalAlignmentContextIterator.java:69) ; ; at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.next(IntervalAlignmentContextIterator.java:21) ; ; at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:120) ; ; at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112) ; ; at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35) ; ; at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192) ; ; at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Mai,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:1728,Safety,detect,detect,1728,"ools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:14:32.573 INFO HaplotypeCaller - Executing as ngs@ngs-linux on Linux v5.8.0-59-generic amd64 ; ; 14:14:32.573 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_292-8u292-b10-0ubuntu1~20.04-b10 ; ; 14:14:32.573 INFO HaplotypeCaller - Start Date/Time: 2021年11月1日 下午02时14分31秒 ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.574 INFO HaplotypeCaller - HTSJDK Version: 2.24.0 ; ; 14:14:32.574 INFO Hap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:1393,Testability,log,log,1393,".broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:14:32.573 INFO HaplotypeCaller - Executing as ngs@ngs-linux on Linux v5.8.0-59-generic amd64 ; ; 14:14:32.573 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_292-8u292-b10-0ubuntu1~20.04-b10 ; ; 14:14:32.573 INFO HaplotypeCall",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:10097,Testability,Assert,AssertingIterator,10097,OrFewer(BinaryCodec.java:421) ; ; at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:394) ; ; at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380) ; ; at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:282) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:866) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:1005) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:840) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:834) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:802) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1058) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1048) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1012) ; ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:591) ; ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:570) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:119) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:156) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/issues/7582:10173,Testability,Assert,AssertingIterator,10173,tes(BinaryCodec.java:394) ; ; at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380) ; ; at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:282) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:866) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:1005) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:840) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:834) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:802) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1058) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1048) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1012) ; ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:591) ; ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:570) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:119) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:156) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.loadNextRead(ReadFilteringIterator.java:53) ; ; at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582
https://github.com/broadinstitute/gatk/pull/7583:1917,Modifiability,Enhance,EnhancedBigQueryReadStub,1917,rpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:129); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:102); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:60); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createSortedReferenceRangeCollectionFromBigQuery(ExtractCohortEngine.java:851); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createVariantsFromUnsortedBigQueryRanges(ExtractCohortEngine.java:891); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.traverse(ExtractCohortEngine.java:224); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohort.traverse(ExtractCohort.java:335); 	at org.broadinstitute.hellbender.engin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583
https://github.com/broadinstitute/gatk/pull/7583:1949,Modifiability,Enhance,EnhancedBigQueryReadStub,1949,plBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:129); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:102); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:60); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createSortedReferenceRangeCollectionFromBigQuery(ExtractCohortEngine.java:851); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createVariantsFromUnsortedBigQueryRanges(ExtractCohortEngine.java:891); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.traverse(ExtractCohortEngine.java:224); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohort.traverse(ExtractCohort.java:335); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583
https://github.com/broadinstitute/gatk/pull/7583:1303,Security,access,access,1303,"eue; SEVERE: *~*~*~ Channel ManagedChannelImpl{logId=121, target=bigquerystorage.googleapis.com:443} was not shutdown properly!!! ~*~*~*; Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.; java.lang.RuntimeException: ManagedChannel allocation site; 	at io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93); 	at io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53); 	at io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44); 	at io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:129); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:102); 	at org.broadinstitute.hellbender.utils.bigquery.St",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583
https://github.com/broadinstitute/gatk/pull/7583:75,Testability,log,logs,75,"When running with multiple table (ie _001, _002) this would show up in the logs due to the bigquery client not being closed. It didn't cause any harm, but is alarming! Closing the ranges-based readers appropriately now. ```; Nov 23, 2021 5:45:53 AM io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference cleanQueue; SEVERE: *~*~*~ Channel ManagedChannelImpl{logId=121, target=bigquerystorage.googleapis.com:443} was not shutdown properly!!! ~*~*~*; Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.; java.lang.RuntimeException: ManagedChannel allocation site; 	at io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93); 	at io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53); 	at io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44); 	at io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583
https://github.com/broadinstitute/gatk/pull/7583:372,Testability,log,logId,372,"When running with multiple table (ie _001, _002) this would show up in the logs due to the bigquery client not being closed. It didn't cause any harm, but is alarming! Closing the ranges-based readers appropriately now. ```; Nov 23, 2021 5:45:53 AM io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference cleanQueue; SEVERE: *~*~*~ Channel ManagedChannelImpl{logId=121, target=bigquerystorage.googleapis.com:443} was not shutdown properly!!! ~*~*~*; Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.; java.lang.RuntimeException: ManagedChannel allocation site; 	at io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93); 	at io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53); 	at io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44); 	at io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583
https://github.com/broadinstitute/gatk/pull/7583:1912,Testability,stub,stub,1912,a:44); 	at io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:129); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:102); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:60); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createSortedReferenceRangeCollectionFromBigQuery(ExtractCohortEngine.java:851); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createVariantsFromUnsortedBigQueryRanges(ExtractCohortEngine.java:891); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.traverse(ExtractCohortEngine.java:224); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohort.traverse(ExtractCohort.java:335); 	at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583
https://github.com/broadinstitute/gatk/pull/7584:174,Deployability,update,updated,174,I also need to add documentation still. - document places where --ignore-above-gq-threshold is used and why / what it means; - if we went back to GQ60 what would need to get updated?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7584
https://github.com/broadinstitute/gatk/issues/7586:1152,Deployability,upgrade,upgrade,1152,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:1172,Deployability,release,release,1172,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:121,Integrability,interface,interface,121,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:1008,Integrability,contract,contract,1008,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:430,Security,hash,hashCode,430,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:737,Security,access,access,737,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:229,Testability,test,testCombineAnnotations,229,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:253,Testability,test,test,253,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:923,Testability,test,test,923,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:141,Usability,Simpl,SimpleAllele,141,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7586:1100,Usability,clear,clear,1100,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586
https://github.com/broadinstitute/gatk/issues/7587:174,Integrability,message,message,174,"Hi,. I want to launch dockstore mutect2_pon and mutect_2 workflow without gnomad option because I don't have germline reference with AF information for mouse but I have this message : ; ```; Required workflow input 'Mutect2_Panel.gnomad_idx' not specified; Required workflow input 'Mutect2_Panel.gnomad' not specified; ```; Is there a way to launch this two workflow without gnomad and gnomad_idx option ?; Thanks in advance for help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7587
https://github.com/broadinstitute/gatk/issues/7589:341,Availability,error,error,341,"## Bug Report. ### Affected tool(s) or class(es); Reblock | JointGenotype. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; We found a bug while running the latest JointGenotype pipeline (2.0.2). We are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7589:372,Availability,ERROR,ERROR,372,"## Bug Report. ### Affected tool(s) or class(es); Reblock | JointGenotype. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; We found a bug while running the latest JointGenotype pipeline (2.0.2). We are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7589:5474,Availability,error,error,5474,"should contain A element(s).; In file/stream ""2105614020_IVRN_stream"", at contig ""chr14"", position 60604048, for sample ""Einstein"", the field AF has 3 elements; expected 2; Using GATK jar /gatk/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms8g -jar /gatk/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate. </p>; </details>. #### Steps to reproduce; Run ReblockGVCF with Dragen 3.6.3 gvcf output. #### Expected behavior; Do the parse as expected. #### Actual behavior; You can see below 2 different examples that return the same error.; <br>-- First; * Dragen output; ```; chr14 60604048 . TCACACACACACA TCACACA,T,<NON_REF> 135.20 PASS DP=44;MQ=250.00;MQRankSum=1.636;ReadPosRankSum=1.540;FractionInformativeReads=0.818;R2_5P_bias=0.000 GT:AD:AF:DP:F1R2:F2R1:GQ:PL:SPL:ICNT:GP:PRI:SB:MB 1/1:1,34,1,0:0.944,0.028,0.000:36:1,20,1,0:0,14,0,0:82:140,88,0,967,86,138,935,101,985,948:255,0,220:0,29:1.3520e+02,8.5204e+01,0.0000e+00,4.5000e+02,8.5278e+01,1.3828e+02,4.5000e+02,1.3246e+02,4.5000e+02,4.5000e+02:0.00,2.00,5.00,2.00,4.00,5.00,34.77,36.77,36.77,37.77:1,0,18,17:1,0,22,13; ```; * ReblockGVCF (4.2.3.0) output; ```; chr14 60604048 . TCACACA T,<NON_REF> 135.20 . AS_QUALapprox=|140|0;AS_VarDP=1|34|0;DP=44;MQ=250.00;MQRankSum=1.636;QUALapprox=140;RAW_GT_COUNT=0,0,1;RAW_MQandDP=2750000,44;ReadPosRankSum=1.540;VarDP=35 GT:AD:AF:DP:F1R2:F2R1:GQ:ICNT:MB:PL:PRI:SB:SPL 1/1:1,34,0:0.944,0.028,0.000:36:1,20,1,0:0,14,0,0:88:0,29:1,0,22,13:140,88,0,935,101,948:0.00,2.00,5.00,2.00,4.00,5.00,34.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7589:6908,Availability,error,error,6908,"204e+01,0.0000e+00,4.5000e+02,8.5278e+01,1.3828e+02,4.5000e+02,1.3246e+02,4.5000e+02,4.5000e+02:0.00,2.00,5.00,2.00,4.00,5.00,34.77,36.77,36.77,37.77:1,0,18,17:1,0,22,13; ```; * ReblockGVCF (4.2.3.0) output; ```; chr14 60604048 . TCACACA T,<NON_REF> 135.20 . AS_QUALapprox=|140|0;AS_VarDP=1|34|0;DP=44;MQ=250.00;MQRankSum=1.636;QUALapprox=140;RAW_GT_COUNT=0,0,1;RAW_MQandDP=2750000,44;ReadPosRankSum=1.540;VarDP=35 GT:AD:AF:DP:F1R2:F2R1:GQ:ICNT:MB:PL:PRI:SB:SPL 1/1:1,34,0:0.944,0.028,0.000:36:1,20,1,0:0,14,0,0:88:0,29:1,0,22,13:140,88,0,935,101,948:0.00,2.00,5.00,2.00,4.00,5.00,34.77,36.77,36.77,37.77:1,0,18,17:255,0,220; ```. As you can see above, there are small issues related to ReblockGVCF output. First, as you do some kind of ""LeftAlignment"" normalization on the data, you are dropping one of the Alt Variants (in this case, variant ""TCACACA"") but, unfortunately, the FORMAT information is not following this change. The AF still has 3 values from the Dragen output when it was supposed to be 2 (This is the main reason for the GenomicsDBImport error shown here). AD drops one of the reads count, but DP don't follow it (dragen AD = 1,34,1,0 ; DP = 36 ---- reblock AD = 1,34,0 ; DP = 36 <-- it was supposed to be 35). Plus, I'm not sure why, but Reblock doesn't keep the GP format information from dragen. <br>-- Second; * dragen; ```; chrX 25031465 . G GTT,GTTT,<NON_REF> 73.68 PASS DP=9;MQ=241.99;FractionInformativeReads=0.667 GT:AD:AF:DP:F1R2:F2R1:GQ:PL:SPL:ICNT:GP:PRI:SB:MB 1:0,5,1,0:0.833,0.167,0.000:6:0,4,0,0:0,1,1,0:57:78,0,57,73:119,0:0,6:7.3684e+01,8.0247e-06,5.7494e+01,1.0405e+02:0.00,4.00,4.00,34.77:0,0,2,4:0,0,2,4; ```; * ReblockGVCF; ```; chrX 25031465 . G GTT,<NON_REF> 73.68 . AS_QUALapprox=|78|0;AS_VarDP=0|5|0;DP=9;MQ=241.99;QUALapprox=78;RAW_GT_COUNT=0,0,1;RAW_MQandDP=527032,9;VarDP=5 GT:AD:AF:DP:F1R2:F2R1:GQ:ICNT:MB:PL:PRI:SB:SPL 1:0,5,0:0.833,0.167,0.000:6:0,4,0,0:0,1,1,0:73:0,6:0,0,2,4:78,0,73:0.00,4.00,4.00,34.77:0,0,2,4:119,0; ```. Mostly the same happens in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7589:120,Deployability,release,release,120,"## Bug Report. ### Affected tool(s) or class(es); Reblock | JointGenotype. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; We found a bug while running the latest JointGenotype pipeline (2.0.2). We are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7589:270,Deployability,pipeline,pipeline,270,"## Bug Report. ### Affected tool(s) or class(es); Reblock | JointGenotype. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; We found a bug while running the latest JointGenotype pipeline (2.0.2). We are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7589:1041,Performance,Load,Loading,1041,"class(es); Reblock | JointGenotype. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; We found a bug while running the latest JointGenotype pipeline (2.0.2). We are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18:46:55.895 INFO GenomicsDBImport - ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7589:1296,Safety,detect,detect,1296,"are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18:46:55.895 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.895 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.895 INFO GenomicsDBImport - HTSJDK Version: 2.24.1; 18:46:55.895 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7589:190,Testability,test,test,190,"## Bug Report. ### Affected tool(s) or class(es); Reblock | JointGenotype. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; We found a bug while running the latest JointGenotype pipeline (2.0.2). We are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589
https://github.com/broadinstitute/gatk/issues/7591:136,Availability,error,error,136,"The first issue in this post has been solved by updating GATK versions but there appears to be a bug causing an ""unrecognized argument"" error when running GermlineCNVCaller. . This request was created from a contribution made by Ahmed S. Chakroun on December 05, 2021 13:52 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4412031649691-GermlineCNVCaller-prefix-string-too-short](https://gatk.broadinstitute.org/hc/en-us/community/posts/4412031649691-GermlineCNVCaller-prefix-string-too-short). \--. Hi everybody,. Unfortunately, I am getting stuck because the filename length of my hd5 is too short (< 3) to be used as prefix by the GermlineCNVCaller. So, I am posting just to check that nothing is left for me to do at this level of the analysis I am running other than renaming my input files and re-running everything from the scratch :-(. Accordingly, here is the needed info:. GATK version used: 4.2.0.0 ; ; openjdk version ""11.0.11"" 2021-04-20 ; ; OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04) ; ; OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing). Command:. gatk GermlineCNVCaller \\ ; ; \--run-mode COHORT \\ ; ; \-L Twist\_Exome\_Target\_hg38\_preprocessed\_annotated\_gc-filtered.interval\_list \\ ; ; \-imr OVERLAPPING\_ONLY \\ ; ; \--contig-ploidy-calls ploidy-calls \\ ; ; \--annotated-intervals Twist\_Exome\_Target\_hg38\_preprocessed\_annotated.interval\_list \\ ; ; \-I 13-20.counts.hd5 \\ ; ; \-I 722.counts.hd5 \\ ; ; \-I D19047.counts.hd5 \\ ; ; \-I F24F1.counts.hd5 \\ ; ; \-I NS.counts.hd5 \\ ; ; \-I TBC039.counts.hd5 \\ ; ; \-I VP.counts.hd5 \\ ; ; \-I WES002.counts.hd5 \\ ; ; \-I WES02.counts.hd5 \\ ; ; \-I 17062-T1-.counts.hd5 \\ ; ; \-I 18001-M1-.counts.hd5 \\ ; ; \-I 516.counts.hd5 \\ ; ; \-I 533.counts.hd5 \\ ; ; \-I NBH.counts.hd5 \\ ; ; \-I ADN492.counts.hd5 \\ ; ; \-I WES607.counts.hd5 \\ ; ; \--class-coherence-length 1000.0 \\ ; ; \--cnv-coherence-length 1000.0 \\ ; ; \--enable-bias",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7591
https://github.com/broadinstitute/gatk/issues/7591:2289,Availability,Error,Error,2289,"-imr OVERLAPPING\_ONLY \\ ; ; \--contig-ploidy-calls ploidy-calls \\ ; ; \--annotated-intervals Twist\_Exome\_Target\_hg38\_preprocessed\_annotated.interval\_list \\ ; ; \-I 13-20.counts.hd5 \\ ; ; \-I 722.counts.hd5 \\ ; ; \-I D19047.counts.hd5 \\ ; ; \-I F24F1.counts.hd5 \\ ; ; \-I NS.counts.hd5 \\ ; ; \-I TBC039.counts.hd5 \\ ; ; \-I VP.counts.hd5 \\ ; ; \-I WES002.counts.hd5 \\ ; ; \-I WES02.counts.hd5 \\ ; ; \-I 17062-T1-.counts.hd5 \\ ; ; \-I 18001-M1-.counts.hd5 \\ ; ; \-I 516.counts.hd5 \\ ; ; \-I 533.counts.hd5 \\ ; ; \-I NBH.counts.hd5 \\ ; ; \-I ADN492.counts.hd5 \\ ; ; \-I WES607.counts.hd5 \\ ; ; \--class-coherence-length 1000.0 \\ ; ; \--cnv-coherence-length 1000.0 \\ ; ; \--enable-bias-factors true \\ ; ; \--interval-psi-scale 1.0E-6 \\ ; ; \--log-mean-bias-standard-deviation 0.01 \\ ; ; \--sample-psi-scale 1.0E-6 \\ ; ; \--output cohort16 \\ ; ; \--output-prefix cohort16 \\ ; ; \--verbosity DEBUG \\ ; ; \--java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true'. Error:. java.lang.IllegalArgumentException: Prefix string ""NS"" too short: length must be at least 3 ; ; at java.base/java.io.File.createTempFile(File.java:2104) ; ; at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685) ; ; at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666) ; ; at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$1(GermlineCNVCaller.java:430) ; ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; ; at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ; at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ; at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7591
https://github.com/broadinstitute/gatk/issues/7591:3356,Integrability,wrap,wrapAndCopyInto,3356,st be at least 3 ; ; at java.base/java.io.File.createTempFile(File.java:2104) ; ; at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685) ; ; at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666) ; ; at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$1(GermlineCNVCaller.java:430) ; ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; ; at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ; at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ; at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ; ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497) ; ; at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.writeIntervalSubsetReadCountFiles(GermlineCNVCaller.java:429) ; ; at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:319) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7591
https://github.com/broadinstitute/gatk/issues/7591:2061,Testability,log,log-mean-bias-standard-deviation,2061," (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing). Command:. gatk GermlineCNVCaller \\ ; ; \--run-mode COHORT \\ ; ; \-L Twist\_Exome\_Target\_hg38\_preprocessed\_annotated\_gc-filtered.interval\_list \\ ; ; \-imr OVERLAPPING\_ONLY \\ ; ; \--contig-ploidy-calls ploidy-calls \\ ; ; \--annotated-intervals Twist\_Exome\_Target\_hg38\_preprocessed\_annotated.interval\_list \\ ; ; \-I 13-20.counts.hd5 \\ ; ; \-I 722.counts.hd5 \\ ; ; \-I D19047.counts.hd5 \\ ; ; \-I F24F1.counts.hd5 \\ ; ; \-I NS.counts.hd5 \\ ; ; \-I TBC039.counts.hd5 \\ ; ; \-I VP.counts.hd5 \\ ; ; \-I WES002.counts.hd5 \\ ; ; \-I WES02.counts.hd5 \\ ; ; \-I 17062-T1-.counts.hd5 \\ ; ; \-I 18001-M1-.counts.hd5 \\ ; ; \-I 516.counts.hd5 \\ ; ; \-I 533.counts.hd5 \\ ; ; \-I NBH.counts.hd5 \\ ; ; \-I ADN492.counts.hd5 \\ ; ; \-I WES607.counts.hd5 \\ ; ; \--class-coherence-length 1000.0 \\ ; ; \--cnv-coherence-length 1000.0 \\ ; ; \--enable-bias-factors true \\ ; ; \--interval-psi-scale 1.0E-6 \\ ; ; \--log-mean-bias-standard-deviation 0.01 \\ ; ; \--sample-psi-scale 1.0E-6 \\ ; ; \--output cohort16 \\ ; ; \--output-prefix cohort16 \\ ; ; \--verbosity DEBUG \\ ; ; \--java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true'. Error:. java.lang.IllegalArgumentException: Prefix string ""NS"" too short: length must be at least 3 ; ; at java.base/java.io.File.createTempFile(File.java:2104) ; ; at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685) ; ; at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666) ; ; at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$1(GermlineCNVCaller.java:430) ; ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; ; at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ; at java.base/java.util.stream.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7591
https://github.com/broadinstitute/gatk/pull/7594:18,Availability,error,error,18,"Prevent a git lfs error that was caused by accidentally checking storing a .gitattributes file; in gitlfs. When checkout out the repository for the first time or moving from an old commit to a newish one, there's been an error report from git lfs. This was caused by accidentally checking a .gitattributes file into git-lfs which then is read as part of the git lfs checkout process, but since the file is tracked by lfs at the point of checkout it is an lfs stub and throws an error. The problem was introduced here: #6694. See below to reproduce:; ```; git checkout 9951f77c6; git checkout f548ccd708009ddcdfead6525edd23a68d73027b; https://git-lfs.github.com/spec/v1 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:1; sha256:cb156adb10b491dd3ba88c2b491bfb021b3c94fc956d36310c67492504fcdc58 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:2; Updating files: 100% (363/363), done.; Note: switching to 'f548ccd708009ddcdfead6525edd23a68d73027b'.; ```. This fixes the problem going forward by removing the file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594
https://github.com/broadinstitute/gatk/pull/7594:221,Availability,error,error,221,"Prevent a git lfs error that was caused by accidentally checking storing a .gitattributes file; in gitlfs. When checkout out the repository for the first time or moving from an old commit to a newish one, there's been an error report from git lfs. This was caused by accidentally checking a .gitattributes file into git-lfs which then is read as part of the git lfs checkout process, but since the file is tracked by lfs at the point of checkout it is an lfs stub and throws an error. The problem was introduced here: #6694. See below to reproduce:; ```; git checkout 9951f77c6; git checkout f548ccd708009ddcdfead6525edd23a68d73027b; https://git-lfs.github.com/spec/v1 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:1; sha256:cb156adb10b491dd3ba88c2b491bfb021b3c94fc956d36310c67492504fcdc58 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:2; Updating files: 100% (363/363), done.; Note: switching to 'f548ccd708009ddcdfead6525edd23a68d73027b'.; ```. This fixes the problem going forward by removing the file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594
https://github.com/broadinstitute/gatk/pull/7594:478,Availability,error,error,478,"Prevent a git lfs error that was caused by accidentally checking storing a .gitattributes file; in gitlfs. When checkout out the repository for the first time or moving from an old commit to a newish one, there's been an error report from git lfs. This was caused by accidentally checking a .gitattributes file into git-lfs which then is read as part of the git lfs checkout process, but since the file is tracked by lfs at the point of checkout it is an lfs stub and throws an error. The problem was introduced here: #6694. See below to reproduce:; ```; git checkout 9951f77c6; git checkout f548ccd708009ddcdfead6525edd23a68d73027b; https://git-lfs.github.com/spec/v1 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:1; sha256:cb156adb10b491dd3ba88c2b491bfb021b3c94fc956d36310c67492504fcdc58 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:2; Updating files: 100% (363/363), done.; Note: switching to 'f548ccd708009ddcdfead6525edd23a68d73027b'.; ```. This fixes the problem going forward by removing the file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594
https://github.com/broadinstitute/gatk/pull/7594:459,Testability,stub,stub,459,"Prevent a git lfs error that was caused by accidentally checking storing a .gitattributes file; in gitlfs. When checkout out the repository for the first time or moving from an old commit to a newish one, there's been an error report from git lfs. This was caused by accidentally checking a .gitattributes file into git-lfs which then is read as part of the git lfs checkout process, but since the file is tracked by lfs at the point of checkout it is an lfs stub and throws an error. The problem was introduced here: #6694. See below to reproduce:; ```; git checkout 9951f77c6; git checkout f548ccd708009ddcdfead6525edd23a68d73027b; https://git-lfs.github.com/spec/v1 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:1; sha256:cb156adb10b491dd3ba88c2b491bfb021b3c94fc956d36310c67492504fcdc58 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:2; Updating files: 100% (363/363), done.; Note: switching to 'f548ccd708009ddcdfead6525edd23a68d73027b'.; ```. This fixes the problem going forward by removing the file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594
https://github.com/broadinstitute/gatk/pull/7594:704,Testability,test,test,704,"Prevent a git lfs error that was caused by accidentally checking storing a .gitattributes file; in gitlfs. When checkout out the repository for the first time or moving from an old commit to a newish one, there's been an error report from git lfs. This was caused by accidentally checking a .gitattributes file into git-lfs which then is read as part of the git lfs checkout process, but since the file is tracked by lfs at the point of checkout it is an lfs stub and throws an error. The problem was introduced here: #6694. See below to reproduce:; ```; git checkout 9951f77c6; git checkout f548ccd708009ddcdfead6525edd23a68d73027b; https://git-lfs.github.com/spec/v1 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:1; sha256:cb156adb10b491dd3ba88c2b491bfb021b3c94fc956d36310c67492504fcdc58 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:2; Updating files: 100% (363/363), done.; Note: switching to 'f548ccd708009ddcdfead6525edd23a68d73027b'.; ```. This fixes the problem going forward by removing the file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594
https://github.com/broadinstitute/gatk/pull/7594:874,Testability,test,test,874,"Prevent a git lfs error that was caused by accidentally checking storing a .gitattributes file; in gitlfs. When checkout out the repository for the first time or moving from an old commit to a newish one, there's been an error report from git lfs. This was caused by accidentally checking a .gitattributes file into git-lfs which then is read as part of the git lfs checkout process, but since the file is tracked by lfs at the point of checkout it is an lfs stub and throws an error. The problem was introduced here: #6694. See below to reproduce:; ```; git checkout 9951f77c6; git checkout f548ccd708009ddcdfead6525edd23a68d73027b; https://git-lfs.github.com/spec/v1 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:1; sha256:cb156adb10b491dd3ba88c2b491bfb021b3c94fc956d36310c67492504fcdc58 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:2; Updating files: 100% (363/363), done.; Note: switching to 'f548ccd708009ddcdfead6525edd23a68d73027b'.; ```. This fixes the problem going forward by removing the file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594
https://github.com/broadinstitute/gatk/issues/7596:793,Deployability,update,updated,793,"## Documentation request. ### Tool(s) or class(es) involved; [Mutect2 WDL's README](https://github.com/broadinstitute/gatk/tree/2e6045a259ed2ded3e9036a5b44a1f8ba330860d/scripts/mutect2_wdl) references several template JSONs that do not seem to exist in this repository, specifically:; * mutect2_multi_sample_template.json; * mutect2_template.json; * mutect2-replicate-validation_template.json. There is [one JSON](https://github.com/broadinstitute/gatk/blob/2e6045a259ed2ded3e9036a5b44a1f8ba330860d/scripts/mutect2_wdl/mutect_resources_json/mutect_resources_process_gnomAD_2.1.json) in the folder, although it's not immediately clear which of these three (if any) it is meant to replace. ### Description ; I did find some JSONs in the deprecated repo, although I'm not sure if they need to be updated. https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7596
https://github.com/broadinstitute/gatk/issues/7596:628,Usability,clear,clear,628,"## Documentation request. ### Tool(s) or class(es) involved; [Mutect2 WDL's README](https://github.com/broadinstitute/gatk/tree/2e6045a259ed2ded3e9036a5b44a1f8ba330860d/scripts/mutect2_wdl) references several template JSONs that do not seem to exist in this repository, specifically:; * mutect2_multi_sample_template.json; * mutect2_template.json; * mutect2-replicate-validation_template.json. There is [one JSON](https://github.com/broadinstitute/gatk/blob/2e6045a259ed2ded3e9036a5b44a1f8ba330860d/scripts/mutect2_wdl/mutect_resources_json/mutect_resources_process_gnomAD_2.1.json) in the folder, although it's not immediately clear which of these three (if any) it is meant to replace. ### Description ; I did find some JSONs in the deprecated repo, although I'm not sure if they need to be updated. https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7596
https://github.com/broadinstitute/gatk/issues/7598:631,Availability,error,error,631,"## Bug Report. ### Affected tool(s) or class(es); GenomeDBImport. ### Affected version(s); ```; 01:22:35.395 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0; 01:22:35.395 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:22:35.481 INFO GenomicsDBImport - Executing as vr6@node-14-20 on Linux v5.4.0-90-generic amd64; 01:22:35.481 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_282-b08; 01:22:35.482 INFO GenomicsDBImport - Start Date/Time: 10 December 2021 01:22:34 UTC. ```. ### Description . It seems that is possible for some IO error affecting the production of the output tile-db file/folder that is ignored by the reslt of the tool run resulting in a falsely succesful completion. One won't realize of it unil tries to use that db with genotype-gvcfs. STDERR: . ```; Dec 10, 2021 1:22:35 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 01:22:35.395 INFO GenomicsDBImport - ------------------------------------------------------------; 01:22:35.395 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0; 01:22:35.395 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:22:35.481 INFO GenomicsDBImport - Executing as vr6@node-14-20 on Linux v5.4.0-90-generic amd64; 01:22:35.481 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_282-b08; 01:22:35.482 INFO GenomicsDBImport - Start Date/Time: 10 December 2021 01:22:34 UTC; 01:22:35.482 INFO GenomicsDBImport - ------------------------------------------------------------; 01:22:35.482 INFO GenomicsDBImport - ------------------------------------------------------------; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Version: 2.24.0; 01:22:35.483 INFO GenomicsDBImport - Picard Version: 2.25.0; 01:22:35.483 INFO GenomicsDBImport - Built for Spa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:4102,Availability,Error,Error,4102,DBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB compl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:4160,Availability,error,error,4160,DBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB compl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:4327,Availability,error,error,4327,laria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it repr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:4356,Availability,Error,Error,4356,laria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it repr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:4414,Availability,error,error,4414,laria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it repr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:4581,Availability,error,error,4581,te VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create so,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:5192,Availability,down,down,5192,78 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create some read-only conflicting file s. #### Expected behavior. No low-level error messages as the ones above... and that the output can be use for genotype-gvcfs without issue . #### Actual behavior. Error messages coming from the jni dependency. The tool finishes succesfully in apperance but the output file is missing some content render it unusable for VCF calling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:5499,Availability,error,errors,5499,78 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create some read-only conflicting file s. #### Expected behavior. No low-level error messages as the ones above... and that the output can be use for genotype-gvcfs without issue . #### Actual behavior. Error messages coming from the jni dependency. The tool finishes succesfully in apperance but the output file is missing some content render it unusable for VCF calling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:5646,Availability,error,error,5646,78 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create some read-only conflicting file s. #### Expected behavior. No low-level error messages as the ones above... and that the output can be use for genotype-gvcfs without issue . #### Actual behavior. Error messages coming from the jni dependency. The tool finishes succesfully in apperance but the output file is missing some content render it unusable for VCF calling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:5770,Availability,Error,Error,5770,78 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create some read-only conflicting file s. #### Expected behavior. No low-level error messages as the ones above... and that the output can be use for genotype-gvcfs without issue . #### Actual behavior. Error messages coming from the jni dependency. The tool finishes succesfully in apperance but the output file is missing some content render it unusable for VCF calling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:2814,Deployability,update,update,2814,---------------------------------------------; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Version: 2.24.0; 01:22:35.483 INFO GenomicsDBImport - Picard Version: 2.25.0; 01:22:35.483 INFO GenomicsDBImport - Built for Spark Version: 2.4.5; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:22:35.483 INFO GenomicsDBImport - Deflater: IntelDeflater; 01:22:35.483 INFO GenomicsDBImport - Inflater: IntelInflater; 01:22:35.483 INFO GenomicsDBImport - GCS max retries/reopens: 20; 01:22:35.483 INFO GenomicsDBImport - Requester pays: disabled; 01:22:35.484 INFO GenomicsDBImport - Initializing engine; 01:24:58.683 INFO FeatureManager - Using codec BEDCodec to read file file:///lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - I,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:3238,Deployability,update,update,3238,omicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:22:35.483 INFO GenomicsDBImport - Deflater: IntelDeflater; 01:22:35.483 INFO GenomicsDBImport - Inflater: IntelInflater; 01:22:35.483 INFO GenomicsDBImport - GCS max retries/reopens: 20; 01:22:35.483 INFO GenomicsDBImport - Requester pays: disabled; 01:22:35.484 INFO GenomicsDBImport - Initializing engine; 01:24:58.683 INFO FeatureManager - Using codec BEDCodec to read file file:///lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/per,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:3450,Deployability,update,update,3450,: IntelDeflater; 01:22:35.483 INFO GenomicsDBImport - Inflater: IntelInflater; 01:22:35.483 INFO GenomicsDBImport - GCS max retries/reopens: 20; 01:22:35.483 INFO GenomicsDBImport - Requester pays: disabled; 01:22:35.484 INFO GenomicsDBImport - Initializing engine; 01:24:58.683 INFO FeatureManager - Using codec BEDCodec to read file file:///lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error;,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:3661,Deployability,update,update,3661,2:35.484 INFO GenomicsDBImport - Initializing engine; 01:24:58.683 INFO FeatureManager - Using codec BEDCodec to read file file:///lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:3859,Deployability,update,update,3859,9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:4224,Deployability,update,update,4224,DBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB compl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:4478,Deployability,update,update,4478,laria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it repr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:5652,Integrability,message,messages,5652,78 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create some read-only conflicting file s. #### Expected behavior. No low-level error messages as the ones above... and that the output can be use for genotype-gvcfs without issue . #### Actual behavior. Error messages coming from the jni dependency. The tool finishes succesfully in apperance but the output file is missing some content render it unusable for VCF calling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:5776,Integrability,message,messages,5776,78 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create some read-only conflicting file s. #### Expected behavior. No low-level error messages as the ones above... and that the output can be use for genotype-gvcfs without issue . #### Actual behavior. Error messages coming from the jni dependency. The tool finishes succesfully in apperance but the output file is missing some content render it unusable for VCF calling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:5805,Integrability,depend,dependency,5805,78 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create some read-only conflicting file s. #### Expected behavior. No low-level error messages as the ones above... and that the output can be use for genotype-gvcfs without issue . #### Actual behavior. Error messages coming from the jni dependency. The tool finishes succesfully in apperance but the output file is missing some content render it unusable for VCF calling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/issues/7598:1001,Safety,detect,detect,1001,"## Bug Report. ### Affected tool(s) or class(es); GenomeDBImport. ### Affected version(s); ```; 01:22:35.395 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0; 01:22:35.395 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:22:35.481 INFO GenomicsDBImport - Executing as vr6@node-14-20 on Linux v5.4.0-90-generic amd64; 01:22:35.481 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_282-b08; 01:22:35.482 INFO GenomicsDBImport - Start Date/Time: 10 December 2021 01:22:34 UTC. ```. ### Description . It seems that is possible for some IO error affecting the production of the output tile-db file/folder that is ignored by the reslt of the tool run resulting in a falsely succesful completion. One won't realize of it unil tries to use that db with genotype-gvcfs. STDERR: . ```; Dec 10, 2021 1:22:35 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 01:22:35.395 INFO GenomicsDBImport - ------------------------------------------------------------; 01:22:35.395 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0; 01:22:35.395 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:22:35.481 INFO GenomicsDBImport - Executing as vr6@node-14-20 on Linux v5.4.0-90-generic amd64; 01:22:35.481 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_282-b08; 01:22:35.482 INFO GenomicsDBImport - Start Date/Time: 10 December 2021 01:22:34 UTC; 01:22:35.482 INFO GenomicsDBImport - ------------------------------------------------------------; 01:22:35.482 INFO GenomicsDBImport - ------------------------------------------------------------; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Version: 2.24.0; 01:22:35.483 INFO GenomicsDBImport - Picard Version: 2.25.0; 01:22:35.483 INFO GenomicsDBImport - Built for Spa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598
https://github.com/broadinstitute/gatk/pull/7601:347,Availability,error,errors,347,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/pull/7601:258,Deployability,release,release,258,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/pull/7601:385,Deployability,Release,Release,385,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/pull/7601:513,Deployability,Release,Release,513,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/pull/7601:100,Energy Efficiency,monitor,monitoring,100,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/pull/7601:266,Testability,log,logs,266,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/pull/7601:308,Testability,log,logging,308,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/pull/7601:393,Testability,log,log,393,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/pull/7601:521,Testability,log,log,521,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601
https://github.com/broadinstitute/gatk/issues/7603:1306,Deployability,release,release,1306,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Please let us know when log4j Vulnerability",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7603
https://github.com/broadinstitute/gatk/issues/7603:2336,Deployability,release,released,2336,"y exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Please let us know when log4j Vulnerability issue can be released to Conda? we are using this package and waiting log4j issue fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7603
https://github.com/broadinstitute/gatk/issues/7603:1376,Testability,test,test,1376,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Please let us know when log4j Vulnerability",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7603
https://github.com/broadinstitute/gatk/issues/7603:1476,Testability,log,logs,1476,"y exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Please let us know when log4j Vulnerability issue can be released to Conda? we are using this package and waiting log4j issue fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7603
https://github.com/broadinstitute/gatk/issues/7606:550,Availability,down,down,550,"I am trying version 4.2.3.0. I set `--java-options ""-Xmx128g""` and it gradually increased to consume all of the RAM. ```; PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND; 3286969 dario 20 0 136.7g 133.2g 27012 S 1478 26.4 725:33.44 java; ```. The input is a modest whole genome sequencing BAM file. ```; $ ls -lh CSCC_0163-B1.final*; -r-------- 1 dario stgrad 9.2M Dec 15 10:22 CSCC_0163-B1.final.bai; -r-------- 1 dario stgrad 86G Dec 15 10:21 CSCC_0163-B1.final.bam; ```. After about one hour without any progress messages, the process shuts down due to running out of heap space. ```; 12:03:07.666 INFO ProgressMeter - Starting traversal; 12:03:07.666 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 13:08:22.336 INFO GetPileupSummaries - Shutting down engine; [December 15, 2021 at 1:08:22 PM AEDT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 84.45 minutes.; Runtime.totalMemory()=30333206528; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; ```. The two other key parameters were `-L af-only-gnomad.hg38.vcf.gz -V af-only-gnomad.hg38.vcf.gz`. I think this module is far too inefficient to be in production, if this is how it typically works.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7606
https://github.com/broadinstitute/gatk/issues/7606:788,Availability,down,down,788,"I am trying version 4.2.3.0. I set `--java-options ""-Xmx128g""` and it gradually increased to consume all of the RAM. ```; PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND; 3286969 dario 20 0 136.7g 133.2g 27012 S 1478 26.4 725:33.44 java; ```. The input is a modest whole genome sequencing BAM file. ```; $ ls -lh CSCC_0163-B1.final*; -r-------- 1 dario stgrad 9.2M Dec 15 10:22 CSCC_0163-B1.final.bai; -r-------- 1 dario stgrad 86G Dec 15 10:21 CSCC_0163-B1.final.bam; ```. After about one hour without any progress messages, the process shuts down due to running out of heap space. ```; 12:03:07.666 INFO ProgressMeter - Starting traversal; 12:03:07.666 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 13:08:22.336 INFO GetPileupSummaries - Shutting down engine; [December 15, 2021 at 1:08:22 PM AEDT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 84.45 minutes.; Runtime.totalMemory()=30333206528; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; ```. The two other key parameters were `-L af-only-gnomad.hg38.vcf.gz -V af-only-gnomad.hg38.vcf.gz`. I think this module is far too inefficient to be in production, if this is how it typically works.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7606
https://github.com/broadinstitute/gatk/issues/7606:522,Integrability,message,messages,522,"I am trying version 4.2.3.0. I set `--java-options ""-Xmx128g""` and it gradually increased to consume all of the RAM. ```; PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND; 3286969 dario 20 0 136.7g 133.2g 27012 S 1478 26.4 725:33.44 java; ```. The input is a modest whole genome sequencing BAM file. ```; $ ls -lh CSCC_0163-B1.final*; -r-------- 1 dario stgrad 9.2M Dec 15 10:22 CSCC_0163-B1.final.bai; -r-------- 1 dario stgrad 86G Dec 15 10:21 CSCC_0163-B1.final.bam; ```. After about one hour without any progress messages, the process shuts down due to running out of heap space. ```; 12:03:07.666 INFO ProgressMeter - Starting traversal; 12:03:07.666 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 13:08:22.336 INFO GetPileupSummaries - Shutting down engine; [December 15, 2021 at 1:08:22 PM AEDT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 84.45 minutes.; Runtime.totalMemory()=30333206528; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; ```. The two other key parameters were `-L af-only-gnomad.hg38.vcf.gz -V af-only-gnomad.hg38.vcf.gz`. I think this module is far too inefficient to be in production, if this is how it typically works.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7606
https://github.com/broadinstitute/gatk/issues/7608:1139,Availability,error,error,1139,"### Affected tool(s) or class(es). VariantFiltration. ### Affected version(s). Master 2021-01 onwards. ### Description . VariantFiltration's --invalidate-previous-filters prevent any output variant to be marked as PASS even if it passes the filters that are passed to that run of the filtration tool. What I would expect is that variants that pass the current set of filters will be marked as PASS regarless whether they were filtered or not filtered (PASS, or '.') in the input. . Perhaps the case is that this option is not meant.to be used with current/new filters being specified as to provide a way to revert previous filtration, but in that case it should fail if new filters are specified. #### Steps to reproduce. Just try it out with a VCF with some filters already applied and run VF with additional filters that will not result in all variants to be filtered. . You will see that no output variant is set To PASS but rather are kept as unknown '.' . #### Expected behavior. Either it does fail if a new filter is applied in the same tool run or passing variants filter are setlp to Pw. #### Actual behavior. Described above, no error message and no pass variants only ""."" ones.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7608
https://github.com/broadinstitute/gatk/issues/7608:1145,Integrability,message,message,1145,"### Affected tool(s) or class(es). VariantFiltration. ### Affected version(s). Master 2021-01 onwards. ### Description . VariantFiltration's --invalidate-previous-filters prevent any output variant to be marked as PASS even if it passes the filters that are passed to that run of the filtration tool. What I would expect is that variants that pass the current set of filters will be marked as PASS regarless whether they were filtered or not filtered (PASS, or '.') in the input. . Perhaps the case is that this option is not meant.to be used with current/new filters being specified as to provide a way to revert previous filtration, but in that case it should fail if new filters are specified. #### Steps to reproduce. Just try it out with a VCF with some filters already applied and run VF with additional filters that will not result in all variants to be filtered. . You will see that no output variant is set To PASS but rather are kept as unknown '.' . #### Expected behavior. Either it does fail if a new filter is applied in the same tool run or passing variants filter are setlp to Pw. #### Actual behavior. Described above, no error message and no pass variants only ""."" ones.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7608
https://github.com/broadinstitute/gatk/pull/7609:81,Availability,down,download,81,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:61,Deployability,upgrade,upgraded,61,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:183,Deployability,install,install,183,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:403,Deployability,release,release,403,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:90,Modifiability,plugin,plugins,90,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:158,Modifiability,plugin,plugin,158,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:437,Performance,perform,perform,437,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:304,Testability,test,test,304,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:343,Testability,test,test,343,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7609:478,Testability,test,test,478,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609
https://github.com/broadinstitute/gatk/pull/7611:53,Deployability,release,releases,53,"[Tag 4.2.4.0](https://github.com/broadinstitute/gatk/releases/tag/) doesn't appear to address the fact that WDLs still reference older GATK Docker images and therefore are still vulnerable. This is a quick replacement of all references to outdated GATK images that I could find in this repo's WDLs and WDL-specific JSONs. Note that these changes may be breaking, especially for older workflows; I do not have the bandwidth to individually test each one. The following images were not updated as I couldn't find a suitable replacement, although I suspect several could be replaced with the standard GATK image; * pkrusche/hap.py (I'm not sure this would even be affected by the vulnerability); * broad-gotc-prod/genomes-in-the-cloud; * gatksv/sv-base-mini -- referenced as gatksv/sv-base-mini:b3af2e3 in joint_call_exome_cnvs.wdl; * broadinstitute/oncotator -- referenced as broadinstitute/oncotator:1.9.5.0-eval-gatk-protected in cnv_somatic_oncotator_workflow.wdl, but is a fallback option; * us.gcr.io/broad-dsde-methods/haplochecker; * us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.4.2-1552931386. If this PR is accepted, note that all affected WDLs should also have their default tag on Dockstore changed -- only [MitochondriaPipeline](https://dockstore.org/workflows/github.com/broadinstitute/gatk/MitochondriaPipeline:master?tab=files) defaults to master in Dockstore if I recall correctly. [gatk4-rnaseq-germline-snps-indels](https://dockstore.org/workflows/github.com/gatk-workflows/gatk4-rnaseq-germline-snps-indels/gatk4-rnaseq-germline-snps-indels:master?tab=versions) defaults to master but is in the gatk-workflows repo, not this one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7611
https://github.com/broadinstitute/gatk/pull/7611:484,Deployability,update,updated,484,"[Tag 4.2.4.0](https://github.com/broadinstitute/gatk/releases/tag/) doesn't appear to address the fact that WDLs still reference older GATK Docker images and therefore are still vulnerable. This is a quick replacement of all references to outdated GATK images that I could find in this repo's WDLs and WDL-specific JSONs. Note that these changes may be breaking, especially for older workflows; I do not have the bandwidth to individually test each one. The following images were not updated as I couldn't find a suitable replacement, although I suspect several could be replaced with the standard GATK image; * pkrusche/hap.py (I'm not sure this would even be affected by the vulnerability); * broad-gotc-prod/genomes-in-the-cloud; * gatksv/sv-base-mini -- referenced as gatksv/sv-base-mini:b3af2e3 in joint_call_exome_cnvs.wdl; * broadinstitute/oncotator -- referenced as broadinstitute/oncotator:1.9.5.0-eval-gatk-protected in cnv_somatic_oncotator_workflow.wdl, but is a fallback option; * us.gcr.io/broad-dsde-methods/haplochecker; * us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.4.2-1552931386. If this PR is accepted, note that all affected WDLs should also have their default tag on Dockstore changed -- only [MitochondriaPipeline](https://dockstore.org/workflows/github.com/broadinstitute/gatk/MitochondriaPipeline:master?tab=files) defaults to master in Dockstore if I recall correctly. [gatk4-rnaseq-germline-snps-indels](https://dockstore.org/workflows/github.com/gatk-workflows/gatk4-rnaseq-germline-snps-indels/gatk4-rnaseq-germline-snps-indels:master?tab=versions) defaults to master but is in the gatk-workflows repo, not this one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7611
https://github.com/broadinstitute/gatk/pull/7611:439,Testability,test,test,439,"[Tag 4.2.4.0](https://github.com/broadinstitute/gatk/releases/tag/) doesn't appear to address the fact that WDLs still reference older GATK Docker images and therefore are still vulnerable. This is a quick replacement of all references to outdated GATK images that I could find in this repo's WDLs and WDL-specific JSONs. Note that these changes may be breaking, especially for older workflows; I do not have the bandwidth to individually test each one. The following images were not updated as I couldn't find a suitable replacement, although I suspect several could be replaced with the standard GATK image; * pkrusche/hap.py (I'm not sure this would even be affected by the vulnerability); * broad-gotc-prod/genomes-in-the-cloud; * gatksv/sv-base-mini -- referenced as gatksv/sv-base-mini:b3af2e3 in joint_call_exome_cnvs.wdl; * broadinstitute/oncotator -- referenced as broadinstitute/oncotator:1.9.5.0-eval-gatk-protected in cnv_somatic_oncotator_workflow.wdl, but is a fallback option; * us.gcr.io/broad-dsde-methods/haplochecker; * us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.4.2-1552931386. If this PR is accepted, note that all affected WDLs should also have their default tag on Dockstore changed -- only [MitochondriaPipeline](https://dockstore.org/workflows/github.com/broadinstitute/gatk/MitochondriaPipeline:master?tab=files) defaults to master in Dockstore if I recall correctly. [gatk4-rnaseq-germline-snps-indels](https://dockstore.org/workflows/github.com/gatk-workflows/gatk4-rnaseq-germline-snps-indels/gatk4-rnaseq-germline-snps-indels:master?tab=versions) defaults to master but is in the gatk-workflows repo, not this one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7611
https://github.com/broadinstitute/gatk/pull/7612:154,Deployability,pipeline,pipeline,154,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:174,Deployability,patch,patch,174,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:231,Integrability,message,message,231,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:131,Testability,log,logging,131,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:222,Testability,log,log,222,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:289,Testability,log,logic,289,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:338,Testability,log,logic,338,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:620,Testability,log,log,620,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:678,Testability,test,tests,678,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:771,Testability,test,tests,771,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/pull/7612:817,Testability,log,logic,817,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612
https://github.com/broadinstitute/gatk/issues/7614:172,Availability,error,error,172,"Hello,. I use GATK(4.2.0.0) HaplotypeCaller call SNPs and CombineGVCFs combining the .g.vcf files. everything goes well.; However, when I use GenotypeGVCFs, I encounter an error [htsjdk.samtools.SAMFormatException: Did not inflate expected amount]. And I see the same Error in [#7582](https://github.com/broadinstitute/gatk/issues/7582), I try to add two parameters --use-jdk-deflater & --use-jdk-inflater, but I get the new Error [java.util.zip.DataFormatException: invalid code lengths set]. ----. ### Version and Tools; GATK 4.2.0.0 GenotypeGVCFs. ### The commend ; java -Xmx10g -jar /home/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs --use-jdk-inflater true --use-jdk-deflater true -R ref.rename.fa -V test.vcf.gz -O test_geno.vcf.gz. ### Error log 1. 21:12:43.028 INFO GenotypeGVCFs - Shutting down engine; [2021年12月20日 下午09时12分43秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.12 minutes.; Runtime.totalMemory()=4856479744; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:268,Availability,Error,Error,268,"Hello,. I use GATK(4.2.0.0) HaplotypeCaller call SNPs and CombineGVCFs combining the .g.vcf files. everything goes well.; However, when I use GenotypeGVCFs, I encounter an error [htsjdk.samtools.SAMFormatException: Did not inflate expected amount]. And I see the same Error in [#7582](https://github.com/broadinstitute/gatk/issues/7582), I try to add two parameters --use-jdk-deflater & --use-jdk-inflater, but I get the new Error [java.util.zip.DataFormatException: invalid code lengths set]. ----. ### Version and Tools; GATK 4.2.0.0 GenotypeGVCFs. ### The commend ; java -Xmx10g -jar /home/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs --use-jdk-inflater true --use-jdk-deflater true -R ref.rename.fa -V test.vcf.gz -O test_geno.vcf.gz. ### Error log 1. 21:12:43.028 INFO GenotypeGVCFs - Shutting down engine; [2021年12月20日 下午09时12分43秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.12 minutes.; Runtime.totalMemory()=4856479744; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:425,Availability,Error,Error,425,"Hello,. I use GATK(4.2.0.0) HaplotypeCaller call SNPs and CombineGVCFs combining the .g.vcf files. everything goes well.; However, when I use GenotypeGVCFs, I encounter an error [htsjdk.samtools.SAMFormatException: Did not inflate expected amount]. And I see the same Error in [#7582](https://github.com/broadinstitute/gatk/issues/7582), I try to add two parameters --use-jdk-deflater & --use-jdk-inflater, but I get the new Error [java.util.zip.DataFormatException: invalid code lengths set]. ----. ### Version and Tools; GATK 4.2.0.0 GenotypeGVCFs. ### The commend ; java -Xmx10g -jar /home/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs --use-jdk-inflater true --use-jdk-deflater true -R ref.rename.fa -V test.vcf.gz -O test_geno.vcf.gz. ### Error log 1. 21:12:43.028 INFO GenotypeGVCFs - Shutting down engine; [2021年12月20日 下午09时12分43秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.12 minutes.; Runtime.totalMemory()=4856479744; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:765,Availability,Error,Error,765,"Hello,. I use GATK(4.2.0.0) HaplotypeCaller call SNPs and CombineGVCFs combining the .g.vcf files. everything goes well.; However, when I use GenotypeGVCFs, I encounter an error [htsjdk.samtools.SAMFormatException: Did not inflate expected amount]. And I see the same Error in [#7582](https://github.com/broadinstitute/gatk/issues/7582), I try to add two parameters --use-jdk-deflater & --use-jdk-inflater, but I get the new Error [java.util.zip.DataFormatException: invalid code lengths set]. ----. ### Version and Tools; GATK 4.2.0.0 GenotypeGVCFs. ### The commend ; java -Xmx10g -jar /home/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs --use-jdk-inflater true --use-jdk-deflater true -R ref.rename.fa -V test.vcf.gz -O test_geno.vcf.gz. ### Error log 1. 21:12:43.028 INFO GenotypeGVCFs - Shutting down engine; [2021年12月20日 下午09时12分43秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.12 minutes.; Runtime.totalMemory()=4856479744; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:821,Availability,down,down,821,"Hello,. I use GATK(4.2.0.0) HaplotypeCaller call SNPs and CombineGVCFs combining the .g.vcf files. everything goes well.; However, when I use GenotypeGVCFs, I encounter an error [htsjdk.samtools.SAMFormatException: Did not inflate expected amount]. And I see the same Error in [#7582](https://github.com/broadinstitute/gatk/issues/7582), I try to add two parameters --use-jdk-deflater & --use-jdk-inflater, but I get the new Error [java.util.zip.DataFormatException: invalid code lengths set]. ----. ### Version and Tools; GATK 4.2.0.0 GenotypeGVCFs. ### The commend ; java -Xmx10g -jar /home/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs --use-jdk-inflater true --use-jdk-deflater true -R ref.rename.fa -V test.vcf.gz -O test_geno.vcf.gz. ### Error log 1. 21:12:43.028 INFO GenotypeGVCFs - Shutting down engine; [2021年12月20日 下午09时12分43秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.12 minutes.; Runtime.totalMemory()=4856479744; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:1671,Availability,avail,available,1671,-jdk-deflater true -R ref.rename.fa -V test.vcf.gz -O test_geno.vcf.gz. ### Error log 1. 21:12:43.028 INFO GenotypeGVCFs - Shutting down engine; [2021年12月20日 下午09时12分43秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.12 minutes.; Runtime.totalMemory()=4856479744; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132); 	at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:84); 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:300); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:4325,Availability,Error,Error,4325,achOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). ### Error log 2.; [2021年12月20日 下午08时36分52秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 7.54 minutes.; Runtime.totalMemory()=4957667328; htsjdk.samtools.util.RuntimeIOException: java.util.zip.DataFormatException: invalid code lengths set; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:161); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:5209,Availability,avail,available,5209,er.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). ### Error log 2.; [2021年12月20日 下午08时36分52秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 7.54 minutes.; Runtime.totalMemory()=4957667328; htsjdk.samtools.util.RuntimeIOException: java.util.zip.DataFormatException: invalid code lengths set; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:161); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132); 	at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:84); 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:300); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:3248,Integrability,wrap,wrapAndCopyInto,3248,oder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:300); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:6786,Integrability,wrap,wrapAndCopyInto,6786,oder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:300); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:728,Testability,test,test,728,"Hello,. I use GATK(4.2.0.0) HaplotypeCaller call SNPs and CombineGVCFs combining the .g.vcf files. everything goes well.; However, when I use GenotypeGVCFs, I encounter an error [htsjdk.samtools.SAMFormatException: Did not inflate expected amount]. And I see the same Error in [#7582](https://github.com/broadinstitute/gatk/issues/7582), I try to add two parameters --use-jdk-deflater & --use-jdk-inflater, but I get the new Error [java.util.zip.DataFormatException: invalid code lengths set]. ----. ### Version and Tools; GATK 4.2.0.0 GenotypeGVCFs. ### The commend ; java -Xmx10g -jar /home/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs --use-jdk-inflater true --use-jdk-deflater true -R ref.rename.fa -V test.vcf.gz -O test_geno.vcf.gz. ### Error log 1. 21:12:43.028 INFO GenotypeGVCFs - Shutting down engine; [2021年12月20日 下午09时12分43秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.12 minutes.; Runtime.totalMemory()=4856479744; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:771,Testability,log,log,771,"Hello,. I use GATK(4.2.0.0) HaplotypeCaller call SNPs and CombineGVCFs combining the .g.vcf files. everything goes well.; However, when I use GenotypeGVCFs, I encounter an error [htsjdk.samtools.SAMFormatException: Did not inflate expected amount]. And I see the same Error in [#7582](https://github.com/broadinstitute/gatk/issues/7582), I try to add two parameters --use-jdk-deflater & --use-jdk-inflater, but I get the new Error [java.util.zip.DataFormatException: invalid code lengths set]. ----. ### Version and Tools; GATK 4.2.0.0 GenotypeGVCFs. ### The commend ; java -Xmx10g -jar /home/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs --use-jdk-inflater true --use-jdk-deflater true -R ref.rename.fa -V test.vcf.gz -O test_geno.vcf.gz. ### Error log 1. 21:12:43.028 INFO GenotypeGVCFs - Shutting down engine; [2021年12月20日 下午09时12分43秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.12 minutes.; Runtime.totalMemory()=4856479744; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/issues/7614:4331,Testability,log,log,4331,achOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). ### Error log 2.; [2021年12月20日 下午08时36分52秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 7.54 minutes.; Runtime.totalMemory()=4957667328; htsjdk.samtools.util.RuntimeIOException: java.util.zip.DataFormatException: invalid code lengths set; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:161); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614
https://github.com/broadinstitute/gatk/pull/7620:140,Availability,error,error,140,"The BigQuery library upgrade for extract, broke ingest :/ This fixes that. It also rethrows an exception we were eating that I noticed. The error seen during ingest was. ```; java.lang.IllegalArgumentException: JSONObject does not have a bytes field at root.sample_id.; 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.fillField(JsonToProtoMessage.java:306); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessageImpl(JsonToProtoMessage.java:138); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessage(JsonToProtoMessage.java:86); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:110); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:90); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.writeLoadStatus(CreateVariantIngestFiles.java:202); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.onTraversalSuccess(CreateVariantIngestFiles.java:369); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7620
https://github.com/broadinstitute/gatk/pull/7620:21,Deployability,upgrade,upgrade,21,"The BigQuery library upgrade for extract, broke ingest :/ This fixes that. It also rethrows an exception we were eating that I noticed. The error seen during ingest was. ```; java.lang.IllegalArgumentException: JSONObject does not have a bytes field at root.sample_id.; 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.fillField(JsonToProtoMessage.java:306); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessageImpl(JsonToProtoMessage.java:138); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessage(JsonToProtoMessage.java:86); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:110); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:90); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.writeLoadStatus(CreateVariantIngestFiles.java:202); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.onTraversalSuccess(CreateVariantIngestFiles.java:369); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7620
https://github.com/broadinstitute/gatk/issues/7621:2323,Deployability,release,releases,2323,"e adopt a new default branch name and retire the use of 'master'.*. The use of 'master' as the default branch is quickly tipping into the realm of being archaic, and present the image of being increasingly tone deaf. 'main' is the commonly accepted replacement on GitHub, but I'm stopping short of suggesting the replacement name, just asking ""please retire master"". . ### 'master has a specific technical meaning' . It does. And is also an example of structural racism, which; > refers to the complex interactions of large scale societal systems, practices, ideologies, and programs that produce and and perpetuate inequities for racial minorities. The key aspect of structural or systematic racism is that these macro-level mechanisms operate independent of the intentions and actions of individuals, so that even if individual racism is not present, the adverse conditions and inequalities for racial minorities will continue to exist - [Gee & Ford, 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4306458/). _And if you just felt as if you were accused of being a racist, please re-read the above definition again_ I'm addressing the bureaucracy ( which I can not realistically effect much change with, but some of you can).; ; Ultimately, a fair number of people are to varying degrees uncomfortable or threatened by this trope. And on these merits alone are a good reason to ditch master. [The process is straight forward and documentation abounds](https://www.git-tower.com/learn/git/faq/git-rename-master-to-main), [there are even tools to help automate the conversion](https://github.com/dsyer/main-branch-switch). But it will take time, and is not the most exciting work in the world. . Perhaps it's a sticky change as part of all major releases, or otherwise planned for? So, that's my vote, if I were to be asked to vote that is. And, if there are detailed plans in place to make this change, horray! Link them here, and now you have a(nother?) nice honeypot for this topic. John Major",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7621
https://github.com/broadinstitute/gatk/issues/7621:1884,Security,threat,threatened,1884,"e adopt a new default branch name and retire the use of 'master'.*. The use of 'master' as the default branch is quickly tipping into the realm of being archaic, and present the image of being increasingly tone deaf. 'main' is the commonly accepted replacement on GitHub, but I'm stopping short of suggesting the replacement name, just asking ""please retire master"". . ### 'master has a specific technical meaning' . It does. And is also an example of structural racism, which; > refers to the complex interactions of large scale societal systems, practices, ideologies, and programs that produce and and perpetuate inequities for racial minorities. The key aspect of structural or systematic racism is that these macro-level mechanisms operate independent of the intentions and actions of individuals, so that even if individual racism is not present, the adverse conditions and inequalities for racial minorities will continue to exist - [Gee & Ford, 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4306458/). _And if you just felt as if you were accused of being a racist, please re-read the above definition again_ I'm addressing the bureaucracy ( which I can not realistically effect much change with, but some of you can).; ; Ultimately, a fair number of people are to varying degrees uncomfortable or threatened by this trope. And on these merits alone are a good reason to ditch master. [The process is straight forward and documentation abounds](https://www.git-tower.com/learn/git/faq/git-rename-master-to-main), [there are even tools to help automate the conversion](https://github.com/dsyer/main-branch-switch). But it will take time, and is not the most exciting work in the world. . Perhaps it's a sticky change as part of all major releases, or otherwise planned for? So, that's my vote, if I were to be asked to vote that is. And, if there are detailed plans in place to make this change, horray! Link them here, and now you have a(nother?) nice honeypot for this topic. John Major",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7621
https://github.com/broadinstitute/gatk/issues/7621:2057,Usability,learn,learn,2057,"e adopt a new default branch name and retire the use of 'master'.*. The use of 'master' as the default branch is quickly tipping into the realm of being archaic, and present the image of being increasingly tone deaf. 'main' is the commonly accepted replacement on GitHub, but I'm stopping short of suggesting the replacement name, just asking ""please retire master"". . ### 'master has a specific technical meaning' . It does. And is also an example of structural racism, which; > refers to the complex interactions of large scale societal systems, practices, ideologies, and programs that produce and and perpetuate inequities for racial minorities. The key aspect of structural or systematic racism is that these macro-level mechanisms operate independent of the intentions and actions of individuals, so that even if individual racism is not present, the adverse conditions and inequalities for racial minorities will continue to exist - [Gee & Ford, 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4306458/). _And if you just felt as if you were accused of being a racist, please re-read the above definition again_ I'm addressing the bureaucracy ( which I can not realistically effect much change with, but some of you can).; ; Ultimately, a fair number of people are to varying degrees uncomfortable or threatened by this trope. And on these merits alone are a good reason to ditch master. [The process is straight forward and documentation abounds](https://www.git-tower.com/learn/git/faq/git-rename-master-to-main), [there are even tools to help automate the conversion](https://github.com/dsyer/main-branch-switch). But it will take time, and is not the most exciting work in the world. . Perhaps it's a sticky change as part of all major releases, or otherwise planned for? So, that's my vote, if I were to be asked to vote that is. And, if there are detailed plans in place to make this change, horray! Link them here, and now you have a(nother?) nice honeypot for this topic. John Major",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7621
https://github.com/broadinstitute/gatk/issues/7622:0,Deployability,UPDATE,UPDATE,0,"UPDATE: @lbergelson and I started creating this as an extension to SplitIntervals, but it quickly because very complex to fit it into that framework/abstraction so we decided to create a specialized tool for GVS. It would be valuable for SplitIntervals to be able to split intervals based not on number of genomic bases, but by using a set of weights. Ideally this new mode would read in a BED file containing the weights in the score field and attempt to produce a series of intervals that have equal total weights. . Note: ` --dont-mix-contigs` should still continue to work. ** Why? **; In the Genomic Variant Store, we have found that scattering work by ""# of genomic bases"" does not lead to even runtimes for the shards. ![image](https://user-images.githubusercontent.com/1423491/147964102-d2c83dea-8486-4699-9e7a-eef3dc759732.png). Instead we have found that an excellent proxy for runtime is the number of variants contained in a given interval:. ![image](https://user-images.githubusercontent.com/1423491/147964259-333f4058-a701-4b31-b410-242518d1b3b2.png). And furthermore, that this generalizes even when we use a subset of a different dataset. ![image](https://user-images.githubusercontent.com/1423491/147964392-17d045e9-e2f2-467b-8eae-77bd63291902.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7622
https://github.com/broadinstitute/gatk/issues/7625:74,Deployability,update,updated,74,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:2320,Deployability,install,installDist,2320,"' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see them when doing the upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:2451,Deployability,upgrade,upgrade,2451,"' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see them when doing the upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:503,Integrability,depend,dependency,503,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:566,Integrability,depend,depending,566,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:993,Integrability,depend,dependency,993,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:1056,Integrability,depend,depending,1056,"gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:1493,Integrability,depend,dependency,1493,"ndardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:1556,Integrability,depend,depending,1556,"' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see them when doing the upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:1974,Modifiability,plugin,plugins,1974,"' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see them when doing the upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:153,Performance,optimiz,optimizations,153,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:2092,Performance,optimiz,optimizations,2092,"' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see them when doing the upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:269,Safety,detect,detected,269,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:762,Safety,detect,detected,762,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7625:1252,Safety,detect,detected,1252,"llowing location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these task",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625
https://github.com/broadinstitute/gatk/issues/7626:78,Integrability,depend,dependencies,78,Hi everyone. . It may sound ridiculous but is it possible to strip all log4j2 dependencies from gatk and get a lean and free from potential vulnerabilities version. . Regards.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7626
https://github.com/broadinstitute/gatk/pull/7627:51,Performance,concurren,concurrent,51,@mmorgantaylor was seeing . | Caused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: Rows must be specified. Due to flush() trying to write an empty buffer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7627
https://github.com/broadinstitute/gatk/issues/7628:207,Security,expose,expose,207,Currently the only way to create a sequence dictionary from within a GATK tool is to call into the CreateSequenceDictionary tool as if it was being executed from the command-line. This is a hack. We need to expose a method that other tools can call into that will create a sequence dictionary for files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7628
https://github.com/broadinstitute/gatk/issues/7630:930,Integrability,message,message,930,"If you train a VQSR model on a subset of the variants under consideration and serialize it, then provide it via `--input-model` to subsequent VQSR runs on sets of variants including those other than in the original training subset (the only use case for which you'd bother serializing a model in the first place, probably), this will result in variants in those subsequent sets being incorrectly labeled as `NEGATIVE_TRAINING_SITE`. The culprit lines of code are: https://github.com/broadinstitute/gatk/blob/5b8736715724d6ba7994613acbc37e1bf187f538/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/VariantRecalibrator.java#L644-L650. Instead, only those negative training sites taken from the original subset and used to generate the serialized negative model should be labeled as such. Note also that we needlessly select the worst scoring variants here, which leads to the bad behavior as well as a misleading log message. There is also some residual funkiness from the fact that we require the specification of the number of training variants in order to construct a GMM from a serialized model, which is a result of further funkiness from the fact that we require the size of the array of responsibilities to be initialized using this number. Unfortunately, I don't think there's an easy fix, since we don't actually pass either the correct positive/negative training labels (in the recal table generated by the original run) or the necessary information to reconstruct them otherwise (e.g., we could repeat the VQSLOD calculations, given the original training subset sites and VQSLOD cutoffs used to originally derive the negative labels) to these subsequent runs. (Unless we actually want to pass those things, which seems messy.). Note also that the `POSITIVE_TRAIN_SITE` label is still applied to sites that get dropped because of standard deviation thresholding and are hence not used to train the positive GMM, which seems slightly inconsistent (since the `NEGATIVE_TRAIN_SITE`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7630
https://github.com/broadinstitute/gatk/issues/7630:926,Testability,log,log,926,"If you train a VQSR model on a subset of the variants under consideration and serialize it, then provide it via `--input-model` to subsequent VQSR runs on sets of variants including those other than in the original training subset (the only use case for which you'd bother serializing a model in the first place, probably), this will result in variants in those subsequent sets being incorrectly labeled as `NEGATIVE_TRAINING_SITE`. The culprit lines of code are: https://github.com/broadinstitute/gatk/blob/5b8736715724d6ba7994613acbc37e1bf187f538/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/VariantRecalibrator.java#L644-L650. Instead, only those negative training sites taken from the original subset and used to generate the serialized negative model should be labeled as such. Note also that we needlessly select the worst scoring variants here, which leads to the bad behavior as well as a misleading log message. There is also some residual funkiness from the fact that we require the specification of the number of training variants in order to construct a GMM from a serialized model, which is a result of further funkiness from the fact that we require the size of the array of responsibilities to be initialized using this number. Unfortunately, I don't think there's an easy fix, since we don't actually pass either the correct positive/negative training labels (in the recal table generated by the original run) or the necessary information to reconstruct them otherwise (e.g., we could repeat the VQSLOD calculations, given the original training subset sites and VQSLOD cutoffs used to originally derive the negative labels) to these subsequent runs. (Unless we actually want to pass those things, which seems messy.). Note also that the `POSITIVE_TRAIN_SITE` label is still applied to sites that get dropped because of standard deviation thresholding and are hence not used to train the positive GMM, which seems slightly inconsistent (since the `NEGATIVE_TRAIN_SITE`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7630
https://github.com/broadinstitute/gatk/issues/7632:122,Deployability,pipeline,pipeline,122,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632
https://github.com/broadinstitute/gatk/issues/7632:319,Deployability,integrat,integration,319,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632
https://github.com/broadinstitute/gatk/issues/7632:319,Integrability,integrat,integration,319,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632
https://github.com/broadinstitute/gatk/issues/7632:226,Testability,test,tested,226,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632
https://github.com/broadinstitute/gatk/issues/7632:331,Testability,test,tests,331,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632
https://github.com/broadinstitute/gatk/issues/7632:534,Testability,test,tests,534,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632
https://github.com/broadinstitute/gatk/issues/7632:830,Testability,test,tests,830,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632
https://github.com/broadinstitute/gatk/issues/7632:511,Usability,simpl,simply,511,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632
https://github.com/broadinstitute/gatk/issues/7633:331,Availability,error,error,331,"Hi, GATK team! I'm working on GATK WGS somatic CNV calling pipeline. . When I tried gatk --java-options ""-Xmx2800g"" ModelSegments --denoised-copy-ratios ${tumor}.denoisedCR.tsv --allelic-counts ${tumor}.allelicCounts.tsv --normal-allelic-counts ${normal}.allelicCounts.tsv --output-prefix ${tumor} -O ${outdir}, I got this type of error:. 10:00:18.408 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/acct-medliuyb/medliuyb-user1/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 10, 2022 10:00:18 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:00:18.544 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.544 INFO ModelSegments - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:00:18.544 INFO ModelSegments - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:18.544 INFO ModelSegments - Executing as medliuyb-user1@huge2.pi.sjtu.edu.cn on Linux v3.10.0-1062.el7.x86_64 amd64; 10:00:18.545 INFO ModelSegments - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 10:00:18.545 INFO ModelSegments - Start Date/Time: January 10, 2022 at 10:00:18 AM CST; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - HTSJDK Version: 2.24.0; 10:00:18.545 INFO ModelSegments - Picard Version: 2.25.0; 10:00:18.545 INFO ModelSegments - Built for Spark Version: 2.4.5; 10:00:18.545 INFO ModelSegments - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633
https://github.com/broadinstitute/gatk/issues/7633:2783,Availability,down,down,2783,"elSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:00:18.546 INFO ModelSegments - Deflater: IntelDeflater; 10:00:18.546 INFO ModelSegments - Inflater: IntelInflater; 10:00:18.546 INFO ModelSegments - GCS max retries/reopens: 20; 10:00:18.546 INFO ModelSegments - Requester pays: disabled; 10:00:18.546 INFO ModelSegments - Initializing engine; 10:00:18.546 INFO ModelSegments - Done initializing engine; 10:00:18.549 INFO ModelSegments - Reading file (/lustre/home/acct-medliuyb/medliuyb-user1/PP_JOB/HiC_PC/seq-data/GATK_practise/CNV/WGS_1_T.denoisedCR.tsv)...; 10:00:22.289 INFO ModelSegments - Reading file (/lustre/home/acct-medliuyb/medliuyb-user1/PP_JOB/HiC_PC/seq-data/GATK_practise/CNV/CollectAllelicCounts/WGS_1_T.allelicCounts.tsv)...; 10:19:45.750 INFO ModelSegments - Shutting down engine; [January 10, 2022 at 10:19:45 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 19.46 minutes.; Runtime.totalMemory()=2628889083904; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; 	at java.base/java.util.Arrays.copyOf(Arrays.java:3688); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.getReadyToExpandTo(ImmutableList.java:768); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.add(ImmutableList.java:787); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.add(ImmutableList.java:748); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableCollection$Builder.addAll(ImmutableCollection.java:456); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.addAll(ImmutableList.java:847); 	at org.broadinstitute.hellbender.rel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633
https://github.com/broadinstitute/gatk/issues/7633:5447,Availability,error,error,5447,"dd(ImmutableList.java:748); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableCollection$Builder.addAll(ImmutableCollection.java:456); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.addAll(ImmutableList.java:847); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList.copyOf(ImmutableList.java:275); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractRecordCollection.<init>(AbstractRecordCollection.java:83); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.<init>(AbstractLocatableCollection.java:58); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractSampleLocatableCollection.<init>(AbstractSampleLocatableCollection.java:44); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection.<init>(AllelicCountCollection.java:58); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments$$Lambda$168/595759572.apply(Unknown Source); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments.readOptionalFileOrNull(ModelSegments.java:599); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments.doWork(ModelSegments.java:481); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). My allelicCounts.tsv is about 69 G. I know it's array size problem, java can not load allelicCounts file that big. I wonder if you know some ways to solve this error. thaks!. Peng Pu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633
https://github.com/broadinstitute/gatk/issues/7633:59,Deployability,pipeline,pipeline,59,"Hi, GATK team! I'm working on GATK WGS somatic CNV calling pipeline. . When I tried gatk --java-options ""-Xmx2800g"" ModelSegments --denoised-copy-ratios ${tumor}.denoisedCR.tsv --allelic-counts ${tumor}.allelicCounts.tsv --normal-allelic-counts ${normal}.allelicCounts.tsv --output-prefix ${tumor} -O ${outdir}, I got this type of error:. 10:00:18.408 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/acct-medliuyb/medliuyb-user1/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 10, 2022 10:00:18 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:00:18.544 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.544 INFO ModelSegments - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:00:18.544 INFO ModelSegments - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:18.544 INFO ModelSegments - Executing as medliuyb-user1@huge2.pi.sjtu.edu.cn on Linux v3.10.0-1062.el7.x86_64 amd64; 10:00:18.545 INFO ModelSegments - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 10:00:18.545 INFO ModelSegments - Start Date/Time: January 10, 2022 at 10:00:18 AM CST; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - HTSJDK Version: 2.24.0; 10:00:18.545 INFO ModelSegments - Picard Version: 2.25.0; 10:00:18.545 INFO ModelSegments - Built for Spark Version: 2.4.5; 10:00:18.545 INFO ModelSegments - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633
https://github.com/broadinstitute/gatk/issues/7633:379,Performance,Load,Loading,379,"Hi, GATK team! I'm working on GATK WGS somatic CNV calling pipeline. . When I tried gatk --java-options ""-Xmx2800g"" ModelSegments --denoised-copy-ratios ${tumor}.denoisedCR.tsv --allelic-counts ${tumor}.allelicCounts.tsv --normal-allelic-counts ${normal}.allelicCounts.tsv --output-prefix ${tumor} -O ${outdir}, I got this type of error:. 10:00:18.408 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/acct-medliuyb/medliuyb-user1/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 10, 2022 10:00:18 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:00:18.544 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.544 INFO ModelSegments - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:00:18.544 INFO ModelSegments - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:18.544 INFO ModelSegments - Executing as medliuyb-user1@huge2.pi.sjtu.edu.cn on Linux v3.10.0-1062.el7.x86_64 amd64; 10:00:18.545 INFO ModelSegments - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 10:00:18.545 INFO ModelSegments - Start Date/Time: January 10, 2022 at 10:00:18 AM CST; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - HTSJDK Version: 2.24.0; 10:00:18.545 INFO ModelSegments - Picard Version: 2.25.0; 10:00:18.545 INFO ModelSegments - Built for Spark Version: 2.4.5; 10:00:18.545 INFO ModelSegments - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633
https://github.com/broadinstitute/gatk/issues/7633:5368,Performance,load,load,5368,"dd(ImmutableList.java:748); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableCollection$Builder.addAll(ImmutableCollection.java:456); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.addAll(ImmutableList.java:847); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList.copyOf(ImmutableList.java:275); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractRecordCollection.<init>(AbstractRecordCollection.java:83); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.<init>(AbstractLocatableCollection.java:58); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractSampleLocatableCollection.<init>(AbstractSampleLocatableCollection.java:44); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection.<init>(AllelicCountCollection.java:58); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments$$Lambda$168/595759572.apply(Unknown Source); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments.readOptionalFileOrNull(ModelSegments.java:599); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments.doWork(ModelSegments.java:481); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). My allelicCounts.tsv is about 69 G. I know it's array size problem, java can not load allelicCounts file that big. I wonder if you know some ways to solve this error. thaks!. Peng Pu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633
https://github.com/broadinstitute/gatk/issues/7633:684,Safety,detect,detect,684,"Hi, GATK team! I'm working on GATK WGS somatic CNV calling pipeline. . When I tried gatk --java-options ""-Xmx2800g"" ModelSegments --denoised-copy-ratios ${tumor}.denoisedCR.tsv --allelic-counts ${tumor}.allelicCounts.tsv --normal-allelic-counts ${normal}.allelicCounts.tsv --output-prefix ${tumor} -O ${outdir}, I got this type of error:. 10:00:18.408 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/acct-medliuyb/medliuyb-user1/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 10, 2022 10:00:18 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:00:18.544 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.544 INFO ModelSegments - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:00:18.544 INFO ModelSegments - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:18.544 INFO ModelSegments - Executing as medliuyb-user1@huge2.pi.sjtu.edu.cn on Linux v3.10.0-1062.el7.x86_64 amd64; 10:00:18.545 INFO ModelSegments - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 10:00:18.545 INFO ModelSegments - Start Date/Time: January 10, 2022 at 10:00:18 AM CST; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - HTSJDK Version: 2.24.0; 10:00:18.545 INFO ModelSegments - Picard Version: 2.25.0; 10:00:18.545 INFO ModelSegments - Built for Spark Version: 2.4.5; 10:00:18.545 INFO ModelSegments - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633
https://github.com/broadinstitute/gatk/pull/7634:16,Deployability,update,updated,16,We have already updated those GATK3 tests to point to a newer version output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7634
https://github.com/broadinstitute/gatk/pull/7634:36,Testability,test,tests,36,We have already updated those GATK3 tests to point to a newer version output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7634
https://github.com/broadinstitute/gatk/issues/7636:167,Availability,error,error,167,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:175,Availability,FAILURE,FAILURE,175,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:375,Deployability,configurat,configuration,375,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:375,Modifiability,config,configuration,375,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:445,Safety,predict,predictor,445,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:557,Safety,predict,predictor,557,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:581,Safety,predict,predictor-,581,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:682,Safety,predict,predictor,682,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:706,Safety,predict,predictor-,706,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:802,Safety,predict,predictor,802,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:826,Safety,predict,predictor-,826,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:892,Safety,predict,predictor,892,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:916,Safety,predict,predictor-,916,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7636:1074,Testability,log,log,1074,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636
https://github.com/broadinstitute/gatk/issues/7639:6593,Availability,down,down,6593," of genotypes will NOT be added for this location. Chromosome chr2L position 19311 (TileDB column 19310) has too many alleles in the combined VCF record : 16 : current limit : 6. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location. 19:44:17.535 INFO ProgressMeter - chr2L:19364 2.8 15000 5324.0; Chromosome chr2L position 19835 (TileDB column 19834) has too many alleles in the combined VCF record : 11 : current limit : 6. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location. 19:44:34.904 INFO ProgressMeter - chr2L:21364 3.1 17000 5471.6; 19:44:47.867 INFO ProgressMeter - chr2L:23364 3.3 19000 5717.8; Chromosome chr2L position 25349 (TileDB column 25348) has too many alleles in the combined VCF record : 7 : current limit : 6. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location. 19:45:00.952 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),57.82864317899994,Cpu time(s),49.25545264700008; [January 13, 2022 7:45:01 PM EST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.22 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalStateException: Genotype [Ark CTTT/CTTT GQ 24 DP 8 AD 0,8,0,0,0,0,0,0 {SB=0,0,4,4}] does not contain likelihoods necessary to calculate posteriors.; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.log10NormalizedGenotypePosteriors(AlleleFrequencyCalculator.java:89); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.effectiveAlleleCounts(AlleleFrequencyCalculator.java:258); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.calculate(AlleleFrequencyCalculator.java:141); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:147); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639
https://github.com/broadinstitute/gatk/issues/7639:8675,Integrability,wrap,wrapAndCopyInto,8675,ateGenotypes(GenotypeGVCFsEngine.java:244); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.regenotypeVC(GenotypeGVCFsEngine.java:152); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:135); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:283); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); (base) [adagilis@longleaf-login4 logs]$; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadins,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639
https://github.com/broadinstitute/gatk/issues/7639:1108,Performance,Load,Loading,1108,"ning 4.1.9.0. . Their complete program log is below: . This request was created from a contribution made by Andrius Jonas Dagilis on January 06, 2022 16:24 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360073212811-GenotypeGVCFs-stalls-while-using-all-sites#community\_comment\_4414746059419](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073212811-GenotypeGVCFs-stalls-while-using-all-sites#community_comment_4414746059419). ```; Using GATK jar /nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx190g -jar /nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R /proj/matutelb/projects/drosophila/melanogaster/dmel6_ref.fasta -V gendb://all_mels_chr2L -L chr2L -O all_mels_chr2L.vcf.gz; 19:40:48.803 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 13, 2022 7:40:50 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:40:50.088 INFO GenotypeGVCFs - ------------------------------------------------------------; 19:40:50.089 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 19:40:50.090 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:40:50.102 INFO GenotypeGVCFs - Executing as adagilis@t0601.ll.unc.edu on Linux v3.10.0-1160.2.2.el7.x86_64 amd64; 19:40:50.103 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 19:40:50.103 INFO GenotypeGVCFs - Start Date/Time: January 13, 2022 7:40:48 PM EST; 19:40:50.103 INFO GenotypeGVCFs - -----------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639
https://github.com/broadinstitute/gatk/issues/7639:1402,Safety,detect,detect,1402,"_comment\_4414746059419](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073212811-GenotypeGVCFs-stalls-while-using-all-sites#community_comment_4414746059419). ```; Using GATK jar /nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx190g -jar /nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R /proj/matutelb/projects/drosophila/melanogaster/dmel6_ref.fasta -V gendb://all_mels_chr2L -L chr2L -O all_mels_chr2L.vcf.gz; 19:40:48.803 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 13, 2022 7:40:50 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:40:50.088 INFO GenotypeGVCFs - ------------------------------------------------------------; 19:40:50.089 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 19:40:50.090 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:40:50.102 INFO GenotypeGVCFs - Executing as adagilis@t0601.ll.unc.edu on Linux v3.10.0-1160.2.2.el7.x86_64 amd64; 19:40:50.103 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 19:40:50.103 INFO GenotypeGVCFs - Start Date/Time: January 13, 2022 7:40:48 PM EST; 19:40:50.103 INFO GenotypeGVCFs - ------------------------------------------------------------; 19:40:50.103 INFO GenotypeGVCFs - ------------------------------------------------------------; 19:40:50.104 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 19:40:50.104 INFO GenotypeGVCFs - Picard Version: 2.25.4; 19:40:50.104 INFO GenotypeGVCFs - Built for Spark Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639
https://github.com/broadinstitute/gatk/issues/7639:145,Testability,log,log,145,"A user running GenotypeGVCFs with a GenomicsDB ran into a new issue with 4.2.4.1. They were previously running 4.1.9.0. . Their complete program log is below: . This request was created from a contribution made by Andrius Jonas Dagilis on January 06, 2022 16:24 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360073212811-GenotypeGVCFs-stalls-while-using-all-sites#community\_comment\_4414746059419](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073212811-GenotypeGVCFs-stalls-while-using-all-sites#community_comment_4414746059419). ```; Using GATK jar /nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx190g -jar /nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R /proj/matutelb/projects/drosophila/melanogaster/dmel6_ref.fasta -V gendb://all_mels_chr2L -L chr2L -O all_mels_chr2L.vcf.gz; 19:40:48.803 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 13, 2022 7:40:50 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:40:50.088 INFO GenotypeGVCFs - ------------------------------------------------------------; 19:40:50.089 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 19:40:50.090 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:40:50.102 INFO GenotypeGVCFs - Executing as adagilis@t0601.ll.unc.edu on Linux v3.10.0-1160.2.2.el7.x86_64 amd64; 19:40:50.103 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 19:40:50.103 INFO GenotypeGVCFs - Start Date/Tim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639
https://github.com/broadinstitute/gatk/issues/7639:8556,Testability,log,logs,8556,s.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:147); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.calculateGenotypes(GenotypeGVCFsEngine.java:244); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.regenotypeVC(GenotypeGVCFsEngine.java:152); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:135); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:283); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); (base) [adagilis@longleaf-login4 logs]$; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639
https://github.com/broadinstitute/gatk/pull/7640:247,Availability,down,down,247,"Some notes from the 10k tieout:; ### Prepare Step; - ~20 min per full ref_ranges table to insert; - ~7 min per full vet table to insert; - ""bytes scanned"" are same as data table size. ### Extract. **Original Run - 293 min**. - 103 minutes pulling down data, scanning 237 GB; - 43 min on 20m vet records (20:26 - > 21:09); - 60 min on 291m vet records (21:09 -> 22:10). - 190 minutes writing the VCF; ; **Prepare Extract with minor tuning of sorting - 134 min**. - 25 minutes pulling down data ( faster), scanning 10 GB (50x reduction); - 4 min on 20m vet records(02:43 -> 02:47) - NOTE 103s of that was sorting (44s) and spilling to disk (59 s); - 21 min on 291m vet records (02:47 -> 03:08) - NOTE 9 min of that was sorting (6 min) and spilling to disk (3 min). - 109 minutes writing the VCF (this is the change to pre-sort the sample set merged to ah_var_store on 1/12/22) . **Tieout is identical**. ```; kcibul@kc-specops-tiny:~/stroke_tieout$ md5sum gold.jointcallset_0.vcf.gz; 496178eae4afe63c4391d8eba64a9947 gold.jointcallset_0.vcf.gz. kcibul@kc-specops-tiny:~/stroke_tieout$ md5sum trial.full.jointcallset_0.vcf.gz; 496178eae4afe63c4391d8eba64a9947 trial.full.jointcallset_0.vcf.gz; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7640
https://github.com/broadinstitute/gatk/pull/7640:483,Availability,down,down,483,"Some notes from the 10k tieout:; ### Prepare Step; - ~20 min per full ref_ranges table to insert; - ~7 min per full vet table to insert; - ""bytes scanned"" are same as data table size. ### Extract. **Original Run - 293 min**. - 103 minutes pulling down data, scanning 237 GB; - 43 min on 20m vet records (20:26 - > 21:09); - 60 min on 291m vet records (21:09 -> 22:10). - 190 minutes writing the VCF; ; **Prepare Extract with minor tuning of sorting - 134 min**. - 25 minutes pulling down data ( faster), scanning 10 GB (50x reduction); - 4 min on 20m vet records(02:43 -> 02:47) - NOTE 103s of that was sorting (44s) and spilling to disk (59 s); - 21 min on 291m vet records (02:47 -> 03:08) - NOTE 9 min of that was sorting (6 min) and spilling to disk (3 min). - 109 minutes writing the VCF (this is the change to pre-sort the sample set merged to ah_var_store on 1/12/22) . **Tieout is identical**. ```; kcibul@kc-specops-tiny:~/stroke_tieout$ md5sum gold.jointcallset_0.vcf.gz; 496178eae4afe63c4391d8eba64a9947 gold.jointcallset_0.vcf.gz. kcibul@kc-specops-tiny:~/stroke_tieout$ md5sum trial.full.jointcallset_0.vcf.gz; 496178eae4afe63c4391d8eba64a9947 trial.full.jointcallset_0.vcf.gz; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7640
https://github.com/broadinstitute/gatk/pull/7643:75,Security,validat,validated,75,"See Issue #7622 for more details. In addition to unit tests, here is how I validated:. ```; ##; # Split the WGS list; ##; rm -rf test_split. ./gatk --java-options ""-Xmx4g $DEBUG"" \; WeightedSplitIntervals \; --scatter-count 100 \; --weight-bed-file gvs_vet_weights_1kb.bed \; -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta \; --dont-mix-contigs true \; -L wgs_calling_regions.hg38.noCentromeres.noTelomeres.interval_list \; --output test_split. ##; # merge all the intervals lists back into one; ##; IL=""""; for f in test_split/*-scattered.interval_list; do; IL=""${IL} -I $f ""; done; ./gatk IntervalListTools --ACTION UNION $IL -O test_split/merged.interval_list. #; # compare it to the original; ##; ./gatk CompareIntervalLists \; -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta \; -L wgs_calling_regions.hg38.noCentromeres.noTelomeres.interval_list \; -L2 test_split/merged.interval_list. ##; # A visual check to see that the ordering is the same, and that the only splits; # are across file boundaries; ##; cat test_split/*-scattered.interval_list | grep -v ""@"" | cut -f1-3 > test_split/combined.txt; cat wgs_calling_regions.hg38.noCentromeres.noTelomeres.interval_list | grep -v ""@"" | cut -f1-3 > test_split/orig.txt; diff -y test_split/orig.txt test_split/combined.txt; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7643
https://github.com/broadinstitute/gatk/pull/7643:54,Testability,test,tests,54,"See Issue #7622 for more details. In addition to unit tests, here is how I validated:. ```; ##; # Split the WGS list; ##; rm -rf test_split. ./gatk --java-options ""-Xmx4g $DEBUG"" \; WeightedSplitIntervals \; --scatter-count 100 \; --weight-bed-file gvs_vet_weights_1kb.bed \; -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta \; --dont-mix-contigs true \; -L wgs_calling_regions.hg38.noCentromeres.noTelomeres.interval_list \; --output test_split. ##; # merge all the intervals lists back into one; ##; IL=""""; for f in test_split/*-scattered.interval_list; do; IL=""${IL} -I $f ""; done; ./gatk IntervalListTools --ACTION UNION $IL -O test_split/merged.interval_list. #; # compare it to the original; ##; ./gatk CompareIntervalLists \; -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta \; -L wgs_calling_regions.hg38.noCentromeres.noTelomeres.interval_list \; -L2 test_split/merged.interval_list. ##; # A visual check to see that the ordering is the same, and that the only splits; # are across file boundaries; ##; cat test_split/*-scattered.interval_list | grep -v ""@"" | cut -f1-3 > test_split/combined.txt; cat wgs_calling_regions.hg38.noCentromeres.noTelomeres.interval_list | grep -v ""@"" | cut -f1-3 > test_split/orig.txt; diff -y test_split/orig.txt test_split/combined.txt; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7643
https://github.com/broadinstitute/gatk/issues/7646:29,Deployability,pipeline,pipeline,29,"[GenomicsDB]. As part of our pipeline, we are running the below mentioned command.; What this step does is, reads the vcf files in batch of 50 so in total 4 batches will run for 156 samples, and then for each batch it writes the tables/data to the “genomicsdb” folder. ; ; Parent Command : python /gatk/gatk --java-options -Xmx4g -Xms4g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; Child Process : java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /gatk/gatk-package-4.1.8.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; The above command took approx. 3.5 hrs to run while writing to local mount of ec2 i.e. EBS volume.; The same command took 8+ hrs (still running as of this email) to run while writing to FSx for luster mount. And surprisingly through AWS Batch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:1264,Deployability,pipeline,pipeline,1264,"to the “genomicsdb” folder. ; ; Parent Command : python /gatk/gatk --java-options -Xmx4g -Xms4g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; Child Process : java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /gatk/gatk-package-4.1.8.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; The above command took approx. 3.5 hrs to run while writing to local mount of ec2 i.e. EBS volume.; The same command took 8+ hrs (still running as of this email) to run while writing to FSx for luster mount. And surprisingly through AWS Batch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file system",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:1340,Performance,cache,cached,1340,"path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; Child Process : java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /gatk/gatk-package-4.1.8.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; The above command took approx. 3.5 hrs to run while writing to local mount of ec2 i.e. EBS volume.; The same command took 8+ hrs (still running as of this email) to run while writing to FSx for luster mount. And surprisingly through AWS Batch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file systems, the FSx shows a much greater speed/performance.; ; Command : dd if=/dev/zero of=<Local/FSx>/test.img bs=1G count=5 oflag=dsync; ; Ra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:1744,Performance,throughput,throughput,1744,"s4g -jar /gatk/gatk-package-4.1.8.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; The above command took approx. 3.5 hrs to run while writing to local mount of ec2 i.e. EBS volume.; The same command took 8+ hrs (still running as of this email) to run while writing to FSx for luster mount. And surprisingly through AWS Batch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file systems, the FSx shows a much greater speed/performance.; ; Command : dd if=/dev/zero of=<Local/FSx>/test.img bs=1G count=5 oflag=dsync; ; Ran to write on local (ec2 EBS mount) :; ; root@6ece7fab91ec:/app# dd if=/dev/zero of=/app/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 51.5764 s, 104 MB/s; root@6ece7fab91ec:/app#; ; ; Ran to write on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:2280,Performance,perform,performance,2280,"ch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file systems, the FSx shows a much greater speed/performance.; ; Command : dd if=/dev/zero of=<Local/FSx>/test.img bs=1G count=5 oflag=dsync; ; Ran to write on local (ec2 EBS mount) :; ; root@6ece7fab91ec:/app# dd if=/dev/zero of=/app/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 51.5764 s, 104 MB/s; root@6ece7fab91ec:/app#; ; ; Ran to write on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:4413,Performance,load,loaded,4413,"genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions were done on different EC2 instances with 4 CPU and 30 GB memory (each running 1 job only at a given time). FSx executions were done one after the other. Please help us with troubleshooting this performance issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:4680,Performance,perform,performance,4680,"genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions were done on different EC2 instances with 4 CPU and 30 GB memory (each running 1 job only at a given time). FSx executions were done one after the other. Please help us with troubleshooting this performance issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:1485,Testability,test,testing,1485,"-pays broad-dsde-methods; ; Child Process : java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /gatk/gatk-package-4.1.8.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; The above command took approx. 3.5 hrs to run while writing to local mount of ec2 i.e. EBS volume.; The same command took 8+ hrs (still running as of this email) to run while writing to FSx for luster mount. And surprisingly through AWS Batch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file systems, the FSx shows a much greater speed/performance.; ; Command : dd if=/dev/zero of=<Local/FSx>/test.img bs=1G count=5 oflag=dsync; ; Ran to write on local (ec2 EBS mount) :; ; root@6ece7fab91ec:/app# dd if=/dev/zero of=/app/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:2196,Testability,test,test,2196,"ch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file systems, the FSx shows a much greater speed/performance.; ; Command : dd if=/dev/zero of=<Local/FSx>/test.img bs=1G count=5 oflag=dsync; ; Ran to write on local (ec2 EBS mount) :; ; root@6ece7fab91ec:/app# dd if=/dev/zero of=/app/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 51.5764 s, 104 MB/s; root@6ece7fab91ec:/app#; ; ; Ran to write on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:2337,Testability,test,test,2337,"is process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file systems, the FSx shows a much greater speed/performance.; ; Command : dd if=/dev/zero of=<Local/FSx>/test.img bs=1G count=5 oflag=dsync; ; Ran to write on local (ec2 EBS mount) :; ; root@6ece7fab91ec:/app# dd if=/dev/zero of=/app/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 51.5764 s, 104 MB/s; root@6ece7fab91ec:/app#; ; ; Ran to write on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:2466,Testability,test,test,2466,"w and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file systems, the FSx shows a much greater speed/performance.; ; Command : dd if=/dev/zero of=<Local/FSx>/test.img bs=1G count=5 oflag=dsync; ; Ran to write on local (ec2 EBS mount) :; ; root@6ece7fab91ec:/app# dd if=/dev/zero of=/app/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 51.5764 s, 104 MB/s; root@6ece7fab91ec:/app#; ; ; Ran to write on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:3112,Testability,test,test,3112,"f 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file systems, the FSx shows a much greater speed/performance.; ; Command : dd if=/dev/zero of=<Local/FSx>/test.img bs=1G count=5 oflag=dsync; ; Ran to write on local (ec2 EBS mount) :; ; root@6ece7fab91ec:/app# dd if=/dev/zero of=/app/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 51.5764 s, 104 MB/s; root@6ece7fab91ec:/app#; ; ; Ran to write on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:3492,Testability,log,log,3492,"5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 51.5764 s, 104 MB/s; root@6ece7fab91ec:/app#; ; ; Ran to write on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions we",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:3606,Testability,log,logs,3606,"91ec:/app#; ; ; Ran to write on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions were done on different EC2 instances with 4 CPU and 30 GB memory (each running 1 job only at a given time). FSx executions were ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:3632,Testability,log,log,3632," on FSx for luster mount on ec2:; ; [root@ip-10-76-63-158 genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions were done on different EC2 instances with 4 CPU and 30 GB memory (each running 1 job only at a given time). FSx executions were done one after the other. P",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:3662,Testability,log,logs,3662,"genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions were done on different EC2 instances with 4 CPU and 30 GB memory (each running 1 job only at a given time). FSx executions were done one after the other. Please help us with troubleshooting this performance issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:3797,Testability,log,logs,3797,"genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions were done on different EC2 instances with 4 CPU and 30 GB memory (each running 1 job only at a given time). FSx executions were done one after the other. Please help us with troubleshooting this performance issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:3821,Testability,log,log,3821,"genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions were done on different EC2 instances with 4 CPU and 30 GB memory (each running 1 job only at a given time). FSx executions were done one after the other. Please help us with troubleshooting this performance issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7646:4321,Testability,test,tests,4321,"genomicsdb]# dd if=/dev/zero of=/gfb-dev-sv-fsx-results-us-east-2/cromwell-execution/GATKSVPipelineBatch/087bd722-5f51-43eb-a89e-70846a1da89f/call-GATKSVPipelinePhase1/GATKSVPipelinePhase1/38595c13-b874-4753-a554-81c09f6449f8/call-GatherBatchEvidence/GatherBatchEvidence/c8120761-6d9f-4bd3-b450-f528b7be817c/call-BAFFromGVCFs/BAFFromGVCFs/d5032666-9c09-4857-a8d7-41042927cf89/call-ImportGVCFs/shard-389/genomicsdb/test.img bs=1G count=5 oflag=dsync; 5+0 records in; 5+0 records out; 5368709120 bytes (5.4 GB) copied, 23.5143 s, 228 MB/s; [root@ip-10-76-63-158 genomicsdb]#. We also ran the jobs with strace enabled and we found that there are millions of FUTEX_WAIT_PRIVATE processes while we run the jobs for fsx writing as compared to just 26 when we write to EBS. # Local EBS writing strace log (Ran around 3.5 hrs); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./local-write-logs/strace_local_writing.log | wc -l; 26. # FSx strace logs; # --reader-threads 5 (Ran around 7 hours); [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./fsx-write-logs/strace_fsx_writing.log | wc -l; 24378265. # --reader-threads 1; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_1_fsx.txt | wc -l; 8745113. #--reader-threads 2; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_2_fsx.txt | wc -l; 13946622. #--reader-threads 10; [root@ip-10-76-62-193 importvcf-job]# egrep ""FUTEX_WAIT_PRIVATE, 0, NULL"" ./strace_thread_10_fsx.txt | wc -l; 13535883; [root@ip-10-76-62-193 importvcf-job]#. The last 3 i.e. tests for thread 1, 2 and 10 were only executed for 20 mins and in those 20 minutes it only loaded around 220 MBs to the genomicsdb. Note that the above executions were done on different EC2 instances with 4 CPU and 30 GB memory (each running 1 job only at a given time). FSx executions were done one after the other. Please help us with troubleshooting this performance issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646
https://github.com/broadinstitute/gatk/issues/7647:105,Energy Efficiency,efficient,efficient,105,There has been a request to do some more work on the HMM again and its become clear that there is not an efficient way to rapidly generate large amounts of test data based on the old HMM results. It would be helpful to add an option to dump the hmm scores out to the command line in an easily machine parseable format. Here is an example of how it has been done in the past (and probably how we should do it this time): https://github.com/Intel-HLS/GKL/blob/master/src/test/resources/pairhmm-testdata.txt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7647
https://github.com/broadinstitute/gatk/issues/7647:156,Testability,test,test,156,There has been a request to do some more work on the HMM again and its become clear that there is not an efficient way to rapidly generate large amounts of test data based on the old HMM results. It would be helpful to add an option to dump the hmm scores out to the command line in an easily machine parseable format. Here is an example of how it has been done in the past (and probably how we should do it this time): https://github.com/Intel-HLS/GKL/blob/master/src/test/resources/pairhmm-testdata.txt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7647
https://github.com/broadinstitute/gatk/issues/7647:469,Testability,test,test,469,There has been a request to do some more work on the HMM again and its become clear that there is not an efficient way to rapidly generate large amounts of test data based on the old HMM results. It would be helpful to add an option to dump the hmm scores out to the command line in an easily machine parseable format. Here is an example of how it has been done in the past (and probably how we should do it this time): https://github.com/Intel-HLS/GKL/blob/master/src/test/resources/pairhmm-testdata.txt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7647
https://github.com/broadinstitute/gatk/issues/7647:492,Testability,test,testdata,492,There has been a request to do some more work on the HMM again and its become clear that there is not an efficient way to rapidly generate large amounts of test data based on the old HMM results. It would be helpful to add an option to dump the hmm scores out to the command line in an easily machine parseable format. Here is an example of how it has been done in the past (and probably how we should do it this time): https://github.com/Intel-HLS/GKL/blob/master/src/test/resources/pairhmm-testdata.txt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7647
https://github.com/broadinstitute/gatk/issues/7647:78,Usability,clear,clear,78,There has been a request to do some more work on the HMM again and its become clear that there is not an efficient way to rapidly generate large amounts of test data based on the old HMM results. It would be helpful to add an option to dump the hmm scores out to the command line in an easily machine parseable format. Here is an example of how it has been done in the past (and probably how we should do it this time): https://github.com/Intel-HLS/GKL/blob/master/src/test/resources/pairhmm-testdata.txt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7647
https://github.com/broadinstitute/gatk/issues/7649:1385,Modifiability,refactor,refactors,1385,"Not really an issue, just wanted to document some surprising behavior. @tmelman has been reviving/reimplementing some ancient CNV/ModelSegments evaluations (dating as far back as 4.0.2.1!) and trying to understand whether observed differences---intentional or otherwise---are due to method changes I might have made, or if she might've introduced changes in her reimplementation of the evaluation code. I ran some checks on the stability of ModelSegments using an old set of inputs (normal/tumor allelic counts and denoised copy ratios for SM-74P4M WES). Behavior has remained largely stable since at least 4.1.0.0. Namely:. 1) We evaluated and signed off on a change that went into 4.1.0.0. See comments in https://github.com/broadinstitute/gatk/pull/5575.; 2) A slight numerical difference in the MCMC-sampled allele fractions was introduced by changes made to some MathUtils code for calculating logs/factorials/etc. between 4.1.0.0 and 4.1.1.0 in https://github.com/broadinstitute/gatk/pull/5814. Note that no CNV code was directly changed, it's just that we call out to that changed MathUtils code---namely, to calculate log10factorial. The overall result in my test was a very slight change to the number of segments found, from 516 to 522.; 3) No further numerical changes have been introduced through the current 4.2.4.1, so any additional code changes I made were indeed true refactors, at least from the perspective of this simple test. Phew!. I was indeed surprised to find that very slight differences in the log10factorial behavior (which result from changing the recursive calculation of cached values to a direct one, and appear in something like the 13th decimal place) led to non-negligible changes in the MCMC estimates of the allele fractions---and thus, changes in the number of segments. Although these are also relatively slight differences in terms of practical impact, they are perhaps much larger than one might guess, given their humble origins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649
https://github.com/broadinstitute/gatk/issues/7649:1602,Performance,cache,cached,1602,"Not really an issue, just wanted to document some surprising behavior. @tmelman has been reviving/reimplementing some ancient CNV/ModelSegments evaluations (dating as far back as 4.0.2.1!) and trying to understand whether observed differences---intentional or otherwise---are due to method changes I might have made, or if she might've introduced changes in her reimplementation of the evaluation code. I ran some checks on the stability of ModelSegments using an old set of inputs (normal/tumor allelic counts and denoised copy ratios for SM-74P4M WES). Behavior has remained largely stable since at least 4.1.0.0. Namely:. 1) We evaluated and signed off on a change that went into 4.1.0.0. See comments in https://github.com/broadinstitute/gatk/pull/5575.; 2) A slight numerical difference in the MCMC-sampled allele fractions was introduced by changes made to some MathUtils code for calculating logs/factorials/etc. between 4.1.0.0 and 4.1.1.0 in https://github.com/broadinstitute/gatk/pull/5814. Note that no CNV code was directly changed, it's just that we call out to that changed MathUtils code---namely, to calculate log10factorial. The overall result in my test was a very slight change to the number of segments found, from 516 to 522.; 3) No further numerical changes have been introduced through the current 4.2.4.1, so any additional code changes I made were indeed true refactors, at least from the perspective of this simple test. Phew!. I was indeed surprised to find that very slight differences in the log10factorial behavior (which result from changing the recursive calculation of cached values to a direct one, and appear in something like the 13th decimal place) led to non-negligible changes in the MCMC estimates of the allele fractions---and thus, changes in the number of segments. Although these are also relatively slight differences in terms of practical impact, they are perhaps much larger than one might guess, given their humble origins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649
https://github.com/broadinstitute/gatk/issues/7649:899,Testability,log,logs,899,"Not really an issue, just wanted to document some surprising behavior. @tmelman has been reviving/reimplementing some ancient CNV/ModelSegments evaluations (dating as far back as 4.0.2.1!) and trying to understand whether observed differences---intentional or otherwise---are due to method changes I might have made, or if she might've introduced changes in her reimplementation of the evaluation code. I ran some checks on the stability of ModelSegments using an old set of inputs (normal/tumor allelic counts and denoised copy ratios for SM-74P4M WES). Behavior has remained largely stable since at least 4.1.0.0. Namely:. 1) We evaluated and signed off on a change that went into 4.1.0.0. See comments in https://github.com/broadinstitute/gatk/pull/5575.; 2) A slight numerical difference in the MCMC-sampled allele fractions was introduced by changes made to some MathUtils code for calculating logs/factorials/etc. between 4.1.0.0 and 4.1.1.0 in https://github.com/broadinstitute/gatk/pull/5814. Note that no CNV code was directly changed, it's just that we call out to that changed MathUtils code---namely, to calculate log10factorial. The overall result in my test was a very slight change to the number of segments found, from 516 to 522.; 3) No further numerical changes have been introduced through the current 4.2.4.1, so any additional code changes I made were indeed true refactors, at least from the perspective of this simple test. Phew!. I was indeed surprised to find that very slight differences in the log10factorial behavior (which result from changing the recursive calculation of cached values to a direct one, and appear in something like the 13th decimal place) led to non-negligible changes in the MCMC estimates of the allele fractions---and thus, changes in the number of segments. Although these are also relatively slight differences in terms of practical impact, they are perhaps much larger than one might guess, given their humble origins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649
https://github.com/broadinstitute/gatk/issues/7649:1167,Testability,test,test,1167,"Not really an issue, just wanted to document some surprising behavior. @tmelman has been reviving/reimplementing some ancient CNV/ModelSegments evaluations (dating as far back as 4.0.2.1!) and trying to understand whether observed differences---intentional or otherwise---are due to method changes I might have made, or if she might've introduced changes in her reimplementation of the evaluation code. I ran some checks on the stability of ModelSegments using an old set of inputs (normal/tumor allelic counts and denoised copy ratios for SM-74P4M WES). Behavior has remained largely stable since at least 4.1.0.0. Namely:. 1) We evaluated and signed off on a change that went into 4.1.0.0. See comments in https://github.com/broadinstitute/gatk/pull/5575.; 2) A slight numerical difference in the MCMC-sampled allele fractions was introduced by changes made to some MathUtils code for calculating logs/factorials/etc. between 4.1.0.0 and 4.1.1.0 in https://github.com/broadinstitute/gatk/pull/5814. Note that no CNV code was directly changed, it's just that we call out to that changed MathUtils code---namely, to calculate log10factorial. The overall result in my test was a very slight change to the number of segments found, from 516 to 522.; 3) No further numerical changes have been introduced through the current 4.2.4.1, so any additional code changes I made were indeed true refactors, at least from the perspective of this simple test. Phew!. I was indeed surprised to find that very slight differences in the log10factorial behavior (which result from changing the recursive calculation of cached values to a direct one, and appear in something like the 13th decimal place) led to non-negligible changes in the MCMC estimates of the allele fractions---and thus, changes in the number of segments. Although these are also relatively slight differences in terms of practical impact, they are perhaps much larger than one might guess, given their humble origins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649
https://github.com/broadinstitute/gatk/issues/7649:1441,Testability,test,test,1441,"Not really an issue, just wanted to document some surprising behavior. @tmelman has been reviving/reimplementing some ancient CNV/ModelSegments evaluations (dating as far back as 4.0.2.1!) and trying to understand whether observed differences---intentional or otherwise---are due to method changes I might have made, or if she might've introduced changes in her reimplementation of the evaluation code. I ran some checks on the stability of ModelSegments using an old set of inputs (normal/tumor allelic counts and denoised copy ratios for SM-74P4M WES). Behavior has remained largely stable since at least 4.1.0.0. Namely:. 1) We evaluated and signed off on a change that went into 4.1.0.0. See comments in https://github.com/broadinstitute/gatk/pull/5575.; 2) A slight numerical difference in the MCMC-sampled allele fractions was introduced by changes made to some MathUtils code for calculating logs/factorials/etc. between 4.1.0.0 and 4.1.1.0 in https://github.com/broadinstitute/gatk/pull/5814. Note that no CNV code was directly changed, it's just that we call out to that changed MathUtils code---namely, to calculate log10factorial. The overall result in my test was a very slight change to the number of segments found, from 516 to 522.; 3) No further numerical changes have been introduced through the current 4.2.4.1, so any additional code changes I made were indeed true refactors, at least from the perspective of this simple test. Phew!. I was indeed surprised to find that very slight differences in the log10factorial behavior (which result from changing the recursive calculation of cached values to a direct one, and appear in something like the 13th decimal place) led to non-negligible changes in the MCMC estimates of the allele fractions---and thus, changes in the number of segments. Although these are also relatively slight differences in terms of practical impact, they are perhaps much larger than one might guess, given their humble origins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649
https://github.com/broadinstitute/gatk/issues/7649:1434,Usability,simpl,simple,1434,"Not really an issue, just wanted to document some surprising behavior. @tmelman has been reviving/reimplementing some ancient CNV/ModelSegments evaluations (dating as far back as 4.0.2.1!) and trying to understand whether observed differences---intentional or otherwise---are due to method changes I might have made, or if she might've introduced changes in her reimplementation of the evaluation code. I ran some checks on the stability of ModelSegments using an old set of inputs (normal/tumor allelic counts and denoised copy ratios for SM-74P4M WES). Behavior has remained largely stable since at least 4.1.0.0. Namely:. 1) We evaluated and signed off on a change that went into 4.1.0.0. See comments in https://github.com/broadinstitute/gatk/pull/5575.; 2) A slight numerical difference in the MCMC-sampled allele fractions was introduced by changes made to some MathUtils code for calculating logs/factorials/etc. between 4.1.0.0 and 4.1.1.0 in https://github.com/broadinstitute/gatk/pull/5814. Note that no CNV code was directly changed, it's just that we call out to that changed MathUtils code---namely, to calculate log10factorial. The overall result in my test was a very slight change to the number of segments found, from 516 to 522.; 3) No further numerical changes have been introduced through the current 4.2.4.1, so any additional code changes I made were indeed true refactors, at least from the perspective of this simple test. Phew!. I was indeed surprised to find that very slight differences in the log10factorial behavior (which result from changing the recursive calculation of cached values to a direct one, and appear in something like the 13th decimal place) led to non-negligible changes in the MCMC estimates of the allele fractions---and thus, changes in the number of segments. Although these are also relatively slight differences in terms of practical impact, they are perhaps much larger than one might guess, given their humble origins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649
https://github.com/broadinstitute/gatk/pull/7652:641,Availability,down,downsampled,641,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:1471,Availability,down,down,1471,"ll/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #5556 because some users were running matched normals with much lower depth than their cases. This meant that many normal sites fell below the default threshold of 30 counts and were thus not pulled from the case, even though the latter had much higher depth. It's conceivable that there will be some use cases for which we might want to relax this and allow a non-zero case threshold; for example,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:175,Deployability,pipeline,pipeline-level,175,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:811,Deployability,pipeline,pipelines,811,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:1516,Security,validat,validation,1516,"test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #5556 because some users were running matched normals with much lower depth than their cases. This meant that many normal sites fell below the default threshold of 30 counts and were thus not pulled from the case, even though the latter had much higher depth. It's conceivable that there will be some use cases for which we might want to relax this and allow a non-zero case threshold; for example, if the case is low depth and there's significant noise in the allele fractions, which would affect the segmentation. But for now, I just added a suggest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:197,Testability,test,testing,197,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:254,Testability,test,tests,254,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:514,Testability,test,test,514,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:549,Testability,test,test,549,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:570,Testability,test,test,570,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:741,Testability,test,test,741,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:846,Testability,test,test,846,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:946,Testability,test,tests,946,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:1078,Testability,test,tests,1078,"#7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #5556 because some users were running matched normals with much lower depth than their cases. This meant that many normal ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:1316,Testability,Test,Tests,1316,"ll/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #5556 because some users were running matched normals with much lower depth than their cases. This meant that many normal sites fell below the default threshold of 30 counts and were thus not pulled from the case, even though the latter had much higher depth. It's conceivable that there will be some use cases for which we might want to relax this and allow a non-zero case threshold; for example,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:2796,Testability,test,test,2796,"hus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #5556 because some users were running matched normals with much lower depth than their cases. This meant that many normal sites fell below the default threshold of 30 counts and were thus not pulled from the case, even though the latter had much higher depth. It's conceivable that there will be some use cases for which we might want to relax this and allow a non-zero case threshold; for example, if the case is low depth and there's significant noise in the allele fractions, which would affect the segmentation. But for now, I just added a suggestion that the allele counts be dropped entirely, in that case. Hopefully, for typical usage, case samples will always be much higher depth. @tmelman hope you don't mind reviewing, as we discussed. There are a lot of new lines here, but mostly from test data (there are only ~150 non-test LOC added) and I've split up the work into separate commits, which will hopefully be easier to review. Happy to answer any questions!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/pull/7652:2831,Testability,test,test,2831,"hus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #5556 because some users were running matched normals with much lower depth than their cases. This meant that many normal sites fell below the default threshold of 30 counts and were thus not pulled from the case, even though the latter had much higher depth. It's conceivable that there will be some use cases for which we might want to relax this and allow a non-zero case threshold; for example, if the case is low depth and there's significant noise in the allele fractions, which would affect the segmentation. But for now, I just added a suggestion that the allele counts be dropped entirely, in that case. Hopefully, for typical usage, case samples will always be much higher depth. @tmelman hope you don't mind reviewing, as we discussed. There are a lot of new lines here, but mostly from test data (there are only ~150 non-test LOC added) and I've split up the work into separate commits, which will hopefully be easier to review. Happy to answer any questions!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652
https://github.com/broadinstitute/gatk/issues/7653:503,Availability,error,error,503,"## Bug Report. ### Affected tool(s) or class(es); `GenomicsDBImport` (writing to a GCS bucket). ### Affected version(s). `4.2.3.0`. ### Description . Running `GenomicsDBImport` on 264 whole genome GVCFs, each of ~1GB size (called with HaplotypeCaller and blocked with only one bin threshold `-GQB 20`). Doing that on 50 genome intervals generated by `SplitIntervals`, so 50 `GenomicsDBImport` jobs write 50 DBs to a GCS bucket. 49 of them worked successfully, however one have failed with the following error:. ```sh; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribbl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:539,Availability,Error,Error,539,"## Bug Report. ### Affected tool(s) or class(es); `GenomicsDBImport` (writing to a GCS bucket). ### Affected version(s). `4.2.3.0`. ### Description . Running `GenomicsDBImport` on 264 whole genome GVCFs, each of ~1GB size (called with HaplotypeCaller and blocked with only one bin threshold `-GQB 20`). Doing that on 50 genome intervals generated by `SplitIntervals`, so 50 `GenomicsDBImport` jobs write 50 DBs to a GCS bucket. 49 of them worked successfully, however one have failed with the following error:. ```sh; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribbl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:1371,Availability,error,error,1371,"csDBImport` jobs write 50 DBs to a GCS bucket. 49 of them worked successfully, however one have failed with the following error:. ```sh; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:1450,Availability,error,error,1450,"d successfully, however one have failed with the following error:. ```sh; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:5811,Availability,Error,Error,5811,"ten to gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/callset.json; 14:26:53.640 INFO GenomicsDBImport - Complete VCF Header will be written to gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/vcfheader.vcf; 14:26:53.640 INFO GenomicsDBImport - Importing to workspace - gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50; 14:26:56.113 INFO GenomicsDBImport - Starting batch input file preload; 14:26:57.968 INFO GenomicsDBImport - Finished batch preload; 14:26:57.968 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 15:59:12.833 INFO GenomicsDBImport - Done importing batch 5/6; 15:59:12.833 INFO GenomicsDBImport - Starting batch input file preload; 15:59:13.218 INFO GenomicsDBImport - Finished batch preload; 15:59:13.218 INFO GenomicsDBImport - Importing batch 6 with 14 samples; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (gzip_write_buffer) Cannot write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (write_buffer) Cannot compress and/or write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; 16:39:59.490 INFO GenomicsDBImport - Done importing batch 6/6; 16:40:00.293 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 16:40:00.293 INFO GenomicsDBImport - Shutting down engine; [January 27, 2022 at 4:40:00 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 133.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:6139,Availability,Error,Error,6139,"-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/callset.json; 14:26:53.640 INFO GenomicsDBImport - Complete VCF Header will be written to gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/vcfheader.vcf; 14:26:53.640 INFO GenomicsDBImport - Importing to workspace - gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50; 14:26:56.113 INFO GenomicsDBImport - Starting batch input file preload; 14:26:57.968 INFO GenomicsDBImport - Finished batch preload; 14:26:57.968 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 15:59:12.833 INFO GenomicsDBImport - Done importing batch 5/6; 15:59:12.833 INFO GenomicsDBImport - Starting batch input file preload; 15:59:13.218 INFO GenomicsDBImport - Finished batch preload; 15:59:13.218 INFO GenomicsDBImport - Importing batch 6 with 14 samples; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (gzip_write_buffer) Cannot write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (write_buffer) Cannot compress and/or write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; 16:39:59.490 INFO GenomicsDBImport - Done importing batch 6/6; 16:40:00.293 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 16:40:00.293 INFO GenomicsDBImport - Shutting down engine; [January 27, 2022 at 4:40:00 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 133.15 minutes.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:6368,Availability,Error,Error,6368,"-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/callset.json; 14:26:53.640 INFO GenomicsDBImport - Complete VCF Header will be written to gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/vcfheader.vcf; 14:26:53.640 INFO GenomicsDBImport - Importing to workspace - gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50; 14:26:56.113 INFO GenomicsDBImport - Starting batch input file preload; 14:26:57.968 INFO GenomicsDBImport - Finished batch preload; 14:26:57.968 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 15:59:12.833 INFO GenomicsDBImport - Done importing batch 5/6; 15:59:12.833 INFO GenomicsDBImport - Starting batch input file preload; 15:59:13.218 INFO GenomicsDBImport - Finished batch preload; 15:59:13.218 INFO GenomicsDBImport - Importing batch 6 with 14 samples; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (gzip_write_buffer) Cannot write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (write_buffer) Cannot compress and/or write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; 16:39:59.490 INFO GenomicsDBImport - Done importing batch 6/6; 16:40:00.293 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 16:40:00.293 INFO GenomicsDBImport - Shutting down engine; [January 27, 2022 at 4:40:00 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 133.15 minutes.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:6778,Availability,down,down,6778,"-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/callset.json; 14:26:53.640 INFO GenomicsDBImport - Complete VCF Header will be written to gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/vcfheader.vcf; 14:26:53.640 INFO GenomicsDBImport - Importing to workspace - gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50; 14:26:56.113 INFO GenomicsDBImport - Starting batch input file preload; 14:26:57.968 INFO GenomicsDBImport - Finished batch preload; 14:26:57.968 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 15:59:12.833 INFO GenomicsDBImport - Done importing batch 5/6; 15:59:12.833 INFO GenomicsDBImport - Starting batch input file preload; 15:59:13.218 INFO GenomicsDBImport - Finished batch preload; 15:59:13.218 INFO GenomicsDBImport - Importing batch 6 with 14 samples; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (gzip_write_buffer) Cannot write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (write_buffer) Cannot compress and/or write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; 16:39:59.490 INFO GenomicsDBImport - Done importing batch 6/6; 16:40:00.293 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 16:40:00.293 INFO GenomicsDBImport - Shutting down engine; [January 27, 2022 at 4:40:00 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 133.15 minutes.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:1623,Deployability,configurat,configuration,1623,"y increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:26:51.130 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:1623,Modifiability,config,configuration,1623,"y increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:26:51.130 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:2481,Performance,Load,Loading,2481,"ith the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:26:51.130 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:26:51.270 INFO GenomicsDBImport - ------------------------------------------------------------; 14:26:51.270 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 14:26:51.270 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:26:51.271 INFO GenomicsDBImport - Executing as root@hostname-e65b745354 on Linux v5.11.0-1020-gcp amd64; 14:26:51.271 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 14:26:51.271 INFO GenomicsDBImport - Start Date/Time: January 27, 2022 at 2:26:51 PM UTC; 14:26:51.271 INFO GenomicsDBImport - ------------------------------------------------------------; 14:26:51.271 INFO GenomicsDBImport - ------------------------------------------------------------; 14:26:51.272 INFO GenomicsDBImport - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/issues/7653:1769,Testability,log,log,1769,"it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:26:51.130 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:26:51.270 INFO GenomicsDBImport - ------------------------------------------------------------; 14:26:51.270 INFO GenomicsDBIm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653
https://github.com/broadinstitute/gatk/pull/7658:11,Testability,test,tested,11,"Sizes were tested, and vm's were made smaller!!!!!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7658
https://github.com/broadinstitute/gatk/pull/7665:42,Availability,down,down,42,We had removed it because it was shutting down and our tests showed it was unnecessary. It turns out that it's only unnecessary because we're getting jcenter results cached in the artifactory. JCenter is now planning to remain online for the immediate future in read only mode so it should be fine to put it back. Fixes https://github.com/broadinstitute/gatk/issues/7636,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7665
https://github.com/broadinstitute/gatk/pull/7665:166,Performance,cache,cached,166,We had removed it because it was shutting down and our tests showed it was unnecessary. It turns out that it's only unnecessary because we're getting jcenter results cached in the artifactory. JCenter is now planning to remain online for the immediate future in read only mode so it should be fine to put it back. Fixes https://github.com/broadinstitute/gatk/issues/7636,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7665
https://github.com/broadinstitute/gatk/pull/7665:55,Testability,test,tests,55,We had removed it because it was shutting down and our tests showed it was unnecessary. It turns out that it's only unnecessary because we're getting jcenter results cached in the artifactory. JCenter is now planning to remain online for the immediate future in read only mode so it should be fine to put it back. Fixes https://github.com/broadinstitute/gatk/issues/7636,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7665
https://github.com/broadinstitute/gatk/pull/7666:270,Security,expose,exposed,270,"The table names in GvsAssignId were not quoted with backticks, which is fine **except** if your dataset name starts with a number… which is a total valid identifier, but requires quoting. . Recently we had a customer (AoU) supply a dataset with the name `1kg_wgs` which exposed this problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7666
https://github.com/broadinstitute/gatk/issues/7668:41,Performance,perform,performance,41,"Using 1kG and/or other cohorts, evaluate performance of current implementation of gCNV model against the alternative described in #4988.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7668
https://github.com/broadinstitute/gatk/issues/7669:448,Performance,perform,performance,448,"This ticket is for tracking the progress of the concordance project described here which is in collaboration with @davidbenjamin from DSP. The output of GATK Mutect2 is in VCF format, and there is an existing GATK tool to compare the output VCF to a truth VCF containing real mutations. David is working on improving Mutect3, as part of that there is a need for a tool specific to somatic variant calling that can diagnose patterns of good and bad performance. To get started, he provided resources of example VCFs of filtered calls from Mutect2 and a truth vcf. I am working on creating a Python script to count the number of false positives/negatives by comparing the mutations in both the VCFs using cyvcf2 library.; After that the next step is to stratify these values by allele fraction and other filters applied by Mutect2. cc: @fleharty @brigranger",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7669
https://github.com/broadinstitute/gatk/pull/7670:9,Deployability,pipeline,pipeline,9,"The warp pipeline tests caught some cases that we apparently didn't have in our integration tests, but now we do!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7670
https://github.com/broadinstitute/gatk/pull/7670:80,Deployability,integrat,integration,80,"The warp pipeline tests caught some cases that we apparently didn't have in our integration tests, but now we do!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7670
https://github.com/broadinstitute/gatk/pull/7670:80,Integrability,integrat,integration,80,"The warp pipeline tests caught some cases that we apparently didn't have in our integration tests, but now we do!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7670
https://github.com/broadinstitute/gatk/pull/7670:18,Testability,test,tests,18,"The warp pipeline tests caught some cases that we apparently didn't have in our integration tests, but now we do!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7670
https://github.com/broadinstitute/gatk/pull/7670:92,Testability,test,tests,92,"The warp pipeline tests caught some cases that we apparently didn't have in our integration tests, but now we do!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7670
https://github.com/broadinstitute/gatk/issues/7671:452,Performance,perform,performance,452,"Hello,. We are working with ~1700 exome samples. The vcf file generated after joint genotyping is around 80 GB (uncompressed). When we are extracting samples from this master file, each extraction (regardless of singleton or trio exome) takes around 40-60 minutes. . We are using gatk 4.1.8.0 for production, and we tried the latest gatk (4.2.5.0) but we saw roughly 10 min decrease in extraction time. We gzipped/indexed our vcf, we still see similar performance. The variants / minute speed is ~50k. Any idea if our extraction rates are slow, or are we performing as expected?. For what its worth, our data is stored on a HDDs in RAID 5.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671
https://github.com/broadinstitute/gatk/issues/7671:555,Performance,perform,performing,555,"Hello,. We are working with ~1700 exome samples. The vcf file generated after joint genotyping is around 80 GB (uncompressed). When we are extracting samples from this master file, each extraction (regardless of singleton or trio exome) takes around 40-60 minutes. . We are using gatk 4.1.8.0 for production, and we tried the latest gatk (4.2.5.0) but we saw roughly 10 min decrease in extraction time. We gzipped/indexed our vcf, we still see similar performance. The variants / minute speed is ~50k. Any idea if our extraction rates are slow, or are we performing as expected?. For what its worth, our data is stored on a HDDs in RAID 5.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671
https://github.com/broadinstitute/gatk/issues/7672:414,Availability,error,error,414,"This request was created from a contribution made by Rahul Gupta on February 03, 2022 19:42 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4417613488411-In-Mutect2-force-calling-allele-via-alleles-does-not-force-call-the-allele](https://gatk.broadinstitute.org/hc/en-us/community/posts/4417613488411-In-Mutect2-force-calling-allele-via-alleles-does-not-force-call-the-allele). \--. If not an error, choose a category for your question(REQUIRED): ; ; c) Why do I see (......)?. GATK version: 4.2.4.1. Hi all,. I'm running Mutect2 over samples for which I have provided a VCF of alleles that I would like force-called. In 99% of cases this works great, but in a few instances a couple of alleles that I have included for force-calling do not actually make it into the output calls VCF. My command is:. gatk --java-options -Xmx3000m Mutect2 \\ ; ; \-R /data\_in/MY1776/MY1776.self.ref.shifted\_by\_8000\_bases.fasta \\ ; ; \-I /data\_in/MY1776/self\_realigned\_shifted.bam \\ ; ; \--read-filter MateOnSameContigOrNoMappedMateReadFilter \\ ; ; \--read-filter MateUnmappedAndUnmappedReadFilter \\ ; ; \-O /out\_default.vcf \\ ; ; \--alleles /data\_in/MY1776/MY1776.self.ref.reversed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672
https://github.com/broadinstitute/gatk/issues/7672:2471,Availability,recover,recover-all-dangling-branches,2471,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672
https://github.com/broadinstitute/gatk/issues/7672:2509,Availability,avail,avail,2509,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672
https://github.com/broadinstitute/gatk/issues/7672:2832,Deployability,UPDATE,UPDATE,2832,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672
https://github.com/broadinstitute/gatk/issues/7672:2858,Energy Efficiency,adapt,adaptive-pruning,2858,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672
https://github.com/broadinstitute/gatk/issues/7672:2858,Modifiability,adapt,adaptive-pruning,2858,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672
https://github.com/broadinstitute/gatk/issues/7672:2471,Safety,recover,recover-all-dangling-branches,2471,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672
https://github.com/broadinstitute/gatk/issues/7674:367,Availability,error,error,367,"Hello,. We're trying to run GenotypeGVCFs on a large genomicsDB workspace. The command is below, with the output. We run these jobs scatter/gather, with each job getting a define, small interval set. Despite being given a huge amount of RAM (testing >250GB), virtually all of the jobs die without any messages right after the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:564,Availability,error,error,564,"Hello,. We're trying to run GenotypeGVCFs on a large genomicsDB workspace. The command is below, with the output. We run these jobs scatter/gather, with each job getting a define, small interval set. Despite being given a huge amount of RAM (testing >250GB), virtually all of the jobs die without any messages right after the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:4566,Availability,Error,Error,4566,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:4630,Availability,error,error,4630,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:4694,Availability,Error,Error,4694,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:5017,Availability,Error,Error,5017,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:4652,Energy Efficiency,allocate,allocate,4652,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:4977,Energy Efficiency,allocate,allocate,4977,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:301,Integrability,message,messages,301,"Hello,. We're trying to run GenotypeGVCFs on a large genomicsDB workspace. The command is below, with the output. We run these jobs scatter/gather, with each job getting a define, small interval set. Despite being given a huge amount of RAM (testing >250GB), virtually all of the jobs die without any messages right after the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:347,Integrability,message,message,347,"Hello,. We're trying to run GenotypeGVCFs on a large genomicsDB workspace. The command is below, with the output. We run these jobs scatter/gather, with each job getting a define, small interval set. Despite being given a huge amount of RAM (testing >250GB), virtually all of the jobs die without any messages right after the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:373,Integrability,message,messages,373,"Hello,. We're trying to run GenotypeGVCFs on a large genomicsDB workspace. The command is below, with the output. We run these jobs scatter/gather, with each job getting a define, small interval set. Despite being given a huge amount of RAM (testing >250GB), virtually all of the jobs die without any messages right after the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:906,Performance,cache,cachedData,906,"Hello,. We're trying to run GenotypeGVCFs on a large genomicsDB workspace. The command is below, with the output. We run these jobs scatter/gather, with each job getting a define, small interval set. Despite being given a huge amount of RAM (testing >250GB), virtually all of the jobs die without any messages right after the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:1366,Performance,optimiz,optimizations,1366,"the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:31:14.784 INFO GenotypeGVCFs - Executing as labkey_submit@exanode-6-25 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 12:31:14.784 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_60-b27; 12:31:14.784 INFO GenotypeGVCFs - Start Date/Time: February 15",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:1421,Performance,Load,Loading,1421,", you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:31:14.784 INFO GenotypeGVCFs - Executing as labkey_submit@exanode-6-25 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 12:31:14.784 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_60-b27; 12:31:14.784 INFO GenotypeGVCFs - Start Date/Time: February 15, 2022 12:31:14 PM PST; 12:31:14.784 INFO GenotypeGVCFs - --------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:4790,Performance,cache,cachedData,4790,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:5031,Performance,load,load,5031,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:1700,Safety,detect,detect,1700,"java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:31:14.784 INFO GenotypeGVCFs - Executing as labkey_submit@exanode-6-25 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 12:31:14.784 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_60-b27; 12:31:14.784 INFO GenotypeGVCFs - Start Date/Time: February 15, 2022 12:31:14 PM PST; 12:31:14.784 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.784 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.784 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 12:31:14.784 INFO GenotypeGVCFs - Picard Version: 2.25.4; 12:31:14.784 INFO GenotypeGVCFs - Built ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7674:242,Testability,test,testing,242,"Hello,. We're trying to run GenotypeGVCFs on a large genomicsDB workspace. The command is below, with the output. We run these jobs scatter/gather, with each job getting a define, small interval set. Despite being given a huge amount of RAM (testing >250GB), virtually all of the jobs die without any messages right after the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674
https://github.com/broadinstitute/gatk/issues/7675:93,Availability,down,downloaded,93,"Hello,. I am running GATK GenotypeGVCFs v4.2.5.0 using a command similar to the one below. I downloaded the gatk ZIP from github, unzipped, and we run the local JAR (renamed to GenomeAnalysisTK4.jar). I am consistently getting a strange NoClassDefFoundError error (below). I noticed com/google/common/base is relocated in the shadowJar step, and that is the class it's complaining about here. Have you seen an error like this before?. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:258,Availability,error,error,258,"Hello,. I am running GATK GenotypeGVCFs v4.2.5.0 using a command similar to the one below. I downloaded the gatk ZIP from github, unzipped, and we run the local JAR (renamed to GenomeAnalysisTK4.jar). I am consistently getting a strange NoClassDefFoundError error (below). I noticed com/google/common/base is relocated in the shadowJar step, and that is the class it's complaining about here. Have you seen an error like this before?. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:410,Availability,error,error,410,"Hello,. I am running GATK GenotypeGVCFs v4.2.5.0 using a command similar to the one below. I downloaded the gatk ZIP from github, unzipped, and we run the local JAR (renamed to GenomeAnalysisTK4.jar). I am consistently getting a strange NoClassDefFoundError error (below). I noticed com/google/common/base is relocated in the shadowJar step, and that is the class it's complaining about here. Have you seen an error like this before?. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:685,Performance,cache,cachedData,685,"Hello,. I am running GATK GenotypeGVCFs v4.2.5.0 using a command similar to the one below. I downloaded the gatk ZIP from github, unzipped, and we run the local JAR (renamed to GenomeAnalysisTK4.jar). I am consistently getting a strange NoClassDefFoundError error (below). I noticed com/google/common/base is relocated in the shadowJar step, and that is the class it's complaining about here. Have you seen an error like this before?. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:1145,Performance,optimiz,optimizations,1145,"the gatk ZIP from github, unzipped, and we run the local JAR (renamed to GenomeAnalysisTK4.jar). I am consistently getting a strange NoClassDefFoundError error (below). I noticed com/google/common/base is relocated in the shadowJar step, and that is the class it's complaining about here. Have you seen an error like this before?. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC header (bad signature); 	at java.util.zip.ZipFile.read(Native Method); 	at java.util.zip.ZipFile.access$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:1777,Performance,load,loadClass,1777,"enotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC header (bad signature); 	at java.util.zip.ZipFile.read(Native Method); 	at java.util.zip.ZipFile.access$1400(ZipFile.java:60); 	at java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:716); 	at java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:419); 	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158); 	at sun.misc.Resource.getBytes(Resource.java:124); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:462); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:1847,Performance,load,loadClass,1847,"enotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC header (bad signature); 	at java.util.zip.ZipFile.read(Native Method); 	at java.util.zip.ZipFile.access$1400(ZipFile.java:60); 	at java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:716); 	at java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:419); 	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158); 	at sun.misc.Resource.getBytes(Resource.java:124); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:462); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:1903,Performance,load,loadClass,1903,"enotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC header (bad signature); 	at java.util.zip.ZipFile.read(Native Method); 	at java.util.zip.ZipFile.access$1400(ZipFile.java:60); 	at java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:716); 	at java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:419); 	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158); 	at sun.misc.Resource.getBytes(Resource.java:124); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:462); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:1632,Security,secur,security,1632,"enotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC header (bad signature); 	at java.util.zip.ZipFile.read(Native Method); 	at java.util.zip.ZipFile.access$1400(ZipFile.java:60); 	at java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:716); 	at java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:419); 	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158); 	at sun.misc.Resource.getBytes(Resource.java:124); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:462); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:1641,Security,Access,AccessController,1641,"enotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC header (bad signature); 	at java.util.zip.ZipFile.read(Native Method); 	at java.util.zip.ZipFile.access$1400(ZipFile.java:60); 	at java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:716); 	at java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:419); 	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158); 	at sun.misc.Resource.getBytes(Resource.java:124); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:462); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:2097,Security,access,access,2097,"enotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC header (bad signature); 	at java.util.zip.ZipFile.read(Native Method); 	at java.util.zip.ZipFile.access$1400(ZipFile.java:60); 	at java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:716); 	at java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:419); 	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158); 	at sun.misc.Resource.getBytes(Resource.java:124); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:462); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7675:2492,Security,access,access,2492,"enotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC header (bad signature); 	at java.util.zip.ZipFile.read(Native Method); 	at java.util.zip.ZipFile.access$1400(ZipFile.java:60); 	at java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:716); 	at java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:419); 	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158); 	at sun.misc.Resource.getBytes(Resource.java:124); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:462); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675
https://github.com/broadinstitute/gatk/issues/7676:964,Availability,error,error,964,"This request was created from a contribution made by tc on February 09, 2022 17:49 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:1482,Availability,error,errors,1482,"ion workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:2059,Availability,down,down,2059,"segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:6972,Availability,error,error,6972,eWalker.java:99) ; ;     at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ;     at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ;     at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.traverse(FeatureWalker.java:97) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). Here's how my called segment file looks like:. CONTIG    START    END    NUM\_POINTS\_COPY\_RATIO    MEAN\_LOG2\_COPY\_RATIO    CALL ; ; 1    14645    13839497    2764    -0.121225    0 ; ; 1    13839498    55529537    8713    -0.060943    0 ; ; 1    55534430    142797736    6763    0.050711    0 ; ; 1    142803161    143164144    9    -1.797248    - ; ; 1    143186822    156929235    3970    -0.077460    0 ; ; 1    156929872    224009136    8811    0.024671    0 ; ; 1    224116102    224116470    1    -4.545156    - ; ; 1    224124170    249230997    3307    0.004490    0 ; ; 2    41203    137402680    14122    -0.000470    0 ; ; 2    137402681    215911009    8594    0.077261    0 ; ; 2    215914005    243081349    4299    -0.032370    0. I used GATK/4.2.4.1. Would you please kindly let me know the cause for the invalid interval error?Thanks a lot!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/271141'>Zendesk ticket #271141</a>)<br> gz#271141</i>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:4468,Energy Efficiency,Reduce,ReduceOps,4468,.funcotator.DataSourceFuncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:239) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:211) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:182) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForSegment$2(FuncotatorEngine.java:218) ; ;     at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ;     at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ;     at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ;     at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ;     at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ;     at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ;     at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForSegment(FuncotatorEngine.java:221) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:191) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:59) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.lambda$traverse$0(FeatureWalker.java:99) ; ;     at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ;     at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ;     at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.traverse(FeatureWalker.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:4478,Energy Efficiency,Reduce,ReduceOp,4478,.funcotator.DataSourceFuncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:239) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:211) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:182) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForSegment$2(FuncotatorEngine.java:218) ; ;     at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ;     at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ;     at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ;     at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ;     at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ;     at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ;     at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForSegment(FuncotatorEngine.java:221) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:191) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:59) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.lambda$traverse$0(FeatureWalker.java:99) ; ;     at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ;     at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ;     at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.traverse(FeatureWalker.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:4506,Energy Efficiency,Reduce,ReduceOps,4506,ncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:239) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:211) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:182) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForSegment$2(FuncotatorEngine.java:218) ; ;     at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ;     at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ;     at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ;     at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ;     at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ;     at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ;     at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForSegment(FuncotatorEngine.java:221) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:191) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:59) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.lambda$traverse$0(FeatureWalker.java:99) ; ;     at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ;     at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ;     at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.traverse(FeatureWalker.java:97) ; ;     at org.b,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:970,Integrability,message,message,970,"This request was created from a contribution made by tc on February 09, 2022 17:49 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:4397,Integrability,wrap,wrapAndCopyInto,4397,actory.java:2866) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:239) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:211) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:182) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForSegment$2(FuncotatorEngine.java:218) ; ;     at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ;     at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ;     at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ;     at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ;     at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ;     at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ;     at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForSegment(FuncotatorEngine.java:221) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:191) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:59) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.lambda$traverse$0(FeatureWalker.java:99) ; ;     at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ;     at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ;     at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ;     at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:1252,Performance,Perform,Performing,1252,"itute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:471,Safety,detect,detection,471,"This request was created from a contribution made by tc on February 09, 2022 17:49 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:1210,Safety,detect,detected,1210,"valid-interval-in-FuncotateSegments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 mi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:2393,Security,validat,validateArg,2393,"GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2938) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnSegment(GencodeF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:2483,Security,validat,validatePositions,2483,"he Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2938) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnSegment(GencodeFuncotationFactory.java:2866) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:2468,Usability,Simpl,SimpleInterval,2468,"e some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2938) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnSegment(GencodeFuncotationFactory.java:2866) ; ;     at org.broadinstitute.hellbender.tool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:2501,Usability,Simpl,SimpleInterval,2501,"he Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2938) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnSegment(GencodeFuncotationFactory.java:2866) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:2572,Usability,Simpl,SimpleInterval,2572,"e two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2938) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnSegment(GencodeFuncotationFactory.java:2866) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:239) ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/issues/7676:2594,Usability,Simpl,SimpleInterval,2594," ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2938) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnSegment(GencodeFuncotationFactory.java:2866) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:239) ; ;     at org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676
https://github.com/broadinstitute/gatk/pull/7679:163,Integrability,inject,inject,163,Closes #7672. @ldgauthier This is the bug fix for Sarah Calvo. The allele was lost when trimming caused its haplotype to start with a deletion. The solution is to inject force-calling alleles after trimming. Are you able to review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7679
https://github.com/broadinstitute/gatk/pull/7679:163,Security,inject,inject,163,Closes #7672. @ldgauthier This is the bug fix for Sarah Calvo. The allele was lost when trimming caused its haplotype to start with a deletion. The solution is to inject force-calling alleles after trimming. Are you able to review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7679
https://github.com/broadinstitute/gatk/pull/7680:197,Availability,error,error,197,"- reduced retries for task calling write API because if it fails more than once, chances are it will continue to fail because the import process was stopped before completion; - hopefully made the error message less scary, also included table number for easier cleanup. Closes https://broadworkbench.atlassian.net/browse/VS-267",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7680
https://github.com/broadinstitute/gatk/pull/7680:2,Energy Efficiency,reduce,reduced,2,"- reduced retries for task calling write API because if it fails more than once, chances are it will continue to fail because the import process was stopped before completion; - hopefully made the error message less scary, also included table number for easier cleanup. Closes https://broadworkbench.atlassian.net/browse/VS-267",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7680
https://github.com/broadinstitute/gatk/pull/7680:203,Integrability,message,message,203,"- reduced retries for task calling write API because if it fails more than once, chances are it will continue to fail because the import process was stopped before completion; - hopefully made the error message less scary, also included table number for easier cleanup. Closes https://broadworkbench.atlassian.net/browse/VS-267",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7680
https://github.com/broadinstitute/gatk/issues/7681:526,Availability,error,error,526,"## Bug Report. ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - [X] Latest public release version [4.2.5.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; The auto-generated wdl for CombineGVCFs on dockstore won't work because it doesn't have any inputs for the indices of the input vcfs. This means GATK cannot access the vcf indices because they never get localized, so the workflow fails. . #### Steps to reproduce; Take any vcfs and run them through the workflow to get an error about missing indices. . #### Expected behavior; Including the indices in the task inputs will allow them to get localized along with the vcfs so GATK can operate normally. . #### Actual behavior; You get an error saying it requires index files to proceed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7681
https://github.com/broadinstitute/gatk/issues/7681:740,Availability,error,error,740,"## Bug Report. ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - [X] Latest public release version [4.2.5.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; The auto-generated wdl for CombineGVCFs on dockstore won't work because it doesn't have any inputs for the indices of the input vcfs. This means GATK cannot access the vcf indices because they never get localized, so the workflow fails. . #### Steps to reproduce; Take any vcfs and run them through the workflow to get an error about missing indices. . #### Expected behavior; Including the indices in the task inputs will allow them to get localized along with the vcfs so GATK can operate normally. . #### Actual behavior; You get an error saying it requires index files to proceed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7681
https://github.com/broadinstitute/gatk/issues/7681:109,Deployability,release,release,109,"## Bug Report. ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - [X] Latest public release version [4.2.5.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; The auto-generated wdl for CombineGVCFs on dockstore won't work because it doesn't have any inputs for the indices of the input vcfs. This means GATK cannot access the vcf indices because they never get localized, so the workflow fails. . #### Steps to reproduce; Take any vcfs and run them through the workflow to get an error about missing indices. . #### Expected behavior; Including the indices in the task inputs will allow them to get localized along with the vcfs so GATK can operate normally. . #### Actual behavior; You get an error saying it requires index files to proceed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7681
https://github.com/broadinstitute/gatk/issues/7681:361,Security,access,access,361,"## Bug Report. ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - [X] Latest public release version [4.2.5.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; The auto-generated wdl for CombineGVCFs on dockstore won't work because it doesn't have any inputs for the indices of the input vcfs. This means GATK cannot access the vcf indices because they never get localized, so the workflow fails. . #### Steps to reproduce; Take any vcfs and run them through the workflow to get an error about missing indices. . #### Expected behavior; Including the indices in the task inputs will allow them to get localized along with the vcfs so GATK can operate normally. . #### Actual behavior; You get an error saying it requires index files to proceed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7681
https://github.com/broadinstitute/gatk/issues/7681:178,Testability,test,test,178,"## Bug Report. ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - [X] Latest public release version [4.2.5.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; The auto-generated wdl for CombineGVCFs on dockstore won't work because it doesn't have any inputs for the indices of the input vcfs. This means GATK cannot access the vcf indices because they never get localized, so the workflow fails. . #### Steps to reproduce; Take any vcfs and run them through the workflow to get an error about missing indices. . #### Expected behavior; Including the indices in the task inputs will allow them to get localized along with the vcfs so GATK can operate normally. . #### Actual behavior; You get an error saying it requires index files to proceed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7681
https://github.com/broadinstitute/gatk/pull/7682:27,Availability,failure,failure,27,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:417,Availability,error,error,417,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:1249,Availability,down,downloaded,1249,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:122,Deployability,install,install,122,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:250,Deployability,install,installs,250,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:323,Deployability,release,released,323,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:433,Deployability,install,installed,433,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:531,Deployability,install,install,531,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:651,Deployability,update,update,651,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:740,Deployability,release,released,740,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:838,Deployability,install,installed,838,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:907,Deployability,update,updates,907,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:1026,Deployability,update,updated,1026,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:1117,Deployability,update,update,1117,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:1467,Integrability,depend,depends,1467,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7682:1215,Testability,test,tests,1215,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682
https://github.com/broadinstitute/gatk/pull/7686:65,Energy Efficiency,efficient,efficient,65,"Three main performance optimizations:. 1. **Avro Parsing**: More efficient parsing and representation of primitive types in Avro-based records (ExtractCohortRecord, ReferenceRecord). We previously called toString() and then parseLong() on everything, even though it was already the right datatype. 2. **Inferred State**: we keep track of which samples have been seen, so that later we can determine which samples have **not** been seen for each site. The data structures here were slow with 100k samples and lots of variants. Moved to using a TreeSet and BitSet. 3. **Reference Genotypes**: Add reference genotypes in bulk (via ReferenceGenotypeInfo, rather than a heavy Variant Context) rather than one at a time. More Details from profiling. https://docs.google.com/spreadsheets/d/1aA7LKgPsaELiGurw95qVX1PwGt54I5rn1h_fAAhkPMo/edit#gid=0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7686
https://github.com/broadinstitute/gatk/pull/7686:11,Performance,perform,performance,11,"Three main performance optimizations:. 1. **Avro Parsing**: More efficient parsing and representation of primitive types in Avro-based records (ExtractCohortRecord, ReferenceRecord). We previously called toString() and then parseLong() on everything, even though it was already the right datatype. 2. **Inferred State**: we keep track of which samples have been seen, so that later we can determine which samples have **not** been seen for each site. The data structures here were slow with 100k samples and lots of variants. Moved to using a TreeSet and BitSet. 3. **Reference Genotypes**: Add reference genotypes in bulk (via ReferenceGenotypeInfo, rather than a heavy Variant Context) rather than one at a time. More Details from profiling. https://docs.google.com/spreadsheets/d/1aA7LKgPsaELiGurw95qVX1PwGt54I5rn1h_fAAhkPMo/edit#gid=0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7686
https://github.com/broadinstitute/gatk/pull/7686:23,Performance,optimiz,optimizations,23,"Three main performance optimizations:. 1. **Avro Parsing**: More efficient parsing and representation of primitive types in Avro-based records (ExtractCohortRecord, ReferenceRecord). We previously called toString() and then parseLong() on everything, even though it was already the right datatype. 2. **Inferred State**: we keep track of which samples have been seen, so that later we can determine which samples have **not** been seen for each site. The data structures here were slow with 100k samples and lots of variants. Moved to using a TreeSet and BitSet. 3. **Reference Genotypes**: Add reference genotypes in bulk (via ReferenceGenotypeInfo, rather than a heavy Variant Context) rather than one at a time. More Details from profiling. https://docs.google.com/spreadsheets/d/1aA7LKgPsaELiGurw95qVX1PwGt54I5rn1h_fAAhkPMo/edit#gid=0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7686
https://github.com/broadinstitute/gatk/issues/7687:378,Performance,cache,cachedData,378,"Hello,. I'm running GATK 4.2.5.0 with a command line this. Note --max-alternate-alleles and --genomicsdb-max-alternate-alleles. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--genomicsdb-max-alternate-alleles 9 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. I'm getting this NPE after it runs for about 40 minutes, meaning many sites were processed fine:. ```; 18 Feb 2022 11:32:59,897 DEBUG: 	java.lang.NullPointerException; 18 Feb 2022 11:32:59,907 DEBUG: 		at org.broadinstitute.hellbender.utils.GenotypeUtils.computeDiploidGenotypeCounts(GenotypeUtils.java:85); 18 Feb 2022 11:32:59,919 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.calculateEH(ExcessHet.java:96); 18 Feb 2022 11:32:59,943 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.annotate(ExcessHet.java:84); 18 Feb 2022 11:32:59,955 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.addInfoAnnotations(VariantAnnotatorEngine.java:355); 18 Feb 2022 11:32:59,971 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:334); 18 Feb 2022 11:32:59,985 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:306); 18 Feb 2022 11:32:59,995 DEBUG: 		at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7687
https://github.com/broadinstitute/gatk/issues/7687:879,Performance,optimiz,optimizations,879,"Hello,. I'm running GATK 4.2.5.0 with a command line this. Note --max-alternate-alleles and --genomicsdb-max-alternate-alleles. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--genomicsdb-max-alternate-alleles 9 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. I'm getting this NPE after it runs for about 40 minutes, meaning many sites were processed fine:. ```; 18 Feb 2022 11:32:59,897 DEBUG: 	java.lang.NullPointerException; 18 Feb 2022 11:32:59,907 DEBUG: 		at org.broadinstitute.hellbender.utils.GenotypeUtils.computeDiploidGenotypeCounts(GenotypeUtils.java:85); 18 Feb 2022 11:32:59,919 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.calculateEH(ExcessHet.java:96); 18 Feb 2022 11:32:59,943 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.annotate(ExcessHet.java:84); 18 Feb 2022 11:32:59,955 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.addInfoAnnotations(VariantAnnotatorEngine.java:355); 18 Feb 2022 11:32:59,971 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:334); 18 Feb 2022 11:32:59,985 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:306); 18 Feb 2022 11:32:59,995 DEBUG: 		at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7687
https://github.com/broadinstitute/gatk/issues/7690:732,Availability,error,error,732,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:1197,Availability,error,error,1197,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:1235,Availability,ERROR,ERROR,1235,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:1372,Availability,ERROR,ERROR,1372,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:112,Deployability,release,release,112,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:549,Deployability,pipeline,pipeline,549,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:679,Deployability,pipeline,pipeline,679,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:1089,Deployability,pipeline,pipeline,1089,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:182,Testability,test,test,182,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:1527,Testability,log,log,1527,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7690:891,Usability,feedback,feedback,891,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690
https://github.com/broadinstitute/gatk/issues/7691:113,Deployability,release,release,113,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [ ] Latest public release version [4.2.5.0]. ### Description ; My gVCF files are block compressed and indexed, but the files have the file extension "".gvcf.gz"" rather than "".vcf.gz"". When I run `GenomicsDBImport` with `--bypass-feature-reader`, the "".gvcf.gz"" file cannot be recognized as a block compressed vcf file. The code of `GenomicsDBImport` validates if input is block compressed by checking if the file extension is "".vcf.gz"". ```; private static void assertVariantFileIsCompressedAndIndexed(final Path path) {; if (!path.toString().toLowerCase().endsWith(FileExtensions.COMPRESSED_VCF)) {; throw new UserException(""Input variant files must be block compressed vcfs when using "" +; BYPASS_FEATURE_READER + "", but "" + path.toString() + "" does not appear to be"");; }; Path indexPath = path.resolveSibling(path.getFileName() + FileExtensions.COMPRESSED_VCF_INDEX);; IOUtils.assertFileIsReadable(indexPath);; }; ```. I understand that this is an issue on my side because I did not name my gVCF files with the standard extension "".vcf.gz"". Is it possible to make this check less stringent in a future release? Maybe make any "".gz""/"".bgz"" file acceptable, or check the "".tbi"" index file to identify block compression (existing index typically means the file is block compressed and indexed). . Thank you. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7691
https://github.com/broadinstitute/gatk/issues/7691:1200,Deployability,release,release,1200,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [ ] Latest public release version [4.2.5.0]. ### Description ; My gVCF files are block compressed and indexed, but the files have the file extension "".gvcf.gz"" rather than "".vcf.gz"". When I run `GenomicsDBImport` with `--bypass-feature-reader`, the "".gvcf.gz"" file cannot be recognized as a block compressed vcf file. The code of `GenomicsDBImport` validates if input is block compressed by checking if the file extension is "".vcf.gz"". ```; private static void assertVariantFileIsCompressedAndIndexed(final Path path) {; if (!path.toString().toLowerCase().endsWith(FileExtensions.COMPRESSED_VCF)) {; throw new UserException(""Input variant files must be block compressed vcfs when using "" +; BYPASS_FEATURE_READER + "", but "" + path.toString() + "" does not appear to be"");; }; Path indexPath = path.resolveSibling(path.getFileName() + FileExtensions.COMPRESSED_VCF_INDEX);; IOUtils.assertFileIsReadable(indexPath);; }; ```. I understand that this is an issue on my side because I did not name my gVCF files with the standard extension "".vcf.gz"". Is it possible to make this check less stringent in a future release? Maybe make any "".gz""/"".bgz"" file acceptable, or check the "".tbi"" index file to identify block compression (existing index typically means the file is block compressed and indexed). . Thank you. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7691
https://github.com/broadinstitute/gatk/issues/7691:444,Security,validat,validates,444,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [ ] Latest public release version [4.2.5.0]. ### Description ; My gVCF files are block compressed and indexed, but the files have the file extension "".gvcf.gz"" rather than "".vcf.gz"". When I run `GenomicsDBImport` with `--bypass-feature-reader`, the "".gvcf.gz"" file cannot be recognized as a block compressed vcf file. The code of `GenomicsDBImport` validates if input is block compressed by checking if the file extension is "".vcf.gz"". ```; private static void assertVariantFileIsCompressedAndIndexed(final Path path) {; if (!path.toString().toLowerCase().endsWith(FileExtensions.COMPRESSED_VCF)) {; throw new UserException(""Input variant files must be block compressed vcfs when using "" +; BYPASS_FEATURE_READER + "", but "" + path.toString() + "" does not appear to be"");; }; Path indexPath = path.resolveSibling(path.getFileName() + FileExtensions.COMPRESSED_VCF_INDEX);; IOUtils.assertFileIsReadable(indexPath);; }; ```. I understand that this is an issue on my side because I did not name my gVCF files with the standard extension "".vcf.gz"". Is it possible to make this check less stringent in a future release? Maybe make any "".gz""/"".bgz"" file acceptable, or check the "".tbi"" index file to identify block compression (existing index typically means the file is block compressed and indexed). . Thank you. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7691
https://github.com/broadinstitute/gatk/issues/7691:556,Testability,assert,assertVariantFileIsCompressedAndIndexed,556,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [ ] Latest public release version [4.2.5.0]. ### Description ; My gVCF files are block compressed and indexed, but the files have the file extension "".gvcf.gz"" rather than "".vcf.gz"". When I run `GenomicsDBImport` with `--bypass-feature-reader`, the "".gvcf.gz"" file cannot be recognized as a block compressed vcf file. The code of `GenomicsDBImport` validates if input is block compressed by checking if the file extension is "".vcf.gz"". ```; private static void assertVariantFileIsCompressedAndIndexed(final Path path) {; if (!path.toString().toLowerCase().endsWith(FileExtensions.COMPRESSED_VCF)) {; throw new UserException(""Input variant files must be block compressed vcfs when using "" +; BYPASS_FEATURE_READER + "", but "" + path.toString() + "" does not appear to be"");; }; Path indexPath = path.resolveSibling(path.getFileName() + FileExtensions.COMPRESSED_VCF_INDEX);; IOUtils.assertFileIsReadable(indexPath);; }; ```. I understand that this is an issue on my side because I did not name my gVCF files with the standard extension "".vcf.gz"". Is it possible to make this check less stringent in a future release? Maybe make any "".gz""/"".bgz"" file acceptable, or check the "".tbi"" index file to identify block compression (existing index typically means the file is block compressed and indexed). . Thank you. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7691
https://github.com/broadinstitute/gatk/issues/7691:975,Testability,assert,assertFileIsReadable,975,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [ ] Latest public release version [4.2.5.0]. ### Description ; My gVCF files are block compressed and indexed, but the files have the file extension "".gvcf.gz"" rather than "".vcf.gz"". When I run `GenomicsDBImport` with `--bypass-feature-reader`, the "".gvcf.gz"" file cannot be recognized as a block compressed vcf file. The code of `GenomicsDBImport` validates if input is block compressed by checking if the file extension is "".vcf.gz"". ```; private static void assertVariantFileIsCompressedAndIndexed(final Path path) {; if (!path.toString().toLowerCase().endsWith(FileExtensions.COMPRESSED_VCF)) {; throw new UserException(""Input variant files must be block compressed vcfs when using "" +; BYPASS_FEATURE_READER + "", but "" + path.toString() + "" does not appear to be"");; }; Path indexPath = path.resolveSibling(path.getFileName() + FileExtensions.COMPRESSED_VCF_INDEX);; IOUtils.assertFileIsReadable(indexPath);; }; ```. I understand that this is an issue on my side because I did not name my gVCF files with the standard extension "".vcf.gz"". Is it possible to make this check less stringent in a future release? Maybe make any "".gz""/"".bgz"" file acceptable, or check the "".tbi"" index file to identify block compression (existing index typically means the file is block compressed and indexed). . Thank you. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7691
https://github.com/broadinstitute/gatk/issues/7693:58,Availability,failure,failures,58,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:543,Availability,failure,failures,543,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:601,Availability,failure,failures,601,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:615,Availability,error,errors,615,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:690,Availability,error,error,690,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:1764,Availability,failure,failures,1764,"arp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where HaplotypeCaller is using more memory across longer shard lengths. Given that these are not throwing java garbage collection exceptions makes me suspicious that this might be related to the non-java gkl code thats being run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:170,Deployability,pipeline,pipeline,170,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:237,Deployability,pipeline,pipelines,237,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:696,Integrability,message,message,696,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:1051,Integrability,message,messages,1051,"t were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where HaplotypeCaller is using more memory a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7693:113,Testability,test,test,113,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693
https://github.com/broadinstitute/gatk/issues/7696:108,Deployability,release,release,108,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - Latest public release version [4.2.5.0]. ### Description . #### Steps to reproduce; Specify `--mate-too-distant-length` parameter. #### Expected behavior; Use specified value to filter paired-end reads. #### Actual behavior; Java exception:. ```; Using GATK jar /home/rwilton/tools/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/rwilton/tools/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar HaplotypeCaller; --reference /datascope/rwilton/scratch/GATK/GRCh38/chr14.fna; --intervals chr14; --emit-ref-confidence GVCF; --sample-name HG002; --smith-waterman FASTEST_AVAILABLE; --native-pair-hmm-threads 48; --read-filter MateDistantReadFilter; --mate-too-distant-length 1500; --minimum-mapping-quality 10; --mapping-quality-threshold-for-genotyping 10; --input full.chr14.bam; --output vcf1/full.chr14.g.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: Couldn't set field value for mateTooDistantLength in org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter@55fdf7f9 with value 1500.; at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:680); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setScalarValue(NamedArgumentDefinition.java:380); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValues(NamedArgumentDefinition.java:293); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.propagateParsedValues(CommandLineArgumentParser.java:490); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:233,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7696
https://github.com/broadinstitute/gatk/issues/7696:2511,Safety,Unsafe,UnsafeFieldAccessorImpl,2511,-output vcf1/full.chr14.g.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: Couldn't set field value for mateTooDistantLength in org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter@55fdf7f9 with value 1500.; at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:680); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setScalarValue(NamedArgumentDefinition.java:380); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValues(NamedArgumentDefinition.java:293); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.propagateParsedValues(CommandLineArgumentParser.java:490); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:233); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:207); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalAccessException: Can not set static final int field org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter.mateTooDistantLength to java.lang.Integer; at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); at sun.reflect.UnsafeQualifiedStaticIntegerFieldAccessorImpl.set(UnsafeQualifiedStaticIntegerFieldAccessorImpl.java:77); at java.lang.reflect.Field.set(Field.java:764); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:677); ... 9 more; HaplotypeCaller ends (3); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7696
https://github.com/broadinstitute/gatk/issues/7696:2573,Safety,Unsafe,UnsafeFieldAccessorImpl,2573,-output vcf1/full.chr14.g.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: Couldn't set field value for mateTooDistantLength in org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter@55fdf7f9 with value 1500.; at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:680); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setScalarValue(NamedArgumentDefinition.java:380); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValues(NamedArgumentDefinition.java:293); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.propagateParsedValues(CommandLineArgumentParser.java:490); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:233); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:207); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalAccessException: Can not set static final int field org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter.mateTooDistantLength to java.lang.Integer; at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); at sun.reflect.UnsafeQualifiedStaticIntegerFieldAccessorImpl.set(UnsafeQualifiedStaticIntegerFieldAccessorImpl.java:77); at java.lang.reflect.Field.set(Field.java:764); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:677); ... 9 more; HaplotypeCaller ends (3); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7696
https://github.com/broadinstitute/gatk/issues/7696:2622,Safety,Unsafe,UnsafeFieldAccessorImpl,2622,-output vcf1/full.chr14.g.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: Couldn't set field value for mateTooDistantLength in org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter@55fdf7f9 with value 1500.; at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:680); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setScalarValue(NamedArgumentDefinition.java:380); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValues(NamedArgumentDefinition.java:293); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.propagateParsedValues(CommandLineArgumentParser.java:490); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:233); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:207); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalAccessException: Can not set static final int field org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter.mateTooDistantLength to java.lang.Integer; at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); at sun.reflect.UnsafeQualifiedStaticIntegerFieldAccessorImpl.set(UnsafeQualifiedStaticIntegerFieldAccessorImpl.java:77); at java.lang.reflect.Field.set(Field.java:764); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:677); ... 9 more; HaplotypeCaller ends (3); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7696
https://github.com/broadinstitute/gatk/issues/7696:2684,Safety,Unsafe,UnsafeFieldAccessorImpl,2684,-output vcf1/full.chr14.g.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: Couldn't set field value for mateTooDistantLength in org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter@55fdf7f9 with value 1500.; at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:680); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setScalarValue(NamedArgumentDefinition.java:380); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValues(NamedArgumentDefinition.java:293); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.propagateParsedValues(CommandLineArgumentParser.java:490); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:233); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:207); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalAccessException: Can not set static final int field org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter.mateTooDistantLength to java.lang.Integer; at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); at sun.reflect.UnsafeQualifiedStaticIntegerFieldAccessorImpl.set(UnsafeQualifiedStaticIntegerFieldAccessorImpl.java:77); at java.lang.reflect.Field.set(Field.java:764); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:677); ... 9 more; HaplotypeCaller ends (3); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7696
https://github.com/broadinstitute/gatk/issues/7696:2733,Safety,Unsafe,UnsafeQualifiedStaticIntegerFieldAccessorImpl,2733,-output vcf1/full.chr14.g.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: Couldn't set field value for mateTooDistantLength in org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter@55fdf7f9 with value 1500.; at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:680); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setScalarValue(NamedArgumentDefinition.java:380); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValues(NamedArgumentDefinition.java:293); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.propagateParsedValues(CommandLineArgumentParser.java:490); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:233); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:207); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalAccessException: Can not set static final int field org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter.mateTooDistantLength to java.lang.Integer; at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); at sun.reflect.UnsafeQualifiedStaticIntegerFieldAccessorImpl.set(UnsafeQualifiedStaticIntegerFieldAccessorImpl.java:77); at java.lang.reflect.Field.set(Field.java:764); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:677); ... 9 more; HaplotypeCaller ends (3); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7696
https://github.com/broadinstitute/gatk/issues/7696:2783,Safety,Unsafe,UnsafeQualifiedStaticIntegerFieldAccessorImpl,2783,-output vcf1/full.chr14.g.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: Couldn't set field value for mateTooDistantLength in org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter@55fdf7f9 with value 1500.; at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:680); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setScalarValue(NamedArgumentDefinition.java:380); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValues(NamedArgumentDefinition.java:293); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.propagateParsedValues(CommandLineArgumentParser.java:490); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:233); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:207); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalAccessException: Can not set static final int field org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter.mateTooDistantLength to java.lang.Integer; at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); at sun.reflect.UnsafeQualifiedStaticIntegerFieldAccessorImpl.set(UnsafeQualifiedStaticIntegerFieldAccessorImpl.java:77); at java.lang.reflect.Field.set(Field.java:764); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:677); ... 9 more; HaplotypeCaller ends (3); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7696
https://github.com/broadinstitute/gatk/issues/7697:1441,Deployability,install,installed,1441," Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Feature request. ### Tool(s) or class(es) involved. VariantRecalibrator. ### Description. VariantRecalibrator automatically runs the generated Rscript to produce recalibration plots. This is usually good and convenient, but it requires that all *R* dependencies must be installed in the same environment in the current running GATK environment. This is not necessarily the case for sandbox-based package managers e.g. docker or conda. A viable fix on the user's side is to include R dependencies with GATK in e.g. docker or conda. But I think I would prefer if my packages were as independent of each other as possible. It would be great if **VariantRecalibrator had an option to write but not run the Rscript for recalibration plots.** Then, the user can call the Rscript in an appropriate e.g. R conda environment or docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7697
https://github.com/broadinstitute/gatk/issues/7697:1420,Integrability,depend,dependencies,1420," Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Feature request. ### Tool(s) or class(es) involved. VariantRecalibrator. ### Description. VariantRecalibrator automatically runs the generated Rscript to produce recalibration plots. This is usually good and convenient, but it requires that all *R* dependencies must be installed in the same environment in the current running GATK environment. This is not necessarily the case for sandbox-based package managers e.g. docker or conda. A viable fix on the user's side is to include R dependencies with GATK in e.g. docker or conda. But I think I would prefer if my packages were as independent of each other as possible. It would be great if **VariantRecalibrator had an option to write but not run the Rscript for recalibration plots.** Then, the user can call the Rscript in an appropriate e.g. R conda environment or docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7697
https://github.com/broadinstitute/gatk/issues/7697:1654,Integrability,depend,dependencies,1654," Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Feature request. ### Tool(s) or class(es) involved. VariantRecalibrator. ### Description. VariantRecalibrator automatically runs the generated Rscript to produce recalibration plots. This is usually good and convenient, but it requires that all *R* dependencies must be installed in the same environment in the current running GATK environment. This is not necessarily the case for sandbox-based package managers e.g. docker or conda. A viable fix on the user's side is to include R dependencies with GATK in e.g. docker or conda. But I think I would prefer if my packages were as independent of each other as possible. It would be great if **VariantRecalibrator had an option to write but not run the Rscript for recalibration plots.** Then, the user can call the Rscript in an appropriate e.g. R conda environment or docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7697
https://github.com/broadinstitute/gatk/issues/7697:1553,Modifiability,sandbox,sandbox-based,1553," Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Feature request. ### Tool(s) or class(es) involved. VariantRecalibrator. ### Description. VariantRecalibrator automatically runs the generated Rscript to produce recalibration plots. This is usually good and convenient, but it requires that all *R* dependencies must be installed in the same environment in the current running GATK environment. This is not necessarily the case for sandbox-based package managers e.g. docker or conda. A viable fix on the user's side is to include R dependencies with GATK in e.g. docker or conda. But I think I would prefer if my packages were as independent of each other as possible. It would be great if **VariantRecalibrator had an option to write but not run the Rscript for recalibration plots.** Then, the user can call the Rscript in an appropriate e.g. R conda environment or docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7697
https://github.com/broadinstitute/gatk/issues/7697:1553,Testability,sandbox,sandbox-based,1553," Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Feature request. ### Tool(s) or class(es) involved. VariantRecalibrator. ### Description. VariantRecalibrator automatically runs the generated Rscript to produce recalibration plots. This is usually good and convenient, but it requires that all *R* dependencies must be installed in the same environment in the current running GATK environment. This is not necessarily the case for sandbox-based package managers e.g. docker or conda. A viable fix on the user's side is to include R dependencies with GATK in e.g. docker or conda. But I think I would prefer if my packages were as independent of each other as possible. It would be great if **VariantRecalibrator had an option to write but not run the Rscript for recalibration plots.** Then, the user can call the Rscript in an appropriate e.g. R conda environment or docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7697
https://github.com/broadinstitute/gatk/issues/7702:511,Deployability,pipeline,pipeline,511,"Here is an issue ticket for adding the diagnosetarget feature to DepthofCoverage. This request was created from a contribution made by Benoit Dewitte on February 09, 2022 13:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-). \--. Hi! ; ; I'm currentlly refactoring ours pipeline which use depthofcoverage and diagnosetarget from GATK 3.8. Despite  google researchs I can not found the equivalent of diagnosetarget for gatk 4 and the depthofcoverage tool that i found is still in beta. I found this [github post](https://github.com/broadinstitute/gatk/pull/5913) which talk about pushing the diagnosetarget feature in depthofcoverage but I was not able to find any more informations. Should I stay on the gatk 3.8 version or upgrade our pipe to gatk 4?. I appreciate very much if someone could enlighten me.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270769'>Zendesk ticket #270769</a>)<br> gz#270769</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7702
https://github.com/broadinstitute/gatk/issues/7702:965,Deployability,upgrade,upgrade,965,"Here is an issue ticket for adding the diagnosetarget feature to DepthofCoverage. This request was created from a contribution made by Benoit Dewitte on February 09, 2022 13:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-). \--. Hi! ; ; I'm currentlly refactoring ours pipeline which use depthofcoverage and diagnosetarget from GATK 3.8. Despite  google researchs I can not found the equivalent of diagnosetarget for gatk 4 and the depthofcoverage tool that i found is still in beta. I found this [github post](https://github.com/broadinstitute/gatk/pull/5913) which talk about pushing the diagnosetarget feature in depthofcoverage but I was not able to find any more informations. Should I stay on the gatk 3.8 version or upgrade our pipe to gatk 4?. I appreciate very much if someone could enlighten me.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270769'>Zendesk ticket #270769</a>)<br> gz#270769</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7702
https://github.com/broadinstitute/gatk/issues/7702:494,Modifiability,refactor,refactoring,494,"Here is an issue ticket for adding the diagnosetarget feature to DepthofCoverage. This request was created from a contribution made by Benoit Dewitte on February 09, 2022 13:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-). \--. Hi! ; ; I'm currentlly refactoring ours pipeline which use depthofcoverage and diagnosetarget from GATK 3.8. Despite  google researchs I can not found the equivalent of diagnosetarget for gatk 4 and the depthofcoverage tool that i found is still in beta. I found this [github post](https://github.com/broadinstitute/gatk/pull/5913) which talk about pushing the diagnosetarget feature in depthofcoverage but I was not able to find any more informations. Should I stay on the gatk 3.8 version or upgrade our pipe to gatk 4?. I appreciate very much if someone could enlighten me.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270769'>Zendesk ticket #270769</a>)<br> gz#270769</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7702
https://github.com/broadinstitute/gatk/pull/7704:451,Deployability,pipeline,pipeline,451,"GvsCreateFilterSet.wdl failed recently for Morgan because of this bug. When run in a brand new project, filter model creation fails because we expect the project to have a hard coded dataset named ""temp_tables"" which is likely does not have. The workaround is simply to manually create one. This ticket removes the need for this dataset altogether. This is removed, and instead, the default dataset is used (that the many other tables created in this pipeline use as the default). able to reproduce with a dummy dataset name:; <img width=""1278"" alt=""Screen Shot 2022-03-03 at 10 44 39 PM"" src=""https://user-images.githubusercontent.com/6863459/156822409-a99d7068-169c-48a2-83ff-5bcc81cdbd2e.png"">. tested here:; https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_ingest/job_history/1dd27d90-82c4-44e6-8172-15c10c8a9c7f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7704
https://github.com/broadinstitute/gatk/pull/7704:698,Testability,test,tested,698,"GvsCreateFilterSet.wdl failed recently for Morgan because of this bug. When run in a brand new project, filter model creation fails because we expect the project to have a hard coded dataset named ""temp_tables"" which is likely does not have. The workaround is simply to manually create one. This ticket removes the need for this dataset altogether. This is removed, and instead, the default dataset is used (that the many other tables created in this pipeline use as the default). able to reproduce with a dummy dataset name:; <img width=""1278"" alt=""Screen Shot 2022-03-03 at 10 44 39 PM"" src=""https://user-images.githubusercontent.com/6863459/156822409-a99d7068-169c-48a2-83ff-5bcc81cdbd2e.png"">. tested here:; https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_ingest/job_history/1dd27d90-82c4-44e6-8172-15c10c8a9c7f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7704
https://github.com/broadinstitute/gatk/pull/7704:260,Usability,simpl,simply,260,"GvsCreateFilterSet.wdl failed recently for Morgan because of this bug. When run in a brand new project, filter model creation fails because we expect the project to have a hard coded dataset named ""temp_tables"" which is likely does not have. The workaround is simply to manually create one. This ticket removes the need for this dataset altogether. This is removed, and instead, the default dataset is used (that the many other tables created in this pipeline use as the default). able to reproduce with a dummy dataset name:; <img width=""1278"" alt=""Screen Shot 2022-03-03 at 10 44 39 PM"" src=""https://user-images.githubusercontent.com/6863459/156822409-a99d7068-169c-48a2-83ff-5bcc81cdbd2e.png"">. tested here:; https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_ingest/job_history/1dd27d90-82c4-44e6-8172-15c10c8a9c7f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7704
https://github.com/broadinstitute/gatk/pull/7705:727,Availability,down,down,727,"* Added MinGqVariantFilterBase; * * loads VCF, pedigree, UCSC genome tract, and truth data; * * calculates variant overlap with genome tracts; * * forms matrices, tensors, and other helping data for machine learning; * * provides for TRAIN and FILTER modes; * * provides functions for calculating loss given assigned min GQ values; * * computes best estimate of truth data used for training xgboost model; * Added XGBoostMinGqVariantFilter; * * calculates new GQ based on gradient boosting; * Added PropertiesTable for loading VCF properties into tensors; * Added TractOverlapDetector for computing overlap properties with; UCSC genome tracts. Training loss is based on weighted combination of heredity and truth; data, broken down by variant category.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7705
https://github.com/broadinstitute/gatk/pull/7705:36,Performance,load,loads,36,"* Added MinGqVariantFilterBase; * * loads VCF, pedigree, UCSC genome tract, and truth data; * * calculates variant overlap with genome tracts; * * forms matrices, tensors, and other helping data for machine learning; * * provides for TRAIN and FILTER modes; * * provides functions for calculating loss given assigned min GQ values; * * computes best estimate of truth data used for training xgboost model; * Added XGBoostMinGqVariantFilter; * * calculates new GQ based on gradient boosting; * Added PropertiesTable for loading VCF properties into tensors; * Added TractOverlapDetector for computing overlap properties with; UCSC genome tracts. Training loss is based on weighted combination of heredity and truth; data, broken down by variant category.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7705
https://github.com/broadinstitute/gatk/pull/7705:519,Performance,load,loading,519,"* Added MinGqVariantFilterBase; * * loads VCF, pedigree, UCSC genome tract, and truth data; * * calculates variant overlap with genome tracts; * * forms matrices, tensors, and other helping data for machine learning; * * provides for TRAIN and FILTER modes; * * provides functions for calculating loss given assigned min GQ values; * * computes best estimate of truth data used for training xgboost model; * Added XGBoostMinGqVariantFilter; * * calculates new GQ based on gradient boosting; * Added PropertiesTable for loading VCF properties into tensors; * Added TractOverlapDetector for computing overlap properties with; UCSC genome tracts. Training loss is based on weighted combination of heredity and truth; data, broken down by variant category.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7705
https://github.com/broadinstitute/gatk/pull/7705:207,Usability,learn,learning,207,"* Added MinGqVariantFilterBase; * * loads VCF, pedigree, UCSC genome tract, and truth data; * * calculates variant overlap with genome tracts; * * forms matrices, tensors, and other helping data for machine learning; * * provides for TRAIN and FILTER modes; * * provides functions for calculating loss given assigned min GQ values; * * computes best estimate of truth data used for training xgboost model; * Added XGBoostMinGqVariantFilter; * * calculates new GQ based on gradient boosting; * Added PropertiesTable for loading VCF properties into tensors; * Added TractOverlapDetector for computing overlap properties with; UCSC genome tracts. Training loss is based on weighted combination of heredity and truth; data, broken down by variant category.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7705
https://github.com/broadinstitute/gatk/issues/7706:680,Availability,error,error,680,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706
https://github.com/broadinstitute/gatk/issues/7706:237,Deployability,release,release,237,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706
https://github.com/broadinstitute/gatk/issues/7706:854,Security,validat,validateArg,854,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706
https://github.com/broadinstitute/gatk/issues/7706:962,Security,validat,validateArgum,962,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706
https://github.com/broadinstitute/gatk/issues/7706:1212,Testability,log,log,1212,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706
https://github.com/broadinstitute/gatk/issues/7706:1275,Testability,log,log,1275,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706
https://github.com/broadinstitute/gatk/issues/7706:1412,Testability,log,log,1412,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706
https://github.com/broadinstitute/gatk/issues/7706:1477,Testability,log,log,1477,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706
https://github.com/broadinstitute/gatk/issues/7707:3063,Availability,down,down,3063,"rsion: 2.24.1; 19:10:31.439 INFO CalculateContamination - Picard Version: 2.25.4; 19:10:31.439 INFO CalculateContamination - Built for Spark Version: 2.4.5; 19:10:31.439 INFO CalculateContamination - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:10:31.439 INFO CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:10:31.439 INFO CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:10:31.439 INFO CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:10:31.439 INFO CalculateContamination - Deflater: IntelDeflater; 19:10:31.439 INFO CalculateContamination - Inflater: IntelInflater; 19:10:31.439 INFO CalculateContamination - GCS max retries/reopens: 20; 19:10:31.439 INFO CalculateContamination - Requester pays: disabled; 19:10:31.439 INFO CalculateContamination - Initializing engine; 19:10:31.439 INFO CalculateContamination - Done initializing engine; 19:10:31.451 INFO CalculateContamination - Shutting down engine; [March 6, 2022 7:10:31 PM CST] org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2141192192; java.lang.IllegalArgumentException: there is no such column: contig; 	at org.broadinstitute.hellbender.utils.tsv.DataLine.columnIndex(DataLine.java:483); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:452); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:581); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:193); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:188); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.fetchNextRecord(TableReader.java:364); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.access$200(TableReader.java:99); 	at org.broadinstitute.hellbender.utils",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707
https://github.com/broadinstitute/gatk/issues/7707:4443,Energy Efficiency,Reduce,ReduceOps,4443,sv.DataLine.get(DataLine.java:452); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:581); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:193); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:188); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.fetchNextRecord(TableReader.java:364); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.access$200(TableReader.java:99); 	at org.broadinstitute.hellbender.utils.tsv.TableReader$1.hasNext(TableReader.java:472); 	at java.util.Iterator.forEachRemaining(Iterator.java:115); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.toList(TableReader.java:532); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary.readFromFile(PileupSummary.java:139); 	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707
https://github.com/broadinstitute/gatk/issues/7707:4453,Energy Efficiency,Reduce,ReduceOp,4453,sv.DataLine.get(DataLine.java:452); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:581); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:193); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:188); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.fetchNextRecord(TableReader.java:364); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.access$200(TableReader.java:99); 	at org.broadinstitute.hellbender.utils.tsv.TableReader$1.hasNext(TableReader.java:472); 	at java.util.Iterator.forEachRemaining(Iterator.java:115); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.toList(TableReader.java:532); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary.readFromFile(PileupSummary.java:139); 	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707
https://github.com/broadinstitute/gatk/issues/7707:4481,Energy Efficiency,Reduce,ReduceOps,4481,.java:452); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:581); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:193); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:188); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.fetchNextRecord(TableReader.java:364); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.access$200(TableReader.java:99); 	at org.broadinstitute.hellbender.utils.tsv.TableReader$1.hasNext(TableReader.java:472); 	at java.util.Iterator.forEachRemaining(Iterator.java:115); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.toList(TableReader.java:532); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary.readFromFile(PileupSummary.java:139); 	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Ma,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707
https://github.com/broadinstitute/gatk/issues/7707:4378,Integrability,wrap,wrapAndCopyInto,4378,aLine.java:483); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:452); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:581); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:193); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:188); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.fetchNextRecord(TableReader.java:364); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.access$200(TableReader.java:99); 	at org.broadinstitute.hellbender.utils.tsv.TableReader$1.hasNext(TableReader.java:472); 	at java.util.Iterator.forEachRemaining(Iterator.java:115); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.toList(TableReader.java:532); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary.readFromFile(PileupSummary.java:139); 	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707
https://github.com/broadinstitute/gatk/issues/7707:823,Performance,Load,Loading,823,"##The CalculateContamination Bug Report. Hello, I have a problem to ask you:. I running this command in the gatk4-4.2.3.0-0:; `gatk CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table`. the following information is displayed:. Using GATK jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table; 19:10:31.163 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 7:10:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:10:31.437 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.437 INFO CalculateContamination - The Genome Analysis Toolkit (GATK) v4.2.3.0; 19:10:31.437 INFO CalculateContamination - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:10:31.438 INFO CalculateContamination - Executing as haojie@node1 on Linux v3.10.0-957.el7.x86_64 amd64; 19:10:31.438 INFO CalculateContamination - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 19:10:31.438 INFO CalculateContamination - Start Date/Time: March 6, 2022 7:10:31 PM CST; 19:10:31.438 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.438 INFO CalculateContamination - --------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707
https://github.com/broadinstitute/gatk/issues/7707:1135,Safety,detect,detect,1135,"k CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table`. the following information is displayed:. Using GATK jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table; 19:10:31.163 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 7:10:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:10:31.437 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.437 INFO CalculateContamination - The Genome Analysis Toolkit (GATK) v4.2.3.0; 19:10:31.437 INFO CalculateContamination - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:10:31.438 INFO CalculateContamination - Executing as haojie@node1 on Linux v3.10.0-957.el7.x86_64 amd64; 19:10:31.438 INFO CalculateContamination - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 19:10:31.438 INFO CalculateContamination - Start Date/Time: March 6, 2022 7:10:31 PM CST; 19:10:31.438 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.438 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.439 INFO CalculateContamination - HTSJDK Version: 2.24.1; 19:10:31.439 INFO CalculateContamination - Picard ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707
https://github.com/broadinstitute/gatk/issues/7707:3994,Security,access,access,3994,"gine; 19:10:31.451 INFO CalculateContamination - Shutting down engine; [March 6, 2022 7:10:31 PM CST] org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2141192192; java.lang.IllegalArgumentException: there is no such column: contig; 	at org.broadinstitute.hellbender.utils.tsv.DataLine.columnIndex(DataLine.java:483); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:452); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:581); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:193); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:188); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.fetchNextRecord(TableReader.java:364); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.access$200(TableReader.java:99); 	at org.broadinstitute.hellbender.utils.tsv.TableReader$1.hasNext(TableReader.java:472); 	at java.util.Iterator.forEachRemaining(Iterator.java:115); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.toList(TableReader.java:532); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary.readFromFile(PileupSummary.java:139); 	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:116); 	at org.broadinstitute.hell",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707
https://github.com/broadinstitute/gatk/issues/7708:112,Availability,error,error,112,"Hello, ; I want to merge multi HaplotypeCaller GVCF files into a single GVCF with **CombineGVCFs**, but I got a error warnings, **Key END found in VariantContext field INFO at Gbar_A01:24359 but this key isn't defined in the VCFHeader. We require all VCFs to have complete VCF headers by default.**; I ran the gatk with 4.1.6 version, here is my command.; `java -Xmx200g -jar /public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar CombineGVCFs -O Gb.gatk.vcf -R /data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/Ref/Gbarbadense_3-79_HAU_v2.fasta -V Y2003/Y2003.gatk.vcf -V Y2010/Y2010.gatk.vcf -V Y2013/Y2013.gatk.vcf `,. I recheck my gvcf files and find the variant in the Y2010.gatk.vcf, which is a delete variant ; **Gbar_A01 24359 . GA G 847.03 . AC=2;AF=1.00;AN=2;DP=25;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=46.41;QD=27.24;SOR=0.963 GT:AD:DP:GQ:PL 1/1:0,23:23:69:861,69,0** . I have no ideal to solve it, did anyone eocounter the same error warnings,. Best wishes,; Qingying. And here is my log . 21:09:58.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 9:09:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:09:59.072 INFO CombineGVCFs - ------------------------------------------------------------; 21:09:59.073 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.6.0; 21:09:59.073 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:09:59.073 INFO CombineGVCFs - Executing as qymeng@s004 on Linux v3.10.0-862.el7.x86_64 amd64; 21:09:59.073 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_92-b15; 21:09:59.073 INFO CombineGVCFs - Start Date/Time: March 6, 2022 9:09:58 PM CST; 21:09:59.073 INFO CombineGVCFs - ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7708
https://github.com/broadinstitute/gatk/issues/7708:967,Availability,error,error,967,"Hello, ; I want to merge multi HaplotypeCaller GVCF files into a single GVCF with **CombineGVCFs**, but I got a error warnings, **Key END found in VariantContext field INFO at Gbar_A01:24359 but this key isn't defined in the VCFHeader. We require all VCFs to have complete VCF headers by default.**; I ran the gatk with 4.1.6 version, here is my command.; `java -Xmx200g -jar /public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar CombineGVCFs -O Gb.gatk.vcf -R /data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/Ref/Gbarbadense_3-79_HAU_v2.fasta -V Y2003/Y2003.gatk.vcf -V Y2010/Y2010.gatk.vcf -V Y2013/Y2013.gatk.vcf `,. I recheck my gvcf files and find the variant in the Y2010.gatk.vcf, which is a delete variant ; **Gbar_A01 24359 . GA G 847.03 . AC=2;AF=1.00;AN=2;DP=25;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=46.41;QD=27.24;SOR=0.963 GT:AD:DP:GQ:PL 1/1:0,23:23:69:861,69,0** . I have no ideal to solve it, did anyone eocounter the same error warnings,. Best wishes,; Qingying. And here is my log . 21:09:58.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 9:09:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:09:59.072 INFO CombineGVCFs - ------------------------------------------------------------; 21:09:59.073 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.6.0; 21:09:59.073 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:09:59.073 INFO CombineGVCFs - Executing as qymeng@s004 on Linux v3.10.0-862.el7.x86_64 amd64; 21:09:59.073 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_92-b15; 21:09:59.073 INFO CombineGVCFs - Start Date/Time: March 6, 2022 9:09:58 PM CST; 21:09:59.073 INFO CombineGVCFs - ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7708
https://github.com/broadinstitute/gatk/issues/7708:3637,Availability,down,down,3637,"later: IntelDeflater; 21:09:59.074 INFO CombineGVCFs - Inflater: IntelInflater; 21:09:59.074 INFO CombineGVCFs - GCS max retries/reopens: 20; 21:09:59.074 INFO CombineGVCFs - Requester pays: disabled; 21:09:59.074 INFO CombineGVCFs - Initializing engine; 21:09:59.773 INFO FeatureManager - Using codec VCFCodec to read file file:///data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/02_Haplotype/Y2003/Y2003.gatk.vcf; 21:09:59.982 INFO FeatureManager - Using codec VCFCodec to read file file:///data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/02_Haplotype/Y2010/Y2010.gatk.vcf; 21:10:00.200 INFO FeatureManager - Using codec VCFCodec to read file file:///data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/02_Haplotype/Y2013/Y2013.gatk.vcf; 21:10:41.729 INFO CombineGVCFs - Done initializing engine; 21:10:41.789 INFO ProgressMeter - Starting traversal; 21:10:41.790 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:10:42.082 INFO CombineGVCFs - Shutting down engine; [March 6, 2022 9:10:42 PM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.73 minutes.; Runtime.totalMemory()=4515692544; java.lang.IllegalStateException: Key END found in VariantContext field INFO at Gbar_A01:24359 but this key isn't defined in the VCFHeader. We require all VCFs to have complete VCF headers by default.; at htsjdk.variant.vcf.VCFEncoder.fieldIsMissingFromHeaderError(VCFEncoder.java:213); at htsjdk.variant.vcf.VCFEncoder.write(VCFEncoder.java:146); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:250); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:408); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:217); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:162); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7708
https://github.com/broadinstitute/gatk/issues/7708:5453,Integrability,wrap,wrapAndCopyInto,5453,ender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:162); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:131); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:106); at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:120); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:118); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:163); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Mai,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7708
https://github.com/broadinstitute/gatk/issues/7708:1069,Performance,Load,Loading,1069,"ngle GVCF with **CombineGVCFs**, but I got a error warnings, **Key END found in VariantContext field INFO at Gbar_A01:24359 but this key isn't defined in the VCFHeader. We require all VCFs to have complete VCF headers by default.**; I ran the gatk with 4.1.6 version, here is my command.; `java -Xmx200g -jar /public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar CombineGVCFs -O Gb.gatk.vcf -R /data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/Ref/Gbarbadense_3-79_HAU_v2.fasta -V Y2003/Y2003.gatk.vcf -V Y2010/Y2010.gatk.vcf -V Y2013/Y2013.gatk.vcf `,. I recheck my gvcf files and find the variant in the Y2010.gatk.vcf, which is a delete variant ; **Gbar_A01 24359 . GA G 847.03 . AC=2;AF=1.00;AN=2;DP=25;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=46.41;QD=27.24;SOR=0.963 GT:AD:DP:GQ:PL 1/1:0,23:23:69:861,69,0** . I have no ideal to solve it, did anyone eocounter the same error warnings,. Best wishes,; Qingying. And here is my log . 21:09:58.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 9:09:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:09:59.072 INFO CombineGVCFs - ------------------------------------------------------------; 21:09:59.073 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.6.0; 21:09:59.073 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:09:59.073 INFO CombineGVCFs - Executing as qymeng@s004 on Linux v3.10.0-862.el7.x86_64 amd64; 21:09:59.073 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_92-b15; 21:09:59.073 INFO CombineGVCFs - Start Date/Time: March 6, 2022 9:09:58 PM CST; 21:09:59.073 INFO CombineGVCFs - ------------------------------------------------------------; 21:09:5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7708
https://github.com/broadinstitute/gatk/issues/7708:1359,Safety,detect,detect,1359,"; `java -Xmx200g -jar /public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar CombineGVCFs -O Gb.gatk.vcf -R /data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/Ref/Gbarbadense_3-79_HAU_v2.fasta -V Y2003/Y2003.gatk.vcf -V Y2010/Y2010.gatk.vcf -V Y2013/Y2013.gatk.vcf `,. I recheck my gvcf files and find the variant in the Y2010.gatk.vcf, which is a delete variant ; **Gbar_A01 24359 . GA G 847.03 . AC=2;AF=1.00;AN=2;DP=25;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=46.41;QD=27.24;SOR=0.963 GT:AD:DP:GQ:PL 1/1:0,23:23:69:861,69,0** . I have no ideal to solve it, did anyone eocounter the same error warnings,. Best wishes,; Qingying. And here is my log . 21:09:58.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 9:09:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:09:59.072 INFO CombineGVCFs - ------------------------------------------------------------; 21:09:59.073 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.6.0; 21:09:59.073 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:09:59.073 INFO CombineGVCFs - Executing as qymeng@s004 on Linux v3.10.0-862.el7.x86_64 amd64; 21:09:59.073 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_92-b15; 21:09:59.073 INFO CombineGVCFs - Start Date/Time: March 6, 2022 9:09:58 PM CST; 21:09:59.073 INFO CombineGVCFs - ------------------------------------------------------------; 21:09:59.073 INFO CombineGVCFs - ------------------------------------------------------------; 21:09:59.074 INFO CombineGVCFs - HTSJDK Version: 2.21.2; 21:09:59.074 INFO CombineGVCFs - Picard Version: 2.21.9; 21:09:59.074 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:09:59.074 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7708
https://github.com/broadinstitute/gatk/issues/7708:1023,Testability,log,log,1023,"o merge multi HaplotypeCaller GVCF files into a single GVCF with **CombineGVCFs**, but I got a error warnings, **Key END found in VariantContext field INFO at Gbar_A01:24359 but this key isn't defined in the VCFHeader. We require all VCFs to have complete VCF headers by default.**; I ran the gatk with 4.1.6 version, here is my command.; `java -Xmx200g -jar /public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar CombineGVCFs -O Gb.gatk.vcf -R /data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/Ref/Gbarbadense_3-79_HAU_v2.fasta -V Y2003/Y2003.gatk.vcf -V Y2010/Y2010.gatk.vcf -V Y2013/Y2013.gatk.vcf `,. I recheck my gvcf files and find the variant in the Y2010.gatk.vcf, which is a delete variant ; **Gbar_A01 24359 . GA G 847.03 . AC=2;AF=1.00;AN=2;DP=25;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=46.41;QD=27.24;SOR=0.963 GT:AD:DP:GQ:PL 1/1:0,23:23:69:861,69,0** . I have no ideal to solve it, did anyone eocounter the same error warnings,. Best wishes,; Qingying. And here is my log . 21:09:58.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/qymeng/biosoft/gatk-4.1.6.0/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 9:09:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:09:59.072 INFO CombineGVCFs - ------------------------------------------------------------; 21:09:59.073 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.6.0; 21:09:59.073 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:09:59.073 INFO CombineGVCFs - Executing as qymeng@s004 on Linux v3.10.0-862.el7.x86_64 amd64; 21:09:59.073 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_92-b15; 21:09:59.073 INFO CombineGVCFs - Start Date/Time: March 6, 2022 9:09:58 PM CST; 21:09:59.073 INFO CombineGVCFs - --------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7708
https://github.com/broadinstitute/gatk/pull/7709:8,Deployability,patch,patch,8,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1120,Deployability,update,updated,1120,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1200,Deployability,Integrat,IntegrationTestSpec,1200,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1373,Deployability,integrat,integration,1373,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1200,Integrability,Integrat,IntegrationTestSpec,1200,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1373,Integrability,integrat,integration,1373,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1302,Safety,sanity check,sanity check,1302,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1043,Security,validat,validation,1043,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1144,Testability,test,tests,1144,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1220,Testability,assert,assertEqualTextFiles,1220,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1385,Testability,test,tests,1385,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1436,Testability,test,tests,1436,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:807,Usability,learn,learn,807,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:820,Usability,learn,learn,820,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/pull/7709:1185,Usability,simpl,simply,1185,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709
https://github.com/broadinstitute/gatk/issues/7710:2825,Availability,error,error,2825,"ndLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). This request was created from a contribution made by Domniki Manousi on March 07, 2022 12:01 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4556136866843-FindBreakpointEvidenceSpark-sudden-shutdown](https://gatk.broadinstitute.org/hc/en-us/community/posts/4556136866843-FindBreakpointEvidenceSpark-sudden-shutdown). \--. Hi, I am trying to run the tool FindBreakpointEvidenceSpark. I have successfully  produced the required kmers and the tool seems to run for several minutes until it finally stops without producing output. I have read in past issues that memory usage might be a problem and have tried to accomodate for it using the -Xmx option. . a) GATK version used: gatk4: 4.2.0.0 through singularity (/cvmfs/singularity.galaxyproject.org/all/gatk4:4.2.0.0--0) . b) Exact command used: . singularity exec /cvmfs/singularity.galaxyproject.org/all/gatk4:4.2.0.0--0 gatk --java-options ""-Xmx75g -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true"" FindBreakpointEvidenceSpark \\ ; ;    -R /mnt/SCRATCH/domniman/references/ssa\_selected/Simon\_Final2021\_Ssa\_selected.fa -I /mnt/SCRATCH/domniman/2014G\_NO\_Males\_1169\_D03\_RG.bam \\ ; ;    --aligner-index-image /mnt/SCRATCH/domniman/references/ssa\_selected/Simon\_Final2021\_Ssa\_selected.fa.img \\ ; ;    --kmers-to-ignore /mnt/users/domniman/ag\_fish/kmers\_to\_ignore.txt -O /mnt/SCRATCH/domniman/assembly.sam \\ ; ;    --tmp-dir /mnt/SCRATCH/domniman/tmp -L ssa03. Entire error log: ; ; Due to length of the complete log (37.671 lines) I attach it as a separate link: [https://www.dropbox.com/s/n7q5dco4z5t3moz/gatk%20error%20log%20.txt?dl=0](https://www.dropbox.com/s/n7q5dco4z5t3moz/gatk%20error%20log%20.txt?dl=0). Best,. Domniki<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/275546'>Zendesk ticket #275546</a>)<br> gz#275546</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7710
https://github.com/broadinstitute/gatk/issues/7710:2831,Testability,log,log,2831,"ndLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). This request was created from a contribution made by Domniki Manousi on March 07, 2022 12:01 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4556136866843-FindBreakpointEvidenceSpark-sudden-shutdown](https://gatk.broadinstitute.org/hc/en-us/community/posts/4556136866843-FindBreakpointEvidenceSpark-sudden-shutdown). \--. Hi, I am trying to run the tool FindBreakpointEvidenceSpark. I have successfully  produced the required kmers and the tool seems to run for several minutes until it finally stops without producing output. I have read in past issues that memory usage might be a problem and have tried to accomodate for it using the -Xmx option. . a) GATK version used: gatk4: 4.2.0.0 through singularity (/cvmfs/singularity.galaxyproject.org/all/gatk4:4.2.0.0--0) . b) Exact command used: . singularity exec /cvmfs/singularity.galaxyproject.org/all/gatk4:4.2.0.0--0 gatk --java-options ""-Xmx75g -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true"" FindBreakpointEvidenceSpark \\ ; ;    -R /mnt/SCRATCH/domniman/references/ssa\_selected/Simon\_Final2021\_Ssa\_selected.fa -I /mnt/SCRATCH/domniman/2014G\_NO\_Males\_1169\_D03\_RG.bam \\ ; ;    --aligner-index-image /mnt/SCRATCH/domniman/references/ssa\_selected/Simon\_Final2021\_Ssa\_selected.fa.img \\ ; ;    --kmers-to-ignore /mnt/users/domniman/ag\_fish/kmers\_to\_ignore.txt -O /mnt/SCRATCH/domniman/assembly.sam \\ ; ;    --tmp-dir /mnt/SCRATCH/domniman/tmp -L ssa03. Entire error log: ; ; Due to length of the complete log (37.671 lines) I attach it as a separate link: [https://www.dropbox.com/s/n7q5dco4z5t3moz/gatk%20error%20log%20.txt?dl=0](https://www.dropbox.com/s/n7q5dco4z5t3moz/gatk%20error%20log%20.txt?dl=0). Best,. Domniki<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/275546'>Zendesk ticket #275546</a>)<br> gz#275546</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7710
https://github.com/broadinstitute/gatk/issues/7710:2870,Testability,log,log,2870,"ndLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). This request was created from a contribution made by Domniki Manousi on March 07, 2022 12:01 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4556136866843-FindBreakpointEvidenceSpark-sudden-shutdown](https://gatk.broadinstitute.org/hc/en-us/community/posts/4556136866843-FindBreakpointEvidenceSpark-sudden-shutdown). \--. Hi, I am trying to run the tool FindBreakpointEvidenceSpark. I have successfully  produced the required kmers and the tool seems to run for several minutes until it finally stops without producing output. I have read in past issues that memory usage might be a problem and have tried to accomodate for it using the -Xmx option. . a) GATK version used: gatk4: 4.2.0.0 through singularity (/cvmfs/singularity.galaxyproject.org/all/gatk4:4.2.0.0--0) . b) Exact command used: . singularity exec /cvmfs/singularity.galaxyproject.org/all/gatk4:4.2.0.0--0 gatk --java-options ""-Xmx75g -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true"" FindBreakpointEvidenceSpark \\ ; ;    -R /mnt/SCRATCH/domniman/references/ssa\_selected/Simon\_Final2021\_Ssa\_selected.fa -I /mnt/SCRATCH/domniman/2014G\_NO\_Males\_1169\_D03\_RG.bam \\ ; ;    --aligner-index-image /mnt/SCRATCH/domniman/references/ssa\_selected/Simon\_Final2021\_Ssa\_selected.fa.img \\ ; ;    --kmers-to-ignore /mnt/users/domniman/ag\_fish/kmers\_to\_ignore.txt -O /mnt/SCRATCH/domniman/assembly.sam \\ ; ;    --tmp-dir /mnt/SCRATCH/domniman/tmp -L ssa03. Entire error log: ; ; Due to length of the complete log (37.671 lines) I attach it as a separate link: [https://www.dropbox.com/s/n7q5dco4z5t3moz/gatk%20error%20log%20.txt?dl=0](https://www.dropbox.com/s/n7q5dco4z5t3moz/gatk%20error%20log%20.txt?dl=0). Best,. Domniki<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/275546'>Zendesk ticket #275546</a>)<br> gz#275546</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7710
https://github.com/broadinstitute/gatk/issues/7712:202,Availability,ERROR,ERROR,202,"I calculated ASECount of genome resequence data by GATK-3.8 , but I want do the same test by GATK-4.0 , It's so strange when I use GATK-4.0 argument ""--variants"" to substitute ""sites"" of GATK-3.8 , the ERROR remaind me that the ""SNP site is not hetero"" , so l want to ask ; What is the mean of ASECountReader ""sites"" argument of GATK-3.8 ? and what is the corresponding argument in GATK-4.0 ? the answer is undocumented in instruction of ""GATK-3.8 --help"", so l want get exact answer, thank you !",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7712
https://github.com/broadinstitute/gatk/issues/7712:85,Testability,test,test,85,"I calculated ASECount of genome resequence data by GATK-3.8 , but I want do the same test by GATK-4.0 , It's so strange when I use GATK-4.0 argument ""--variants"" to substitute ""sites"" of GATK-3.8 , the ERROR remaind me that the ""SNP site is not hetero"" , so l want to ask ; What is the mean of ASECountReader ""sites"" argument of GATK-3.8 ? and what is the corresponding argument in GATK-4.0 ? the answer is undocumented in instruction of ""GATK-3.8 --help"", so l want get exact answer, thank you !",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7712
https://github.com/broadinstitute/gatk/issues/7712:407,Usability,undo,undocumented,407,"I calculated ASECount of genome resequence data by GATK-3.8 , but I want do the same test by GATK-4.0 , It's so strange when I use GATK-4.0 argument ""--variants"" to substitute ""sites"" of GATK-3.8 , the ERROR remaind me that the ""SNP site is not hetero"" , so l want to ask ; What is the mean of ASECountReader ""sites"" argument of GATK-3.8 ? and what is the corresponding argument in GATK-4.0 ? the answer is undocumented in instruction of ""GATK-3.8 --help"", so l want get exact answer, thank you !",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7712
https://github.com/broadinstitute/gatk/pull/7714:27,Security,validat,validation,27,"-Adds start/end coordinate validation to `SVCallRecord`, checking contigs and positions against the sequence dictionary and their ordering.; -Adds some checks for invalid coordinates in places where `SimpleInterval.expandWithinContig()` can potentially return `null`.; -Addresses an issue where inter-chromosomal records' end positions were incorrectly disallowed from preceding the start position during record collapsing (despite being on a different contig).; -Deletes unused `SVCallRecordWithEvidence`. Includes regression tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7714
https://github.com/broadinstitute/gatk/pull/7714:527,Testability,test,tests,527,"-Adds start/end coordinate validation to `SVCallRecord`, checking contigs and positions against the sequence dictionary and their ordering.; -Adds some checks for invalid coordinates in places where `SimpleInterval.expandWithinContig()` can potentially return `null`.; -Addresses an issue where inter-chromosomal records' end positions were incorrectly disallowed from preceding the start position during record collapsing (despite being on a different contig).; -Deletes unused `SVCallRecordWithEvidence`. Includes regression tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7714
https://github.com/broadinstitute/gatk/pull/7714:200,Usability,Simpl,SimpleInterval,200,"-Adds start/end coordinate validation to `SVCallRecord`, checking contigs and positions against the sequence dictionary and their ordering.; -Adds some checks for invalid coordinates in places where `SimpleInterval.expandWithinContig()` can potentially return `null`.; -Addresses an issue where inter-chromosomal records' end positions were incorrectly disallowed from preceding the start position during record collapsing (despite being on a different contig).; -Deletes unused `SVCallRecordWithEvidence`. Includes regression tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7714
https://github.com/broadinstitute/gatk/issues/7716:437,Availability,error,error,437,"As reported by @jkobject testing our latest gatk-nightly image, certain non-requester-pays accesses fail with the latest google-cloud-nio version (0.123.23) when `--gcs-project-for-requester-pays` is specified. . The specific issue appears to be checks for the existence of non-existent files in non-requester-pays buckets when `--gcs-project-for-requester-pays` is set, resulting in a ""User project specified in the request is invalid"" error:. ```; code: 400; message: User project specified in the request is invalid.; reason: invalid; location: null; retryable: false; com.google.cloud.storage.StorageException: User project specified in the request is invalid.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:233); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.list(HttpStorageRpc.java:376); 	at com.google.cloud.storage.StorageImpl.lambda$listBlobs$11(StorageImpl.java:391); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.Retrying.run(Retrying.java:51); 	at com.google.cloud.storage.StorageImpl.listBlobs(StorageImpl.java:388); 	at com.google.cloud.storage.StorageImpl.list(StorageImpl.java:359); 	at com.google.cloud.storage.contrib.nio.CloudStoragePath.seemsLikeADirectoryAndUsePseudoDirectories(CloudStoragePath.java:118); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:743); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:418); 	at htsjdk.tribble.TribbleIndexedFeatureReader.loadIndex(TribbleIndexedFeatureReader.java:162); 	at htsjdk.tribble.TribbleIndexedFeatureReader.hasIndex(TribbleIndexedFeatureReader.java:228); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSourc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:4416,Availability,error,errors,4416,"ommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; GET https://storage.googleapis.com/storage/v1/b/fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/o?maxResults=1&prefix=5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list.idx/&projection=full&userProject; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""User project specified in the request is invalid."",; ""reason"" : ""invalid""; } ],; ""message"" : ""User project specified in the request is invalid.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:5912,Availability,error,error,5912,"Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; GET https://storage.googleapis.com/storage/v1/b/fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/o?maxResults=1&prefix=5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list.idx/&projection=full&userProject; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""User project specified in the request is invalid."",; ""reason"" : ""invalid""; } ],; ""message"" : ""User project specified in the request is invalid.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.list(HttpStorageRpc.java:366); 	... 33 more; ```. This can be triggered by providing an `interval_list` file in a non-RP bucket to the `-L` argument, since GATK will check for the existence of an index on the `interval_list` file and fail with the above error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:461,Integrability,message,message,461,"As reported by @jkobject testing our latest gatk-nightly image, certain non-requester-pays accesses fail with the latest google-cloud-nio version (0.123.23) when `--gcs-project-for-requester-pays` is specified. . The specific issue appears to be checks for the existence of non-existent files in non-requester-pays buckets when `--gcs-project-for-requester-pays` is set, resulting in a ""User project specified in the request is invalid"" error:. ```; code: 400; message: User project specified in the request is invalid.; reason: invalid; location: null; retryable: false; com.google.cloud.storage.StorageException: User project specified in the request is invalid.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:233); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.list(HttpStorageRpc.java:376); 	at com.google.cloud.storage.StorageImpl.lambda$listBlobs$11(StorageImpl.java:391); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.Retrying.run(Retrying.java:51); 	at com.google.cloud.storage.StorageImpl.listBlobs(StorageImpl.java:388); 	at com.google.cloud.storage.StorageImpl.list(StorageImpl.java:359); 	at com.google.cloud.storage.contrib.nio.CloudStoragePath.seemsLikeADirectoryAndUsePseudoDirectories(CloudStoragePath.java:118); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:743); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:418); 	at htsjdk.tribble.TribbleIndexedFeatureReader.loadIndex(TribbleIndexedFeatureReader.java:162); 	at htsjdk.tribble.TribbleIndexedFeatureReader.hasIndex(TribbleIndexedFeatureReader.java:228); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSourc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:4454,Integrability,message,message,4454,"ommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; GET https://storage.googleapis.com/storage/v1/b/fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/o?maxResults=1&prefix=5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list.idx/&projection=full&userProject; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""User project specified in the request is invalid."",; ""reason"" : ""invalid""; } ],; ""message"" : ""User project specified in the request is invalid.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:4548,Integrability,message,message,4548,"ceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; GET https://storage.googleapis.com/storage/v1/b/fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/o?maxResults=1&prefix=5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list.idx/&projection=full&userProject; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""User project specified in the request is invalid."",; ""reason"" : ""invalid""; } ],; ""message"" : ""User project specified in the request is invalid.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:1775,Performance,load,loadIndex,1775,torageRpc.list(HttpStorageRpc.java:376); 	at com.google.cloud.storage.StorageImpl.lambda$listBlobs$11(StorageImpl.java:391); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.Retrying.run(Retrying.java:51); 	at com.google.cloud.storage.StorageImpl.listBlobs(StorageImpl.java:388); 	at com.google.cloud.storage.StorageImpl.list(StorageImpl.java:359); 	at com.google.cloud.storage.contrib.nio.CloudStoragePath.seemsLikeADirectoryAndUsePseudoDirectories(CloudStoragePath.java:118); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:743); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:418); 	at htsjdk.tribble.TribbleIndexedFeatureReader.loadIndex(TribbleIndexedFeatureReader.java:162); 	at htsjdk.tribble.TribbleIndexedFeatureReader.hasIndex(TribbleIndexedFeatureReader.java:228); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:331); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:236); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:204); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:191); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:154); 	at org.broadinstitute.hellbender.utils.IntervalUtils.featureFileToIntervals(IntervalUtils.java:356); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:319); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:239); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:2652,Performance,load,loadIntervals,2652,bble.util.ParsingUtils.resourceExists(ParsingUtils.java:418); 	at htsjdk.tribble.TribbleIndexedFeatureReader.loadIndex(TribbleIndexedFeatureReader.java:162); 	at htsjdk.tribble.TribbleIndexedFeatureReader.hasIndex(TribbleIndexedFeatureReader.java:228); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:331); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:236); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:204); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:191); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:154); 	at org.broadinstitute.hellbender.utils.IntervalUtils.featureFileToIntervals(IntervalUtils.java:356); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:319); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:239); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:200); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:180); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeIntervals(GATKTool.java:525); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:728); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:91,Security,access,accesses,91,"As reported by @jkobject testing our latest gatk-nightly image, certain non-requester-pays accesses fail with the latest google-cloud-nio version (0.123.23) when `--gcs-project-for-requester-pays` is specified. . The specific issue appears to be checks for the existence of non-existent files in non-requester-pays buckets when `--gcs-project-for-requester-pays` is set, resulting in a ""User project specified in the request is invalid"" error:. ```; code: 400; message: User project specified in the request is invalid.; reason: invalid; location: null; retryable: false; com.google.cloud.storage.StorageException: User project specified in the request is invalid.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:233); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.list(HttpStorageRpc.java:376); 	at com.google.cloud.storage.StorageImpl.lambda$listBlobs$11(StorageImpl.java:391); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.Retrying.run(Retrying.java:51); 	at com.google.cloud.storage.StorageImpl.listBlobs(StorageImpl.java:388); 	at com.google.cloud.storage.StorageImpl.list(StorageImpl.java:359); 	at com.google.cloud.storage.contrib.nio.CloudStoragePath.seemsLikeADirectoryAndUsePseudoDirectories(CloudStoragePath.java:118); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:743); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:418); 	at htsjdk.tribble.TribbleIndexedFeatureReader.loadIndex(TribbleIndexedFeatureReader.java:162); 	at htsjdk.tribble.TribbleIndexedFeatureReader.hasIndex(TribbleIndexedFeatureReader.java:228); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSourc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:4064,Security,secur,secure-,4064,"KTool.java:525); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:728); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; GET https://storage.googleapis.com/storage/v1/b/fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/o?maxResults=1&prefix=5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list.idx/&projection=full&userProject; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""User project specified in the request is invalid."",; ""reason"" : ""invalid""; } ],; ""message"" : ""User project specified in the request is invalid.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428); 	at com.google.api.client.http.HttpRequest.e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/issues/7716:25,Testability,test,testing,25,"As reported by @jkobject testing our latest gatk-nightly image, certain non-requester-pays accesses fail with the latest google-cloud-nio version (0.123.23) when `--gcs-project-for-requester-pays` is specified. . The specific issue appears to be checks for the existence of non-existent files in non-requester-pays buckets when `--gcs-project-for-requester-pays` is set, resulting in a ""User project specified in the request is invalid"" error:. ```; code: 400; message: User project specified in the request is invalid.; reason: invalid; location: null; retryable: false; com.google.cloud.storage.StorageException: User project specified in the request is invalid.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:233); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.list(HttpStorageRpc.java:376); 	at com.google.cloud.storage.StorageImpl.lambda$listBlobs$11(StorageImpl.java:391); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.Retrying.run(Retrying.java:51); 	at com.google.cloud.storage.StorageImpl.listBlobs(StorageImpl.java:388); 	at com.google.cloud.storage.StorageImpl.list(StorageImpl.java:359); 	at com.google.cloud.storage.contrib.nio.CloudStoragePath.seemsLikeADirectoryAndUsePseudoDirectories(CloudStoragePath.java:118); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:743); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:418); 	at htsjdk.tribble.TribbleIndexedFeatureReader.loadIndex(TribbleIndexedFeatureReader.java:162); 	at htsjdk.tribble.TribbleIndexedFeatureReader.hasIndex(TribbleIndexedFeatureReader.java:228); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSourc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716
https://github.com/broadinstitute/gatk/pull/7718:345,Availability,avail,available,345,"Summary of VQSR Changes; 	- ONLY populate AS_RAW_MQRankSum or AS_RAW_ReadPosRankSum for ref-alt genotypes (0/1, 0/2) not 1/1/,1/2,2,2; 	- AS_RAW_MQ for Non AS... Assign full MQ to alternate allele (don't distribute); 	- Compute SUM(AD) for future use; 	- provide command line option to force the AS-Approximate path even when AS annotations are available (for benchmarking/comparisons)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7718
https://github.com/broadinstitute/gatk/pull/7718:360,Testability,benchmark,benchmarking,360,"Summary of VQSR Changes; 	- ONLY populate AS_RAW_MQRankSum or AS_RAW_ReadPosRankSum for ref-alt genotypes (0/1, 0/2) not 1/1/,1/2,2,2; 	- AS_RAW_MQ for Non AS... Assign full MQ to alternate allele (don't distribute); 	- Compute SUM(AD) for future use; 	- provide command line option to force the AS-Approximate path even when AS annotations are available (for benchmarking/comparisons)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7718
https://github.com/broadinstitute/gatk/issues/7721:603,Availability,redundant,redundant,603,"Cromwell still struggles with call caching and metadata bloat in our gCNV workflows. Specific improvements to reduce overhead will modify scattered tasks:. 1. `GermlineCNVCallerCohort(Case)Mode` - Replace input `Array[File] read_count_files` with a list of files, i.e. `File read_count_file_paths`. This should be generated using `WritePathList`, rather than using `write_lines()` which does not function in WDL workflow blocks on some Cromwell servers. Replace output `Array[File] gcnv_call_tars` with a single tarball `File gcnv_call_tar` containing all of the calls. It appears there are a number of redundant outputs - kernel version, denoising configs, output file lists, etc. that were added with the [joint calling pipeline](https://github.com/broadinstitute/gatk/commit/31df35bb9204b5551cc1a3ee7468e2b0e577215d). We should rework that to extract/generate those in joint calling workflow when needed and eliminate these outputs.; 2. Add a transpose task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_call_tar` output, and generates a sample-sharded `Array[File] gcnv_calls_by_sample` output of tarballs.; 2. Add a model bundling task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_model_tar` output, extracts the files, and tarballs all of them together to produce a single tarball output. Make this the output of the cohort workflow and input to the case mode workflows, rather than an array of model tars (retain the current `Array[File]` input as an optional type `Array[File]?` that will be used as the default if provided to case mode in order to support users still working with the old paradigm).; 3. `PostprocessGermlineCNVCalls` - replace input `Array[File] gcnv_calls_tars` with `File gcnv_sample_calls`, the sample-sharded output from the aforementioned transpose task. Delete inputs `calling_configs`, `denoising_configs`, `gcnvkernel_version`, `sharded_interval_lists`, as the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7721
https://github.com/broadinstitute/gatk/issues/7721:722,Deployability,pipeline,pipeline,722,"Cromwell still struggles with call caching and metadata bloat in our gCNV workflows. Specific improvements to reduce overhead will modify scattered tasks:. 1. `GermlineCNVCallerCohort(Case)Mode` - Replace input `Array[File] read_count_files` with a list of files, i.e. `File read_count_file_paths`. This should be generated using `WritePathList`, rather than using `write_lines()` which does not function in WDL workflow blocks on some Cromwell servers. Replace output `Array[File] gcnv_call_tars` with a single tarball `File gcnv_call_tar` containing all of the calls. It appears there are a number of redundant outputs - kernel version, denoising configs, output file lists, etc. that were added with the [joint calling pipeline](https://github.com/broadinstitute/gatk/commit/31df35bb9204b5551cc1a3ee7468e2b0e577215d). We should rework that to extract/generate those in joint calling workflow when needed and eliminate these outputs.; 2. Add a transpose task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_call_tar` output, and generates a sample-sharded `Array[File] gcnv_calls_by_sample` output of tarballs.; 2. Add a model bundling task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_model_tar` output, extracts the files, and tarballs all of them together to produce a single tarball output. Make this the output of the cohort workflow and input to the case mode workflows, rather than an array of model tars (retain the current `Array[File]` input as an optional type `Array[File]?` that will be used as the default if provided to case mode in order to support users still working with the old paradigm).; 3. `PostprocessGermlineCNVCalls` - replace input `Array[File] gcnv_calls_tars` with `File gcnv_sample_calls`, the sample-sharded output from the aforementioned transpose task. Delete inputs `calling_configs`, `denoising_configs`, `gcnvkernel_version`, `sharded_interval_lists`, as the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7721
https://github.com/broadinstitute/gatk/issues/7721:110,Energy Efficiency,reduce,reduce,110,"Cromwell still struggles with call caching and metadata bloat in our gCNV workflows. Specific improvements to reduce overhead will modify scattered tasks:. 1. `GermlineCNVCallerCohort(Case)Mode` - Replace input `Array[File] read_count_files` with a list of files, i.e. `File read_count_file_paths`. This should be generated using `WritePathList`, rather than using `write_lines()` which does not function in WDL workflow blocks on some Cromwell servers. Replace output `Array[File] gcnv_call_tars` with a single tarball `File gcnv_call_tar` containing all of the calls. It appears there are a number of redundant outputs - kernel version, denoising configs, output file lists, etc. that were added with the [joint calling pipeline](https://github.com/broadinstitute/gatk/commit/31df35bb9204b5551cc1a3ee7468e2b0e577215d). We should rework that to extract/generate those in joint calling workflow when needed and eliminate these outputs.; 2. Add a transpose task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_call_tar` output, and generates a sample-sharded `Array[File] gcnv_calls_by_sample` output of tarballs.; 2. Add a model bundling task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_model_tar` output, extracts the files, and tarballs all of them together to produce a single tarball output. Make this the output of the cohort workflow and input to the case mode workflows, rather than an array of model tars (retain the current `Array[File]` input as an optional type `Array[File]?` that will be used as the default if provided to case mode in order to support users still working with the old paradigm).; 3. `PostprocessGermlineCNVCalls` - replace input `Array[File] gcnv_calls_tars` with `File gcnv_sample_calls`, the sample-sharded output from the aforementioned transpose task. Delete inputs `calling_configs`, `denoising_configs`, `gcnvkernel_version`, `sharded_interval_lists`, as the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7721
https://github.com/broadinstitute/gatk/issues/7721:649,Modifiability,config,configs,649,"Cromwell still struggles with call caching and metadata bloat in our gCNV workflows. Specific improvements to reduce overhead will modify scattered tasks:. 1. `GermlineCNVCallerCohort(Case)Mode` - Replace input `Array[File] read_count_files` with a list of files, i.e. `File read_count_file_paths`. This should be generated using `WritePathList`, rather than using `write_lines()` which does not function in WDL workflow blocks on some Cromwell servers. Replace output `Array[File] gcnv_call_tars` with a single tarball `File gcnv_call_tar` containing all of the calls. It appears there are a number of redundant outputs - kernel version, denoising configs, output file lists, etc. that were added with the [joint calling pipeline](https://github.com/broadinstitute/gatk/commit/31df35bb9204b5551cc1a3ee7468e2b0e577215d). We should rework that to extract/generate those in joint calling workflow when needed and eliminate these outputs.; 2. Add a transpose task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_call_tar` output, and generates a sample-sharded `Array[File] gcnv_calls_by_sample` output of tarballs.; 2. Add a model bundling task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_model_tar` output, extracts the files, and tarballs all of them together to produce a single tarball output. Make this the output of the cohort workflow and input to the case mode workflows, rather than an array of model tars (retain the current `Array[File]` input as an optional type `Array[File]?` that will be used as the default if provided to case mode in order to support users still working with the old paradigm).; 3. `PostprocessGermlineCNVCalls` - replace input `Array[File] gcnv_calls_tars` with `File gcnv_sample_calls`, the sample-sharded output from the aforementioned transpose task. Delete inputs `calling_configs`, `denoising_configs`, `gcnvkernel_version`, `sharded_interval_lists`, as the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7721
https://github.com/broadinstitute/gatk/issues/7721:603,Safety,redund,redundant,603,"Cromwell still struggles with call caching and metadata bloat in our gCNV workflows. Specific improvements to reduce overhead will modify scattered tasks:. 1. `GermlineCNVCallerCohort(Case)Mode` - Replace input `Array[File] read_count_files` with a list of files, i.e. `File read_count_file_paths`. This should be generated using `WritePathList`, rather than using `write_lines()` which does not function in WDL workflow blocks on some Cromwell servers. Replace output `Array[File] gcnv_call_tars` with a single tarball `File gcnv_call_tar` containing all of the calls. It appears there are a number of redundant outputs - kernel version, denoising configs, output file lists, etc. that were added with the [joint calling pipeline](https://github.com/broadinstitute/gatk/commit/31df35bb9204b5551cc1a3ee7468e2b0e577215d). We should rework that to extract/generate those in joint calling workflow when needed and eliminate these outputs.; 2. Add a transpose task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_call_tar` output, and generates a sample-sharded `Array[File] gcnv_calls_by_sample` output of tarballs.; 2. Add a model bundling task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_model_tar` output, extracts the files, and tarballs all of them together to produce a single tarball output. Make this the output of the cohort workflow and input to the case mode workflows, rather than an array of model tars (retain the current `Array[File]` input as an optional type `Array[File]?` that will be used as the default if provided to case mode in order to support users still working with the old paradigm).; 3. `PostprocessGermlineCNVCalls` - replace input `Array[File] gcnv_calls_tars` with `File gcnv_sample_calls`, the sample-sharded output from the aforementioned transpose task. Delete inputs `calling_configs`, `denoising_configs`, `gcnvkernel_version`, `sharded_interval_lists`, as the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7721
https://github.com/broadinstitute/gatk/issues/7722:156,Availability,error,error,156,"Following up on a [forum thread](https://gatk.broadinstitute.org/hc/en-us/community/posts/4493905884699/comments/4682115320731) reporting a runtime compile error for gCNV:; ```; Exception: ('Compilation failed (return status=1): collect2: error: ld returned 1 exit status. ', '[Elemwise{second,no_inplace}(v, <TensorType(float64, (True,))>)]'); 19:40:38.193 INFO GermlineCNVCaller - Shutting down engine; ```; which could be fixed by explicitly setting the home and tmp directories. This is a pervasive issue in WDLs, as these directories are set by the OS. It is known that when running in Cromwell on GCP, tmp is by default on the boot disk and should be overridden for any tasks that use temporary storage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7722
https://github.com/broadinstitute/gatk/issues/7722:239,Availability,error,error,239,"Following up on a [forum thread](https://gatk.broadinstitute.org/hc/en-us/community/posts/4493905884699/comments/4682115320731) reporting a runtime compile error for gCNV:; ```; Exception: ('Compilation failed (return status=1): collect2: error: ld returned 1 exit status. ', '[Elemwise{second,no_inplace}(v, <TensorType(float64, (True,))>)]'); 19:40:38.193 INFO GermlineCNVCaller - Shutting down engine; ```; which could be fixed by explicitly setting the home and tmp directories. This is a pervasive issue in WDLs, as these directories are set by the OS. It is known that when running in Cromwell on GCP, tmp is by default on the boot disk and should be overridden for any tasks that use temporary storage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7722
https://github.com/broadinstitute/gatk/issues/7722:392,Availability,down,down,392,"Following up on a [forum thread](https://gatk.broadinstitute.org/hc/en-us/community/posts/4493905884699/comments/4682115320731) reporting a runtime compile error for gCNV:; ```; Exception: ('Compilation failed (return status=1): collect2: error: ld returned 1 exit status. ', '[Elemwise{second,no_inplace}(v, <TensorType(float64, (True,))>)]'); 19:40:38.193 INFO GermlineCNVCaller - Shutting down engine; ```; which could be fixed by explicitly setting the home and tmp directories. This is a pervasive issue in WDLs, as these directories are set by the OS. It is known that when running in Cromwell on GCP, tmp is by default on the boot disk and should be overridden for any tasks that use temporary storage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7722
https://github.com/broadinstitute/gatk/pull/7727:59,Testability,test,test,59,* previously the docker build would unnecessarily pull the test files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7727
https://github.com/broadinstitute/gatk/issues/7728:243,Availability,error,error,243,"----. ## Bug Report. ### Affected tool(s) or class(es); gatk CombineGVCFs. ### Affected version(s); - [v4.1.8.1]. ### Description ; When I am using CombineGVCFs to join the two raw_variants.vcf files derived from HaplotypeCaller, it throws an error: KEY END found in VariantContext field INFO at chr1:20094 but this key isn't defined in the VCFHeader. However, when I checked original files, there is no KEY END in INFO. #### Steps to reproduce; The command line I used is:; `~/biosoft/gatk-4.1.8.1/gatk --jave-options ""-Xmx30G"" CombineGVCFs -R ${REF} -V first_raw_variants.vcf -V second_variants.vcf -O cohort.g.vcf.gz`; In which, ${REF} refers to the human reference file. #### Expected behavior; It should return a combined gvcf file. #### Actual behavior; An error happened, as described in above. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7728
https://github.com/broadinstitute/gatk/issues/7728:763,Availability,error,error,763,"----. ## Bug Report. ### Affected tool(s) or class(es); gatk CombineGVCFs. ### Affected version(s); - [v4.1.8.1]. ### Description ; When I am using CombineGVCFs to join the two raw_variants.vcf files derived from HaplotypeCaller, it throws an error: KEY END found in VariantContext field INFO at chr1:20094 but this key isn't defined in the VCFHeader. However, when I checked original files, there is no KEY END in INFO. #### Steps to reproduce; The command line I used is:; `~/biosoft/gatk-4.1.8.1/gatk --jave-options ""-Xmx30G"" CombineGVCFs -R ${REF} -V first_raw_variants.vcf -V second_variants.vcf -O cohort.g.vcf.gz`; In which, ${REF} refers to the human reference file. #### Expected behavior; It should return a combined gvcf file. #### Actual behavior; An error happened, as described in above. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7728
https://github.com/broadinstitute/gatk/issues/7729:170,Availability,error,error,170,"This is probably complicated by a bug in the htsjdk warning from previous versions, which should be fixed in the latest master now. There's probably still a bug, but the error will be more informative now. There may be a ploidy-related bug since the somatic genotypes are a little funky that way. I don't like the fact that this is calling a biallelic method. @fleharty if you still care about this, can you run it again with the latest master?. _Originally posted by @ldgauthier in https://github.com/broadinstitute/gatk/issues/6689#issuecomment-898580381_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7729
https://github.com/broadinstitute/gatk/pull/7730:141,Deployability,patch,patches,141,* The previous attempt to fix requester pays didn't fix it in many cases.; This incorporates a newer version of the NIO library with several patches to fix; edge cases we were hitting.; * https://github.com/googleapis/java-storage-nio/issues/849; * https://github.com/googleapis/java-storage-nio/issues/856; * https://github.com/googleapis/java-storage-nio/issues/857; * upgrade com.google.cloud:google-cloud-nio:0.123.23 ->0.123.25; * fixes https://github.com/broadinstitute/gatk/issues/7716,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7730
https://github.com/broadinstitute/gatk/pull/7730:371,Deployability,upgrade,upgrade,371,* The previous attempt to fix requester pays didn't fix it in many cases.; This incorporates a newer version of the NIO library with several patches to fix; edge cases we were hitting.; * https://github.com/googleapis/java-storage-nio/issues/849; * https://github.com/googleapis/java-storage-nio/issues/856; * https://github.com/googleapis/java-storage-nio/issues/857; * upgrade com.google.cloud:google-cloud-nio:0.123.23 ->0.123.25; * fixes https://github.com/broadinstitute/gatk/issues/7716,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7730
https://github.com/broadinstitute/gatk/issues/7731:107,Availability,redundant,redundant,107,"Hi, . In the Mutect2.wdl file, the section of task definition for M2, I found the following argument maybe redundant, but I am not quite sure. -L ~{variants_for_contamination} . Best regards!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731
https://github.com/broadinstitute/gatk/issues/7731:107,Safety,redund,redundant,107,"Hi, . In the Mutect2.wdl file, the section of task definition for M2, I found the following argument maybe redundant, but I am not quite sure. -L ~{variants_for_contamination} . Best regards!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731
https://github.com/broadinstitute/gatk/issues/7732:210,Safety,detect,detected,210,"Apparently `SelectVariants` is ~10x slower when the samples in the VCF header are not sorted, due to the need to reorder the genotypes on output. We should at least warn the user when unsorted sample names are detected in the input. (discovered by @epiercehoffman)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7732
https://github.com/broadinstitute/gatk/issues/7737:236,Availability,error,error,236,"## Bug Report; ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - version 4.1.0.8. ### Description ; I know this issue has been brought up before but I still cannot find a solution. When I run CombineGVCFs, the error indicates the first locus where there is a NON_REF allele. For example:. HiC_scaffold_2 12497 . T TC,* . My full process is as follows:; I run HaplotypeCaller on individual sample files using the -ERC GVCF command.; I combine the individual g.vcf files using CombineGVCFs. I do this separately for the three species I ultimately want to merge, although they are all mapped to the same reference genome.; I call multisample genotypes with GenotypeGVCFs, again for each species separately. At this point a sample that will fail the process in the future will look like:; HiC_scaffold_2 12497 . T TC,<NON_REF>. I now merge the multispecies genotyped files together using CombineGVCFs.; But then when I go to run GenotypeGVCFs, there is an error wherever there are more than two alleles. And as shown at the top of the message what was a <NON_REF> becomes a * and I get the message: ERROR input alleles must contain <NON_REF>. #### Expected behavior; I expected the g.vcf files to genotype across multiple samples so I can move forward with analyses. Is the problem that I'm running CombineGVCFs and GenotypeGVCFs twice? And somehow that is changing <NON_REF> into * ?. I appreciate any advice.; With thanks,; Emily",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737
https://github.com/broadinstitute/gatk/issues/7737:978,Availability,error,error,978,"## Bug Report; ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - version 4.1.0.8. ### Description ; I know this issue has been brought up before but I still cannot find a solution. When I run CombineGVCFs, the error indicates the first locus where there is a NON_REF allele. For example:. HiC_scaffold_2 12497 . T TC,* . My full process is as follows:; I run HaplotypeCaller on individual sample files using the -ERC GVCF command.; I combine the individual g.vcf files using CombineGVCFs. I do this separately for the three species I ultimately want to merge, although they are all mapped to the same reference genome.; I call multisample genotypes with GenotypeGVCFs, again for each species separately. At this point a sample that will fail the process in the future will look like:; HiC_scaffold_2 12497 . T TC,<NON_REF>. I now merge the multispecies genotyped files together using CombineGVCFs.; But then when I go to run GenotypeGVCFs, there is an error wherever there are more than two alleles. And as shown at the top of the message what was a <NON_REF> becomes a * and I get the message: ERROR input alleles must contain <NON_REF>. #### Expected behavior; I expected the g.vcf files to genotype across multiple samples so I can move forward with analyses. Is the problem that I'm running CombineGVCFs and GenotypeGVCFs twice? And somehow that is changing <NON_REF> into * ?. I appreciate any advice.; With thanks,; Emily",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737
https://github.com/broadinstitute/gatk/issues/7737:1121,Availability,ERROR,ERROR,1121,"## Bug Report; ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - version 4.1.0.8. ### Description ; I know this issue has been brought up before but I still cannot find a solution. When I run CombineGVCFs, the error indicates the first locus where there is a NON_REF allele. For example:. HiC_scaffold_2 12497 . T TC,* . My full process is as follows:; I run HaplotypeCaller on individual sample files using the -ERC GVCF command.; I combine the individual g.vcf files using CombineGVCFs. I do this separately for the three species I ultimately want to merge, although they are all mapped to the same reference genome.; I call multisample genotypes with GenotypeGVCFs, again for each species separately. At this point a sample that will fail the process in the future will look like:; HiC_scaffold_2 12497 . T TC,<NON_REF>. I now merge the multispecies genotyped files together using CombineGVCFs.; But then when I go to run GenotypeGVCFs, there is an error wherever there are more than two alleles. And as shown at the top of the message what was a <NON_REF> becomes a * and I get the message: ERROR input alleles must contain <NON_REF>. #### Expected behavior; I expected the g.vcf files to genotype across multiple samples so I can move forward with analyses. Is the problem that I'm running CombineGVCFs and GenotypeGVCFs twice? And somehow that is changing <NON_REF> into * ?. I appreciate any advice.; With thanks,; Emily",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737
https://github.com/broadinstitute/gatk/issues/7737:1057,Integrability,message,message,1057,"## Bug Report; ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - version 4.1.0.8. ### Description ; I know this issue has been brought up before but I still cannot find a solution. When I run CombineGVCFs, the error indicates the first locus where there is a NON_REF allele. For example:. HiC_scaffold_2 12497 . T TC,* . My full process is as follows:; I run HaplotypeCaller on individual sample files using the -ERC GVCF command.; I combine the individual g.vcf files using CombineGVCFs. I do this separately for the three species I ultimately want to merge, although they are all mapped to the same reference genome.; I call multisample genotypes with GenotypeGVCFs, again for each species separately. At this point a sample that will fail the process in the future will look like:; HiC_scaffold_2 12497 . T TC,<NON_REF>. I now merge the multispecies genotyped files together using CombineGVCFs.; But then when I go to run GenotypeGVCFs, there is an error wherever there are more than two alleles. And as shown at the top of the message what was a <NON_REF> becomes a * and I get the message: ERROR input alleles must contain <NON_REF>. #### Expected behavior; I expected the g.vcf files to genotype across multiple samples so I can move forward with analyses. Is the problem that I'm running CombineGVCFs and GenotypeGVCFs twice? And somehow that is changing <NON_REF> into * ?. I appreciate any advice.; With thanks,; Emily",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737
https://github.com/broadinstitute/gatk/issues/7737:1112,Integrability,message,message,1112,"## Bug Report; ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - version 4.1.0.8. ### Description ; I know this issue has been brought up before but I still cannot find a solution. When I run CombineGVCFs, the error indicates the first locus where there is a NON_REF allele. For example:. HiC_scaffold_2 12497 . T TC,* . My full process is as follows:; I run HaplotypeCaller on individual sample files using the -ERC GVCF command.; I combine the individual g.vcf files using CombineGVCFs. I do this separately for the three species I ultimately want to merge, although they are all mapped to the same reference genome.; I call multisample genotypes with GenotypeGVCFs, again for each species separately. At this point a sample that will fail the process in the future will look like:; HiC_scaffold_2 12497 . T TC,<NON_REF>. I now merge the multispecies genotyped files together using CombineGVCFs.; But then when I go to run GenotypeGVCFs, there is an error wherever there are more than two alleles. And as shown at the top of the message what was a <NON_REF> becomes a * and I get the message: ERROR input alleles must contain <NON_REF>. #### Expected behavior; I expected the g.vcf files to genotype across multiple samples so I can move forward with analyses. Is the problem that I'm running CombineGVCFs and GenotypeGVCFs twice? And somehow that is changing <NON_REF> into * ?. I appreciate any advice.; With thanks,; Emily",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737
https://github.com/broadinstitute/gatk/pull/7739:453,Deployability,pipeline,pipeline,453,"Some read tags get lost when we convert SAM to fastq. This tool allows us to get those tags back once we are done processing the fastqs (some tools e.g. adapter clippers cannot take SAMs as input so the conversion is unavoidable.) So this tool works like Picard MergeBamAlignment, except that we are putting the tags from the unaligned bam to the aligned bam, rather than adding alignment info to the unaligned bam. We will use this in our new TCap RNA pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739
https://github.com/broadinstitute/gatk/pull/7739:153,Energy Efficiency,adapt,adapter,153,"Some read tags get lost when we convert SAM to fastq. This tool allows us to get those tags back once we are done processing the fastqs (some tools e.g. adapter clippers cannot take SAMs as input so the conversion is unavoidable.) So this tool works like Picard MergeBamAlignment, except that we are putting the tags from the unaligned bam to the aligned bam, rather than adding alignment info to the unaligned bam. We will use this in our new TCap RNA pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739
https://github.com/broadinstitute/gatk/pull/7739:153,Integrability,adapter,adapter,153,"Some read tags get lost when we convert SAM to fastq. This tool allows us to get those tags back once we are done processing the fastqs (some tools e.g. adapter clippers cannot take SAMs as input so the conversion is unavoidable.) So this tool works like Picard MergeBamAlignment, except that we are putting the tags from the unaligned bam to the aligned bam, rather than adding alignment info to the unaligned bam. We will use this in our new TCap RNA pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739
https://github.com/broadinstitute/gatk/pull/7739:153,Modifiability,adapt,adapter,153,"Some read tags get lost when we convert SAM to fastq. This tool allows us to get those tags back once we are done processing the fastqs (some tools e.g. adapter clippers cannot take SAMs as input so the conversion is unavoidable.) So this tool works like Picard MergeBamAlignment, except that we are putting the tags from the unaligned bam to the aligned bam, rather than adding alignment info to the unaligned bam. We will use this in our new TCap RNA pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739
https://github.com/broadinstitute/gatk/issues/7741:1805,Availability,error,errors,1805,"00     GT:AD:DP        0/0:37:37 ; ; chr5    125895106       .       G       .       73.78   .       AN=2;DP=28;MQ=60.00     GT:AD:DP        0/0:28:28 ; ; chr5    125895113       .       G       .       68.78   .       AN=2;DP=25;MQ=60.00     GT:AD:DP        0/0:25:25. Position 125894866 is present in the --alleles file, and is genotyped correctrly as homozygous reference in the current sample. The following three positions are not present in the --alleles file, and do not contain an ALT allele in the output file ( dot for ALT).   without '--alleles', these positions are not outputted. . Is this expected behaviour, and if so, why are they emitted ?. notes : ; ; \- same output is observed without threading , ; ; \- same output is observed without dragen mode.  ; ; \-  --alleles is taken from a normal HC run.  ; ; \-  roughly the same heterozygous calls & hom.ALT calls are made with/without --alleles (which is expected behaviour). \=======================. REQUIRED for all errors and issues: ; ; a) GATK version used: 4.2.5.0. b) Exact command used:. gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller \\ ; ;   -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta \\ ; ;   -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam \\ ; ;   -O results/wesep-229191-f.vcf \\ ; ;   --alleles affected\_alleles.vcf \\ ; ;   -L 0005-scattered.interval\_list \\ ; ;   -bamout results/wesep-229191-f.variants.bam \\ ; ;   -G StandardAnnotation -G StandardHCAnnotation \\ ; ;   --dragen-mode \\ ; ;   --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params \\ ; ;   --native-pair-hmm-threads 2.   ; ; c) Entire program log:. (ELPREP) gvandeweyer@ngsvm-pipelines:~/elprep\_streaming/VariantCalling\_Test/scattered$ gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller    -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta    -I /home/gvandeweyer/elprep\_streaming/results/wesep- ; ; 229191-f.bam    -O resul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:4133,Availability,Redundant,Redundant,4133,"me.log.txt ; ; Using GATK jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-packa ; ; ge-4.2.5.0-local.jar HaplotypeCaller -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.54",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:4281,Availability,Redundant,Redundant,4281,"-Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-packa ; ; ge-4.2.5.0-local.jar HaplotypeCaller -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic am",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:10583,Availability,Avail,Available,10583," HaplotypeCaller - \* of the above arguments please manually construct the command.         \* ; ; 22:06:40.415 WARN  HaplotypeCaller - \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* ; ; 22:06:40.437 INFO  HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output ; ; 22:06:40.484 INFO  NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 22:06:40.485 INFO  NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 22:06:40.515 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 22:06:40.516 INFO  IntelPairHmm - Available threads: 4 ; ; 22:06:40.516 INFO  IntelPairHmm - Requested threads: 4 ; ; 22:06:40.517 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 22:06:40.545 INFO  ProgressMeter - Starting traversal ; ; 22:06:40.545 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Regions Processed   Regions/Minute ; ; 22:06:41.344 WARN  InbreedingCoeff - InbreedingCoeff will not be calculated at position chr4:57843320 and possibly subsequent; at least 10 samples must have called genotypes ; ; 22:06:50.557 INFO  ProgressMeter -        chr4:69816964              0.2                   570           3415.9 ; ; 22:07:00.633 INFO  ProgressMeter -        chr4:74352584              0.3                  1340           4002.4 ; ; 22:07:10.827 INFO  ProgressMeter -        chr4:79856475              0.5                  2370           4695.9 ; ; 22:07:20.846 INFO  ProgressMeter -        chr4:88243684              0.7                  3370           5017.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:15408,Availability,down,down,15408,"              4.5                 21980           4841.3 ; ; 22:11:23.401 INFO  ProgressMeter -       chr5:115188609              4.7                 23170           4914.9 ; ; 22:11:33.498 INFO  ProgressMeter -       chr5:127089898              4.9                 24050           4925.7 ; ; 22:11:33.815 INFO  HaplotypeCaller - 69572 read(s) filtered by: MappingQualityReadFilter   ; ; 0 read(s) filtered by: MappingQualityAvailableReadFilter   ; ; 0 read(s) filtered by: MappedReadFilter   ; ; 380 read(s) filtered by: NotSecondaryAlignmentReadFilter   ; ; 0 read(s) filtered by: NotDuplicateReadFilter   ; ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter   ; ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter   ; ; 0 read(s) filtered by: GoodCigarReadFilter   ; ; 0 read(s) filtered by: WellformedReadFilter   ; ; 69952 total reads filtered ; ; 22:11:33.816 INFO  ProgressMeter -       chr5:127488298              4.9                 24105           4931.6 ; ; 22:11:33.816 INFO  ProgressMeter - Traversal complete. Processed 24105 total regions in 4.9 minutes. ; ; 22:11:33.891 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call : 2.883281574 ; ; 22:11:33.891 INFO  PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 66.287158269 ; ; 22:11:33.891 INFO  SmithWatermanAligner - Total compute time in java Smith-Waterman : 81.56 sec ; ; 22:11:35.558 INFO  HaplotypeCaller - Shutting down engine ; ; \[March 12, 2022 10:11:35 PM CET\] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 4.94 minutes. ; ; Runtime.totalMemory()=1998061568. \===========. See forum topic details at forum guidelines page: [https://gatk.broadinstitute.org/hc/en-us/articles/360053845952-Forum-Guidelines](https://gatk.broadinstitute.org/hc/en-us/articles/360053845952-Forum-Guidelines)<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/276551'>Zendesk ticket #276551</a>)<br> gz#276551</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:2543,Deployability,pipeline,pipelines,2543," dragen mode.  ; ; \-  --alleles is taken from a normal HC run.  ; ; \-  roughly the same heterozygous calls & hom.ALT calls are made with/without --alleles (which is expected behaviour). \=======================. REQUIRED for all errors and issues: ; ; a) GATK version used: 4.2.5.0. b) Exact command used:. gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller \\ ; ;   -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta \\ ; ;   -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam \\ ; ;   -O results/wesep-229191-f.vcf \\ ; ;   --alleles affected\_alleles.vcf \\ ; ;   -L 0005-scattered.interval\_list \\ ; ;   -bamout results/wesep-229191-f.variants.bam \\ ; ;   -G StandardAnnotation -G StandardHCAnnotation \\ ; ;   --dragen-mode \\ ; ;   --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params \\ ; ;   --native-pair-hmm-threads 2.   ; ; c) Entire program log:. (ELPREP) gvandeweyer@ngsvm-pipelines:~/elprep\_streaming/VariantCalling\_Test/scattered$ gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller    -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta    -I /home/gvandeweyer/elprep\_streaming/results/wesep- ; ; 229191-f.bam    -O results/wesep-229191-f.vcf    --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.interval\_list    -bamout results/wesep-229191-f.variants.bam    -G StandardAnnotation -G StandardHCAnnotation    --dragen-mode    --dragstr-params- ; ; path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params 2>&1 | tee Runtime.log.txt ; ; Using GATK jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:5222,Deployability,pipeline,pipelines,5222,"ed for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic amd64 ; ; 22:06:39.543 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_312-b07 ; ; 22:06:39.544 INFO  HaplotypeCaller - Start Date/Time: March 12, 2022 10:06:39 PM CET ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - HTSJDK Version: 2.24.1 ; ; 22:06:39.544 INFO  HaplotypeCaller - Picard Version: 2.25.4 ; ; 22:06:39.544 INFO  HaplotypeCaller - Built for Spark Version: 2.4.5 ; ; 22:06:39.544 INFO  HaplotypeCaller - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 22:06",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:5265,Deployability,pipeline,pipelines,5265,"otationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic amd64 ; ; 22:06:39.543 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_312-b07 ; ; 22:06:39.544 INFO  HaplotypeCaller - Start Date/Time: March 12, 2022 10:06:39 PM CET ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - HTSJDK Version: 2.24.1 ; ; 22:06:39.544 INFO  HaplotypeCaller - Picard Version: 2.25.4 ; ; 22:06:39.544 INFO  HaplotypeCaller - Built for Spark Version: 2.4.5 ; ; 22:06:39.544 INFO  HaplotypeCaller - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:4420,Performance,Load,Loading,4420,"_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-packa ; ; ge-4.2.5.0-local.jar HaplotypeCaller -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic amd64 ; ; 22:06:39.543 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_312-b07 ; ; 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:10052,Performance,Load,Loading,10052,"ce-threshold-for-calling 3.0                   \* ; ; 22:06:40.414 WARN  HaplotypeCaller - \* --use-posteriors-to-calculate-qual                                    \* ; ; 22:06:40.414 WARN  HaplotypeCaller - \* --allele-informative-reads-overlap-margin  1                          \* ; ; 22:06:40.414 WARN  HaplotypeCaller - \*                                                                       \* ; ; 22:06:40.414 WARN  HaplotypeCaller - \* If you would like to run DRAGEN-GATK with different inputs for any    \* ; ; 22:06:40.415 WARN  HaplotypeCaller - \* of the above arguments please manually construct the command.         \* ; ; 22:06:40.415 WARN  HaplotypeCaller - \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* ; ; 22:06:40.437 INFO  HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output ; ; 22:06:40.484 INFO  NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 22:06:40.485 INFO  NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 22:06:40.515 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 22:06:40.516 INFO  IntelPairHmm - Available threads: 4 ; ; 22:06:40.516 INFO  IntelPairHmm - Requested threads: 4 ; ; 22:06:40.517 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 22:06:40.545 INFO  ProgressMeter - Starting traversal ; ; 22:06:40.545 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Regions Processed   Regions/Minute ; ; 22:06:41.344 WARN  InbreedingCoeff - InbreedingCoeff will not be calculated at position chr4:57",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:10269,Performance,Load,Loading,10269,"informative-reads-overlap-margin  1                          \* ; ; 22:06:40.414 WARN  HaplotypeCaller - \*                                                                       \* ; ; 22:06:40.414 WARN  HaplotypeCaller - \* If you would like to run DRAGEN-GATK with different inputs for any    \* ; ; 22:06:40.415 WARN  HaplotypeCaller - \* of the above arguments please manually construct the command.         \* ; ; 22:06:40.415 WARN  HaplotypeCaller - \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* ; ; 22:06:40.437 INFO  HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output ; ; 22:06:40.484 INFO  NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 22:06:40.485 INFO  NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 22:06:40.515 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 22:06:40.516 INFO  IntelPairHmm - Available threads: 4 ; ; 22:06:40.516 INFO  IntelPairHmm - Requested threads: 4 ; ; 22:06:40.517 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 22:06:40.545 INFO  ProgressMeter - Starting traversal ; ; 22:06:40.545 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Regions Processed   Regions/Minute ; ; 22:06:41.344 WARN  InbreedingCoeff - InbreedingCoeff will not be calculated at position chr4:57843320 and possibly subsequent; at least 10 samples must have called genotypes ; ; 22:06:50.557 INFO  ProgressMeter -        chr4:69816964              0.2                   570           3415.9 ; ; 22:07:00.633 INFO  Pro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:10713,Performance,multi-thread,multi-threaded,10713,"\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* ; ; 22:06:40.437 INFO  HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output ; ; 22:06:40.484 INFO  NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 22:06:40.485 INFO  NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 22:06:40.515 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 22:06:40.516 INFO  IntelPairHmm - Available threads: 4 ; ; 22:06:40.516 INFO  IntelPairHmm - Requested threads: 4 ; ; 22:06:40.517 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 22:06:40.545 INFO  ProgressMeter - Starting traversal ; ; 22:06:40.545 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Regions Processed   Regions/Minute ; ; 22:06:41.344 WARN  InbreedingCoeff - InbreedingCoeff will not be calculated at position chr4:57843320 and possibly subsequent; at least 10 samples must have called genotypes ; ; 22:06:50.557 INFO  ProgressMeter -        chr4:69816964              0.2                   570           3415.9 ; ; 22:07:00.633 INFO  ProgressMeter -        chr4:74352584              0.3                  1340           4002.4 ; ; 22:07:10.827 INFO  ProgressMeter -        chr4:79856475              0.5                  2370           4695.9 ; ; 22:07:20.846 INFO  ProgressMeter -        chr4:88243684              0.7                  3370           5017.2 ; ; 22:07:32.170 INFO  ProgressMeter -        chr4:88577717              0.9                  3470           4032.9 ; ; 22:07:42.172 INFO  Pro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:4133,Safety,Redund,Redundant,4133,"me.log.txt ; ; Using GATK jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-packa ; ; ge-4.2.5.0-local.jar HaplotypeCaller -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.54",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:4281,Safety,Redund,Redundant,4281,"-Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-packa ; ; ge-4.2.5.0-local.jar HaplotypeCaller -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic am",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:4742,Safety,detect,detect,4742,"s/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic amd64 ; ; 22:06:39.543 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_312-b07 ; ; 22:06:39.544 INFO  HaplotypeCaller - Start Date/Time: March 12, 2022 10:06:39 PM CET ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  Haploty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:2510,Testability,log,log,2510,"put is observed without threading , ; ; \- same output is observed without dragen mode.  ; ; \-  --alleles is taken from a normal HC run.  ; ; \-  roughly the same heterozygous calls & hom.ALT calls are made with/without --alleles (which is expected behaviour). \=======================. REQUIRED for all errors and issues: ; ; a) GATK version used: 4.2.5.0. b) Exact command used:. gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller \\ ; ;   -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta \\ ; ;   -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam \\ ; ;   -O results/wesep-229191-f.vcf \\ ; ;   --alleles affected\_alleles.vcf \\ ; ;   -L 0005-scattered.interval\_list \\ ; ;   -bamout results/wesep-229191-f.variants.bam \\ ; ;   -G StandardAnnotation -G StandardHCAnnotation \\ ; ;   --dragen-mode \\ ; ;   --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params \\ ; ;   --native-pair-hmm-threads 2.   ; ; c) Entire program log:. (ELPREP) gvandeweyer@ngsvm-pipelines:~/elprep\_streaming/VariantCalling\_Test/scattered$ gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller    -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta    -I /home/gvandeweyer/elprep\_streaming/results/wesep- ; ; 229191-f.bam    -O results/wesep-229191-f.vcf    --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.interval\_list    -bamout results/wesep-229191-f.variants.bam    -G StandardAnnotation -G StandardHCAnnotation    --dragen-mode    --dragstr-params- ; ; path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params 2>&1 | tee Runtime.log.txt ; ; Using GATK jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:3167,Testability,log,log,3167,".vcf \\ ; ;   -L 0005-scattered.interval\_list \\ ; ;   -bamout results/wesep-229191-f.variants.bam \\ ; ;   -G StandardAnnotation -G StandardHCAnnotation \\ ; ;   --dragen-mode \\ ; ;   --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params \\ ; ;   --native-pair-hmm-threads 2.   ; ; c) Entire program log:. (ELPREP) gvandeweyer@ngsvm-pipelines:~/elprep\_streaming/VariantCalling\_Test/scattered$ gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller    -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta    -I /home/gvandeweyer/elprep\_streaming/results/wesep- ; ; 229191-f.bam    -O results/wesep-229191-f.vcf    --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.interval\_list    -bamout results/wesep-229191-f.variants.bam    -G StandardAnnotation -G StandardHCAnnotation    --dragen-mode    --dragstr-params- ; ; path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params 2>&1 | tee Runtime.log.txt ; ; Using GATK jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-packa ; ; ge-4.2.5.0-local.jar HaplotypeCaller -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:15654,Usability,guid,guidelines,15654,"              4.5                 21980           4841.3 ; ; 22:11:23.401 INFO  ProgressMeter -       chr5:115188609              4.7                 23170           4914.9 ; ; 22:11:33.498 INFO  ProgressMeter -       chr5:127089898              4.9                 24050           4925.7 ; ; 22:11:33.815 INFO  HaplotypeCaller - 69572 read(s) filtered by: MappingQualityReadFilter   ; ; 0 read(s) filtered by: MappingQualityAvailableReadFilter   ; ; 0 read(s) filtered by: MappedReadFilter   ; ; 380 read(s) filtered by: NotSecondaryAlignmentReadFilter   ; ; 0 read(s) filtered by: NotDuplicateReadFilter   ; ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter   ; ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter   ; ; 0 read(s) filtered by: GoodCigarReadFilter   ; ; 0 read(s) filtered by: WellformedReadFilter   ; ; 69952 total reads filtered ; ; 22:11:33.816 INFO  ProgressMeter -       chr5:127488298              4.9                 24105           4931.6 ; ; 22:11:33.816 INFO  ProgressMeter - Traversal complete. Processed 24105 total regions in 4.9 minutes. ; ; 22:11:33.891 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call : 2.883281574 ; ; 22:11:33.891 INFO  PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 66.287158269 ; ; 22:11:33.891 INFO  SmithWatermanAligner - Total compute time in java Smith-Waterman : 81.56 sec ; ; 22:11:35.558 INFO  HaplotypeCaller - Shutting down engine ; ; \[March 12, 2022 10:11:35 PM CET\] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 4.94 minutes. ; ; Runtime.totalMemory()=1998061568. \===========. See forum topic details at forum guidelines page: [https://gatk.broadinstitute.org/hc/en-us/articles/360053845952-Forum-Guidelines](https://gatk.broadinstitute.org/hc/en-us/articles/360053845952-Forum-Guidelines)<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/276551'>Zendesk ticket #276551</a>)<br> gz#276551</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:15741,Usability,Guid,Guidelines,15741,"              4.5                 21980           4841.3 ; ; 22:11:23.401 INFO  ProgressMeter -       chr5:115188609              4.7                 23170           4914.9 ; ; 22:11:33.498 INFO  ProgressMeter -       chr5:127089898              4.9                 24050           4925.7 ; ; 22:11:33.815 INFO  HaplotypeCaller - 69572 read(s) filtered by: MappingQualityReadFilter   ; ; 0 read(s) filtered by: MappingQualityAvailableReadFilter   ; ; 0 read(s) filtered by: MappedReadFilter   ; ; 380 read(s) filtered by: NotSecondaryAlignmentReadFilter   ; ; 0 read(s) filtered by: NotDuplicateReadFilter   ; ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter   ; ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter   ; ; 0 read(s) filtered by: GoodCigarReadFilter   ; ; 0 read(s) filtered by: WellformedReadFilter   ; ; 69952 total reads filtered ; ; 22:11:33.816 INFO  ProgressMeter -       chr5:127488298              4.9                 24105           4931.6 ; ; 22:11:33.816 INFO  ProgressMeter - Traversal complete. Processed 24105 total regions in 4.9 minutes. ; ; 22:11:33.891 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call : 2.883281574 ; ; 22:11:33.891 INFO  PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 66.287158269 ; ; 22:11:33.891 INFO  SmithWatermanAligner - Total compute time in java Smith-Waterman : 81.56 sec ; ; 22:11:35.558 INFO  HaplotypeCaller - Shutting down engine ; ; \[March 12, 2022 10:11:35 PM CET\] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 4.94 minutes. ; ; Runtime.totalMemory()=1998061568. \===========. See forum topic details at forum guidelines page: [https://gatk.broadinstitute.org/hc/en-us/articles/360053845952-Forum-Guidelines](https://gatk.broadinstitute.org/hc/en-us/articles/360053845952-Forum-Guidelines)<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/276551'>Zendesk ticket #276551</a>)<br> gz#276551</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/issues/7741:15822,Usability,Guid,Guidelines,15822,"              4.5                 21980           4841.3 ; ; 22:11:23.401 INFO  ProgressMeter -       chr5:115188609              4.7                 23170           4914.9 ; ; 22:11:33.498 INFO  ProgressMeter -       chr5:127089898              4.9                 24050           4925.7 ; ; 22:11:33.815 INFO  HaplotypeCaller - 69572 read(s) filtered by: MappingQualityReadFilter   ; ; 0 read(s) filtered by: MappingQualityAvailableReadFilter   ; ; 0 read(s) filtered by: MappedReadFilter   ; ; 380 read(s) filtered by: NotSecondaryAlignmentReadFilter   ; ; 0 read(s) filtered by: NotDuplicateReadFilter   ; ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter   ; ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter   ; ; 0 read(s) filtered by: GoodCigarReadFilter   ; ; 0 read(s) filtered by: WellformedReadFilter   ; ; 69952 total reads filtered ; ; 22:11:33.816 INFO  ProgressMeter -       chr5:127488298              4.9                 24105           4931.6 ; ; 22:11:33.816 INFO  ProgressMeter - Traversal complete. Processed 24105 total regions in 4.9 minutes. ; ; 22:11:33.891 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call : 2.883281574 ; ; 22:11:33.891 INFO  PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 66.287158269 ; ; 22:11:33.891 INFO  SmithWatermanAligner - Total compute time in java Smith-Waterman : 81.56 sec ; ; 22:11:35.558 INFO  HaplotypeCaller - Shutting down engine ; ; \[March 12, 2022 10:11:35 PM CET\] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 4.94 minutes. ; ; Runtime.totalMemory()=1998061568. \===========. See forum topic details at forum guidelines page: [https://gatk.broadinstitute.org/hc/en-us/articles/360053845952-Forum-Guidelines](https://gatk.broadinstitute.org/hc/en-us/articles/360053845952-Forum-Guidelines)<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/276551'>Zendesk ticket #276551</a>)<br> gz#276551</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741
https://github.com/broadinstitute/gatk/pull/7742:575,Modifiability,Parameteriz,Parameterization,575,"Previously, a temporary table is created as part of extract of the VQSR features, and it goes into a separate `temp_tables` dataset in the current project -- that is no longer true, and it now goes into the default dataset as a short living temp table with the task name and a hash. This pr should:. - default to the current dataset (with the VET etc tables) rather than a different dataset. - give a prefix to the temp tables so we know which one came from which step. - temp table TTL---not a changeable option, but default to 24 hours across the board. Still to discuss:; Parameterization of the location (dataset) to create the temp table in (default to the default dataset); manual clean up/TTL is a changeable option and TTL is parametrizable (currently the TTL is a parameter for the prepare step -- but then we set a default as 24 hrs in the WDL) . ![Screen Shot 2022-06-10 at 1 27 58 PM](https://user-images.githubusercontent.com/6863459/173121781-4486c1d1-ef7a-4ab8-aa62-fdc5018fd3b9.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7742
https://github.com/broadinstitute/gatk/pull/7742:277,Security,hash,hash,277,"Previously, a temporary table is created as part of extract of the VQSR features, and it goes into a separate `temp_tables` dataset in the current project -- that is no longer true, and it now goes into the default dataset as a short living temp table with the task name and a hash. This pr should:. - default to the current dataset (with the VET etc tables) rather than a different dataset. - give a prefix to the temp tables so we know which one came from which step. - temp table TTL---not a changeable option, but default to 24 hours across the board. Still to discuss:; Parameterization of the location (dataset) to create the temp table in (default to the default dataset); manual clean up/TTL is a changeable option and TTL is parametrizable (currently the TTL is a parameter for the prepare step -- but then we set a default as 24 hrs in the WDL) . ![Screen Shot 2022-06-10 at 1 27 58 PM](https://user-images.githubusercontent.com/6863459/173121781-4486c1d1-ef7a-4ab8-aa62-fdc5018fd3b9.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7742
https://github.com/broadinstitute/gatk/pull/7743:80,Availability,down,down,80,"Currently, only a public interval list is allowed--this changes that and copies down the SA before the interval list is used. used here:; https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/AoU_DRC_WGS_12-6-21_beta_ingest/job_history/f5ca0b70-1ae2-4e86-b4fc-3d7d19674347 . with interval list: 	gs://prod-drc-broad/beta-release-99k-v3/0000000000-scattered.interval_list",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7743
https://github.com/broadinstitute/gatk/pull/7743:321,Deployability,release,release-,321,"Currently, only a public interval list is allowed--this changes that and copies down the SA before the interval list is used. used here:; https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/AoU_DRC_WGS_12-6-21_beta_ingest/job_history/f5ca0b70-1ae2-4e86-b4fc-3d7d19674347 . with interval list: 	gs://prod-drc-broad/beta-release-99k-v3/0000000000-scattered.interval_list",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7743
https://github.com/broadinstitute/gatk/issues/7747:152,Availability,error,error,152,"I believe `sites` and `variants` should be equivalent. I see that there is a warning emitted by GATK4 at sites that are not het, but it shouldn't be an error. If you're getting an error can you please post the full message in this thread? . ASEReadCounter will only process het sites in both GATK3 and GATK4, but it's possible that GATK3 silently skipped non-het sites rather than emitting a warning. _Originally posted by @meganshand in https://github.com/broadinstitute/gatk/issues/7712#issuecomment-1084552468_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7747
https://github.com/broadinstitute/gatk/issues/7747:180,Availability,error,error,180,"I believe `sites` and `variants` should be equivalent. I see that there is a warning emitted by GATK4 at sites that are not het, but it shouldn't be an error. If you're getting an error can you please post the full message in this thread? . ASEReadCounter will only process het sites in both GATK3 and GATK4, but it's possible that GATK3 silently skipped non-het sites rather than emitting a warning. _Originally posted by @meganshand in https://github.com/broadinstitute/gatk/issues/7712#issuecomment-1084552468_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7747
https://github.com/broadinstitute/gatk/issues/7747:215,Integrability,message,message,215,"I believe `sites` and `variants` should be equivalent. I see that there is a warning emitted by GATK4 at sites that are not het, but it shouldn't be an error. If you're getting an error can you please post the full message in this thread? . ASEReadCounter will only process het sites in both GATK3 and GATK4, but it's possible that GATK3 silently skipped non-het sites rather than emitting a warning. _Originally posted by @meganshand in https://github.com/broadinstitute/gatk/issues/7712#issuecomment-1084552468_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7747
https://github.com/broadinstitute/gatk/pull/7749:163,Deployability,release,released,163,"Reflect the changes from https://github.com/broadinstitute/picard/pull/1782 in the GATK doc build. Note that this requires a new version of Picard that is not yet released. Also, I suppressed deprecation warnings caused by the use of obsolete Java 8 javadoc classes (FieldDoc) for now, since it causes the Java 11 build to fail. A new metrics category shows up in the doc now, with all of the Picard and GATK metrics:. <img width=""1252"" alt=""Screen Shot 2022-10-18 at 4 42 37 PM"" src=""https://user-images.githubusercontent.com/10062863/196540823-a6108b75-c9e7-44c0-a4ba-1d8f927fe5b6.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7749
https://github.com/broadinstitute/gatk/issues/7751:114,Deployability,release,release,114,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport . ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of Apr 4, 2022. ### Description ; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found. #### Steps to reproduce; Run the first test case for GnarlyGenotyperIntergrationTest::testUsingGenomicsDB() on the branch https://github.com/broadinstitute/gatk/pull/7750. The test contains the argument `--intervals chrX:1000000-5000000`, but I'm not sure why that would be an issue. The tool runs fine and the output is valid. #### Expected behavior; An informative warning or a single output of the existing warning. #### Actual behavior; Excessive logging",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7751
https://github.com/broadinstitute/gatk/issues/7751:493,Testability,test,test,493,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport . ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of Apr 4, 2022. ### Description ; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found. #### Steps to reproduce; Run the first test case for GnarlyGenotyperIntergrationTest::testUsingGenomicsDB() on the branch https://github.com/broadinstitute/gatk/pull/7750. The test contains the argument `--intervals chrX:1000000-5000000`, but I'm not sure why that would be an issue. The tool runs fine and the output is valid. #### Expected behavior; An informative warning or a single output of the existing warning. #### Actual behavior; Excessive logging",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7751
https://github.com/broadinstitute/gatk/issues/7751:540,Testability,test,testUsingGenomicsDB,540,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport . ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of Apr 4, 2022. ### Description ; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found. #### Steps to reproduce; Run the first test case for GnarlyGenotyperIntergrationTest::testUsingGenomicsDB() on the branch https://github.com/broadinstitute/gatk/pull/7750. The test contains the argument `--intervals chrX:1000000-5000000`, but I'm not sure why that would be an issue. The tool runs fine and the output is valid. #### Expected behavior; An informative warning or a single output of the existing warning. #### Actual behavior; Excessive logging",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7751
https://github.com/broadinstitute/gatk/issues/7751:630,Testability,test,test,630,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport . ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of Apr 4, 2022. ### Description ; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found. #### Steps to reproduce; Run the first test case for GnarlyGenotyperIntergrationTest::testUsingGenomicsDB() on the branch https://github.com/broadinstitute/gatk/pull/7750. The test contains the argument `--intervals chrX:1000000-5000000`, but I'm not sure why that would be an issue. The tool runs fine and the output is valid. #### Expected behavior; An informative warning or a single output of the existing warning. #### Actual behavior; Excessive logging",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7751
https://github.com/broadinstitute/gatk/issues/7751:905,Testability,log,logging,905,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport . ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of Apr 4, 2022. ### Description ; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found. #### Steps to reproduce; Run the first test case for GnarlyGenotyperIntergrationTest::testUsingGenomicsDB() on the branch https://github.com/broadinstitute/gatk/pull/7750. The test contains the argument `--intervals chrX:1000000-5000000`, but I'm not sure why that would be an issue. The tool runs fine and the output is valid. #### Expected behavior; An informative warning or a single output of the existing warning. #### Actual behavior; Excessive logging",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7751
https://github.com/broadinstitute/gatk/pull/7753:70,Testability,test,test,70,I found this useful to debug some results that varied under different test conditions (different processors or something like that). It could be useful for debugging in the future.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7753
https://github.com/broadinstitute/gatk/pull/7754:166,Deployability,update,update,166,We are trying to replace travis CI and we must test that the push tests work as expected. This will eventually require we rebase everything. . ; We should get rid of/update our old travis tickets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7754
https://github.com/broadinstitute/gatk/pull/7754:47,Testability,test,test,47,We are trying to replace travis CI and we must test that the push tests work as expected. This will eventually require we rebase everything. . ; We should get rid of/update our old travis tickets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7754
https://github.com/broadinstitute/gatk/pull/7754:66,Testability,test,tests,66,We are trying to replace travis CI and we must test that the push tests work as expected. This will eventually require we rebase everything. . ; We should get rid of/update our old travis tickets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7754
https://github.com/broadinstitute/gatk/issues/7755:271,Availability,error,error,271,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); GATK version 4.2.5.0, run from the `us.gcr.io/broad-gatk/gatk:4.2.5.0` docker image. ### Description ; Rarely (~0.1%) within exomes that were sequenced at Broad (by GP), we encounter the error message whose stack trace is shown below. This occurs during batch processing, but it is specific to the .CRAM files: running Mutect2 on the same file produces the same error, and running Mutect2 on other files with the same arguments works fine. The files that trigger this error have contents that match the Broad GP-produced .md5 checksum, and they also pass `samtools quickcheck`. #### Steps to reproduce; (The variables are filled in as one might reasonably expect.); ```sh; /gatk/gatk --java-options ""-Xmx${RAM}G"" \; Mutect2 \; --input ${cram} \; --reference ${REFERENCE_FASTA} \; --panel-of-normals ${PON} \; --germline-resource ${GNOMAD} \; --intervals ${INTERVALS} \; --output ${unfiltered}; ```. #### Expected behavior; In all other cases, somatic variant calling proceeds successfully. #### Actual behavior; ```; 00:17:31.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:32.225 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.226 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.5.0; 00:17:32.226 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:32.227 INFO Mutect2 - Executing as root@8d398eecd56e on Linux v5.10.90+ amd64; 00:17:32.227 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 00:17:32.228 INFO Mutect2 - Start Date/Time: April 5, 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755
https://github.com/broadinstitute/gatk/issues/7755:446,Availability,error,error,446,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); GATK version 4.2.5.0, run from the `us.gcr.io/broad-gatk/gatk:4.2.5.0` docker image. ### Description ; Rarely (~0.1%) within exomes that were sequenced at Broad (by GP), we encounter the error message whose stack trace is shown below. This occurs during batch processing, but it is specific to the .CRAM files: running Mutect2 on the same file produces the same error, and running Mutect2 on other files with the same arguments works fine. The files that trigger this error have contents that match the Broad GP-produced .md5 checksum, and they also pass `samtools quickcheck`. #### Steps to reproduce; (The variables are filled in as one might reasonably expect.); ```sh; /gatk/gatk --java-options ""-Xmx${RAM}G"" \; Mutect2 \; --input ${cram} \; --reference ${REFERENCE_FASTA} \; --panel-of-normals ${PON} \; --germline-resource ${GNOMAD} \; --intervals ${INTERVALS} \; --output ${unfiltered}; ```. #### Expected behavior; In all other cases, somatic variant calling proceeds successfully. #### Actual behavior; ```; 00:17:31.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:32.225 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.226 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.5.0; 00:17:32.226 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:32.227 INFO Mutect2 - Executing as root@8d398eecd56e on Linux v5.10.90+ amd64; 00:17:32.227 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 00:17:32.228 INFO Mutect2 - Start Date/Time: April 5, 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755
https://github.com/broadinstitute/gatk/issues/7755:552,Availability,error,error,552,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); GATK version 4.2.5.0, run from the `us.gcr.io/broad-gatk/gatk:4.2.5.0` docker image. ### Description ; Rarely (~0.1%) within exomes that were sequenced at Broad (by GP), we encounter the error message whose stack trace is shown below. This occurs during batch processing, but it is specific to the .CRAM files: running Mutect2 on the same file produces the same error, and running Mutect2 on other files with the same arguments works fine. The files that trigger this error have contents that match the Broad GP-produced .md5 checksum, and they also pass `samtools quickcheck`. #### Steps to reproduce; (The variables are filled in as one might reasonably expect.); ```sh; /gatk/gatk --java-options ""-Xmx${RAM}G"" \; Mutect2 \; --input ${cram} \; --reference ${REFERENCE_FASTA} \; --panel-of-normals ${PON} \; --germline-resource ${GNOMAD} \; --intervals ${INTERVALS} \; --output ${unfiltered}; ```. #### Expected behavior; In all other cases, somatic variant calling proceeds successfully. #### Actual behavior; ```; 00:17:31.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:32.225 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.226 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.5.0; 00:17:32.226 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:32.227 INFO Mutect2 - Executing as root@8d398eecd56e on Linux v5.10.90+ amd64; 00:17:32.227 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 00:17:32.228 INFO Mutect2 - Start Date/Time: April 5, 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755
https://github.com/broadinstitute/gatk/issues/7755:2772,Availability,down,down,2772,", 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 INFO Mutect2 - HTSJDK Version: 2.24.1; 00:17:32.230 INFO Mutect2 - Picard Version: 2.25.4; 00:17:32.230 INFO Mutect2 - Built for Spark Version: 2.4.5; 00:17:32.231 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:17:32.231 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:17:32.231 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:17:32.232 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:17:32.232 INFO Mutect2 - Deflater: IntelDeflater; 00:17:32.232 INFO Mutect2 - Inflater: IntelInflater; 00:17:32.233 INFO Mutect2 - GCS max retries/reopens: 20; 00:17:32.233 INFO Mutect2 - Requester pays: disabled; 00:17:32.233 INFO Mutect2 - Initializing engine; 00:17:33.373 INFO Mutect2 - Shutting down engine; [April 5, 2022 12:17:33 AM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=361234432; htsjdk.samtools.cram.CRAMException: Attempt to create a bai entry for an unmapped slice with unexpected alignment start (0) or span (-2147483647) values; 	at htsjdk.samtools.cram.BAIEntry.<init>(BAIEntry.java:60); 	at htsjdk.samtools.cram.BAIEntry.<init>(BAIEntry.java:83); 	at htsjdk.samtools.cram.CRAIIndex.openCraiFileAsBaiStream(CRAIIndex.java:89); 	at htsjdk.samtools.SamIndexes.asBaiSeekableStreamOrNull(SamIndexes.java:91); 	at htsjdk.samtools.CRAMFileReader.initWithStreams(CRAMFileReader.java:203); 	at htsjdk.samtools.CRAMFileReader.<init>(CRAMFileReader.java:194); 	at htsjdk.samtools.SamReaderFactory$SamReaderFactoryImpl.open(SamReaderFactory.java:432); 	at htsjdk.samtools.SamReaderFactory.open(SamReaderFactory.java:106); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.<init>(ReadsPathDataSource.java:245); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755
https://github.com/broadinstitute/gatk/issues/7755:277,Integrability,message,message,277,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); GATK version 4.2.5.0, run from the `us.gcr.io/broad-gatk/gatk:4.2.5.0` docker image. ### Description ; Rarely (~0.1%) within exomes that were sequenced at Broad (by GP), we encounter the error message whose stack trace is shown below. This occurs during batch processing, but it is specific to the .CRAM files: running Mutect2 on the same file produces the same error, and running Mutect2 on other files with the same arguments works fine. The files that trigger this error have contents that match the Broad GP-produced .md5 checksum, and they also pass `samtools quickcheck`. #### Steps to reproduce; (The variables are filled in as one might reasonably expect.); ```sh; /gatk/gatk --java-options ""-Xmx${RAM}G"" \; Mutect2 \; --input ${cram} \; --reference ${REFERENCE_FASTA} \; --panel-of-normals ${PON} \; --germline-resource ${GNOMAD} \; --intervals ${INTERVALS} \; --output ${unfiltered}; ```. #### Expected behavior; In all other cases, somatic variant calling proceeds successfully. #### Actual behavior; ```; 00:17:31.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:32.225 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.226 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.5.0; 00:17:32.226 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:32.227 INFO Mutect2 - Executing as root@8d398eecd56e on Linux v5.10.90+ amd64; 00:17:32.227 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 00:17:32.228 INFO Mutect2 - Start Date/Time: April 5, 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755
https://github.com/broadinstitute/gatk/issues/7755:692,Modifiability,variab,variables,692,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); GATK version 4.2.5.0, run from the `us.gcr.io/broad-gatk/gatk:4.2.5.0` docker image. ### Description ; Rarely (~0.1%) within exomes that were sequenced at Broad (by GP), we encounter the error message whose stack trace is shown below. This occurs during batch processing, but it is specific to the .CRAM files: running Mutect2 on the same file produces the same error, and running Mutect2 on other files with the same arguments works fine. The files that trigger this error have contents that match the Broad GP-produced .md5 checksum, and they also pass `samtools quickcheck`. #### Steps to reproduce; (The variables are filled in as one might reasonably expect.); ```sh; /gatk/gatk --java-options ""-Xmx${RAM}G"" \; Mutect2 \; --input ${cram} \; --reference ${REFERENCE_FASTA} \; --panel-of-normals ${PON} \; --germline-resource ${GNOMAD} \; --intervals ${INTERVALS} \; --output ${unfiltered}; ```. #### Expected behavior; In all other cases, somatic variant calling proceeds successfully. #### Actual behavior; ```; 00:17:31.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:32.225 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.226 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.5.0; 00:17:32.226 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:32.227 INFO Mutect2 - Executing as root@8d398eecd56e on Linux v5.10.90+ amd64; 00:17:32.227 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 00:17:32.228 INFO Mutect2 - Start Date/Time: April 5, 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755
https://github.com/broadinstitute/gatk/issues/7755:1141,Performance,Load,Loading,1141,"k/gatk:4.2.5.0` docker image. ### Description ; Rarely (~0.1%) within exomes that were sequenced at Broad (by GP), we encounter the error message whose stack trace is shown below. This occurs during batch processing, but it is specific to the .CRAM files: running Mutect2 on the same file produces the same error, and running Mutect2 on other files with the same arguments works fine. The files that trigger this error have contents that match the Broad GP-produced .md5 checksum, and they also pass `samtools quickcheck`. #### Steps to reproduce; (The variables are filled in as one might reasonably expect.); ```sh; /gatk/gatk --java-options ""-Xmx${RAM}G"" \; Mutect2 \; --input ${cram} \; --reference ${REFERENCE_FASTA} \; --panel-of-normals ${PON} \; --germline-resource ${GNOMAD} \; --intervals ${INTERVALS} \; --output ${unfiltered}; ```. #### Expected behavior; In all other cases, somatic variant calling proceeds successfully. #### Actual behavior; ```; 00:17:31.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:32.225 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.226 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.5.0; 00:17:32.226 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:32.227 INFO Mutect2 - Executing as root@8d398eecd56e on Linux v5.10.90+ amd64; 00:17:32.227 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 00:17:32.228 INFO Mutect2 - Start Date/Time: April 5, 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 INFO Mutect2 - HTSJDK Version: 2.24.1; 00:17:32.230 INFO Mutect2 - Picard Version: 2.25.4; 00:17:32.230 INFO Mutect2 - Built for Spark Versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755
https://github.com/broadinstitute/gatk/issues/7755:610,Security,checksum,checksum,610,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); GATK version 4.2.5.0, run from the `us.gcr.io/broad-gatk/gatk:4.2.5.0` docker image. ### Description ; Rarely (~0.1%) within exomes that were sequenced at Broad (by GP), we encounter the error message whose stack trace is shown below. This occurs during batch processing, but it is specific to the .CRAM files: running Mutect2 on the same file produces the same error, and running Mutect2 on other files with the same arguments works fine. The files that trigger this error have contents that match the Broad GP-produced .md5 checksum, and they also pass `samtools quickcheck`. #### Steps to reproduce; (The variables are filled in as one might reasonably expect.); ```sh; /gatk/gatk --java-options ""-Xmx${RAM}G"" \; Mutect2 \; --input ${cram} \; --reference ${REFERENCE_FASTA} \; --panel-of-normals ${PON} \; --germline-resource ${GNOMAD} \; --intervals ${INTERVALS} \; --output ${unfiltered}; ```. #### Expected behavior; In all other cases, somatic variant calling proceeds successfully. #### Actual behavior; ```; 00:17:31.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:32.225 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.226 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.5.0; 00:17:32.226 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:32.227 INFO Mutect2 - Executing as root@8d398eecd56e on Linux v5.10.90+ amd64; 00:17:32.227 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 00:17:32.228 INFO Mutect2 - Start Date/Time: April 5, 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755
https://github.com/broadinstitute/gatk/pull/7756:164,Testability,test,test,164,"Docker image added---I think we'll want to implement whatever we are doing for the jars for the docker images in parallel. I'm planning to add an additional python test to this and then I want to dig into how to make sure they run during docker build. outside the scope for this specific round of python testing, but it will be worth reaching out the the coworker who spotted both previous clinvar issues to ask for additional spot testing cases",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7756
https://github.com/broadinstitute/gatk/pull/7756:304,Testability,test,testing,304,"Docker image added---I think we'll want to implement whatever we are doing for the jars for the docker images in parallel. I'm planning to add an additional python test to this and then I want to dig into how to make sure they run during docker build. outside the scope for this specific round of python testing, but it will be worth reaching out the the coworker who spotted both previous clinvar issues to ask for additional spot testing cases",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7756
https://github.com/broadinstitute/gatk/pull/7756:432,Testability,test,testing,432,"Docker image added---I think we'll want to implement whatever we are doing for the jars for the docker images in parallel. I'm planning to add an additional python test to this and then I want to dig into how to make sure they run during docker build. outside the scope for this specific round of python testing, but it will be worth reaching out the the coworker who spotted both previous clinvar issues to ask for additional spot testing cases",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7756
https://github.com/broadinstitute/gatk/issues/7757:265,Integrability,message,message,265,"Funcotator checks the VCF input sequence dictionary to see if it corresponds to the `b37` reference so that it can automatically convert `b37` variants to `hg19`. . In the case where a partial `b37` sequence dictionary is provided, funcotator should emit a warning message stating that the sequence dictionary _could_ be `b37` but because it's incomplete, it cannot do the automatic conversion. The code that must be changed for this is in `FuncotatorEngine::determineReferenceAndDatasourceCompatibility` and `FuncotatorUtils::isSequenceDictionaryUsingB37Reference`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7757
https://github.com/broadinstitute/gatk/issues/7761:1856,Testability,test,tests,1856,"nversion from `b37` to `hg19` yet again has resulted in a bug:. On output of a compressed vcf (`.vcf.gz`), HTSJDK is unable to index the resulting file because the reference dictionary is valid for `b37`, but we have converted the variants to use `hg19` contigs. These contigs are not in `b37` and a null pointer exception is thrown. Three exceptions are actually being thrown, but this is the output stack trace:. ```; java.lang.NullPointerException; 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:233); 	at org.broadinstitute.hellbender.tools.funcotator.vcfOutput.VcfOutputRenderer.close(VcfOutputRenderer.java:137); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:893); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1064); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. To fix this we must put a ""reverse transformer"" in to put the variants back into the basis of `b37` on output. This should happen for both `VCF` and `MAF` output so it is consistent. Many of our tests will have to change to accommodate this, and this is a bandaid for the real fix (which is just having a separate datasource for `b37` in the first place).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7761
https://github.com/broadinstitute/gatk/issues/7763:4,Testability,test,tests,4,"The tests in `RequesterPaysIntegrationTest` were temporarily disabled due to not passing in Travis, despite passing when we run them externally. They should be re-enabled after the Github Actions migration",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7763
https://github.com/broadinstitute/gatk/pull/7764:47,Safety,unsafe,unsafe,47,* Exceptions thrown by a tool can be hidden by unsafe close methods throwing additional exceptions.; Avoid this by making use of try-with-resources suppressed exception handling in order to surface the; primary exception as well as the secondary ones. * Fixes #528,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7764
https://github.com/broadinstitute/gatk/pull/7764:101,Safety,Avoid,Avoid,101,* Exceptions thrown by a tool can be hidden by unsafe close methods throwing additional exceptions.; Avoid this by making use of try-with-resources suppressed exception handling in order to surface the; primary exception as well as the secondary ones. * Fixes #528,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7764
https://github.com/broadinstitute/gatk/pull/7774:46,Deployability,update,updated,46,To remove warning about unfound logger.; Also updated latest version in gatk; Updated to latest version of gatk in wdls.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7774
https://github.com/broadinstitute/gatk/pull/7774:78,Deployability,Update,Updated,78,To remove warning about unfound logger.; Also updated latest version in gatk; Updated to latest version of gatk in wdls.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7774
https://github.com/broadinstitute/gatk/pull/7774:32,Testability,log,logger,32,To remove warning about unfound logger.; Also updated latest version in gatk; Updated to latest version of gatk in wdls.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7774
https://github.com/broadinstitute/gatk/pull/7779:169,Availability,error,error,169,- Moved `OutputSortingBuffer` class used by `SVCluster` into `SVClusterEngine` to unify clustering code across tools; - Fixes an issue where no-call genotypes caused an error in `JointGermlineCNVSegmentation`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7779
https://github.com/broadinstitute/gatk/issues/7782:459,Availability,recover,recover,459,"## Bug Report. ### Affected tool(s) or class(es); Mutect2 `--max-mnp-distance 0`. ### Affected version(s); - [X] Latest public release version [4.2.6.1]. ### Description; Same issue than described here: https://github.com/broadinstitute/gatk/issues/6473; ```; singularity exec docker://broadinstitute/gatk:4.2.6.1 gatk Mutect2 \; -R NC_000962.3.fa \; -I input.bam \; -O output.vcf \; --annotation StrandBiasBySample \; --num-matching-bases-in-dangling-end-to-recover 1 \; --max-reads-per-alignment-start 75 \; --max-mnp-distance 0; ```. And a MNP remains:; ```; grep -P ""NC_000962.3\t761155"" output.vcf; NC_000962.3 761155 . C T,G . . AS_SB_TABLE=0,0|9,9|0,2;DP=20;ECNT=1;MBQ=0,17,23;MFRL=0,311,334;MMQ=60,60,60;MPOS=31,40;POPAF=7.30,7.30;TLOD=44.10,3.01GT:AD:AF:DP:F1R2:F2R1:FAD:SB 0/1/2:0,18,2:0.807,0.143:20:0,5,0:0,4,1:0,15,2:0,0,9,11; ```. #### Expected behavior; ```; grep -P ""NC_000962.3\t761155"" output.vcf; NC_000962.3 761155 . C T [...]; NC_000962.3 761155 . C G [...]; ```. BAM, BAI, and VCF here: [files.zip](https://github.com/broadinstitute/gatk/files/8488204/files.zip). Cheers!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7782
https://github.com/broadinstitute/gatk/issues/7782:127,Deployability,release,release,127,"## Bug Report. ### Affected tool(s) or class(es); Mutect2 `--max-mnp-distance 0`. ### Affected version(s); - [X] Latest public release version [4.2.6.1]. ### Description; Same issue than described here: https://github.com/broadinstitute/gatk/issues/6473; ```; singularity exec docker://broadinstitute/gatk:4.2.6.1 gatk Mutect2 \; -R NC_000962.3.fa \; -I input.bam \; -O output.vcf \; --annotation StrandBiasBySample \; --num-matching-bases-in-dangling-end-to-recover 1 \; --max-reads-per-alignment-start 75 \; --max-mnp-distance 0; ```. And a MNP remains:; ```; grep -P ""NC_000962.3\t761155"" output.vcf; NC_000962.3 761155 . C T,G . . AS_SB_TABLE=0,0|9,9|0,2;DP=20;ECNT=1;MBQ=0,17,23;MFRL=0,311,334;MMQ=60,60,60;MPOS=31,40;POPAF=7.30,7.30;TLOD=44.10,3.01GT:AD:AF:DP:F1R2:F2R1:FAD:SB 0/1/2:0,18,2:0.807,0.143:20:0,5,0:0,4,1:0,15,2:0,0,9,11; ```. #### Expected behavior; ```; grep -P ""NC_000962.3\t761155"" output.vcf; NC_000962.3 761155 . C T [...]; NC_000962.3 761155 . C G [...]; ```. BAM, BAI, and VCF here: [files.zip](https://github.com/broadinstitute/gatk/files/8488204/files.zip). Cheers!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7782
https://github.com/broadinstitute/gatk/issues/7782:459,Safety,recover,recover,459,"## Bug Report. ### Affected tool(s) or class(es); Mutect2 `--max-mnp-distance 0`. ### Affected version(s); - [X] Latest public release version [4.2.6.1]. ### Description; Same issue than described here: https://github.com/broadinstitute/gatk/issues/6473; ```; singularity exec docker://broadinstitute/gatk:4.2.6.1 gatk Mutect2 \; -R NC_000962.3.fa \; -I input.bam \; -O output.vcf \; --annotation StrandBiasBySample \; --num-matching-bases-in-dangling-end-to-recover 1 \; --max-reads-per-alignment-start 75 \; --max-mnp-distance 0; ```. And a MNP remains:; ```; grep -P ""NC_000962.3\t761155"" output.vcf; NC_000962.3 761155 . C T,G . . AS_SB_TABLE=0,0|9,9|0,2;DP=20;ECNT=1;MBQ=0,17,23;MFRL=0,311,334;MMQ=60,60,60;MPOS=31,40;POPAF=7.30,7.30;TLOD=44.10,3.01GT:AD:AF:DP:F1R2:F2R1:FAD:SB 0/1/2:0,18,2:0.807,0.143:20:0,5,0:0,4,1:0,15,2:0,0,9,11; ```. #### Expected behavior; ```; grep -P ""NC_000962.3\t761155"" output.vcf; NC_000962.3 761155 . C T [...]; NC_000962.3 761155 . C G [...]; ```. BAM, BAI, and VCF here: [files.zip](https://github.com/broadinstitute/gatk/files/8488204/files.zip). Cheers!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7782
https://github.com/broadinstitute/gatk/pull/7783:625,Deployability,pipeline,pipelines,625,"This PR changes GvsExtractCallset to optionally pad the output VCFs with a leading zero. They are named in a similar fashion to the (also optionally renamed) interval list shards. If the input `zero_pad_output_vcf_filenames` is set to **true** (the default) the VCFs and interval files will be named as such:; - 0000000000-gg_filterset.vcf.gz; - 0000000000-gg_filterset.vcf.gz.tbi; - 0000000000-gg_filterset.vcf.gz.interval_list; - 0000000001-gg_filterset.vcf.gz; - 0000000001-gg_filterset.vcf.gz.tbi; - 0000000001-gg_filterset.vcf.gz.interval_list; - ... An example workflow is [here](https://app.terra.bio/#workspaces/warp-pipelines/ggrant%20-%20GVS%20Quickstart%20V2%20copy/job_history/3d3fc1e4-7f83-40d7-8f8c-115d4a4de158). If the input `zero_pad_output_vcf_filenames` is set to **false**, the workflow will name things as it has in the past:; - gg_filterset_0.vcf.gz; - gg_filterset_0.vcf.gz.tbi; - 0000000000-scattered.interval_list; - gg_filterset_1.vcf.gz; - gg_filterset_1.vcf.gz.tbi; - 0000000001-scattered.interval_list; - ... An example workflow is [here](https://app.terra.bio/#workspaces/warp-pipelines/ggrant%20-%20GVS%20Quickstart%20V2%20copy/job_history/27bd9b06-4400-4db1-89b4-4bdb5856aaff)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7783
https://github.com/broadinstitute/gatk/pull/7783:1107,Deployability,pipeline,pipelines,1107,"This PR changes GvsExtractCallset to optionally pad the output VCFs with a leading zero. They are named in a similar fashion to the (also optionally renamed) interval list shards. If the input `zero_pad_output_vcf_filenames` is set to **true** (the default) the VCFs and interval files will be named as such:; - 0000000000-gg_filterset.vcf.gz; - 0000000000-gg_filterset.vcf.gz.tbi; - 0000000000-gg_filterset.vcf.gz.interval_list; - 0000000001-gg_filterset.vcf.gz; - 0000000001-gg_filterset.vcf.gz.tbi; - 0000000001-gg_filterset.vcf.gz.interval_list; - ... An example workflow is [here](https://app.terra.bio/#workspaces/warp-pipelines/ggrant%20-%20GVS%20Quickstart%20V2%20copy/job_history/3d3fc1e4-7f83-40d7-8f8c-115d4a4de158). If the input `zero_pad_output_vcf_filenames` is set to **false**, the workflow will name things as it has in the past:; - gg_filterset_0.vcf.gz; - gg_filterset_0.vcf.gz.tbi; - 0000000000-scattered.interval_list; - gg_filterset_1.vcf.gz; - gg_filterset_1.vcf.gz.tbi; - 0000000001-scattered.interval_list; - ... An example workflow is [here](https://app.terra.bio/#workspaces/warp-pipelines/ggrant%20-%20GVS%20Quickstart%20V2%20copy/job_history/27bd9b06-4400-4db1-89b4-4bdb5856aaff)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7783
https://github.com/broadinstitute/gatk/pull/7785:8,Usability,simpl,simply,8,This is simply intended to scope out what breaks with forked PR branches so they can be fixed.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7785
https://github.com/broadinstitute/gatk/pull/7787:72,Availability,error,error,72,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787
https://github.com/broadinstitute/gatk/pull/7787:563,Availability,error,error,563,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787
https://github.com/broadinstitute/gatk/pull/7787:750,Energy Efficiency,reduce,reduce,750,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787
https://github.com/broadinstitute/gatk/pull/7787:142,Integrability,wrap,wrapped,142,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787
https://github.com/broadinstitute/gatk/pull/7787:243,Integrability,wrap,wrapper,243,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787
https://github.com/broadinstitute/gatk/pull/7787:717,Performance,load,load,717,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787
https://github.com/broadinstitute/gatk/pull/7787:693,Testability,log,logic,693,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787
https://github.com/broadinstitute/gatk/pull/7787:312,Usability,simpl,simply,312,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787
https://github.com/broadinstitute/gatk/pull/7788:4,Deployability,update,updated,4,use updated jars---even in split intervals!. One less place to track the jars,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7788
https://github.com/broadinstitute/gatk/issues/7792:110,Deployability,release,release,110,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [X] Latest public release version. ### Description ; GenotypeGVCFs outputs samples (individuals) with no reads (depth) as reference genotypes: ; 0/0:0,0:0:0:.:.:0,0,0. or with very small number of reads, like 1:; 0/0:1,0:1:3:.:.:0,3,31. I believe that this issue was also reported here:; https://gatk.broadinstitute.org/hc/en-us/community/posts/4476803114779-GenotypeGVCFs-Output-no-call-as-reference-genotypes. #### Steps to reproduce; I have followed all GATK steps with default settings. . #### Expected behavior; I believe that samples with no reads (or very small number of reads) should be reported as missing, like:; ./. Thank you for your help,; Marcin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7792
https://github.com/broadinstitute/gatk/pull/7794:27,Availability,down,downloads,27,This fixes the first day's downloads on a new release which previously were set to 0. @jonn-smith This is what I was talking about.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7794
https://github.com/broadinstitute/gatk/pull/7794:46,Deployability,release,release,46,This fixes the first day's downloads on a new release which previously were set to 0. @jonn-smith This is what I was talking about.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7794
https://github.com/broadinstitute/gatk/issues/7796:822,Availability,error,error,822,"Hello again, I have not been able to solve the problem with the launch ReadsPipelineSparkMulticore.wdl. :(; ![qoKs8pEt_-0](https://user-images.githubusercontent.com/55628707/161521914-49b7b10f-3264-43d5-8f5c-739d924656a1.jpg); I use this command:; `java -jar ../cromwell-77.jar run ReadsPipelineSparkMulticore.wdl -i exome/ReadsPiplineSpark_exome.json`; This is my json file:; ![qo2S-PSVs_xNXb5ZysWT8g](https://user-images.githubusercontent.com/55628707/164080985-ae983a19-104a-4742-9401-82ea938ef69e.jpeg); Here is my configuration (CPU and RAM):; ![PEP_IbcPaXyZtql0oefE_A](https://user-images.githubusercontent.com/55628707/164081131-58958f86-5aaf-450f-b985-d7885d4a8de4.jpeg); ![FDmF-6wr4RelT11qqByfGA](https://user-images.githubusercontent.com/55628707/164081137-2d07fe83-845e-463e-ab17-7a5cecb415e0.jpeg). Found this error in my stderr file:; 22/04/06 15:36:17 ERROR Executor: Exception in task 10.0 in stage 11.0 (TID 1596); java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateFractionalErrorArray(BaseRecalibrationEngine.java:440); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:141); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$null$0(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$705/136574652.accept(Unknown Source); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at org.broadinstitute.hellbender.utils.iterators.CloseAtEndIterator.forEachRemaining(CloseAtEndIterator.java:47); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7796:866,Availability,ERROR,ERROR,866,"Hello again, I have not been able to solve the problem with the launch ReadsPipelineSparkMulticore.wdl. :(; ![qoKs8pEt_-0](https://user-images.githubusercontent.com/55628707/161521914-49b7b10f-3264-43d5-8f5c-739d924656a1.jpg); I use this command:; `java -jar ../cromwell-77.jar run ReadsPipelineSparkMulticore.wdl -i exome/ReadsPiplineSpark_exome.json`; This is my json file:; ![qo2S-PSVs_xNXb5ZysWT8g](https://user-images.githubusercontent.com/55628707/164080985-ae983a19-104a-4742-9401-82ea938ef69e.jpeg); Here is my configuration (CPU and RAM):; ![PEP_IbcPaXyZtql0oefE_A](https://user-images.githubusercontent.com/55628707/164081131-58958f86-5aaf-450f-b985-d7885d4a8de4.jpeg); ![FDmF-6wr4RelT11qqByfGA](https://user-images.githubusercontent.com/55628707/164081137-2d07fe83-845e-463e-ab17-7a5cecb415e0.jpeg). Found this error in my stderr file:; 22/04/06 15:36:17 ERROR Executor: Exception in task 10.0 in stage 11.0 (TID 1596); java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateFractionalErrorArray(BaseRecalibrationEngine.java:440); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:141); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$null$0(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$705/136574652.accept(Unknown Source); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at org.broadinstitute.hellbender.utils.iterators.CloseAtEndIterator.forEachRemaining(CloseAtEndIterator.java:47); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7796:519,Deployability,configurat,configuration,519,"Hello again, I have not been able to solve the problem with the launch ReadsPipelineSparkMulticore.wdl. :(; ![qoKs8pEt_-0](https://user-images.githubusercontent.com/55628707/161521914-49b7b10f-3264-43d5-8f5c-739d924656a1.jpg); I use this command:; `java -jar ../cromwell-77.jar run ReadsPipelineSparkMulticore.wdl -i exome/ReadsPiplineSpark_exome.json`; This is my json file:; ![qo2S-PSVs_xNXb5ZysWT8g](https://user-images.githubusercontent.com/55628707/164080985-ae983a19-104a-4742-9401-82ea938ef69e.jpeg); Here is my configuration (CPU and RAM):; ![PEP_IbcPaXyZtql0oefE_A](https://user-images.githubusercontent.com/55628707/164081131-58958f86-5aaf-450f-b985-d7885d4a8de4.jpeg); ![FDmF-6wr4RelT11qqByfGA](https://user-images.githubusercontent.com/55628707/164081137-2d07fe83-845e-463e-ab17-7a5cecb415e0.jpeg). Found this error in my stderr file:; 22/04/06 15:36:17 ERROR Executor: Exception in task 10.0 in stage 11.0 (TID 1596); java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateFractionalErrorArray(BaseRecalibrationEngine.java:440); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:141); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$null$0(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$705/136574652.accept(Unknown Source); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at org.broadinstitute.hellbender.utils.iterators.CloseAtEndIterator.forEachRemaining(CloseAtEndIterator.java:47); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7796:3108,Energy Efficiency,schedul,scheduler,3108,til.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$635/777640102.call(Unknown Source); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). I will be greateful for your help,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7796:3188,Energy Efficiency,schedul,scheduler,3188,til.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$635/777640102.call(Unknown Source); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). I will be greateful for your help,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7796:3268,Energy Efficiency,schedul,scheduler,3268,til.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$635/777640102.call(Unknown Source); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). I will be greateful for your help,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7796:519,Modifiability,config,configuration,519,"Hello again, I have not been able to solve the problem with the launch ReadsPipelineSparkMulticore.wdl. :(; ![qoKs8pEt_-0](https://user-images.githubusercontent.com/55628707/161521914-49b7b10f-3264-43d5-8f5c-739d924656a1.jpg); I use this command:; `java -jar ../cromwell-77.jar run ReadsPipelineSparkMulticore.wdl -i exome/ReadsPiplineSpark_exome.json`; This is my json file:; ![qo2S-PSVs_xNXb5ZysWT8g](https://user-images.githubusercontent.com/55628707/164080985-ae983a19-104a-4742-9401-82ea938ef69e.jpeg); Here is my configuration (CPU and RAM):; ![PEP_IbcPaXyZtql0oefE_A](https://user-images.githubusercontent.com/55628707/164081131-58958f86-5aaf-450f-b985-d7885d4a8de4.jpeg); ![FDmF-6wr4RelT11qqByfGA](https://user-images.githubusercontent.com/55628707/164081137-2d07fe83-845e-463e-ab17-7a5cecb415e0.jpeg). Found this error in my stderr file:; 22/04/06 15:36:17 ERROR Executor: Exception in task 10.0 in stage 11.0 (TID 1596); java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateFractionalErrorArray(BaseRecalibrationEngine.java:440); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:141); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$null$0(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$705/136574652.accept(Unknown Source); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at org.broadinstitute.hellbender.utils.iterators.CloseAtEndIterator.forEachRemaining(CloseAtEndIterator.java:47); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7796:3553,Performance,concurren,concurrent,3553,til.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$635/777640102.call(Unknown Source); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). I will be greateful for your help,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7796:3638,Performance,concurren,concurrent,3638,til.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$635/777640102.call(Unknown Source); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). I will be greateful for your help,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796
https://github.com/broadinstitute/gatk/issues/7797:313,Availability,error,error,313,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x ] Latest public release version [version?] _**GATK 4.2.6.1**_; - [ ] Latest master branch as of [date of test?]. ### Description ; We ran ReblockGVCF in 549 samples with the newest GATK (4.2.6.1). 8 of them returned the error similar to the message below . `org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrM:1 [VC /tmp/scratch/prs-sabe-files/GRAR/2031812880_AJ.hard-filtered.gvcf.gz @ chrM:1 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=1} GT=[[2031812880_AJ G*/G* DP 1691 AD 112,1579 {MIN_DP=1691, SQ=0}]] filters=weak_evidence`. and right below, we could find in all of them; `Caused by: org.broadinstitute.hellbender.exceptions.UserException$BadInput: Bad input: Homozygous reference genotypes must contain GQ or PL. Both are missing for hom ref genotype at chrM:1`. All the ""failed samples"" produced a broken output, in this case, missing the chrM (and the alt chr, such as HLA, chr1_alt etc)... It was weird because on WDL it returned as **_Success_** job... We need all the samples with a proper output to run the JointGenotype pipeline with the Reblocked Dragen samples output. #### Steps to reproduce; I'll share with you the chrM:1 from GVCF from a sample with no error; `chrM	1	.	G	<NON_REF>	.	PASS	END=72	GT:AD:DP:GQ:MIN_DP:PL:SPL:ICNT	0/0:2441,2:2443:99:1613:0,120,1800:0,255,255:40,13`. And now, the chrM:1 from a sample with the error; `chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691`. #### Expected behavior; No broken output. #### Actual behavior; Failing in a few samples, breaking the expected output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797
https://github.com/broadinstitute/gatk/issues/7797:1306,Availability,error,error,1306,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x ] Latest public release version [version?] _**GATK 4.2.6.1**_; - [ ] Latest master branch as of [date of test?]. ### Description ; We ran ReblockGVCF in 549 samples with the newest GATK (4.2.6.1). 8 of them returned the error similar to the message below . `org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrM:1 [VC /tmp/scratch/prs-sabe-files/GRAR/2031812880_AJ.hard-filtered.gvcf.gz @ chrM:1 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=1} GT=[[2031812880_AJ G*/G* DP 1691 AD 112,1579 {MIN_DP=1691, SQ=0}]] filters=weak_evidence`. and right below, we could find in all of them; `Caused by: org.broadinstitute.hellbender.exceptions.UserException$BadInput: Bad input: Homozygous reference genotypes must contain GQ or PL. Both are missing for hom ref genotype at chrM:1`. All the ""failed samples"" produced a broken output, in this case, missing the chrM (and the alt chr, such as HLA, chr1_alt etc)... It was weird because on WDL it returned as **_Success_** job... We need all the samples with a proper output to run the JointGenotype pipeline with the Reblocked Dragen samples output. #### Steps to reproduce; I'll share with you the chrM:1 from GVCF from a sample with no error; `chrM	1	.	G	<NON_REF>	.	PASS	END=72	GT:AD:DP:GQ:MIN_DP:PL:SPL:ICNT	0/0:2441,2:2443:99:1613:0,120,1800:0,255,255:40,13`. And now, the chrM:1 from a sample with the error; `chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691`. #### Expected behavior; No broken output. #### Actual behavior; Failing in a few samples, breaking the expected output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797
https://github.com/broadinstitute/gatk/issues/7797:1476,Availability,error,error,1476,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x ] Latest public release version [version?] _**GATK 4.2.6.1**_; - [ ] Latest master branch as of [date of test?]. ### Description ; We ran ReblockGVCF in 549 samples with the newest GATK (4.2.6.1). 8 of them returned the error similar to the message below . `org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrM:1 [VC /tmp/scratch/prs-sabe-files/GRAR/2031812880_AJ.hard-filtered.gvcf.gz @ chrM:1 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=1} GT=[[2031812880_AJ G*/G* DP 1691 AD 112,1579 {MIN_DP=1691, SQ=0}]] filters=weak_evidence`. and right below, we could find in all of them; `Caused by: org.broadinstitute.hellbender.exceptions.UserException$BadInput: Bad input: Homozygous reference genotypes must contain GQ or PL. Both are missing for hom ref genotype at chrM:1`. All the ""failed samples"" produced a broken output, in this case, missing the chrM (and the alt chr, such as HLA, chr1_alt etc)... It was weird because on WDL it returned as **_Success_** job... We need all the samples with a proper output to run the JointGenotype pipeline with the Reblocked Dragen samples output. #### Steps to reproduce; I'll share with you the chrM:1 from GVCF from a sample with no error; `chrM	1	.	G	<NON_REF>	.	PASS	END=72	GT:AD:DP:GQ:MIN_DP:PL:SPL:ICNT	0/0:2441,2:2443:99:1613:0,120,1800:0,255,255:40,13`. And now, the chrM:1 from a sample with the error; `chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691`. #### Expected behavior; No broken output. #### Actual behavior; Failing in a few samples, breaking the expected output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797
https://github.com/broadinstitute/gatk/issues/7797:109,Deployability,release,release,109,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x ] Latest public release version [version?] _**GATK 4.2.6.1**_; - [ ] Latest master branch as of [date of test?]. ### Description ; We ran ReblockGVCF in 549 samples with the newest GATK (4.2.6.1). 8 of them returned the error similar to the message below . `org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrM:1 [VC /tmp/scratch/prs-sabe-files/GRAR/2031812880_AJ.hard-filtered.gvcf.gz @ chrM:1 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=1} GT=[[2031812880_AJ G*/G* DP 1691 AD 112,1579 {MIN_DP=1691, SQ=0}]] filters=weak_evidence`. and right below, we could find in all of them; `Caused by: org.broadinstitute.hellbender.exceptions.UserException$BadInput: Bad input: Homozygous reference genotypes must contain GQ or PL. Both are missing for hom ref genotype at chrM:1`. All the ""failed samples"" produced a broken output, in this case, missing the chrM (and the alt chr, such as HLA, chr1_alt etc)... It was weird because on WDL it returned as **_Success_** job... We need all the samples with a proper output to run the JointGenotype pipeline with the Reblocked Dragen samples output. #### Steps to reproduce; I'll share with you the chrM:1 from GVCF from a sample with no error; `chrM	1	.	G	<NON_REF>	.	PASS	END=72	GT:AD:DP:GQ:MIN_DP:PL:SPL:ICNT	0/0:2441,2:2443:99:1613:0,120,1800:0,255,255:40,13`. And now, the chrM:1 from a sample with the error; `chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691`. #### Expected behavior; No broken output. #### Actual behavior; Failing in a few samples, breaking the expected output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797
https://github.com/broadinstitute/gatk/issues/7797:1167,Deployability,pipeline,pipeline,1167,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x ] Latest public release version [version?] _**GATK 4.2.6.1**_; - [ ] Latest master branch as of [date of test?]. ### Description ; We ran ReblockGVCF in 549 samples with the newest GATK (4.2.6.1). 8 of them returned the error similar to the message below . `org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrM:1 [VC /tmp/scratch/prs-sabe-files/GRAR/2031812880_AJ.hard-filtered.gvcf.gz @ chrM:1 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=1} GT=[[2031812880_AJ G*/G* DP 1691 AD 112,1579 {MIN_DP=1691, SQ=0}]] filters=weak_evidence`. and right below, we could find in all of them; `Caused by: org.broadinstitute.hellbender.exceptions.UserException$BadInput: Bad input: Homozygous reference genotypes must contain GQ or PL. Both are missing for hom ref genotype at chrM:1`. All the ""failed samples"" produced a broken output, in this case, missing the chrM (and the alt chr, such as HLA, chr1_alt etc)... It was weird because on WDL it returned as **_Success_** job... We need all the samples with a proper output to run the JointGenotype pipeline with the Reblocked Dragen samples output. #### Steps to reproduce; I'll share with you the chrM:1 from GVCF from a sample with no error; `chrM	1	.	G	<NON_REF>	.	PASS	END=72	GT:AD:DP:GQ:MIN_DP:PL:SPL:ICNT	0/0:2441,2:2443:99:1613:0,120,1800:0,255,255:40,13`. And now, the chrM:1 from a sample with the error; `chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691`. #### Expected behavior; No broken output. #### Actual behavior; Failing in a few samples, breaking the expected output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797
https://github.com/broadinstitute/gatk/issues/7797:334,Integrability,message,message,334,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x ] Latest public release version [version?] _**GATK 4.2.6.1**_; - [ ] Latest master branch as of [date of test?]. ### Description ; We ran ReblockGVCF in 549 samples with the newest GATK (4.2.6.1). 8 of them returned the error similar to the message below . `org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrM:1 [VC /tmp/scratch/prs-sabe-files/GRAR/2031812880_AJ.hard-filtered.gvcf.gz @ chrM:1 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=1} GT=[[2031812880_AJ G*/G* DP 1691 AD 112,1579 {MIN_DP=1691, SQ=0}]] filters=weak_evidence`. and right below, we could find in all of them; `Caused by: org.broadinstitute.hellbender.exceptions.UserException$BadInput: Bad input: Homozygous reference genotypes must contain GQ or PL. Both are missing for hom ref genotype at chrM:1`. All the ""failed samples"" produced a broken output, in this case, missing the chrM (and the alt chr, such as HLA, chr1_alt etc)... It was weird because on WDL it returned as **_Success_** job... We need all the samples with a proper output to run the JointGenotype pipeline with the Reblocked Dragen samples output. #### Steps to reproduce; I'll share with you the chrM:1 from GVCF from a sample with no error; `chrM	1	.	G	<NON_REF>	.	PASS	END=72	GT:AD:DP:GQ:MIN_DP:PL:SPL:ICNT	0/0:2441,2:2443:99:1613:0,120,1800:0,255,255:40,13`. And now, the chrM:1 from a sample with the error; `chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691`. #### Expected behavior; No broken output. #### Actual behavior; Failing in a few samples, breaking the expected output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797
https://github.com/broadinstitute/gatk/issues/7797:198,Testability,test,test,198,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x ] Latest public release version [version?] _**GATK 4.2.6.1**_; - [ ] Latest master branch as of [date of test?]. ### Description ; We ran ReblockGVCF in 549 samples with the newest GATK (4.2.6.1). 8 of them returned the error similar to the message below . `org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrM:1 [VC /tmp/scratch/prs-sabe-files/GRAR/2031812880_AJ.hard-filtered.gvcf.gz @ chrM:1 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=1} GT=[[2031812880_AJ G*/G* DP 1691 AD 112,1579 {MIN_DP=1691, SQ=0}]] filters=weak_evidence`. and right below, we could find in all of them; `Caused by: org.broadinstitute.hellbender.exceptions.UserException$BadInput: Bad input: Homozygous reference genotypes must contain GQ or PL. Both are missing for hom ref genotype at chrM:1`. All the ""failed samples"" produced a broken output, in this case, missing the chrM (and the alt chr, such as HLA, chr1_alt etc)... It was weird because on WDL it returned as **_Success_** job... We need all the samples with a proper output to run the JointGenotype pipeline with the Reblocked Dragen samples output. #### Steps to reproduce; I'll share with you the chrM:1 from GVCF from a sample with no error; `chrM	1	.	G	<NON_REF>	.	PASS	END=72	GT:AD:DP:GQ:MIN_DP:PL:SPL:ICNT	0/0:2441,2:2443:99:1613:0,120,1800:0,255,255:40,13`. And now, the chrM:1 from a sample with the error; `chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691`. #### Expected behavior; No broken output. #### Actual behavior; Failing in a few samples, breaking the expected output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797
https://github.com/broadinstitute/gatk/issues/7798:186,Safety,redund,redundancy,186,In light of #7754 the tests run in about an 1:15 minutes for the slowest variant calling docker tests. This can be improved by farther sub-dividing the tests and eliminating some of the redundancy between docker and non-docker tests so the test suite runs faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7798
https://github.com/broadinstitute/gatk/issues/7798:22,Testability,test,tests,22,In light of #7754 the tests run in about an 1:15 minutes for the slowest variant calling docker tests. This can be improved by farther sub-dividing the tests and eliminating some of the redundancy between docker and non-docker tests so the test suite runs faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7798
https://github.com/broadinstitute/gatk/issues/7798:96,Testability,test,tests,96,In light of #7754 the tests run in about an 1:15 minutes for the slowest variant calling docker tests. This can be improved by farther sub-dividing the tests and eliminating some of the redundancy between docker and non-docker tests so the test suite runs faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7798
https://github.com/broadinstitute/gatk/issues/7798:152,Testability,test,tests,152,In light of #7754 the tests run in about an 1:15 minutes for the slowest variant calling docker tests. This can be improved by farther sub-dividing the tests and eliminating some of the redundancy between docker and non-docker tests so the test suite runs faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7798
https://github.com/broadinstitute/gatk/issues/7798:227,Testability,test,tests,227,In light of #7754 the tests run in about an 1:15 minutes for the slowest variant calling docker tests. This can be improved by farther sub-dividing the tests and eliminating some of the redundancy between docker and non-docker tests so the test suite runs faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7798
https://github.com/broadinstitute/gatk/issues/7798:240,Testability,test,test,240,In light of #7754 the tests run in about an 1:15 minutes for the slowest variant calling docker tests. This can be improved by farther sub-dividing the tests and eliminating some of the redundancy between docker and non-docker tests so the test suite runs faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7798
https://github.com/broadinstitute/gatk/issues/7799:160,Availability,down,downloaded,160,With the changes in #7754 the experience for forked-PR tests has improved somewhat given that now the jacoco test reports are uploaded as artifacts that can be downloaded. Unfortunately this is confusing and the reports don't appear in the UI until the entire test suite has finished executing AND there is no comment besides the actions API comments on the merge widiget about the tests failing and where to find out why. This can be improved if we pull out an action that runs on test completion to comment about the finished tests for all users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7799
https://github.com/broadinstitute/gatk/issues/7799:55,Testability,test,tests,55,With the changes in #7754 the experience for forked-PR tests has improved somewhat given that now the jacoco test reports are uploaded as artifacts that can be downloaded. Unfortunately this is confusing and the reports don't appear in the UI until the entire test suite has finished executing AND there is no comment besides the actions API comments on the merge widiget about the tests failing and where to find out why. This can be improved if we pull out an action that runs on test completion to comment about the finished tests for all users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7799
https://github.com/broadinstitute/gatk/issues/7799:109,Testability,test,test,109,With the changes in #7754 the experience for forked-PR tests has improved somewhat given that now the jacoco test reports are uploaded as artifacts that can be downloaded. Unfortunately this is confusing and the reports don't appear in the UI until the entire test suite has finished executing AND there is no comment besides the actions API comments on the merge widiget about the tests failing and where to find out why. This can be improved if we pull out an action that runs on test completion to comment about the finished tests for all users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7799
https://github.com/broadinstitute/gatk/issues/7799:260,Testability,test,test,260,With the changes in #7754 the experience for forked-PR tests has improved somewhat given that now the jacoco test reports are uploaded as artifacts that can be downloaded. Unfortunately this is confusing and the reports don't appear in the UI until the entire test suite has finished executing AND there is no comment besides the actions API comments on the merge widiget about the tests failing and where to find out why. This can be improved if we pull out an action that runs on test completion to comment about the finished tests for all users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7799
https://github.com/broadinstitute/gatk/issues/7799:382,Testability,test,tests,382,With the changes in #7754 the experience for forked-PR tests has improved somewhat given that now the jacoco test reports are uploaded as artifacts that can be downloaded. Unfortunately this is confusing and the reports don't appear in the UI until the entire test suite has finished executing AND there is no comment besides the actions API comments on the merge widiget about the tests failing and where to find out why. This can be improved if we pull out an action that runs on test completion to comment about the finished tests for all users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7799
https://github.com/broadinstitute/gatk/issues/7799:482,Testability,test,test,482,With the changes in #7754 the experience for forked-PR tests has improved somewhat given that now the jacoco test reports are uploaded as artifacts that can be downloaded. Unfortunately this is confusing and the reports don't appear in the UI until the entire test suite has finished executing AND there is no comment besides the actions API comments on the merge widiget about the tests failing and where to find out why. This can be improved if we pull out an action that runs on test completion to comment about the finished tests for all users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7799
https://github.com/broadinstitute/gatk/issues/7799:528,Testability,test,tests,528,With the changes in #7754 the experience for forked-PR tests has improved somewhat given that now the jacoco test reports are uploaded as artifacts that can be downloaded. Unfortunately this is confusing and the reports don't appear in the UI until the entire test suite has finished executing AND there is no comment besides the actions API comments on the merge widiget about the tests failing and where to find out why. This can be improved if we pull out an action that runs on test completion to comment about the finished tests for all users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7799
https://github.com/broadinstitute/gatk/issues/7800:559,Deployability,install,installed,559,"Both Travis and Github actions are failing all of the conda tests at the moment. [Example Log](https://api.travis-ci.com/v3/job/567253185/log.txt) . There seems to be a problem with the conda environment.; ```; java.lang.RuntimeException: A required Python package (""gcnvkernel"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; ```; ```; java.lang.AssertionError: The installed version of sklearn does not match the 0.23.1 version that was requested. Check the build log to see the actual version that was resolved by conda.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7800
https://github.com/broadinstitute/gatk/issues/7800:60,Testability,test,tests,60,"Both Travis and Github actions are failing all of the conda tests at the moment. [Example Log](https://api.travis-ci.com/v3/job/567253185/log.txt) . There seems to be a problem with the conda environment.; ```; java.lang.RuntimeException: A required Python package (""gcnvkernel"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; ```; ```; java.lang.AssertionError: The installed version of sklearn does not match the 0.23.1 version that was requested. Check the build log to see the actual version that was resolved by conda.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7800
https://github.com/broadinstitute/gatk/issues/7800:90,Testability,Log,Log,90,"Both Travis and Github actions are failing all of the conda tests at the moment. [Example Log](https://api.travis-ci.com/v3/job/567253185/log.txt) . There seems to be a problem with the conda environment.; ```; java.lang.RuntimeException: A required Python package (""gcnvkernel"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; ```; ```; java.lang.AssertionError: The installed version of sklearn does not match the 0.23.1 version that was requested. Check the build log to see the actual version that was resolved by conda.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7800
https://github.com/broadinstitute/gatk/issues/7800:138,Testability,log,log,138,"Both Travis and Github actions are failing all of the conda tests at the moment. [Example Log](https://api.travis-ci.com/v3/job/567253185/log.txt) . There seems to be a problem with the conda environment.; ```; java.lang.RuntimeException: A required Python package (""gcnvkernel"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; ```; ```; java.lang.AssertionError: The installed version of sklearn does not match the 0.23.1 version that was requested. Check the build log to see the actual version that was resolved by conda.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7800
https://github.com/broadinstitute/gatk/issues/7800:539,Testability,Assert,AssertionError,539,"Both Travis and Github actions are failing all of the conda tests at the moment. [Example Log](https://api.travis-ci.com/v3/job/567253185/log.txt) . There seems to be a problem with the conda environment.; ```; java.lang.RuntimeException: A required Python package (""gcnvkernel"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; ```; ```; java.lang.AssertionError: The installed version of sklearn does not match the 0.23.1 version that was requested. Check the build log to see the actual version that was resolved by conda.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7800
https://github.com/broadinstitute/gatk/issues/7800:658,Testability,log,log,658,"Both Travis and Github actions are failing all of the conda tests at the moment. [Example Log](https://api.travis-ci.com/v3/job/567253185/log.txt) . There seems to be a problem with the conda environment.; ```; java.lang.RuntimeException: A required Python package (""gcnvkernel"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; ```; ```; java.lang.AssertionError: The installed version of sklearn does not match the 0.23.1 version that was requested. Check the build log to see the actual version that was resolved by conda.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7800
https://github.com/broadinstitute/gatk/pull/7801:13,Integrability,depend,dependencies,13,This list of dependencies was generated by running `$ conda env export` on the nightly docker image `broadinstitute/gatk-nightly:2022-04-17-4.2.6.1-3-g0f6ca4f14-NIGHTLY-SNAPSHOT`. Resolves #7800,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7801
https://github.com/broadinstitute/gatk/pull/7802:506,Testability,test,test,506,"@lbergelson @jamesemery I'm on vacation, but saw #7800 and #7801 go by in the gatk4-github Slack channel. Just curious to see if this will work on Travis and/or Github Actions---can you take it from here, if needed?. I've confirmed that this branch works locally in my Ubuntu environment and that `import gcnvkernel` and `import sklearn` both work in the resulting environment (whereas these imports failed in master, although the environment built successfully---this seems to mirror what is happening on test infrastructure in #7800). In contrast, the approach in #7801 doesn't seem to work for me locally, and we've seen that including conda build strings is a bit restrictive. See e.g. https://github.com/broadinstitute/gatk/pull/5026#issuecomment-628860774. . Closes #7800.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7802
https://github.com/broadinstitute/gatk/pull/7804:220,Performance,optimiz,optimization,220,"Cherry pick of #7754 with very minor conflict resolution. Co-authored-by: Louis Bergelson <louisb@broadinstitute.org>, James Emery <emeryj@broadinstitute.org>. Also cherry picked #7727 to pick up an attempt at a git lfs optimization. Even though there's a bug in that that prevents it from running at all, git lfs not running at all works out to be better than git lfs that runs without limits.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7804
https://github.com/broadinstitute/gatk/pull/7808:43,Deployability,update,updated,43,This was missed in #7754 ; @droazen I even updated the all important badge...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7808
https://github.com/broadinstitute/gatk/issues/7809:552,Safety,detect,detected,552,"This request was created from a contribution made by Francesco Mazzarotto on March 23, 2022 14:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4892502642075-FilterMutectCalls-haplotype-filter-value-assigned-to-variants-with-different-PGT-tag](https://gatk.broadinstitute.org/hc/en-us/community/posts/4892502642075-FilterMutectCalls-haplotype-filter-value-assigned-to-variants-with-different-PGT-tag). \--. Hello,. I am using GATK v4.2.5.0 to process tumor-only samples sequenced with WES. In a sample, one variant that has been detected with Sanger sequencing (chr14-45137087-C-T) gets filtered out as non-PASS (also) because of the 'haplotype' filter value. As far as the 'haplotype' filter value is concerned, the 'guilty' variant seems to be another SNP 3bp upstream (chr14-45137084-C-T). There are no other variants called within 100bp of the Sanger-validated one (see below). chr14    45136964    .    C    T    .    haplotype;weak\_evidence    AS\_FilterStatus=weak\_evidence;AS\_SB\_TABLE=3,0|1,0;DP=4;ECNT=2;GERMQ=25;MBQ=41,37;MFRL=360,390;MMQ=60,60;MPOS=69;POPAF=7.30;ROQ=17;TLOD=3.20    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    0|1:3,1:0.333:4:1,0:1,1:0|1:45136962\_C\_T:45136962:3,0,1,0 ; ; chr14    45137084    .    C    T    .    germline;haplotype;panel\_of\_normals    AS\_FilterStatus=SITE;AS\_SB\_TABLE=9,1|12,5;DP=27;ECNT=2;GERMQ=1;MBQ=41,41;MFRL=297,326;MMQ=60,60;MPOS=45;PON;POPAF=0.830;ROQ=90;TLOD=59.93    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    0|1:10,17:0.615:27:4,13:4,4:0|1:45137084\_C\_T:45137084:9,1,12,5 ; ; chr14    45137087    .    C    T    .    germline;haplotype    AS\_FilterStatus=SITE;AS\_SB\_TABLE=12,5|9,1;DP=27;ECNT=2;GERMQ=1;MBQ=41,41;MFRL=326,297;MMQ=60,60;MPOS=44;POPAF=2.33;ROQ=93;TLOD=31.76    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    1|0:17,10:0.385:27:13,4:4,6:1|0:45137084\_C\_T:45137084:12,5,9,1 ; ; chr14    45149295    .    AC    A    .    haplotype;weak\_evidence    AS\_FilterStatus=weak\_evidence;AS\_SB\_TABLE=0,0|0,0;DP=1;ECNT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7809
https://github.com/broadinstitute/gatk/issues/7809:878,Security,validat,validated,878,"This request was created from a contribution made by Francesco Mazzarotto on March 23, 2022 14:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4892502642075-FilterMutectCalls-haplotype-filter-value-assigned-to-variants-with-different-PGT-tag](https://gatk.broadinstitute.org/hc/en-us/community/posts/4892502642075-FilterMutectCalls-haplotype-filter-value-assigned-to-variants-with-different-PGT-tag). \--. Hello,. I am using GATK v4.2.5.0 to process tumor-only samples sequenced with WES. In a sample, one variant that has been detected with Sanger sequencing (chr14-45137087-C-T) gets filtered out as non-PASS (also) because of the 'haplotype' filter value. As far as the 'haplotype' filter value is concerned, the 'guilty' variant seems to be another SNP 3bp upstream (chr14-45137084-C-T). There are no other variants called within 100bp of the Sanger-validated one (see below). chr14    45136964    .    C    T    .    haplotype;weak\_evidence    AS\_FilterStatus=weak\_evidence;AS\_SB\_TABLE=3,0|1,0;DP=4;ECNT=2;GERMQ=25;MBQ=41,37;MFRL=360,390;MMQ=60,60;MPOS=69;POPAF=7.30;ROQ=17;TLOD=3.20    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    0|1:3,1:0.333:4:1,0:1,1:0|1:45136962\_C\_T:45136962:3,0,1,0 ; ; chr14    45137084    .    C    T    .    germline;haplotype;panel\_of\_normals    AS\_FilterStatus=SITE;AS\_SB\_TABLE=9,1|12,5;DP=27;ECNT=2;GERMQ=1;MBQ=41,41;MFRL=297,326;MMQ=60,60;MPOS=45;PON;POPAF=0.830;ROQ=90;TLOD=59.93    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    0|1:10,17:0.615:27:4,13:4,4:0|1:45137084\_C\_T:45137084:9,1,12,5 ; ; chr14    45137087    .    C    T    .    germline;haplotype    AS\_FilterStatus=SITE;AS\_SB\_TABLE=12,5|9,1;DP=27;ECNT=2;GERMQ=1;MBQ=41,41;MFRL=326,297;MMQ=60,60;MPOS=44;POPAF=2.33;ROQ=93;TLOD=31.76    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    1|0:17,10:0.385:27:13,4:4,6:1|0:45137084\_C\_T:45137084:12,5,9,1 ; ; chr14    45149295    .    AC    A    .    haplotype;weak\_evidence    AS\_FilterStatus=weak\_evidence;AS\_SB\_TABLE=0,0|0,0;DP=1;ECNT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7809
https://github.com/broadinstitute/gatk/issues/7811:833,Availability,error,error,833,"Looks like this java.lang.NullPointerException is from an environment set up issue. . This request was created from a contribution made by Jordi Maggi on April 25, 2022 09:25 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException](https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException). \--. Hi,. I created a conda environment and installed gatk4 through `conda install -c bioconda gatk4`. I have been using this environment to run all steps of the single sample germline variant calling best practices workflow (both gatk and picard). However, I have never been able to run CNNScoreVariants with this setup, as it always results in a java.lang.NullPointerException error. The only way I am able to run this step is by running it through the docker image you provide. That, however, is not ideal for our setup. Any idea as to what I may try to be able to run it directly?. GATK version:. Using GATK jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811
https://github.com/broadinstitute/gatk/issues/7811:5316,Availability,down,down,5316,".670 INFO  CNNScoreVariants - Inflater: IntelInflater ; ; 11:17:58.671 INFO  CNNScoreVariants - GCS max retries/reopens: 20 ; ; 11:17:58.671 INFO  CNNScoreVariants - Requester pays: disabled ; ; 11:17:58.671 INFO  CNNScoreVariants - Initializing engine ; ; WARNING: BAM index file /media/analyst/Data/WES/73318/73318\_WES\_hg19\_recalibrated.sorted.bai is older than BAM /media/analyst/Data/WES/73318/73318\_WES\_hg19\_recalibrated.sorted.bam ; ; 11:17:58.969 INFO  FeatureManager - Using codec VCFCodec to read file file:///media/analyst/Data/WES/73318/73318\_80\_IDTv1.vcf.gz ; ; 11:17:59.079 INFO  CNNScoreVariants - Done initializing engine ; ; 11:17:59.081 INFO  NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 11:17:59.187 INFO  CNNScoreVariants - Done scoring variants with CNN. ; ; 11:17:59.187 INFO  CNNScoreVariants - Shutting down engine ; ; \[April 25, 2022 at 11:17:59 AM CEST\] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.01 minutes. ; ; Runtime.totalMemory()=1895825408 ; ; java.lang.NullPointerException ; ;     at org.broadinstitute.hellbender.utils.runtime.ProcessControllerAckResult.hasMessage(ProcessControllerAckResult.java:49) ; ;     at org.broadinstitute.hellbender.utils.runtime.ProcessControllerAckResult.getDisplayMessage(ProcessControllerAckResult.java:69) ; ;     at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:229) ; ;     at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:216) ; ;     at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:183) ; ;     at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVaria",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811
https://github.com/broadinstitute/gatk/issues/7811:498,Deployability,install,installed,498,"Looks like this java.lang.NullPointerException is from an environment set up issue. . This request was created from a contribution made by Jordi Maggi on April 25, 2022 09:25 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException](https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException). \--. Hi,. I created a conda environment and installed gatk4 through `conda install -c bioconda gatk4`. I have been using this environment to run all steps of the single sample germline variant calling best practices workflow (both gatk and picard). However, I have never been able to run CNNScoreVariants with this setup, as it always results in a java.lang.NullPointerException error. The only way I am able to run this step is by running it through the docker image you provide. That, however, is not ideal for our setup. Any idea as to what I may try to be able to run it directly?. GATK version:. Using GATK jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811
https://github.com/broadinstitute/gatk/issues/7811:529,Deployability,install,install,529,"Looks like this java.lang.NullPointerException is from an environment set up issue. . This request was created from a contribution made by Jordi Maggi on April 25, 2022 09:25 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException](https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException). \--. Hi,. I created a conda environment and installed gatk4 through `conda install -c bioconda gatk4`. I have been using this environment to run all steps of the single sample germline variant calling best practices workflow (both gatk and picard). However, I have never been able to run CNNScoreVariants with this setup, as it always results in a java.lang.NullPointerException error. The only way I am able to run this step is by running it through the docker image you provide. That, however, is not ideal for our setup. Any idea as to what I may try to be able to run it directly?. GATK version:. Using GATK jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811
https://github.com/broadinstitute/gatk/issues/7811:2430,Performance,Load,Loading,2430,"0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor ; ; 11:17:58.509 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Apr 25, 2022 11:17:58 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:17:58.668 INFO  CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.668 INFO  CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 11:17:58.669 INFO  CNNScoreVariants - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:17:58.669 INFO  CNNScoreVariants - Executing as analyst@WGS on Linux v5.13.0-40-generic amd64 ; ; 11:17:58.669 INFO  CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13 ; ; 11:17:58.669 INFO  CNNScoreVariants - Start Date/Time: April 25, 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811
https://github.com/broadinstitute/gatk/issues/7811:5016,Performance,Load,Loading,5016,"_IO\_READ\_FOR\_SAMTOOLS : false ; ; 11:17:58.670 INFO  CNNScoreVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 11:17:58.670 INFO  CNNScoreVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 11:17:58.670 INFO  CNNScoreVariants - Deflater: IntelDeflater ; ; 11:17:58.670 INFO  CNNScoreVariants - Inflater: IntelInflater ; ; 11:17:58.671 INFO  CNNScoreVariants - GCS max retries/reopens: 20 ; ; 11:17:58.671 INFO  CNNScoreVariants - Requester pays: disabled ; ; 11:17:58.671 INFO  CNNScoreVariants - Initializing engine ; ; WARNING: BAM index file /media/analyst/Data/WES/73318/73318\_WES\_hg19\_recalibrated.sorted.bai is older than BAM /media/analyst/Data/WES/73318/73318\_WES\_hg19\_recalibrated.sorted.bam ; ; 11:17:58.969 INFO  FeatureManager - Using codec VCFCodec to read file file:///media/analyst/Data/WES/73318/73318\_80\_IDTv1.vcf.gz ; ; 11:17:59.079 INFO  CNNScoreVariants - Done initializing engine ; ; 11:17:59.081 INFO  NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 11:17:59.187 INFO  CNNScoreVariants - Done scoring variants with CNN. ; ; 11:17:59.187 INFO  CNNScoreVariants - Shutting down engine ; ; \[April 25, 2022 at 11:17:59 AM CEST\] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.01 minutes. ; ; Runtime.totalMemory()=1895825408 ; ; java.lang.NullPointerException ; ;     at org.broadinstitute.hellbender.utils.runtime.ProcessControllerAckResult.hasMessage(ProcessControllerAckResult.java:49) ; ;     at org.broadinstitute.hellbender.utils.runtime.ProcessControllerAckResult.getDisplayMessage(ProcessControllerAckResult.java:69) ; ;     at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:229) ; ;     at org.broadinstitute.hellbender.utils.python.StreamingPythonScri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811
https://github.com/broadinstitute/gatk/issues/7811:2755,Safety,detect,detect,2755,"st/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor ; ; 11:17:58.509 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Apr 25, 2022 11:17:58 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:17:58.668 INFO  CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.668 INFO  CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 11:17:58.669 INFO  CNNScoreVariants - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:17:58.669 INFO  CNNScoreVariants - Executing as analyst@WGS on Linux v5.13.0-40-generic amd64 ; ; 11:17:58.669 INFO  CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13 ; ; 11:17:58.669 INFO  CNNScoreVariants - Start Date/Time: April 25, 2022 at 11:17:58 AM CEST ; ; 11:17:58.669 INFO  CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.669 INFO  CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.670 INFO  CNNScoreVariants - HTSJDK Version: 2.24.1 ; ; 11:17:58.670 INFO  CNN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811
https://github.com/broadinstitute/gatk/issues/7811:1850,Testability,log,log,1850," I am able to run this step is by running it through the docker image you provide. That, however, is not ideal for our setup. Any idea as to what I may try to be able to run it directly?. GATK version:. Using GATK jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor ; ; 11:17:58.509 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Apr 25, 2022 11:17:58 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:17:58.668 INFO  CNNScoreVariants -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811
https://github.com/broadinstitute/gatk/pull/7815:112,Deployability,patch,patch,112,Turns out we disabled codecov 3 years ago for a reason. We can leave it on for informational purposes with this patch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7815
https://github.com/broadinstitute/gatk/pull/7817:19,Testability,test,test,19,"Repost of #7815 to test the ""after_n_builds"" functionality",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7817
https://github.com/broadinstitute/gatk/pull/7818:18,Testability,Test,Testing,18,mutex with #7817. Testing in a fresh branch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7818
https://github.com/broadinstitute/gatk/pull/7822:37,Security,validat,validation,37,Added a github action to run womtool validation on all WDLs; Also fixed two wdls that were failing validation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7822
https://github.com/broadinstitute/gatk/pull/7822:99,Security,validat,validation,99,Added a github action to run womtool validation on all WDLs; Also fixed two wdls that were failing validation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7822
https://github.com/broadinstitute/gatk/pull/7823:240,Testability,test,tests,240,currently clinvar grabs all values in the variants section of the annotation json---this is wrong because nirvana includes clinvar values from all overlapping variants. this change limits the clinvar values to the correct variant only; and tests the change as well,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7823
https://github.com/broadinstitute/gatk/pull/7824:0,Deployability,Update,Update,0,Update cromwell and womtool. See if I feel lucky.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7824
https://github.com/broadinstitute/gatk/issues/7825:386,Availability,down,downstream,386,"## Bug Report. ### Affected tool(s) or class(es): Mutect2. ### Affected version(s); gatk 4.2.5. ### Description ; Like most use cases, I acquired a high-confidence, ""consensus"" VCF from a large batch of samples, and I run force-calling on each individual sample again to:; (1) rescue rare variants.; (2) for variants that are not called in a sample, get the REF/ALT counts for them for downstream analysis. However, compared to the first pass (where Mutect2 is in simple germline calling mode), the second pass (force-calling) is extremely slow. Sorry I have not done any precise measurement, but the difference is quite significant. Given my use case, do you still recommend using force-calling? Or is there any alternative, more efficient method? I tried using bcftools call, but that tool has several issues as well such as omitting indels, not supporting multiallelic force-calling etc. #### Steps to reproduce. My command for force-calling is:; ```; ""gatk Mutect2 ""; ""-alleles {input.q_vcf} ""; ""-L {input.q_vcf} ""; ""--genotype-filtered-alleles ""; ""--max-reads-per-alignment-start {params.mrpas} ""; ""-R {params.REF} ""; ""-I {input.sc_bam} ""; ""-O {output.sc_vcf}; ""; ```. I can upload some BAMs for testing if needed. Thanks in advance!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7825
https://github.com/broadinstitute/gatk/issues/7825:731,Energy Efficiency,efficient,efficient,731,"## Bug Report. ### Affected tool(s) or class(es): Mutect2. ### Affected version(s); gatk 4.2.5. ### Description ; Like most use cases, I acquired a high-confidence, ""consensus"" VCF from a large batch of samples, and I run force-calling on each individual sample again to:; (1) rescue rare variants.; (2) for variants that are not called in a sample, get the REF/ALT counts for them for downstream analysis. However, compared to the first pass (where Mutect2 is in simple germline calling mode), the second pass (force-calling) is extremely slow. Sorry I have not done any precise measurement, but the difference is quite significant. Given my use case, do you still recommend using force-calling? Or is there any alternative, more efficient method? I tried using bcftools call, but that tool has several issues as well such as omitting indels, not supporting multiallelic force-calling etc. #### Steps to reproduce. My command for force-calling is:; ```; ""gatk Mutect2 ""; ""-alleles {input.q_vcf} ""; ""-L {input.q_vcf} ""; ""--genotype-filtered-alleles ""; ""--max-reads-per-alignment-start {params.mrpas} ""; ""-R {params.REF} ""; ""-I {input.sc_bam} ""; ""-O {output.sc_vcf}; ""; ```. I can upload some BAMs for testing if needed. Thanks in advance!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7825
https://github.com/broadinstitute/gatk/issues/7825:1201,Testability,test,testing,1201,"## Bug Report. ### Affected tool(s) or class(es): Mutect2. ### Affected version(s); gatk 4.2.5. ### Description ; Like most use cases, I acquired a high-confidence, ""consensus"" VCF from a large batch of samples, and I run force-calling on each individual sample again to:; (1) rescue rare variants.; (2) for variants that are not called in a sample, get the REF/ALT counts for them for downstream analysis. However, compared to the first pass (where Mutect2 is in simple germline calling mode), the second pass (force-calling) is extremely slow. Sorry I have not done any precise measurement, but the difference is quite significant. Given my use case, do you still recommend using force-calling? Or is there any alternative, more efficient method? I tried using bcftools call, but that tool has several issues as well such as omitting indels, not supporting multiallelic force-calling etc. #### Steps to reproduce. My command for force-calling is:; ```; ""gatk Mutect2 ""; ""-alleles {input.q_vcf} ""; ""-L {input.q_vcf} ""; ""--genotype-filtered-alleles ""; ""--max-reads-per-alignment-start {params.mrpas} ""; ""-R {params.REF} ""; ""-I {input.sc_bam} ""; ""-O {output.sc_vcf}; ""; ```. I can upload some BAMs for testing if needed. Thanks in advance!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7825
https://github.com/broadinstitute/gatk/issues/7825:464,Usability,simpl,simple,464,"## Bug Report. ### Affected tool(s) or class(es): Mutect2. ### Affected version(s); gatk 4.2.5. ### Description ; Like most use cases, I acquired a high-confidence, ""consensus"" VCF from a large batch of samples, and I run force-calling on each individual sample again to:; (1) rescue rare variants.; (2) for variants that are not called in a sample, get the REF/ALT counts for them for downstream analysis. However, compared to the first pass (where Mutect2 is in simple germline calling mode), the second pass (force-calling) is extremely slow. Sorry I have not done any precise measurement, but the difference is quite significant. Given my use case, do you still recommend using force-calling? Or is there any alternative, more efficient method? I tried using bcftools call, but that tool has several issues as well such as omitting indels, not supporting multiallelic force-calling etc. #### Steps to reproduce. My command for force-calling is:; ```; ""gatk Mutect2 ""; ""-alleles {input.q_vcf} ""; ""-L {input.q_vcf} ""; ""--genotype-filtered-alleles ""; ""--max-reads-per-alignment-start {params.mrpas} ""; ""-R {params.REF} ""; ""-I {input.sc_bam} ""; ""-O {output.sc_vcf}; ""; ```. I can upload some BAMs for testing if needed. Thanks in advance!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7825
https://github.com/broadinstitute/gatk/pull/7829:26,Security,validat,validating,26,Renamed existing test for validating generated WDLs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7829
https://github.com/broadinstitute/gatk/pull/7829:17,Testability,test,test,17,Renamed existing test for validating generated WDLs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7829
https://github.com/broadinstitute/gatk/pull/7830:176,Deployability,configurat,configuration,176,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:176,Modifiability,config,configuration,176,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:256,Safety,predict,predictor,256,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:388,Safety,predict,predictor,388,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:412,Safety,predict,predictor-,412,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:523,Safety,predict,predictor,523,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:547,Safety,predict,predictor-,547,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:653,Safety,predict,predictor,653,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:677,Safety,predict,predictor-,677,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:753,Safety,predict,predictor,753,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7830:777,Safety,predict,predictor-,777,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830
https://github.com/broadinstitute/gatk/pull/7831:33,Availability,robust,robust,33,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:234,Availability,failure,failures,234,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:266,Availability,failure,failure,266,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:226,Integrability,inject,injects,226,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:62,Performance,load,loaded,62,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:361,Performance,load,loaded,361,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:425,Performance,load,load,425,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:496,Performance,load,loadings,496,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:226,Security,inject,injects,226,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7831:283,Testability,test,tested,283,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831
https://github.com/broadinstitute/gatk/pull/7835:83,Testability,test,test,83,- Use start position as end for purposes of sorting PE evidence; - Adds regression test to MultiFeatureWalker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7835
https://github.com/broadinstitute/gatk/issues/7838:202,Availability,avail,available,202,"## Bug Report. ### Affected tool(s) or class(es); gatk SortVcf. ### Affected version(s); Mac OS X 10.16 x86_64; OpenJDK 64-Bit Server VM 1.8.0_322-b06; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.4.1. ### Description ; SortVcf finishes sorting and writes out a VCF, but then fails with java.lang.ArrayIndexOutOfBoundsException when generating the tabix index. To work around this, I can run with --CREATE_INDEX false and then run `tabix` to generate the index.; ```; INFO	2022-05-06 12:14:45	SortVcf	wrote 675,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:41,521,469; INFO	2022-05-06 12:14:45	SortVcf	wrote 700,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:61,833,861; INFO	2022-05-06 12:14:45	SortVcf	wrote 725,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:78,534,676; INFO	2022-05-06 12:14:45	SortVcf	wrote 750,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:100,707,682; INFO	2022-05-06 12:14:45	SortVcf	wrote 775,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:117,527,190; INFO	2022-05-06 12:14:45	SortVcf	wrote 800,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:134,613,380; INFO	2022-05-06 12:14:45	SortVcf	wrote 825,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:153,780,108; INFO	2022-05-06 12:14:45	SortVcf	wrote 850,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:173,329,831; INFO	2022-05-06 12:14:46	SortVcf	wrote 875,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:192,133,262; [Fri May 06 12:14:46 EDT 2022] picard.vcf.SortVcf done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=2855272448; To get help, see http://broadinstitute.github.io/picard/index.html#G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7838
https://github.com/broadinstitute/gatk/issues/7838:3163,Availability,error,error,3163,"n: chr3:117,527,190; INFO	2022-05-06 12:14:45	SortVcf	wrote 800,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:134,613,380; INFO	2022-05-06 12:14:45	SortVcf	wrote 825,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:153,780,108; INFO	2022-05-06 12:14:45	SortVcf	wrote 850,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:173,329,831; INFO	2022-05-06 12:14:46	SortVcf	wrote 875,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:192,133,262; [Fri May 06 12:14:46 EDT 2022] picard.vcf.SortVcf done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=2855272448; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp. java.lang.ArrayIndexOutOfBoundsException: 16799; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:102); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:92); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); 	at picard.vcf.SortVcf.writeSortedOutput(SortVcf.java:183); 	at picard.vcf.SortVcf.doWork(SortVcf.java:101); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. #### Expected output. There's almost certainly some format issue with my VCF, but ideally GATK would have a better error message than ArrayIndexOutOfBoundsException.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7838
https://github.com/broadinstitute/gatk/issues/7838:3169,Integrability,message,message,3169,"n: chr3:117,527,190; INFO	2022-05-06 12:14:45	SortVcf	wrote 800,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:134,613,380; INFO	2022-05-06 12:14:45	SortVcf	wrote 825,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:153,780,108; INFO	2022-05-06 12:14:45	SortVcf	wrote 850,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:173,329,831; INFO	2022-05-06 12:14:46	SortVcf	wrote 875,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:192,133,262; [Fri May 06 12:14:46 EDT 2022] picard.vcf.SortVcf done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=2855272448; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp. java.lang.ArrayIndexOutOfBoundsException: 16799; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:102); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:92); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); 	at picard.vcf.SortVcf.writeSortedOutput(SortVcf.java:183); 	at picard.vcf.SortVcf.doWork(SortVcf.java:101); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. #### Expected output. There's almost certainly some format issue with my VCF, but ideally GATK would have a better error message than ArrayIndexOutOfBoundsException.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7838
https://github.com/broadinstitute/gatk/issues/7839:153,Safety,predict,predictor,153,"Hello,. We have a java tool that requires GATK and our docker build began to fail a day or two ago. From what I can tell, it seems like biz.k11i:xgboost-predictor is being ported to ai.h2o:xgboost-predictor and perhaps biz.k11i:xgboost-predictor is falling off the mavenCentral() repos? This is a quick attempt to diagnose this. Are you seeing any problems in your builds? Do you have an alternate maven repo for biz.k11i:xgboost-predictor?. Thanks,; Ben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7839
https://github.com/broadinstitute/gatk/issues/7839:197,Safety,predict,predictor,197,"Hello,. We have a java tool that requires GATK and our docker build began to fail a day or two ago. From what I can tell, it seems like biz.k11i:xgboost-predictor is being ported to ai.h2o:xgboost-predictor and perhaps biz.k11i:xgboost-predictor is falling off the mavenCentral() repos? This is a quick attempt to diagnose this. Are you seeing any problems in your builds? Do you have an alternate maven repo for biz.k11i:xgboost-predictor?. Thanks,; Ben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7839
https://github.com/broadinstitute/gatk/issues/7839:236,Safety,predict,predictor,236,"Hello,. We have a java tool that requires GATK and our docker build began to fail a day or two ago. From what I can tell, it seems like biz.k11i:xgboost-predictor is being ported to ai.h2o:xgboost-predictor and perhaps biz.k11i:xgboost-predictor is falling off the mavenCentral() repos? This is a quick attempt to diagnose this. Are you seeing any problems in your builds? Do you have an alternate maven repo for biz.k11i:xgboost-predictor?. Thanks,; Ben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7839
https://github.com/broadinstitute/gatk/issues/7839:430,Safety,predict,predictor,430,"Hello,. We have a java tool that requires GATK and our docker build began to fail a day or two ago. From what I can tell, it seems like biz.k11i:xgboost-predictor is being ported to ai.h2o:xgboost-predictor and perhaps biz.k11i:xgboost-predictor is falling off the mavenCentral() repos? This is a quick attempt to diagnose this. Are you seeing any problems in your builds? Do you have an alternate maven repo for biz.k11i:xgboost-predictor?. Thanks,; Ben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7839
https://github.com/broadinstitute/gatk/issues/7840:955,Availability,Error,Error,955,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; GenotypeGVCFs won't joint call DRAGEN mitochondrial data because of the DRAGEN somatic output format. We should be able to use the DRAGEN SQ in place of Mutect2's TLOD (see line 279 in GenotypeGVCFsEngine); Note that DRAGEN SQ is a Phred-scaled double. #### Steps to reproduce; DRAGEN somatic GVCF entries from version 3.8.4 look like:; chrM 1 . G <NON_REF> . weak_evidence END=1 GT:AD:DP:SQ:MIN_DP 0/0:112,1579:1691:0:1691. Run GenotypeGVCFs with -V to a file like that (reference GenotypeGVCFsIntegrationTest::testGenotypingForSomaticGVCFs() for more details); Must include `--input-is-somatic` as of now. #### Expected behavior; The task should run to completion, calculating a site quality store using the DRAGEN SQ value. #### Actual behavior; Error from AlleleFrequencyCalculator about not having PLs or GQ.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7840
https://github.com/broadinstitute/gatk/issues/7840:110,Deployability,release,release,110,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; GenotypeGVCFs won't joint call DRAGEN mitochondrial data because of the DRAGEN somatic output format. We should be able to use the DRAGEN SQ in place of Mutect2's TLOD (see line 279 in GenotypeGVCFsEngine); Note that DRAGEN SQ is a Phred-scaled double. #### Steps to reproduce; DRAGEN somatic GVCF entries from version 3.8.4 look like:; chrM 1 . G <NON_REF> . weak_evidence END=1 GT:AD:DP:SQ:MIN_DP 0/0:112,1579:1691:0:1691. Run GenotypeGVCFs with -V to a file like that (reference GenotypeGVCFsIntegrationTest::testGenotypingForSomaticGVCFs() for more details); Must include `--input-is-somatic` as of now. #### Expected behavior; The task should run to completion, calculating a site quality store using the DRAGEN SQ value. #### Actual behavior; Error from AlleleFrequencyCalculator about not having PLs or GQ.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7840
https://github.com/broadinstitute/gatk/issues/7840:180,Testability,test,test,180,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; GenotypeGVCFs won't joint call DRAGEN mitochondrial data because of the DRAGEN somatic output format. We should be able to use the DRAGEN SQ in place of Mutect2's TLOD (see line 279 in GenotypeGVCFsEngine); Note that DRAGEN SQ is a Phred-scaled double. #### Steps to reproduce; DRAGEN somatic GVCF entries from version 3.8.4 look like:; chrM 1 . G <NON_REF> . weak_evidence END=1 GT:AD:DP:SQ:MIN_DP 0/0:112,1579:1691:0:1691. Run GenotypeGVCFs with -V to a file like that (reference GenotypeGVCFsIntegrationTest::testGenotypingForSomaticGVCFs() for more details); Must include `--input-is-somatic` as of now. #### Expected behavior; The task should run to completion, calculating a site quality store using the DRAGEN SQ value. #### Actual behavior; Error from AlleleFrequencyCalculator about not having PLs or GQ.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7840
https://github.com/broadinstitute/gatk/issues/7840:718,Testability,test,testGenotypingForSomaticGVCFs,718,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; GenotypeGVCFs won't joint call DRAGEN mitochondrial data because of the DRAGEN somatic output format. We should be able to use the DRAGEN SQ in place of Mutect2's TLOD (see line 279 in GenotypeGVCFsEngine); Note that DRAGEN SQ is a Phred-scaled double. #### Steps to reproduce; DRAGEN somatic GVCF entries from version 3.8.4 look like:; chrM 1 . G <NON_REF> . weak_evidence END=1 GT:AD:DP:SQ:MIN_DP 0/0:112,1579:1691:0:1691. Run GenotypeGVCFs with -V to a file like that (reference GenotypeGVCFsIntegrationTest::testGenotypingForSomaticGVCFs() for more details); Must include `--input-is-somatic` as of now. #### Expected behavior; The task should run to completion, calculating a site quality store using the DRAGEN SQ value. #### Actual behavior; Error from AlleleFrequencyCalculator about not having PLs or GQ.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7840
https://github.com/broadinstitute/gatk/pull/7841:72,Availability,error,error,72,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:564,Availability,error,error,564,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:1344,Availability,error,errors,1344,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:1404,Availability,error,errors,1404,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:1019,Energy Efficiency,reduce,reduce,1019,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:142,Integrability,wrap,wrapped,142,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:243,Integrability,wrap,wrapper,243,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:859,Performance,load,loading,859,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:986,Performance,load,load,986,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:962,Testability,log,logic,962,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/pull/7841:312,Usability,simpl,simply,312,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841
https://github.com/broadinstitute/gatk/issues/7842:523,Availability,error,error,523,"Hi. I had a question on Java version to use for GATK/PIcard. . https://gatk.broadinstitute.org/hc/en-us/articles/360035889531-What-are-the-requirements-for-running-GATK- ; states that `the Java runtime version should be at 1.8 exactly`. As you might be aware, Java 1.8 has reached its end of life - https://endoflife.date/java . Given this , I tried running MarkDuplicates and some other tools using Amazon Corretto 17 and they seem to work fine. More importantly, I did not encounter the `Unsupported major.minor version` error as mentioned here: https://gatk.broadinstitute.org/hc/en-us/articles/360035532332. Also happened to see this - https://github.com/broadinstitute/gatk/issues/7436. Question: Am I OK using Amazon Corretto for GATK/Picard or do I need to absolutely stick on to Java 1.8 for GATK/Picard. ```; java -version; openjdk version ""17.0.2"" 2022-01-18 LTS; OpenJDK Runtime Environment Corretto-17.0.2.8.1 (build 17.0.2+8-LTS); OpenJDK 64-Bit Server VM Corretto-17.0.2.8.1 (build 17.0.2+8-LTS, mixed mode, sharing); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7842
https://github.com/broadinstitute/gatk/pull/7843:31,Availability,robust,robust,31,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:232,Availability,failure,failures,232,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:264,Availability,failure,failure,264,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:224,Integrability,inject,injects,224,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:60,Performance,load,loaded,60,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:359,Performance,load,loaded,359,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:423,Performance,load,load,423,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:494,Performance,load,loadings,494,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:224,Security,inject,injects,224,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7843:281,Testability,test,tested,281,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843
https://github.com/broadinstitute/gatk/pull/7844:60,Testability,Test,Tested,60,"Allow CH to create custom cohorts with a subset of samples. Tested with sample_id=1 (sample name = ERS4367795). Imported:; <img width=""851"" alt=""Screen Shot 2022-05-13 at 4 13 15 PM"" src=""https://user-images.githubusercontent.com/6863459/168382823-abcfad4a-e641-43eb-a806-6556c49b9a8b.png"">. In the alt-allele; <img width=""1213"" alt=""Screen Shot 2022-05-17 at 6 52 30 PM"" src=""https://user-images.githubusercontent.com/6863459/168925071-dc889fa8-0875-4dcf-b318-330f0f3a27ae.png"">. In the filter:; <img width=""1186"" alt=""Screen Shot 2022-05-17 at 6 51 43 PM"" src=""https://user-images.githubusercontent.com/6863459/168924994-3ceb693e-9223-49b4-adca-50dfaff18c9a.png"">. In the prepare:; <img width=""589"" alt=""Screen Shot 2022-05-17 at 2 47 32 PM"" src=""https://user-images.githubusercontent.com/6863459/168887752-5897a0b9-2a6d-4393-9e3b-450a64c649c7.png"">. In the extract:; <img width=""1780"" alt=""Screen Shot 2022-05-17 at 6 48 17 PM"" src=""https://user-images.githubusercontent.com/6863459/168924703-ad863dee-19c9-4b6e-90b3-696781e86c93.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7844
https://github.com/broadinstitute/gatk/issues/7849:265,Availability,error,error,265,"Hello GATK team!. ## Bug Report. ### Affected tool(s) or class(es). mutect2. ### Affected version(s); - Latest public release version: 4.2.6.1. ### Description . getting fail for all scatter task with the argument ""--emit-ref-confidence GVCF"", no fails without it. error:; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar GetSampleName -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -O tumor_name.txt -encode --gcs-project-for-requester-pays broad-firecloud-ccle; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.b3fd1830; 14:13:40.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:13:40.275 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.276 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.6.1; 14:13:40.277 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:13:40.277 INFO Mutect2 - Executing as root@0b46ce3a6ba5 on Linux v5.10.107+ amd64; 14:13:40.277 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:13:40.278 INFO Mutect2 - Start Date/Time: May 13, 2022 2:13:40 PM GMT; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.279 INFO Mutect2 - HTSJDK Version: 2.24.1; 14:13:40.280 INFO Mutect2 - Picard Version: 2.27.1; 14:13:40.284 INFO Mutect2 - Built for Spark Version: 2.4.5; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:40.285 INFO Mutect2 - H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:3647,Availability,Avail,Available,3647,"t-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; 14:13:54.570 INFO FeatureManager - Using codec IntervalListCodec to read file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 20",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:3765,Availability,avail,available,3765,"d file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 2022 2:14:09 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.49 minutes.; Runtime.to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:4627,Availability,down,down,4627,"lPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 2022 2:14:09 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=850644992; java.lang.ArrayIndexOutOfBoundsException: -1; at java.util.ArrayList.elementData(ArrayList.java:422); at java.util.ArrayList.get(ArrayList.java:435); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$27(SomaticGenotypingEngine.java:376); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.stream.SliceOps$1$1.accept(SliceOps.java:204); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:118,Deployability,release,release,118,"Hello GATK team!. ## Bug Report. ### Affected tool(s) or class(es). mutect2. ### Affected version(s); - Latest public release version: 4.2.6.1. ### Description . getting fail for all scatter task with the argument ""--emit-ref-confidence GVCF"", no fails without it. error:; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar GetSampleName -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -O tumor_name.txt -encode --gcs-project-for-requester-pays broad-firecloud-ccle; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.b3fd1830; 14:13:40.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:13:40.275 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.276 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.6.1; 14:13:40.277 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:13:40.277 INFO Mutect2 - Executing as root@0b46ce3a6ba5 on Linux v5.10.107+ amd64; 14:13:40.277 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:13:40.278 INFO Mutect2 - Start Date/Time: May 13, 2022 2:13:40 PM GMT; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.279 INFO Mutect2 - HTSJDK Version: 2.24.1; 14:13:40.280 INFO Mutect2 - Picard Version: 2.27.1; 14:13:40.284 INFO Mutect2 - Built for Spark Version: 2.4.5; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:40.285 INFO Mutect2 - H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:8220,Deployability,pipeline,pipeline,8220,"e.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -tumor HAP1_1 --germline-resource gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz -L gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF --gcs-project-for-requester-pays broad-firecloud-ccle; ```. #### Steps to reproduce. running the same pipeline as described in previous issues: #7492. But I have added ""--genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF"" as additional args. the rest of the arguments are defaults/basic from the mutect2.wdl pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:8458,Deployability,pipeline,pipeline,8458,"e.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -tumor HAP1_1 --germline-resource gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz -L gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF --gcs-project-for-requester-pays broad-firecloud-ccle; ```. #### Steps to reproduce. running the same pipeline as described in previous issues: #7492. But I have added ""--genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF"" as additional args. the rest of the arguments are defaults/basic from the mutect2.wdl pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:5434,Integrability,wrap,wrapAndCopyInto,5434,"mputeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 2022 2:14:09 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=850644992; java.lang.ArrayIndexOutOfBoundsException: -1; at java.util.ArrayList.elementData(ArrayList.java:422); at java.util.ArrayList.get(ArrayList.java:435); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$27(SomaticGenotypingEngine.java:376); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.stream.SliceOps$1$1.accept(SliceOps.java:204); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:530); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:377); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:354); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:161); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:283); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:300); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:7177,Modifiability,variab,variable,7177,2Engine.callRegion(Mutect2Engine.java:283); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:300); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -tumor HAP1_1 --germline-resource gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz -L gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF --gcs-project-for-requester-pays broad-firecloud-ccle; ```. #### Steps,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:813,Performance,Load,Loading,813,"Hello GATK team!. ## Bug Report. ### Affected tool(s) or class(es). mutect2. ### Affected version(s); - Latest public release version: 4.2.6.1. ### Description . getting fail for all scatter task with the argument ""--emit-ref-confidence GVCF"", no fails without it. error:; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar GetSampleName -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -O tumor_name.txt -encode --gcs-project-for-requester-pays broad-firecloud-ccle; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.b3fd1830; 14:13:40.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:13:40.275 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.276 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.6.1; 14:13:40.277 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:13:40.277 INFO Mutect2 - Executing as root@0b46ce3a6ba5 on Linux v5.10.107+ amd64; 14:13:40.277 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:13:40.278 INFO Mutect2 - Start Date/Time: May 13, 2022 2:13:40 PM GMT; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.279 INFO Mutect2 - HTSJDK Version: 2.24.1; 14:13:40.280 INFO Mutect2 - Picard Version: 2.27.1; 14:13:40.284 INFO Mutect2 - Built for Spark Version: 2.4.5; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:40.285 INFO Mutect2 - H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:3248,Performance,Load,Loading,3248,"r; 14:13:40.286 INFO Mutect2 - GCS max retries/reopens: 20; 14:13:40.286 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 14:13:40.286 INFO Mutect2 - Initializing engine; 14:13:46.660 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz; 14:13:48.823 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; 14:13:54.570 INFO FeatureManager - Using codec IntervalListCodec to read file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:3402,Performance,Load,Loading,3402,"6 INFO Mutect2 - Initializing engine; 14:13:46.660 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz; 14:13:48.823 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; 14:13:54.570 INFO FeatureManager - Using codec IntervalListCodec to read file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:3851,Performance,multi-thread,multi-threaded,3851,"6/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 2022 2:14:09 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=850644992; java.lang.ArrayIndexOutOfBoundsException: -1; at java.util.ArrayList.ele",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:2789,Security,secur,secure-,2789," : 2; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:40.285 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:13:40.285 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:13:40.285 INFO Mutect2 - Deflater: IntelDeflater; 14:13:40.285 INFO Mutect2 - Inflater: IntelInflater; 14:13:40.286 INFO Mutect2 - GCS max retries/reopens: 20; 14:13:40.286 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 14:13:40.286 INFO Mutect2 - Initializing engine; 14:13:46.660 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz; 14:13:48.823 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; 14:13:54.570 INFO FeatureManager - Using codec IntervalListCodec to read file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/issues/7849:7714,Security,secur,secure-,7714,"e.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -tumor HAP1_1 --germline-resource gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz -L gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF --gcs-project-for-requester-pays broad-firecloud-ccle; ```. #### Steps to reproduce. running the same pipeline as described in previous issues: #7492. But I have added ""--genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF"" as additional args. the rest of the arguments are defaults/basic from the mutect2.wdl pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849
https://github.com/broadinstitute/gatk/pull/7850:8,Security,Validat,Validation,8,Add VAT Validation check that aa_change and exon_number are consistentally set.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7850
https://github.com/broadinstitute/gatk/pull/7853:23,Performance,Load,Load,23,Multiple bug fixes:; - Load entity_types; - Properly handle case where gvcf or index NOT found.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7853
https://github.com/broadinstitute/gatk/pull/7857:1118,Deployability,integrat,integration,1118,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:856,Integrability,interface,interface,856,"**Spanning Deletion Changes**. 1. Created a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:1118,Integrability,integrat,integration,1118,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:931,Modifiability,Refactor,Refactoring,931,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:1871,Performance,perform,performance,1871,"ecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for reference data); 3. ExtractCohortRecord (pre-variant context record)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:992,Testability,test,testing,992,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:1098,Testability,test,test,1098,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:1141,Testability,test,test,1141,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:1339,Testability,test,tests,1339,"ecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for reference data); 3. ExtractCohortRecord (pre-variant context record)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/pull/7857:1742,Testability,test,tests,1742,"ecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for reference data); 3. ExtractCohortRecord (pre-variant context record)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857
https://github.com/broadinstitute/gatk/issues/7859:346,Availability,error,error,346,"## Bug Report. ### Affected tool(s) or class(es); GatherTranches. ### Affected version(s); Latest public release version 4.2.6.1. ### Description ; I ran `VariantRecalibrator` in scattered (using intervals) mode and now trying to gather the scattered tranches into a single file but somehow the number of novel variants is < 0. This is the exact error:; `Invalid tranche - no. variants is < 0 : known 90357410 novel -1894637320`. #### Steps to reproduce; ```; inputs_cmdl = ' '.join([f'--input {t}' for t in tranches]); j.command(; f""""""set -euo pipefail; gatk --java-options -Xms6g \\; GatherTranches \\; --mode SNP \\; {inputs_cmdl} \\; --output {j.out_tranches}""""""; ); ```. #### Expected behavior; Gathered scattered VQSLOD tranches into a single file. #### Actual behavior; Fails because of what seems like an integer overflow according to @ldgauthier; ```; org.broadinstitute.hellbender.exceptions.GATKException: Invalid tranche - no. variants is < 0 : known 90357410 novel -1894637320; 	at org.broadinstitute.hellbender.tools.walkers.vqsr.Tranche.<init>(Tranche.java:37); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.<init>(VQSLODTranche.java:37); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.mergeAndConvertTranches(VQSLODTranche.java:205); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.mergeAndConvertTranches(VQSLODTranche.java:139); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.GatherTranches.doWork(GatherTranches.java:80); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7859
https://github.com/broadinstitute/gatk/issues/7859:105,Deployability,release,release,105,"## Bug Report. ### Affected tool(s) or class(es); GatherTranches. ### Affected version(s); Latest public release version 4.2.6.1. ### Description ; I ran `VariantRecalibrator` in scattered (using intervals) mode and now trying to gather the scattered tranches into a single file but somehow the number of novel variants is < 0. This is the exact error:; `Invalid tranche - no. variants is < 0 : known 90357410 novel -1894637320`. #### Steps to reproduce; ```; inputs_cmdl = ' '.join([f'--input {t}' for t in tranches]); j.command(; f""""""set -euo pipefail; gatk --java-options -Xms6g \\; GatherTranches \\; --mode SNP \\; {inputs_cmdl} \\; --output {j.out_tranches}""""""; ); ```. #### Expected behavior; Gathered scattered VQSLOD tranches into a single file. #### Actual behavior; Fails because of what seems like an integer overflow according to @ldgauthier; ```; org.broadinstitute.hellbender.exceptions.GATKException: Invalid tranche - no. variants is < 0 : known 90357410 novel -1894637320; 	at org.broadinstitute.hellbender.tools.walkers.vqsr.Tranche.<init>(Tranche.java:37); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.<init>(VQSLODTranche.java:37); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.mergeAndConvertTranches(VQSLODTranche.java:205); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.mergeAndConvertTranches(VQSLODTranche.java:139); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.GatherTranches.doWork(GatherTranches.java:80); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7859
https://github.com/broadinstitute/gatk/pull/7860:170,Integrability,wrap,wrapping,170,"Setting a default batch size of 5 since that seemed to work out well in the two Stroke Anderson runs, overridable at the workflow level. Also cleaned up some logging and wrapping exceptions before throwing to avoid confusing stack traces when scanning through logs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7860
https://github.com/broadinstitute/gatk/pull/7860:209,Safety,avoid,avoid,209,"Setting a default batch size of 5 since that seemed to work out well in the two Stroke Anderson runs, overridable at the workflow level. Also cleaned up some logging and wrapping exceptions before throwing to avoid confusing stack traces when scanning through logs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7860
https://github.com/broadinstitute/gatk/pull/7860:158,Testability,log,logging,158,"Setting a default batch size of 5 since that seemed to work out well in the two Stroke Anderson runs, overridable at the workflow level. Also cleaned up some logging and wrapping exceptions before throwing to avoid confusing stack traces when scanning through logs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7860
https://github.com/broadinstitute/gatk/pull/7860:260,Testability,log,logs,260,"Setting a default batch size of 5 since that seemed to work out well in the two Stroke Anderson runs, overridable at the workflow level. Also cleaned up some logging and wrapping exceptions before throwing to avoid confusing stack traces when scanning through logs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7860
https://github.com/broadinstitute/gatk/pull/7861:125,Safety,redund,redundancies,125,1. Allows negative BAFs (so that we can process legacy files).; 2. Skips same-site vcf records so that we can glide over the redundancies in the dbsnp sites vcf.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7861
https://github.com/broadinstitute/gatk/issues/7865:171,Availability,Error,Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations,171,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:316,Availability,Error,Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations,316,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:429,Availability,error,error,429,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:512,Availability,error,errors,512,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:627,Availability,down,downloaded,627,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:988,Availability,error,error,988,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:5142,Availability,ERROR,ERROR,5142,"rFuncotations - Done initializing engine ; ; 02:00:35.260 INFO  ProgressMeter - Starting traversal ; ; 02:00:35.261 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute ; ; 02:00:35.262 INFO  FilterFuncotations - Starting pass 0 through the variants ; ; 02:00:35.778 ERROR FuncotationMap - Keys:  Gencode\_34\_hugoSymbol, Gencode\_34\_ncbiBuild, Gencode\_34\_chromosome, Gencode\_34\_start, Gencode\_34\_end, Gencode\_34\_variantClassification, Gencode\_34\_secondaryVariantClassification, Gencode\_34\_variantType, Gencode\_34\_refAllele, Gencode\_34\_tumorSeqAllele1, Gencode\_34\_tumorSeqAllele2, Gencode\_34\_genomeChange, Gencode\_34\_annotationTranscript, Gencode\_34\_transcriptStrand, Gencode\_34\_transcriptExon, Gencode\_34\_transcriptPos, Gencode\_34\_cDnaChange, Gencode\_34\_codonChange, Gencode\_34\_proteinChange, Gencode\_34\_gcContent, Gencode\_34\_referenceContext, Gencode\_34\_otherTranscripts, ACMGLMMLof\_LOF\_Mechanism, ACMGLMMLof\_Mode\_of\_Inheritance, ACMGLMMLof\_Notes, ACMG\_recommendation\_Disease\_Name, ClinVar\_VCF\_AF\_ESP, ClinVar\_VCF\_AF\_EXAC, ClinVar\_VCF\_AF\_TGP, ClinVar\_VCF\_ALLELEID, ClinVar\_VCF\_CLNDISDB, ClinVar\_VCF\_CLNDISDBINCL, ClinVar\_VCF\_CLNDN, ClinVar\_VCF\_CLNDNINCL, ClinVar\_VCF\_CLNHGVS, ClinVar\_VCF\_CLNREVSTAT, ClinVar\_VCF\_CLNSIG, ClinVar\_VCF\_CLNSIGCONF, ClinVar\_VCF\_CLNSIGINCL, ClinVar\_VCF\_CLNVC, ClinVar\_VCF\_CLNVCSO, ClinVar\_VCF\_CLNVI, ClinVar\_VCF\_DBVARID, ClinVar\_VCF\_GENEINFO, ClinVar\_VCF\_MC, ClinVar\_VCF\_ORIGIN, ClinVar\_VCF\_RS, ClinVar\_VCF\_SSR, ClinVar\_VCF\_ID, ClinVar\_VCF\_FILTER, LMMKnown\_LMM\_FLAGGED, LMMKnown\_ID, LMMKnown\_FILTER ; ; 02:00:35.778 ERROR FuncotationMap - Values:  , , , , , , , , , , , , , , , , , , , , , , , , , , , , false, ,  ; ; 02:00:35.793 INFO  FilterFuncotations - Shutting down engine ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:6525,Availability,ERROR,ERROR,6525,"4\_transcriptExon, Gencode\_34\_transcriptPos, Gencode\_34\_cDnaChange, Gencode\_34\_codonChange, Gencode\_34\_proteinChange, Gencode\_34\_gcContent, Gencode\_34\_referenceContext, Gencode\_34\_otherTranscripts, ACMGLMMLof\_LOF\_Mechanism, ACMGLMMLof\_Mode\_of\_Inheritance, ACMGLMMLof\_Notes, ACMG\_recommendation\_Disease\_Name, ClinVar\_VCF\_AF\_ESP, ClinVar\_VCF\_AF\_EXAC, ClinVar\_VCF\_AF\_TGP, ClinVar\_VCF\_ALLELEID, ClinVar\_VCF\_CLNDISDB, ClinVar\_VCF\_CLNDISDBINCL, ClinVar\_VCF\_CLNDN, ClinVar\_VCF\_CLNDNINCL, ClinVar\_VCF\_CLNHGVS, ClinVar\_VCF\_CLNREVSTAT, ClinVar\_VCF\_CLNSIG, ClinVar\_VCF\_CLNSIGCONF, ClinVar\_VCF\_CLNSIGINCL, ClinVar\_VCF\_CLNVC, ClinVar\_VCF\_CLNVCSO, ClinVar\_VCF\_CLNVI, ClinVar\_VCF\_DBVARID, ClinVar\_VCF\_GENEINFO, ClinVar\_VCF\_MC, ClinVar\_VCF\_ORIGIN, ClinVar\_VCF\_RS, ClinVar\_VCF\_SSR, ClinVar\_VCF\_ID, ClinVar\_VCF\_FILTER, LMMKnown\_LMM\_FLAGGED, LMMKnown\_ID, LMMKnown\_FILTER ; ; 02:00:35.778 ERROR FuncotationMap - Values:  , , , , , , , , , , , , , , , , , , , , , , , , , , , , false, ,  ; ; 02:00:35.793 INFO  FilterFuncotations - Shutting down engine ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53 ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:6676,Availability,down,down,6676,"_34\_proteinChange, Gencode\_34\_gcContent, Gencode\_34\_referenceContext, Gencode\_34\_otherTranscripts, ACMGLMMLof\_LOF\_Mechanism, ACMGLMMLof\_Mode\_of\_Inheritance, ACMGLMMLof\_Notes, ACMG\_recommendation\_Disease\_Name, ClinVar\_VCF\_AF\_ESP, ClinVar\_VCF\_AF\_EXAC, ClinVar\_VCF\_AF\_TGP, ClinVar\_VCF\_ALLELEID, ClinVar\_VCF\_CLNDISDB, ClinVar\_VCF\_CLNDISDBINCL, ClinVar\_VCF\_CLNDN, ClinVar\_VCF\_CLNDNINCL, ClinVar\_VCF\_CLNHGVS, ClinVar\_VCF\_CLNREVSTAT, ClinVar\_VCF\_CLNSIG, ClinVar\_VCF\_CLNSIGCONF, ClinVar\_VCF\_CLNSIGINCL, ClinVar\_VCF\_CLNVC, ClinVar\_VCF\_CLNVCSO, ClinVar\_VCF\_CLNVI, ClinVar\_VCF\_DBVARID, ClinVar\_VCF\_GENEINFO, ClinVar\_VCF\_MC, ClinVar\_VCF\_ORIGIN, ClinVar\_VCF\_RS, ClinVar\_VCF\_SSR, ClinVar\_VCF\_ID, ClinVar\_VCF\_FILTER, LMMKnown\_LMM\_FLAGGED, LMMKnown\_ID, LMMKnown\_FILTER ; ; 02:00:35.778 ERROR FuncotationMap - Values:  , , , , , , , , , , , , , , , , , , , , , , , , , , , , false, ,  ; ; 02:00:35.793 INFO  FilterFuncotations - Shutting down engine ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53 ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(St",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:7466,Energy Efficiency,Reduce,ReduceOps,7466,"MMKnown\_ID, LMMKnown\_FILTER ; ; 02:00:35.778 ERROR FuncotationMap - Values:  , , , , , , , , , , , , , , , , , , , , , , , , , , , , false, ,  ; ; 02:00:35.793 INFO  FilterFuncotations - Shutting down engine ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53 ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ;     at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ;     at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ;     at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ;     at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; ;     at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAlleleToFuncotationMapFromFuncotationVcfAttribute(FuncotatorUtils.java:2255) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:7497,Energy Efficiency,Reduce,ReduceOps,7497,"n\_FILTER ; ; 02:00:35.778 ERROR FuncotationMap - Values:  , , , , , , , , , , , , , , , , , , , , , , , , , , , , false, ,  ; ; 02:00:35.793 INFO  FilterFuncotations - Shutting down engine ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53 ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ;     at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ;     at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ;     at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ;     at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; ;     at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAlleleToFuncotationMapFromFuncotationVcfAttribute(FuncotatorUtils.java:2255) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFil",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:8016,Energy Efficiency,Reduce,ReduceOps,8016, ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ;     at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ;     at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ;     at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ;     at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; ;     at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAlleleToFuncotationMapFromFuncotationVcfAttribute(FuncotatorUtils.java:2255) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.buildArHetByGene(ArHetvarFilter.java:77) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.firstPassApply(ArHetvarFilter.java:50) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.firstPassApply(FilterFuncotations.java:161) ; ;     at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:17) ; ;     at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:8026,Energy Efficiency,Reduce,ReduceOp,8026, ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ;     at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ;     at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ;     at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ;     at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; ;     at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAlleleToFuncotationMapFromFuncotationVcfAttribute(FuncotatorUtils.java:2255) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.buildArHetByGene(ArHetvarFilter.java:77) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.firstPassApply(ArHetvarFilter.java:50) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.firstPassApply(FilterFuncotations.java:161) ; ;     at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:17) ; ;     at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:8054,Energy Efficiency,Reduce,ReduceOps,8054,titute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ;     at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ;     at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ;     at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ;     at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; ;     at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAlleleToFuncotationMapFromFuncotationVcfAttribute(FuncotatorUtils.java:2255) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.buildArHetByGene(ArHetvarFilter.java:77) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.firstPassApply(ArHetvarFilter.java:50) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.firstPassApply(FilterFuncotations.java:161) ; ;     at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:17) ; ;     at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ;     at org.bro,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:7935,Integrability,wrap,wrapAndCopyInto,7935,on: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53 ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ;     at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ;     at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ;     at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ;     at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; ;     at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAlleleToFuncotationMapFromFuncotationVcfAttribute(FuncotatorUtils.java:2255) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.buildArHetByGene(ArHetvarFilter.java:77) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.firstPassApply(ArHetvarFilter.java:50) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.firstPassApply(FilterFuncotations.java:161) ; ;     at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:17) ; ;     at org.broadinstitute.hellbender.engine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:9682,Integrability,wrap,wrapAndCopyInto,9682,ute.hellbender.tools.funcotator.FilterFuncotations.firstPassApply(FilterFuncotations.java:161) ; ;     at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:17) ; ;     at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ;     at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; ;     at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; ;     at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177) ; ;     at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) ; ;     at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ;     at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ;     at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ;     at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; ;     at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ; ;     at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497) ; ;     at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75) ; ;     at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:2427,Performance,Load,Loading,2427,"tute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf ; ; 02:00:34.173 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 02:00:34.368 INFO  FilterFuncotations - ------------------------------------------------------------ ; ; 02:00:34.369 INFO  FilterFuncotations - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 02:00:34.369 INFO  FilterFuncotations - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 02:00:34.369 INFO  FilterFuncotations - Executing as aru@BioinformaticsVM on Linux v5.13.0-39-generic amd64 ; ; 02:00:34.369 INFO  FilterFuncotations - Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04 ; ; 02:00:34.369 INFO  FilterFuncotations - Start Date/Time: April 25, 2022 at 2:00:34 AM EDT ; ; 02:00:34.369 INFO  FilterFuncotations - ------------------------------------------------------------ ; ; 02:00:34.369 INFO  FilterFuncotations - ------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7865:491,Security,Validat,ValidateVariants,491,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865
https://github.com/broadinstitute/gatk/issues/7866:2052,Availability,Redundant,Redundant,2052,"ded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022年5月20日 上",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866
https://github.com/broadinstitute/gatk/issues/7866:2185,Performance,Load,Loading,2185,"### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022年5月20日 上午11时58分13秒; 11:58:19.060 INFO GenotypeGVCFs - -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866
https://github.com/broadinstitute/gatk/issues/7866:2052,Safety,Redund,Redundant,2052,"ded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022年5月20日 上",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866
https://github.com/broadinstitute/gatk/issues/7866:2475,Safety,detect,detect,2475,"ing GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022年5月20日 上午11时58分13秒; 11:58:19.060 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.061 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 11:58:19.061 INFO GenotypeGVCFs - Picard Version: 2.23.3; 11:58:19.061 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866
https://github.com/broadinstitute/gatk/issues/7866:1272,Testability,test,test,1272,"ute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866
https://github.com/broadinstitute/gatk/issues/7866:1372,Testability,log,logs,1372,"ar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866
https://github.com/broadinstitute/gatk/issues/7872:170,Availability,error,error,170,## Bug Report. ### Affected tool(s) or class(es); Mutect2 in tumour-normal mode. ### Affected version(s); - 4.2.6.1; - 4.2.0.0. ### Description ; Mutect2 crashes with an error:; ```; 16:53:19.984 INFO Mutect2 - Shutting down engine; [25 May 2022 16:53:19 BST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=1632632832; java.lang.NullPointerException; 	at htsjdk.samtools.ComparableSamRecordIterator.compareTo(ComparableSamRecordIterator.java:68); 	at htsjdk.samtools.ComparableSamRecordIterator.compareTo(ComparableSamRecordIterator.java:36); 	at java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:656); 	at java.util.PriorityQueue.siftUp(PriorityQueue.java:647); 	at java.util.PriorityQueue.offer(PriorityQueue.java:344); 	at htsjdk.samtools.MergingSamRecordIterator.addIfNotEmpty(MergingSamRecordIterator.java:161); 	at htsjdk.samtools.MergingSamRecordIterator.<init>(MergingSamRecordIterator.java:94); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:429); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instan,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872
https://github.com/broadinstitute/gatk/issues/7872:220,Availability,down,down,220,## Bug Report. ### Affected tool(s) or class(es); Mutect2 in tumour-normal mode. ### Affected version(s); - 4.2.6.1; - 4.2.0.0. ### Description ; Mutect2 crashes with an error:; ```; 16:53:19.984 INFO Mutect2 - Shutting down engine; [25 May 2022 16:53:19 BST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=1632632832; java.lang.NullPointerException; 	at htsjdk.samtools.ComparableSamRecordIterator.compareTo(ComparableSamRecordIterator.java:68); 	at htsjdk.samtools.ComparableSamRecordIterator.compareTo(ComparableSamRecordIterator.java:36); 	at java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:656); 	at java.util.PriorityQueue.siftUp(PriorityQueue.java:647); 	at java.util.PriorityQueue.offer(PriorityQueue.java:344); 	at htsjdk.samtools.MergingSamRecordIterator.addIfNotEmpty(MergingSamRecordIterator.java:161); 	at htsjdk.samtools.MergingSamRecordIterator.<init>(MergingSamRecordIterator.java:94); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:429); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instan,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872
https://github.com/broadinstitute/gatk/issues/7872:2274,Availability,error,error,2274,ltiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; The error occurs when running a command: ; ```; gatk Mutect2 -R /home/genome/gatk.hg38/Homo_sapiens_assembly38.fasta -L panel_collapsed.bed -I bam/tumour_recalibrated.bam -I bam/normal_recalibrated.bam -tumor tumour -normal normal -germline-resource /home/genome/gatk.hg38/af-only-gnomad.hg38.vcf.gz -pon /home/genome/pon/PON_B1.vcf --genotype-pon-sites --f1r2-tar-gz results/learnOrientation/tumour_lo.tar.gz -O results/Mutect2/tumour.s.vcf.gz -bamout bam/tumour.mutect2.bam --disable-read-filter MateOnSameContigOrNoMappedMateReadFilter --af-of-alleles-not-in-resource 0.000001; ```. #### Expected behavior; Mutect2 producing outputs. #### Actual behavior; Full log: ; [Mutect2_error.txt](https://github.com/broadinstitute/gatk/files/8772744/Mutect2_error.txt). ---. I would be grateful if you could help me to investigate the cause of this error. I couldn't find any clues when googling it and tried `picard ValidateSamFile` but it returns no errors or warnings. Many thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872
https://github.com/broadinstitute/gatk/issues/7872:3113,Availability,error,error,3113,ltiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; The error occurs when running a command: ; ```; gatk Mutect2 -R /home/genome/gatk.hg38/Homo_sapiens_assembly38.fasta -L panel_collapsed.bed -I bam/tumour_recalibrated.bam -I bam/normal_recalibrated.bam -tumor tumour -normal normal -germline-resource /home/genome/gatk.hg38/af-only-gnomad.hg38.vcf.gz -pon /home/genome/pon/PON_B1.vcf --genotype-pon-sites --f1r2-tar-gz results/learnOrientation/tumour_lo.tar.gz -O results/Mutect2/tumour.s.vcf.gz -bamout bam/tumour.mutect2.bam --disable-read-filter MateOnSameContigOrNoMappedMateReadFilter --af-of-alleles-not-in-resource 0.000001; ```. #### Expected behavior; Mutect2 producing outputs. #### Actual behavior; Full log: ; [Mutect2_error.txt](https://github.com/broadinstitute/gatk/files/8772744/Mutect2_error.txt). ---. I would be grateful if you could help me to investigate the cause of this error. I couldn't find any clues when googling it and tried `picard ValidateSamFile` but it returns no errors or warnings. Many thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872
https://github.com/broadinstitute/gatk/issues/7872:3216,Availability,error,errors,3216,ltiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; The error occurs when running a command: ; ```; gatk Mutect2 -R /home/genome/gatk.hg38/Homo_sapiens_assembly38.fasta -L panel_collapsed.bed -I bam/tumour_recalibrated.bam -I bam/normal_recalibrated.bam -tumor tumour -normal normal -germline-resource /home/genome/gatk.hg38/af-only-gnomad.hg38.vcf.gz -pon /home/genome/pon/PON_B1.vcf --genotype-pon-sites --f1r2-tar-gz results/learnOrientation/tumour_lo.tar.gz -O results/Mutect2/tumour.s.vcf.gz -bamout bam/tumour.mutect2.bam --disable-read-filter MateOnSameContigOrNoMappedMateReadFilter --af-of-alleles-not-in-resource 0.000001; ```. #### Expected behavior; Mutect2 producing outputs. #### Actual behavior; Full log: ; [Mutect2_error.txt](https://github.com/broadinstitute/gatk/files/8772744/Mutect2_error.txt). ---. I would be grateful if you could help me to investigate the cause of this error. I couldn't find any clues when googling it and tried `picard ValidateSamFile` but it returns no errors or warnings. Many thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872
https://github.com/broadinstitute/gatk/issues/7872:3181,Security,Validat,ValidateSamFile,3181,ltiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; The error occurs when running a command: ; ```; gatk Mutect2 -R /home/genome/gatk.hg38/Homo_sapiens_assembly38.fasta -L panel_collapsed.bed -I bam/tumour_recalibrated.bam -I bam/normal_recalibrated.bam -tumor tumour -normal normal -germline-resource /home/genome/gatk.hg38/af-only-gnomad.hg38.vcf.gz -pon /home/genome/pon/PON_B1.vcf --genotype-pon-sites --f1r2-tar-gz results/learnOrientation/tumour_lo.tar.gz -O results/Mutect2/tumour.s.vcf.gz -bamout bam/tumour.mutect2.bam --disable-read-filter MateOnSameContigOrNoMappedMateReadFilter --af-of-alleles-not-in-resource 0.000001; ```. #### Expected behavior; Mutect2 producing outputs. #### Actual behavior; Full log: ; [Mutect2_error.txt](https://github.com/broadinstitute/gatk/files/8772744/Mutect2_error.txt). ---. I would be grateful if you could help me to investigate the cause of this error. I couldn't find any clues when googling it and tried `picard ValidateSamFile` but it returns no errors or warnings. Many thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872
https://github.com/broadinstitute/gatk/issues/7872:2934,Testability,log,log,2934,ltiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; The error occurs when running a command: ; ```; gatk Mutect2 -R /home/genome/gatk.hg38/Homo_sapiens_assembly38.fasta -L panel_collapsed.bed -I bam/tumour_recalibrated.bam -I bam/normal_recalibrated.bam -tumor tumour -normal normal -germline-resource /home/genome/gatk.hg38/af-only-gnomad.hg38.vcf.gz -pon /home/genome/pon/PON_B1.vcf --genotype-pon-sites --f1r2-tar-gz results/learnOrientation/tumour_lo.tar.gz -O results/Mutect2/tumour.s.vcf.gz -bamout bam/tumour.mutect2.bam --disable-read-filter MateOnSameContigOrNoMappedMateReadFilter --af-of-alleles-not-in-resource 0.000001; ```. #### Expected behavior; Mutect2 producing outputs. #### Actual behavior; Full log: ; [Mutect2_error.txt](https://github.com/broadinstitute/gatk/files/8772744/Mutect2_error.txt). ---. I would be grateful if you could help me to investigate the cause of this error. I couldn't find any clues when googling it and tried `picard ValidateSamFile` but it returns no errors or warnings. Many thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872
https://github.com/broadinstitute/gatk/issues/7872:2646,Usability,learn,learnOrientation,2646,ltiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; The error occurs when running a command: ; ```; gatk Mutect2 -R /home/genome/gatk.hg38/Homo_sapiens_assembly38.fasta -L panel_collapsed.bed -I bam/tumour_recalibrated.bam -I bam/normal_recalibrated.bam -tumor tumour -normal normal -germline-resource /home/genome/gatk.hg38/af-only-gnomad.hg38.vcf.gz -pon /home/genome/pon/PON_B1.vcf --genotype-pon-sites --f1r2-tar-gz results/learnOrientation/tumour_lo.tar.gz -O results/Mutect2/tumour.s.vcf.gz -bamout bam/tumour.mutect2.bam --disable-read-filter MateOnSameContigOrNoMappedMateReadFilter --af-of-alleles-not-in-resource 0.000001; ```. #### Expected behavior; Mutect2 producing outputs. #### Actual behavior; Full log: ; [Mutect2_error.txt](https://github.com/broadinstitute/gatk/files/8772744/Mutect2_error.txt). ---. I would be grateful if you could help me to investigate the cause of this error. I couldn't find any clues when googling it and tried `picard ValidateSamFile` but it returns no errors or warnings. Many thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872
https://github.com/broadinstitute/gatk/issues/7873:1875,Integrability,message,messages,1875,"DUPLICATE);; filters.add(ReadFilterLibrary.PASSES_VENDOR_QUALITY_CHECK);; filters.add(ReadFilterLibrary.NON_ZERO_REFERENCE_LENGTH_ALIGNMENT);; filters.add(ReadFilterLibrary.GOOD_CIGAR);; filters.add(new WellformedReadFilter());. return filters;; }; ```. And the `WellformedReadFilter` includes these filters:; ```; private void createFilter() {; final AlignmentAgreesWithHeaderReadFilter alignmentAgreesWithHeader = new AlignmentAgreesWithHeaderReadFilter(samHeader);. wellFormedFilter = ReadFilterLibrary.VALID_ALIGNMENT_START; .and(ReadFilterLibrary.VALID_ALIGNMENT_END); .and(alignmentAgreesWithHeader); .and(ReadFilterLibrary.HAS_READ_GROUP); .and(ReadFilterLibrary.HAS_MATCHING_BASES_AND_QUALS); .and(ReadFilterLibrary.READLENGTH_EQUALS_CIGARLENGTH); .and(ReadFilterLibrary.SEQ_IS_STORED); .and(ReadFilterLibrary.CIGAR_CONTAINS_NO_N_OPERATOR);; }; ```. These all seem sensible and consistent, but I was finding that when I would apply these filters to the output of other tools like samtools view, I'd get different number of reported reads compared to what seemed to be being discovered within HaplotypeCaller (via adding debug messages). For example, using this SRA sample: https://www.ncbi.nlm.nih.gov/sra/SRR12324251 and mapping the reads against these references using minimap2: https://s3.amazonaws.com/zymo-files/BioPool/ZymoBIOMICS.STD.refseq.v2.zip, and then observing the alignments that appear at position 97201 on the main E. Coli Chromosome we can see that there are 873 alignments overlapping that position:; `samtools view gatk_bams/Escherichia_coli_complete_genome.fasta.SRR12324251_1.fastq.bam.bam Escherichia_coli_chromosome:97201-97201 | less -S`. Applying the above filters to these alignments reports 871 alignments. However, GATK HaplotypeCaller reports only 743 valid alignments (before variant calling). Below is some code where I added debug messages to retrieve the read names that HaplotypeCaller determines to be valid:. *~line 476 of `HaplotypeCallerEngine.java`*; ``",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873
https://github.com/broadinstitute/gatk/issues/7873:2613,Integrability,message,messages,2613,"tent, but I was finding that when I would apply these filters to the output of other tools like samtools view, I'd get different number of reported reads compared to what seemed to be being discovered within HaplotypeCaller (via adding debug messages). For example, using this SRA sample: https://www.ncbi.nlm.nih.gov/sra/SRR12324251 and mapping the reads against these references using minimap2: https://s3.amazonaws.com/zymo-files/BioPool/ZymoBIOMICS.STD.refseq.v2.zip, and then observing the alignments that appear at position 97201 on the main E. Coli Chromosome we can see that there are 873 alignments overlapping that position:; `samtools view gatk_bams/Escherichia_coli_complete_genome.fasta.SRR12324251_1.fastq.bam.bam Escherichia_coli_chromosome:97201-97201 | less -S`. Applying the above filters to these alignments reports 871 alignments. However, GATK HaplotypeCaller reports only 743 valid alignments (before variant calling). Below is some code where I added debug messages to retrieve the read names that HaplotypeCaller determines to be valid:. *~line 476 of `HaplotypeCallerEngine.java`*; ```; public ActivityProfileState isActive( final AlignmentContext context, final ReferenceContext ref, final FeatureContext features ) {; if ( forceCallingAllelesPresent && features.getValues(hcArgs.alleles, ref).stream().anyMatch(vc -> hcArgs.forceCallFiltered || vc.isNotFiltered())) {; return new ActivityProfileState(ref.getInterval(), 1.0);; }. if( context == null || context.getBasePileup().isEmpty() ) {; // if we don't have any data, just abort early; return new ActivityProfileState(ref.getInterval(), 0.0);; }. final boolean debug = (context.getPosition() - 1) % 100000 == 0 || (context.getPosition() >= 97200 && context.getPosition() <= 97350) || (context.getPosition() >= 641000 && context.getPosition() <= 650000);; if (debug) {; System.out.println(""Position "" + context.getPosition() + "" Reads at position "" + context.size());; if (context.getPosition() == 97201 && context.getCon",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873
https://github.com/broadinstitute/gatk/issues/7873:3187,Safety,abort,abort,3187,"li Chromosome we can see that there are 873 alignments overlapping that position:; `samtools view gatk_bams/Escherichia_coli_complete_genome.fasta.SRR12324251_1.fastq.bam.bam Escherichia_coli_chromosome:97201-97201 | less -S`. Applying the above filters to these alignments reports 871 alignments. However, GATK HaplotypeCaller reports only 743 valid alignments (before variant calling). Below is some code where I added debug messages to retrieve the read names that HaplotypeCaller determines to be valid:. *~line 476 of `HaplotypeCallerEngine.java`*; ```; public ActivityProfileState isActive( final AlignmentContext context, final ReferenceContext ref, final FeatureContext features ) {; if ( forceCallingAllelesPresent && features.getValues(hcArgs.alleles, ref).stream().anyMatch(vc -> hcArgs.forceCallFiltered || vc.isNotFiltered())) {; return new ActivityProfileState(ref.getInterval(), 1.0);; }. if( context == null || context.getBasePileup().isEmpty() ) {; // if we don't have any data, just abort early; return new ActivityProfileState(ref.getInterval(), 0.0);; }. final boolean debug = (context.getPosition() - 1) % 100000 == 0 || (context.getPosition() >= 97200 && context.getPosition() <= 97350) || (context.getPosition() >= 641000 && context.getPosition() <= 650000);; if (debug) {; System.out.println(""Position "" + context.getPosition() + "" Reads at position "" + context.size());; if (context.getPosition() == 97201 && context.getContig().equals(""Escherichia_coli_chromosome"")) {; System.out.println(""##READS"");; context.getBasePileup().getReads().forEach(read -> System.out.println(read.getName()));; System.out.println(""##END READS"");; }; }; ...; ...; }; ```. So HaplotypeCaller is filtering more reads which is fine, but the problem appears when we look at which alignments are actually being filtered. Below is a table with four example alignments at the previously described position on the E. coli genome. The main output from samtools view is included as well as a column describ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873
https://github.com/broadinstitute/gatk/issues/7873:80,Testability,benchmark,benchmarks,80,"----. ## Bug Report; Hi GATK team,. I've just been running some variant calling benchmarks on a set of bacterial isolates and I was trying to understand how GATK HaplotypeCaller chooses which reads to use in its analyses. From what I could tell within the code, HaplotypeCaller sets these default read filters:. *line ~336 `HaplotypeCallerEngine.java`*; ```; public static List<ReadFilter> makeStandardHCReadFilters() {; List<ReadFilter> filters = new ArrayList<>();; filters.add(new MappingQualityReadFilter(DEFAULT_READ_QUALITY_FILTER_THRESHOLD));; filters.add(ReadFilterLibrary.MAPPING_QUALITY_AVAILABLE);; filters.add(ReadFilterLibrary.MAPPED);; filters.add(ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);; filters.add(ReadFilterLibrary.NOT_DUPLICATE);; filters.add(ReadFilterLibrary.PASSES_VENDOR_QUALITY_CHECK);; filters.add(ReadFilterLibrary.NON_ZERO_REFERENCE_LENGTH_ALIGNMENT);; filters.add(ReadFilterLibrary.GOOD_CIGAR);; filters.add(new WellformedReadFilter());. return filters;; }; ```. And the `WellformedReadFilter` includes these filters:; ```; private void createFilter() {; final AlignmentAgreesWithHeaderReadFilter alignmentAgreesWithHeader = new AlignmentAgreesWithHeaderReadFilter(samHeader);. wellFormedFilter = ReadFilterLibrary.VALID_ALIGNMENT_START; .and(ReadFilterLibrary.VALID_ALIGNMENT_END); .and(alignmentAgreesWithHeader); .and(ReadFilterLibrary.HAS_READ_GROUP); .and(ReadFilterLibrary.HAS_MATCHING_BASES_AND_QUALS); .and(ReadFilterLibrary.READLENGTH_EQUALS_CIGARLENGTH); .and(ReadFilterLibrary.SEQ_IS_STORED); .and(ReadFilterLibrary.CIGAR_CONTAINS_NO_N_OPERATOR);; }; ```. These all seem sensible and consistent, but I was finding that when I would apply these filters to the output of other tools like samtools view, I'd get different number of reported reads compared to what seemed to be being discovered within HaplotypeCaller (via adding debug messages). For example, using this SRA sample: https://www.ncbi.nlm.nih.gov/sra/SRR12324251 and mapping the reads against thes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873
https://github.com/broadinstitute/gatk/pull/7874:42,Testability,test,test,42,"Similar issues to last time. Do we have a test case yet that simulates AoU's usage?. In addition to this, the workflow seems to function on an older version of GVS, as long as I make the following modifications:. - Add columns `sample_info.withdrawn`, `sample_info.is_control`; - Backfill true/false accordingly to these columns (backfilling withdrawn seems unnecessary)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7874
https://github.com/broadinstitute/gatk/pull/7880:52,Deployability,integrat,integration,52,"~Draft PR for mobbing discussion~ Ready for review, integration tested [here](https://app.terra.bio/#workspaces/broad-firecloud-dsde/VS-415%20GVS%20Quickstart%20Default%20Extract%20Scatter/job_history/7ef604ff-46e8-45d9-be39-e88276db993b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7880
https://github.com/broadinstitute/gatk/pull/7880:52,Integrability,integrat,integration,52,"~Draft PR for mobbing discussion~ Ready for review, integration tested [here](https://app.terra.bio/#workspaces/broad-firecloud-dsde/VS-415%20GVS%20Quickstart%20Default%20Extract%20Scatter/job_history/7ef604ff-46e8-45d9-be39-e88276db993b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7880
https://github.com/broadinstitute/gatk/pull/7880:64,Testability,test,tested,64,"~Draft PR for mobbing discussion~ Ready for review, integration tested [here](https://app.terra.bio/#workspaces/broad-firecloud-dsde/VS-415%20GVS%20Quickstart%20Default%20Extract%20Scatter/job_history/7ef604ff-46e8-45d9-be39-e88276db993b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7880
https://github.com/broadinstitute/gatk/pull/7881:0,Deployability,Update,Updates,0,Updates expectations for the X/Y scaling changes that went in a couple of weeks ago and slip in a few unrelated minor improvements.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7881
https://github.com/broadinstitute/gatk/issues/7884:108,Deployability,release,release,108,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x] Latest public release version [4.2.6.1]; - [x] Latest master branch (probably). ### Description ; First position on a contig can be missing if that position is low quality in the input GVCF. #### Steps to reproduce; Run Reblock GVCF with the following parameters: --floor-blocks true --gvcf-gq-bands 20 --gvcf-gq-bands 30 --gvcf-gq-bands 40 --do-qual-score-approximation true --variant $inputVC -R $hg38. where inputVC contains; chr13	18173860	.	A	C,<NON_REF>	0	.	AS_RAW_BaseQRankSum=|-4.9,1|NaN;AS_RAW_MQ=51256.00|4709.00|0.00;AS_RAW_MQRankSum=|0.5,1|NaN;AS_RAW_ReadPosRankSum=|1.2,1|NaN;AS_SB_TABLE=23,2|1,1|0,0;BaseQRankSum=-4.896;DP=28;ExcessHet=0.0000;MLEAC=0,0;MLEAF=0.00,0.00;MQRankSum=0.564;RAW_MQandDP=59565,28;ReadPosRankSum=1.252	GT:AD:DP:GQ:PL:SB	0/0:25,2,0:27:61:0,61,946,75,951,965:23,2,1,1; appears to be dropped in output. Full input GVCF at gs://broad-dsde-methods-gauthier/reblocking-bug/. #### Expected behavior; QUAL 0 VC should be replaced with a GQ0 reference block. #### Actual behavior; Output GVCF is missing position chr13:18173860 and fails ValidateVCF task",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7884
https://github.com/broadinstitute/gatk/issues/7884:1162,Security,Validat,ValidateVCF,1162,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x] Latest public release version [4.2.6.1]; - [x] Latest master branch (probably). ### Description ; First position on a contig can be missing if that position is low quality in the input GVCF. #### Steps to reproduce; Run Reblock GVCF with the following parameters: --floor-blocks true --gvcf-gq-bands 20 --gvcf-gq-bands 30 --gvcf-gq-bands 40 --do-qual-score-approximation true --variant $inputVC -R $hg38. where inputVC contains; chr13	18173860	.	A	C,<NON_REF>	0	.	AS_RAW_BaseQRankSum=|-4.9,1|NaN;AS_RAW_MQ=51256.00|4709.00|0.00;AS_RAW_MQRankSum=|0.5,1|NaN;AS_RAW_ReadPosRankSum=|1.2,1|NaN;AS_SB_TABLE=23,2|1,1|0,0;BaseQRankSum=-4.896;DP=28;ExcessHet=0.0000;MLEAC=0,0;MLEAF=0.00,0.00;MQRankSum=0.564;RAW_MQandDP=59565,28;ReadPosRankSum=1.252	GT:AD:DP:GQ:PL:SB	0/0:25,2,0:27:61:0,61,946,75,951,965:23,2,1,1; appears to be dropped in output. Full input GVCF at gs://broad-dsde-methods-gauthier/reblocking-bug/. #### Expected behavior; QUAL 0 VC should be replaced with a GQ0 reference block. #### Actual behavior; Output GVCF is missing position chr13:18173860 and fails ValidateVCF task",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7884
https://github.com/broadinstitute/gatk/issues/7886:89,Deployability,update,updated,89,"## Feature request. ### Tool(s) or class(es) involved; The docker image would need to be updated (gatkbase docker image). See line ~17 in `scripts/docker/gatkbase/Dockerfile`. ### Description; Can you include a later version of samtools in the GATK image? The current samtools version (1.7) does not support crams. ; I believe that you would need to update gatkbase to make this change. . Additional suggestion, which should not be a requirement for closing this issue: you may want to fix the versions of the software in gatkbase (lines ~6-25 in `scripts/docker/gatkbase/Dockerfile`). Note: I have only replicated this issue in `us.gcr.io/broad-gatk/gatk:4.2.6.1`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7886
https://github.com/broadinstitute/gatk/issues/7886:350,Deployability,update,update,350,"## Feature request. ### Tool(s) or class(es) involved; The docker image would need to be updated (gatkbase docker image). See line ~17 in `scripts/docker/gatkbase/Dockerfile`. ### Description; Can you include a later version of samtools in the GATK image? The current samtools version (1.7) does not support crams. ; I believe that you would need to update gatkbase to make this change. . Additional suggestion, which should not be a requirement for closing this issue: you may want to fix the versions of the software in gatkbase (lines ~6-25 in `scripts/docker/gatkbase/Dockerfile`). Note: I have only replicated this issue in `us.gcr.io/broad-gatk/gatk:4.2.6.1`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7886
https://github.com/broadinstitute/gatk/pull/7888:0,Deployability,Integrat,Integration,0,Integration test successful https://app.terra.bio/#workspaces/broad-firecloud-dsde/VS-415%20GVS%20Quickstart%20Default%20Extract%20Scatter/job_history/73ac71db-0488-46be-a8e8-7f00e795edec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7888
https://github.com/broadinstitute/gatk/pull/7888:0,Integrability,Integrat,Integration,0,Integration test successful https://app.terra.bio/#workspaces/broad-firecloud-dsde/VS-415%20GVS%20Quickstart%20Default%20Extract%20Scatter/job_history/73ac71db-0488-46be-a8e8-7f00e795edec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7888
https://github.com/broadinstitute/gatk/pull/7888:12,Testability,test,test,12,Integration test successful https://app.terra.bio/#workspaces/broad-firecloud-dsde/VS-415%20GVS%20Quickstart%20Default%20Extract%20Scatter/job_history/73ac71db-0488-46be-a8e8-7f00e795edec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7888
https://github.com/broadinstitute/gatk/pull/7889:38,Deployability,integrat,integration,38,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:294,Deployability,update,updates,294,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:38,Integrability,integrat,integration,38,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:272,Safety,detect,detect,272,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:50,Testability,test,test,50,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:96,Testability,test,tests,96,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:146,Testability,test,test,146,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:255,Testability,test,test,255,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:392,Testability,test,test,392,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:482,Testability,test,test,482,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/pull/7889:686,Testability,test,test,686,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889
https://github.com/broadinstitute/gatk/issues/7890:260,Performance,multi-thread,multi-threaded,260,"## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_; [DepthOfCoverage](https://gatk.broadinstitute.org/hc/en-us/articles/360041851491-DepthOfCoverage-BETA-). ### Description. Are there plans to make DepthOfCoverage multi-threaded? If not, would it be possible to require such improvements?. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7890
https://github.com/broadinstitute/gatk/pull/7894:24,Availability,down,down,24,A wrapper WDL that cuts down on excess params for our Beta users,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7894
https://github.com/broadinstitute/gatk/pull/7894:2,Integrability,wrap,wrapper,2,A wrapper WDL that cuts down on excess params for our Beta users,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7894
https://github.com/broadinstitute/gatk/issues/7898:1708,Availability,down,down,1708,"zed setting. ### Tool(s) or class(es) involved; GenomicsDBImport v.4.2.6.1 (current). ### Description ; As far as I understand it the joint germline variant calling process is like this (imagine 100 samples):; 1. Call variants using `Haplotypecaller` using the gVCF output flag for each sample; 2. use the multiple gVCFs (1 per sample) and a set of intervals (WGS_intervals.bed as an example) to build a Genomics DB store using `GenomicsDBImport`; 3. Use `GenotypeGVCFs` using the output of `GenomicsDBImport` as the input to consolidate the multiple samples into 1 multi-sample vcf. My question comes from the parallelization/interval splitting during step 2. If I parallelize the GenomicsDBImport across each interval. I would end up with ~300 intervals and subsequently, ~300 GenomicsDB directory paths since I am not adding new samples to an existing DB, then the specified output DB path, ""Must be an empty or non-existent directory"", which will contain the relevant interval calls for the 100 samples. . Am I supposed to use the 300 directory paths as input into a single `GenotypeGVCFs` call? Or process each of the 300 intervals into 300 multi-sample vcf files (each with 100 samples) and then merge those into a single vcf file using `GatherVcfs` or some other merging tool. The examples posted and documentation for `GenomicsDBImport` relay the need for intervals to work effectively, and so does [an old broad lecture recording](https://www.youtube.com/watch?v=XrHt5yBlp80&t=1243s). . Essentially it boils down to when and how to process and merge the same set of samples (100) over the many intervals (300). If I had 300 compute nodes (as an example) I want to parallelize as much of this as possible. so that each node can process an interval set, and at the end of the process I have 1 VCF file with 100 samples covering the entire range of intervals. I hope that was clear. Please let me know if you need any more info, or if I should be asking somewhere else. Thanks in advance!. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7898
https://github.com/broadinstitute/gatk/issues/7898:2073,Usability,clear,clear,2073,"zed setting. ### Tool(s) or class(es) involved; GenomicsDBImport v.4.2.6.1 (current). ### Description ; As far as I understand it the joint germline variant calling process is like this (imagine 100 samples):; 1. Call variants using `Haplotypecaller` using the gVCF output flag for each sample; 2. use the multiple gVCFs (1 per sample) and a set of intervals (WGS_intervals.bed as an example) to build a Genomics DB store using `GenomicsDBImport`; 3. Use `GenotypeGVCFs` using the output of `GenomicsDBImport` as the input to consolidate the multiple samples into 1 multi-sample vcf. My question comes from the parallelization/interval splitting during step 2. If I parallelize the GenomicsDBImport across each interval. I would end up with ~300 intervals and subsequently, ~300 GenomicsDB directory paths since I am not adding new samples to an existing DB, then the specified output DB path, ""Must be an empty or non-existent directory"", which will contain the relevant interval calls for the 100 samples. . Am I supposed to use the 300 directory paths as input into a single `GenotypeGVCFs` call? Or process each of the 300 intervals into 300 multi-sample vcf files (each with 100 samples) and then merge those into a single vcf file using `GatherVcfs` or some other merging tool. The examples posted and documentation for `GenomicsDBImport` relay the need for intervals to work effectively, and so does [an old broad lecture recording](https://www.youtube.com/watch?v=XrHt5yBlp80&t=1243s). . Essentially it boils down to when and how to process and merge the same set of samples (100) over the many intervals (300). If I had 300 compute nodes (as an example) I want to parallelize as much of this as possible. so that each node can process an interval set, and at the end of the process I have 1 VCF file with 100 samples covering the entire range of intervals. I hope that was clear. Please let me know if you need any more info, or if I should be asking somewhere else. Thanks in advance!. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7898
https://github.com/broadinstitute/gatk/pull/7902:300,Deployability,integrat,integration,300,Label names are standard from https://docs.google.com/document/d/1eyPrpCSCMyf1CaBkc-aDi39xV7Bq9hthh07ludpgPNw. branch adds labels to:; - `bg query` calls in WDL command blocks; - queries run within python scripts called by WDLs; - datasets created with Utils.BuildGATKJarAndCreateDataset. Quickstart integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/15442e08-92af-48bc-ab96-2bdd4f39a801,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7902
https://github.com/broadinstitute/gatk/pull/7902:300,Integrability,integrat,integration,300,Label names are standard from https://docs.google.com/document/d/1eyPrpCSCMyf1CaBkc-aDi39xV7Bq9hthh07ludpgPNw. branch adds labels to:; - `bg query` calls in WDL command blocks; - queries run within python scripts called by WDLs; - datasets created with Utils.BuildGATKJarAndCreateDataset. Quickstart integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/15442e08-92af-48bc-ab96-2bdd4f39a801,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7902
https://github.com/broadinstitute/gatk/issues/7904:502,Usability,simpl,simple,502,"This request was created from a contribution made by Brian Wiley on March 18, 2022 22:33 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360073119131-MuTect2-annotations#community\_comment\_4797484535835](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073119131-MuTect2-annotations#community_comment_4797484535835). \--. 394224746911, do you think it would be possible for Broad to start documenting things like this and most of the tags in your VCF files.  More like simple things not like the algorithm for clustered events.  For instance I have variant from which I can get the depth from the bam file with samtools with a variant called by Mutect at position chr17:60663118 C>T.  The depth of my bam file at this position is 170 (after removing duplicates; 222 before removing duplicates which is what shows in IGV). $ samtools depth C484.TCGA-19-2620-10A-01D-1495-08.5\_gdc\_realn.bam -r chr17:60663117-60663119 ; ; chr17    60663117    172 ; ; chr17    60663118    170 ; ; chr17    60663119    173. This coincides exactly with IGV (also shows 170 for position 60663118 when remove duplicates is turned on). $ samtools view C484.TCGA-19-2620-10A-01D-1495-08.5\_gdc\_realn.bam chr17:60663118-60663118 | wc -l ; ; 222. However Mutect has 2 depth DP tags, an INFO/DP and a FMT/DP.  The format DP shows a depth of 163 at this position which makes sense that it's at least lower.  But the INFO/DP shows a depth of DP=318 which makes almost zero sense (that's almost double!!) and in the VCF file it even indicates some reads are filtered... ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">. Additionally the AS\_SB\_TABLE shows ""164,135|3,2"" in which the total is 304 which again makes almost zero sense.  So from a researcher perspective it is absolutely necessary to know how Mutect2 is calculating these numbers else the strand bias filter cannot be trusted at all.  I am using version 4.2.1.0 of GATK an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7904
https://github.com/broadinstitute/gatk/issues/7904:4169,Usability,Learn,LearnReadOrientationModel,4169,"ON\_FISHER=0;CSQ=T|stop\_gained|HIGH|PPM1D|ENSG00000170836|Transcript|ENST00000305921.8|protein\_coding|6/6||ENST00000305921.8:c.1384C>T|ENSP ; ; 00000306682.2:p.Gln462Ter|1606|1384|462|Q/\*|Caa/Taa|CM131995&COSV59955543||1||SNV|HGNC|HGNC:9277|YES|NM\_003620.4||1|P1|CCDS11625.1|ENSP00000306682|O15297.184|A ; ; 0A0S2Z4M2.32|UPI0000130FE8|O15297-1||Ensembl||C|C||1||||||||||||||||||||||||||0&1|1&1||||||||MAGLYSLGVSVFSDQGGRKYMEDVTQIVVEPEPTAEEKPSPRRSLSQPLPPRPSPAALPGGEVSGK ; ; GPAVAAREARDPLPDAGASPAPSRCCRRRSSVAFFAVCDGHGGREAAQFAREHLWGFIKKQKGFTSSEPAKVCAAIRKGFLACHLAMWKKLAEWPKTMTGLPSTSGTTASVVIIRGMKMYVAHVGDSGVVLGIQDDPKDDFVRAVEVTQDHKPELPKER ; ; ERIEGLGGSVMNKSGVNRVVWKRPRLTHNGPVRRSTVIDQIPFLAVARALGDLWSYDFFSGEFVVSPEPDTSVHTLDPQKHKYIILGSDGLWNMIPPQDAISMCQDQEEKKYLMGEHGQSCAKMLVNRALGRWRQRMLRADNTSAIVICISPEVDNQGN ; ; FTNEDELYLNLTDSPSYNSQETCVMTPSPCSTPPVKSLEEDPWPRVNSKDHIPALVRSNAFSENFLEVSAEIARENVQGVVIPSKDPEPLEENCAKALTLRIHDSLNNSLPIGLVPTNSTNTVMDQKNLKMSTPGQMKAQEIERTPPTNFKRTLEESNS ; ; GPLMKKHRRNGLSRSSGAQPASLPTTSQRKNSVKLTMRRRLRGQKKIGNPLLHQHRKTVCVC||||||||||||||||||||||||||||||    GT:AD:AF:DP:F1R2:F2R1:SB        0/1:158,5:0.031:163:70,2:79,2:8 ; ; 7,71,3,2. Call (your guys also sequenced this sample for the TCGA since the center is **\-08** TCGA-19-2620-10A-01D-1495-08):. NORMAL=$(samtools view -H $normal\_bam | /usr/bin/perl -nE 'say $1 if /^\\@RG.+\\tSM:(\[ -~\]+)/' | head -n 1) ; ; TUMOR=$(samtools view -H $tumor\_bam | /usr/bin/perl -nE 'say $1 if /^\\@RG.+\\tSM:(\[ -~\]+)/' | head -n 1). /gatk/gatk Mutect2 --java-options ""-Xmx8g"" -O $1 -R $2 -I $3 -tumor ""$TUMOR"" -I $4 -normal ""$NORMAL"" -L $5 --f1r2-tar-gz f1r2.tar.gz #Running Mutect2. /gatk/gatk LearnReadOrientationModel -I f1r2.tar.gz -O artifact.priors.tar.gz. /gatk/gatk FilterMutectCalls --java-options ""-Xmx8g"" -R $2 -V $1 -O $6 -ob-priors artifact.priors.tar.gz #Running FilterMutectCalls on the output vcf.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/278802'>Zendesk ticket #278802</a>)<br> gz#278802</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7904
https://github.com/broadinstitute/gatk/pull/7906:0,Deployability,update,update,0,update warp!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7906
https://github.com/broadinstitute/gatk/issues/7914:687,Energy Efficiency,reduce,reduce,687,"## Bug Report. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); - gatk 4.2.6.1. ### Description ; I produced the bam files using STAR, and adjusted the MQ value to 60. I then used sambamba markdup to mark duplicate, then I proceeded to use SplitNCigarReads. The CPU load for SplitNCigarReads was very high and at certain times can spike up to 2400%. I tried limiting the cpu usage with commands like `-XX:ParallelGCThreads=1` and `-XX:ConcGCThreads=1`, but it doesn't seem to have an effect. (The cpu usage sometimes do stay at 100%) I also adjusted the MQ value in STAR to lessen the load in SplitNCigarReads. I also tried to increase the read size to reduce I/O time.; ![image](https://user-images.githubusercontent.com/106958825/175206165-08b28567-d671-45fa-b033-f20c4792edb7.png). #### Steps to reproduce; STAR; ```; STAR \; --genomeDir ${star_reference_path} \; --runThreadN 16 \; --readFilesIn ${file_1} ${file_2} \; --readFilesCommand ""gunzip -c"" \; --sjdbOverhang 149 \; --outSAMtype BAM SortedByCoordinate \; --outBAMsortingThreadN 16 \; --outSAMmultNmax 1 \; --outSAMmapqUnique 60 \; --outSAMattrRGline ID:${id} LB:RNASEQ SM:${sample_name} PL:ILLUMINA PU:${platform_unit} PM:${instrument_id} \; --limitBAMsortRAM 50000000000 \; --twopassMode Basic \; --outFileNamePrefix /rawdata/rnaseq/clean/bam/1.; ```. Mark Duplicate; ```; sambamba markdup \; -t 4 \; --tmpdir=/tmp \; --hash-table-size=262144 \; --overflow-list-size=67108864 \; /rawdata/rnaseq/clean/bam/1.Aligned.sortedByCoord.out.bam \; /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; ```. SplitNCigarReads; ```; gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx20G -XX:ParallelGCThreads=1 -XX:ConcGCThreads=1"" SplitNCigarReads \; -R ${reference_path} \; --tmp-dir /tmp \; -I /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; -O /rawdata/rnaseq/clean/bam_gatk/1.aligned.duplicate_marked.sorted.bam \; --create-output-bam-md5 TRUE \; --max-reads-in-memory 1000000 \; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7914
https://github.com/broadinstitute/gatk/issues/7914:300,Performance,load,load,300,"## Bug Report. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); - gatk 4.2.6.1. ### Description ; I produced the bam files using STAR, and adjusted the MQ value to 60. I then used sambamba markdup to mark duplicate, then I proceeded to use SplitNCigarReads. The CPU load for SplitNCigarReads was very high and at certain times can spike up to 2400%. I tried limiting the cpu usage with commands like `-XX:ParallelGCThreads=1` and `-XX:ConcGCThreads=1`, but it doesn't seem to have an effect. (The cpu usage sometimes do stay at 100%) I also adjusted the MQ value in STAR to lessen the load in SplitNCigarReads. I also tried to increase the read size to reduce I/O time.; ![image](https://user-images.githubusercontent.com/106958825/175206165-08b28567-d671-45fa-b033-f20c4792edb7.png). #### Steps to reproduce; STAR; ```; STAR \; --genomeDir ${star_reference_path} \; --runThreadN 16 \; --readFilesIn ${file_1} ${file_2} \; --readFilesCommand ""gunzip -c"" \; --sjdbOverhang 149 \; --outSAMtype BAM SortedByCoordinate \; --outBAMsortingThreadN 16 \; --outSAMmultNmax 1 \; --outSAMmapqUnique 60 \; --outSAMattrRGline ID:${id} LB:RNASEQ SM:${sample_name} PL:ILLUMINA PU:${platform_unit} PM:${instrument_id} \; --limitBAMsortRAM 50000000000 \; --twopassMode Basic \; --outFileNamePrefix /rawdata/rnaseq/clean/bam/1.; ```. Mark Duplicate; ```; sambamba markdup \; -t 4 \; --tmpdir=/tmp \; --hash-table-size=262144 \; --overflow-list-size=67108864 \; /rawdata/rnaseq/clean/bam/1.Aligned.sortedByCoord.out.bam \; /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; ```. SplitNCigarReads; ```; gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx20G -XX:ParallelGCThreads=1 -XX:ConcGCThreads=1"" SplitNCigarReads \; -R ${reference_path} \; --tmp-dir /tmp \; -I /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; -O /rawdata/rnaseq/clean/bam_gatk/1.aligned.duplicate_marked.sorted.bam \; --create-output-bam-md5 TRUE \; --max-reads-in-memory 1000000 \; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7914
https://github.com/broadinstitute/gatk/issues/7914:619,Performance,load,load,619,"## Bug Report. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); - gatk 4.2.6.1. ### Description ; I produced the bam files using STAR, and adjusted the MQ value to 60. I then used sambamba markdup to mark duplicate, then I proceeded to use SplitNCigarReads. The CPU load for SplitNCigarReads was very high and at certain times can spike up to 2400%. I tried limiting the cpu usage with commands like `-XX:ParallelGCThreads=1` and `-XX:ConcGCThreads=1`, but it doesn't seem to have an effect. (The cpu usage sometimes do stay at 100%) I also adjusted the MQ value in STAR to lessen the load in SplitNCigarReads. I also tried to increase the read size to reduce I/O time.; ![image](https://user-images.githubusercontent.com/106958825/175206165-08b28567-d671-45fa-b033-f20c4792edb7.png). #### Steps to reproduce; STAR; ```; STAR \; --genomeDir ${star_reference_path} \; --runThreadN 16 \; --readFilesIn ${file_1} ${file_2} \; --readFilesCommand ""gunzip -c"" \; --sjdbOverhang 149 \; --outSAMtype BAM SortedByCoordinate \; --outBAMsortingThreadN 16 \; --outSAMmultNmax 1 \; --outSAMmapqUnique 60 \; --outSAMattrRGline ID:${id} LB:RNASEQ SM:${sample_name} PL:ILLUMINA PU:${platform_unit} PM:${instrument_id} \; --limitBAMsortRAM 50000000000 \; --twopassMode Basic \; --outFileNamePrefix /rawdata/rnaseq/clean/bam/1.; ```. Mark Duplicate; ```; sambamba markdup \; -t 4 \; --tmpdir=/tmp \; --hash-table-size=262144 \; --overflow-list-size=67108864 \; /rawdata/rnaseq/clean/bam/1.Aligned.sortedByCoord.out.bam \; /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; ```. SplitNCigarReads; ```; gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx20G -XX:ParallelGCThreads=1 -XX:ConcGCThreads=1"" SplitNCigarReads \; -R ${reference_path} \; --tmp-dir /tmp \; -I /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; -O /rawdata/rnaseq/clean/bam_gatk/1.aligned.duplicate_marked.sorted.bam \; --create-output-bam-md5 TRUE \; --max-reads-in-memory 1000000 \; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7914
https://github.com/broadinstitute/gatk/issues/7914:1418,Security,hash,hash-table-size,1418,"es); gatk SplitNCigarReads. ### Affected version(s); - gatk 4.2.6.1. ### Description ; I produced the bam files using STAR, and adjusted the MQ value to 60. I then used sambamba markdup to mark duplicate, then I proceeded to use SplitNCigarReads. The CPU load for SplitNCigarReads was very high and at certain times can spike up to 2400%. I tried limiting the cpu usage with commands like `-XX:ParallelGCThreads=1` and `-XX:ConcGCThreads=1`, but it doesn't seem to have an effect. (The cpu usage sometimes do stay at 100%) I also adjusted the MQ value in STAR to lessen the load in SplitNCigarReads. I also tried to increase the read size to reduce I/O time.; ![image](https://user-images.githubusercontent.com/106958825/175206165-08b28567-d671-45fa-b033-f20c4792edb7.png). #### Steps to reproduce; STAR; ```; STAR \; --genomeDir ${star_reference_path} \; --runThreadN 16 \; --readFilesIn ${file_1} ${file_2} \; --readFilesCommand ""gunzip -c"" \; --sjdbOverhang 149 \; --outSAMtype BAM SortedByCoordinate \; --outBAMsortingThreadN 16 \; --outSAMmultNmax 1 \; --outSAMmapqUnique 60 \; --outSAMattrRGline ID:${id} LB:RNASEQ SM:${sample_name} PL:ILLUMINA PU:${platform_unit} PM:${instrument_id} \; --limitBAMsortRAM 50000000000 \; --twopassMode Basic \; --outFileNamePrefix /rawdata/rnaseq/clean/bam/1.; ```. Mark Duplicate; ```; sambamba markdup \; -t 4 \; --tmpdir=/tmp \; --hash-table-size=262144 \; --overflow-list-size=67108864 \; /rawdata/rnaseq/clean/bam/1.Aligned.sortedByCoord.out.bam \; /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; ```. SplitNCigarReads; ```; gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx20G -XX:ParallelGCThreads=1 -XX:ConcGCThreads=1"" SplitNCigarReads \; -R ${reference_path} \; --tmp-dir /tmp \; -I /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; -O /rawdata/rnaseq/clean/bam_gatk/1.aligned.duplicate_marked.sorted.bam \; --create-output-bam-md5 TRUE \; --max-reads-in-memory 1000000 \; --skip-mapping-quality-transform TRUE \; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7914
https://github.com/broadinstitute/gatk/pull/7915:386,Deployability,Integrat,Integration,386,- restore job stats collection to create_ranges_cohort_extract_data_table.py; - add writing of cost info to BigQuery table to create_ranges_cohort_extract_data_table.py and populate_alt_allele_table.py; - add task to GvsQuickstartIntegration.wdl that checks that expected cost data was written to BigQuery table; - tweaked schema for cost_observability table to include descriptions; ; Integration test run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/30a2d8ee-13dd-4829-b3a8-4e6a67409705; Closes https://broadworkbench.atlassian.net/browse/VS-480,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7915
https://github.com/broadinstitute/gatk/pull/7915:386,Integrability,Integrat,Integration,386,- restore job stats collection to create_ranges_cohort_extract_data_table.py; - add writing of cost info to BigQuery table to create_ranges_cohort_extract_data_table.py and populate_alt_allele_table.py; - add task to GvsQuickstartIntegration.wdl that checks that expected cost data was written to BigQuery table; - tweaked schema for cost_observability table to include descriptions; ; Integration test run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/30a2d8ee-13dd-4829-b3a8-4e6a67409705; Closes https://broadworkbench.atlassian.net/browse/VS-480,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7915
https://github.com/broadinstitute/gatk/pull/7915:398,Testability,test,test,398,- restore job stats collection to create_ranges_cohort_extract_data_table.py; - add writing of cost info to BigQuery table to create_ranges_cohort_extract_data_table.py and populate_alt_allele_table.py; - add task to GvsQuickstartIntegration.wdl that checks that expected cost data was written to BigQuery table; - tweaked schema for cost_observability table to include descriptions; ; Integration test run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/30a2d8ee-13dd-4829-b3a8-4e6a67409705; Closes https://broadworkbench.atlassian.net/browse/VS-480,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7915
https://github.com/broadinstitute/gatk/issues/7916:66,Testability,test,tests,66,The various overloads of `Utils.concat()` are not covered by unit tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7916
https://github.com/broadinstitute/gatk/pull/7918:37,Testability,test,tests,37,Fixes #7916. Data providers and unit tests for the various overloads of Utils.concat(),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7918
https://github.com/broadinstitute/gatk/issues/7921:109,Deployability,release,releases,109,"We noticed a strong drop in precision for SNVs (~10% for tumor-normal mode, ~20% in tumor-only mode) between releases 4.1.7.0 and 4.2.6.0.; With more testing using the HCC1395 somatic benchmark (https://pubmed.ncbi.nlm.nih.gov/34504347/) and sequencing data provided by the Somatic Mutation Working Group (Fudan university WES tumor-normal data set, 2 x 100x coverage), the drop in performance can be traced to changes between 4.1.8.1 and 4.1.9.0. Here are the performance metrics for selected gatk releases: . ![FD_TN_4170_filter_FD_TN_4181_filter_FD_TN_4190_filter_FD_TN_4200_filter_FD_TN_4260_filter](https://user-images.githubusercontent.com/15612230/176261673-e13b9ada-0462-4cd4-b645-67459895363b.png). The calling was done with essentially default parameters:; `; tools/gatk-${version}/gatk Mutect2 --normal-sample WES_FD_N --output $outvcf --intervals $wesbed --interval-padding 0 --input $inbam_t --input $inbam_n --reference $ref ; `. ` ; tools/gatk-${version}/gatk FilterMutectCalls --output ${outvcf%.vcf}_filtered.vcf --variant $outvcf --intervals $wesbed --reference $ref --stats ${outvcf}.stats --threshold-strategy OPTIMAL_F_SCORE --f-score-beta 1.0 ; `. som.py was used for calculating performance metrics. Curiously, we do not observe a such a substantial drop in precision in WGS data, neither in tumor-only nor in tumor-normal mode.; In the foillowing, our ""v04"" corresponds to gatk 4.1.7.0 and out ""v05"" corresponds to gatk 4.2.6.0:. Tumor-normal:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176270981-2e56bb2e-c3a6-4715-bcce-33cbe0d0cf67.png). Tumor-only:. ![WGS_FD_tumor_reference_workflow_v04_WGS_FD_tumor_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176271103-6863a6f7-26f6-4841-b066-d963221ff735.png). In my opinion, the small gains in recall between 4.1.8.1 and 4.1.9.0. do not justify the drop in precision. This and the fact that WES data is affe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921
https://github.com/broadinstitute/gatk/issues/7921:499,Deployability,release,releases,499,"We noticed a strong drop in precision for SNVs (~10% for tumor-normal mode, ~20% in tumor-only mode) between releases 4.1.7.0 and 4.2.6.0.; With more testing using the HCC1395 somatic benchmark (https://pubmed.ncbi.nlm.nih.gov/34504347/) and sequencing data provided by the Somatic Mutation Working Group (Fudan university WES tumor-normal data set, 2 x 100x coverage), the drop in performance can be traced to changes between 4.1.8.1 and 4.1.9.0. Here are the performance metrics for selected gatk releases: . ![FD_TN_4170_filter_FD_TN_4181_filter_FD_TN_4190_filter_FD_TN_4200_filter_FD_TN_4260_filter](https://user-images.githubusercontent.com/15612230/176261673-e13b9ada-0462-4cd4-b645-67459895363b.png). The calling was done with essentially default parameters:; `; tools/gatk-${version}/gatk Mutect2 --normal-sample WES_FD_N --output $outvcf --intervals $wesbed --interval-padding 0 --input $inbam_t --input $inbam_n --reference $ref ; `. ` ; tools/gatk-${version}/gatk FilterMutectCalls --output ${outvcf%.vcf}_filtered.vcf --variant $outvcf --intervals $wesbed --reference $ref --stats ${outvcf}.stats --threshold-strategy OPTIMAL_F_SCORE --f-score-beta 1.0 ; `. som.py was used for calculating performance metrics. Curiously, we do not observe a such a substantial drop in precision in WGS data, neither in tumor-only nor in tumor-normal mode.; In the foillowing, our ""v04"" corresponds to gatk 4.1.7.0 and out ""v05"" corresponds to gatk 4.2.6.0:. Tumor-normal:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176270981-2e56bb2e-c3a6-4715-bcce-33cbe0d0cf67.png). Tumor-only:. ![WGS_FD_tumor_reference_workflow_v04_WGS_FD_tumor_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176271103-6863a6f7-26f6-4841-b066-d963221ff735.png). In my opinion, the small gains in recall between 4.1.8.1 and 4.1.9.0. do not justify the drop in precision. This and the fact that WES data is affe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921
https://github.com/broadinstitute/gatk/issues/7921:382,Performance,perform,performance,382,"We noticed a strong drop in precision for SNVs (~10% for tumor-normal mode, ~20% in tumor-only mode) between releases 4.1.7.0 and 4.2.6.0.; With more testing using the HCC1395 somatic benchmark (https://pubmed.ncbi.nlm.nih.gov/34504347/) and sequencing data provided by the Somatic Mutation Working Group (Fudan university WES tumor-normal data set, 2 x 100x coverage), the drop in performance can be traced to changes between 4.1.8.1 and 4.1.9.0. Here are the performance metrics for selected gatk releases: . ![FD_TN_4170_filter_FD_TN_4181_filter_FD_TN_4190_filter_FD_TN_4200_filter_FD_TN_4260_filter](https://user-images.githubusercontent.com/15612230/176261673-e13b9ada-0462-4cd4-b645-67459895363b.png). The calling was done with essentially default parameters:; `; tools/gatk-${version}/gatk Mutect2 --normal-sample WES_FD_N --output $outvcf --intervals $wesbed --interval-padding 0 --input $inbam_t --input $inbam_n --reference $ref ; `. ` ; tools/gatk-${version}/gatk FilterMutectCalls --output ${outvcf%.vcf}_filtered.vcf --variant $outvcf --intervals $wesbed --reference $ref --stats ${outvcf}.stats --threshold-strategy OPTIMAL_F_SCORE --f-score-beta 1.0 ; `. som.py was used for calculating performance metrics. Curiously, we do not observe a such a substantial drop in precision in WGS data, neither in tumor-only nor in tumor-normal mode.; In the foillowing, our ""v04"" corresponds to gatk 4.1.7.0 and out ""v05"" corresponds to gatk 4.2.6.0:. Tumor-normal:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176270981-2e56bb2e-c3a6-4715-bcce-33cbe0d0cf67.png). Tumor-only:. ![WGS_FD_tumor_reference_workflow_v04_WGS_FD_tumor_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176271103-6863a6f7-26f6-4841-b066-d963221ff735.png). In my opinion, the small gains in recall between 4.1.8.1 and 4.1.9.0. do not justify the drop in precision. This and the fact that WES data is affe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921
https://github.com/broadinstitute/gatk/issues/7921:461,Performance,perform,performance,461,"We noticed a strong drop in precision for SNVs (~10% for tumor-normal mode, ~20% in tumor-only mode) between releases 4.1.7.0 and 4.2.6.0.; With more testing using the HCC1395 somatic benchmark (https://pubmed.ncbi.nlm.nih.gov/34504347/) and sequencing data provided by the Somatic Mutation Working Group (Fudan university WES tumor-normal data set, 2 x 100x coverage), the drop in performance can be traced to changes between 4.1.8.1 and 4.1.9.0. Here are the performance metrics for selected gatk releases: . ![FD_TN_4170_filter_FD_TN_4181_filter_FD_TN_4190_filter_FD_TN_4200_filter_FD_TN_4260_filter](https://user-images.githubusercontent.com/15612230/176261673-e13b9ada-0462-4cd4-b645-67459895363b.png). The calling was done with essentially default parameters:; `; tools/gatk-${version}/gatk Mutect2 --normal-sample WES_FD_N --output $outvcf --intervals $wesbed --interval-padding 0 --input $inbam_t --input $inbam_n --reference $ref ; `. ` ; tools/gatk-${version}/gatk FilterMutectCalls --output ${outvcf%.vcf}_filtered.vcf --variant $outvcf --intervals $wesbed --reference $ref --stats ${outvcf}.stats --threshold-strategy OPTIMAL_F_SCORE --f-score-beta 1.0 ; `. som.py was used for calculating performance metrics. Curiously, we do not observe a such a substantial drop in precision in WGS data, neither in tumor-only nor in tumor-normal mode.; In the foillowing, our ""v04"" corresponds to gatk 4.1.7.0 and out ""v05"" corresponds to gatk 4.2.6.0:. Tumor-normal:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176270981-2e56bb2e-c3a6-4715-bcce-33cbe0d0cf67.png). Tumor-only:. ![WGS_FD_tumor_reference_workflow_v04_WGS_FD_tumor_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176271103-6863a6f7-26f6-4841-b066-d963221ff735.png). In my opinion, the small gains in recall between 4.1.8.1 and 4.1.9.0. do not justify the drop in precision. This and the fact that WES data is affe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921
https://github.com/broadinstitute/gatk/issues/7921:1202,Performance,perform,performance,1202,"ps://pubmed.ncbi.nlm.nih.gov/34504347/) and sequencing data provided by the Somatic Mutation Working Group (Fudan university WES tumor-normal data set, 2 x 100x coverage), the drop in performance can be traced to changes between 4.1.8.1 and 4.1.9.0. Here are the performance metrics for selected gatk releases: . ![FD_TN_4170_filter_FD_TN_4181_filter_FD_TN_4190_filter_FD_TN_4200_filter_FD_TN_4260_filter](https://user-images.githubusercontent.com/15612230/176261673-e13b9ada-0462-4cd4-b645-67459895363b.png). The calling was done with essentially default parameters:; `; tools/gatk-${version}/gatk Mutect2 --normal-sample WES_FD_N --output $outvcf --intervals $wesbed --interval-padding 0 --input $inbam_t --input $inbam_n --reference $ref ; `. ` ; tools/gatk-${version}/gatk FilterMutectCalls --output ${outvcf%.vcf}_filtered.vcf --variant $outvcf --intervals $wesbed --reference $ref --stats ${outvcf}.stats --threshold-strategy OPTIMAL_F_SCORE --f-score-beta 1.0 ; `. som.py was used for calculating performance metrics. Curiously, we do not observe a such a substantial drop in precision in WGS data, neither in tumor-only nor in tumor-normal mode.; In the foillowing, our ""v04"" corresponds to gatk 4.1.7.0 and out ""v05"" corresponds to gatk 4.2.6.0:. Tumor-normal:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176270981-2e56bb2e-c3a6-4715-bcce-33cbe0d0cf67.png). Tumor-only:. ![WGS_FD_tumor_reference_workflow_v04_WGS_FD_tumor_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176271103-6863a6f7-26f6-4841-b066-d963221ff735.png). In my opinion, the small gains in recall between 4.1.8.1 and 4.1.9.0. do not justify the drop in precision. This and the fact that WES data is affected, while WGS data is not, suggests that this might be a bug rather than a feature. Any suggestions on how to get the WES precision back to v4.1.8.1 levels are appreciated. . Thanks in advance,. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921
https://github.com/broadinstitute/gatk/issues/7921:150,Testability,test,testing,150,"We noticed a strong drop in precision for SNVs (~10% for tumor-normal mode, ~20% in tumor-only mode) between releases 4.1.7.0 and 4.2.6.0.; With more testing using the HCC1395 somatic benchmark (https://pubmed.ncbi.nlm.nih.gov/34504347/) and sequencing data provided by the Somatic Mutation Working Group (Fudan university WES tumor-normal data set, 2 x 100x coverage), the drop in performance can be traced to changes between 4.1.8.1 and 4.1.9.0. Here are the performance metrics for selected gatk releases: . ![FD_TN_4170_filter_FD_TN_4181_filter_FD_TN_4190_filter_FD_TN_4200_filter_FD_TN_4260_filter](https://user-images.githubusercontent.com/15612230/176261673-e13b9ada-0462-4cd4-b645-67459895363b.png). The calling was done with essentially default parameters:; `; tools/gatk-${version}/gatk Mutect2 --normal-sample WES_FD_N --output $outvcf --intervals $wesbed --interval-padding 0 --input $inbam_t --input $inbam_n --reference $ref ; `. ` ; tools/gatk-${version}/gatk FilterMutectCalls --output ${outvcf%.vcf}_filtered.vcf --variant $outvcf --intervals $wesbed --reference $ref --stats ${outvcf}.stats --threshold-strategy OPTIMAL_F_SCORE --f-score-beta 1.0 ; `. som.py was used for calculating performance metrics. Curiously, we do not observe a such a substantial drop in precision in WGS data, neither in tumor-only nor in tumor-normal mode.; In the foillowing, our ""v04"" corresponds to gatk 4.1.7.0 and out ""v05"" corresponds to gatk 4.2.6.0:. Tumor-normal:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176270981-2e56bb2e-c3a6-4715-bcce-33cbe0d0cf67.png). Tumor-only:. ![WGS_FD_tumor_reference_workflow_v04_WGS_FD_tumor_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176271103-6863a6f7-26f6-4841-b066-d963221ff735.png). In my opinion, the small gains in recall between 4.1.8.1 and 4.1.9.0. do not justify the drop in precision. This and the fact that WES data is affe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921
https://github.com/broadinstitute/gatk/issues/7921:184,Testability,benchmark,benchmark,184,"We noticed a strong drop in precision for SNVs (~10% for tumor-normal mode, ~20% in tumor-only mode) between releases 4.1.7.0 and 4.2.6.0.; With more testing using the HCC1395 somatic benchmark (https://pubmed.ncbi.nlm.nih.gov/34504347/) and sequencing data provided by the Somatic Mutation Working Group (Fudan university WES tumor-normal data set, 2 x 100x coverage), the drop in performance can be traced to changes between 4.1.8.1 and 4.1.9.0. Here are the performance metrics for selected gatk releases: . ![FD_TN_4170_filter_FD_TN_4181_filter_FD_TN_4190_filter_FD_TN_4200_filter_FD_TN_4260_filter](https://user-images.githubusercontent.com/15612230/176261673-e13b9ada-0462-4cd4-b645-67459895363b.png). The calling was done with essentially default parameters:; `; tools/gatk-${version}/gatk Mutect2 --normal-sample WES_FD_N --output $outvcf --intervals $wesbed --interval-padding 0 --input $inbam_t --input $inbam_n --reference $ref ; `. ` ; tools/gatk-${version}/gatk FilterMutectCalls --output ${outvcf%.vcf}_filtered.vcf --variant $outvcf --intervals $wesbed --reference $ref --stats ${outvcf}.stats --threshold-strategy OPTIMAL_F_SCORE --f-score-beta 1.0 ; `. som.py was used for calculating performance metrics. Curiously, we do not observe a such a substantial drop in precision in WGS data, neither in tumor-only nor in tumor-normal mode.; In the foillowing, our ""v04"" corresponds to gatk 4.1.7.0 and out ""v05"" corresponds to gatk 4.2.6.0:. Tumor-normal:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176270981-2e56bb2e-c3a6-4715-bcce-33cbe0d0cf67.png). Tumor-only:. ![WGS_FD_tumor_reference_workflow_v04_WGS_FD_tumor_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176271103-6863a6f7-26f6-4841-b066-d963221ff735.png). In my opinion, the small gains in recall between 4.1.8.1 and 4.1.9.0. do not justify the drop in precision. This and the fact that WES data is affe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921
https://github.com/broadinstitute/gatk/pull/7925:214,Performance,perform,performance,214,"Automatically choose appropriate import batch, preemptible, and max retry values for sample sets up to 20K. This makes 20K samples our effective threshold for where we feel comfortable with the current stability / performance of our import code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7925
https://github.com/broadinstitute/gatk/issues/7928:287,Testability,test,tests,287,"The new annotation-based filtering tools make use of the utility methods in the HDF5Utils class for writing intervals, matrices, etc. to HDF5. This class currently lives in the copynumber package, so perhaps it should be moved somewhere more general. It would be nice to expand the unit tests as well. Currently, there is only a single, disabled test that checks for the ability to read/write very large, chunked matrices; this test inherently requires a relatively large amount of memory (>Xmx4G) to run, which is why it's disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7928
https://github.com/broadinstitute/gatk/issues/7928:346,Testability,test,test,346,"The new annotation-based filtering tools make use of the utility methods in the HDF5Utils class for writing intervals, matrices, etc. to HDF5. This class currently lives in the copynumber package, so perhaps it should be moved somewhere more general. It would be nice to expand the unit tests as well. Currently, there is only a single, disabled test that checks for the ability to read/write very large, chunked matrices; this test inherently requires a relatively large amount of memory (>Xmx4G) to run, which is why it's disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7928
https://github.com/broadinstitute/gatk/issues/7928:428,Testability,test,test,428,"The new annotation-based filtering tools make use of the utility methods in the HDF5Utils class for writing intervals, matrices, etc. to HDF5. This class currently lives in the copynumber package, so perhaps it should be moved somewhere more general. It would be nice to expand the unit tests as well. Currently, there is only a single, disabled test that checks for the ability to read/write very large, chunked matrices; this test inherently requires a relatively large amount of memory (>Xmx4G) to run, which is why it's disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7928
https://github.com/broadinstitute/gatk/pull/7932:57,Deployability,pipeline,pipeline,57,"This adds a small test case for the WDL of the filtering pipeline. This still has indels and snps separated out. I can combine them if needed, but we'd like to use different annotations for each mode. This also doesn't actually apply the final filtering (with a threshold) since we still need to add a step to determine the correct threshold. The final VCFs from this workflow should have SCORE INFO annotations for each site. This takes in an array of VCFs (and outputs an array of VCFs) because this is an option for large callsets in the WARP joint genotyping WDL which is where this WDL will eventually be integrated. This test only ensures that the WDL runs and doesn't compare to expected results (the same as the other WDL tests in this repo).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7932
https://github.com/broadinstitute/gatk/pull/7932:610,Deployability,integrat,integrated,610,"This adds a small test case for the WDL of the filtering pipeline. This still has indels and snps separated out. I can combine them if needed, but we'd like to use different annotations for each mode. This also doesn't actually apply the final filtering (with a threshold) since we still need to add a step to determine the correct threshold. The final VCFs from this workflow should have SCORE INFO annotations for each site. This takes in an array of VCFs (and outputs an array of VCFs) because this is an option for large callsets in the WARP joint genotyping WDL which is where this WDL will eventually be integrated. This test only ensures that the WDL runs and doesn't compare to expected results (the same as the other WDL tests in this repo).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7932
https://github.com/broadinstitute/gatk/pull/7932:610,Integrability,integrat,integrated,610,"This adds a small test case for the WDL of the filtering pipeline. This still has indels and snps separated out. I can combine them if needed, but we'd like to use different annotations for each mode. This also doesn't actually apply the final filtering (with a threshold) since we still need to add a step to determine the correct threshold. The final VCFs from this workflow should have SCORE INFO annotations for each site. This takes in an array of VCFs (and outputs an array of VCFs) because this is an option for large callsets in the WARP joint genotyping WDL which is where this WDL will eventually be integrated. This test only ensures that the WDL runs and doesn't compare to expected results (the same as the other WDL tests in this repo).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7932
https://github.com/broadinstitute/gatk/pull/7932:18,Testability,test,test,18,"This adds a small test case for the WDL of the filtering pipeline. This still has indels and snps separated out. I can combine them if needed, but we'd like to use different annotations for each mode. This also doesn't actually apply the final filtering (with a threshold) since we still need to add a step to determine the correct threshold. The final VCFs from this workflow should have SCORE INFO annotations for each site. This takes in an array of VCFs (and outputs an array of VCFs) because this is an option for large callsets in the WARP joint genotyping WDL which is where this WDL will eventually be integrated. This test only ensures that the WDL runs and doesn't compare to expected results (the same as the other WDL tests in this repo).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7932
https://github.com/broadinstitute/gatk/pull/7932:627,Testability,test,test,627,"This adds a small test case for the WDL of the filtering pipeline. This still has indels and snps separated out. I can combine them if needed, but we'd like to use different annotations for each mode. This also doesn't actually apply the final filtering (with a threshold) since we still need to add a step to determine the correct threshold. The final VCFs from this workflow should have SCORE INFO annotations for each site. This takes in an array of VCFs (and outputs an array of VCFs) because this is an option for large callsets in the WARP joint genotyping WDL which is where this WDL will eventually be integrated. This test only ensures that the WDL runs and doesn't compare to expected results (the same as the other WDL tests in this repo).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7932
https://github.com/broadinstitute/gatk/pull/7932:730,Testability,test,tests,730,"This adds a small test case for the WDL of the filtering pipeline. This still has indels and snps separated out. I can combine them if needed, but we'd like to use different annotations for each mode. This also doesn't actually apply the final filtering (with a threshold) since we still need to add a step to determine the correct threshold. The final VCFs from this workflow should have SCORE INFO annotations for each site. This takes in an array of VCFs (and outputs an array of VCFs) because this is an option for large callsets in the WARP joint genotyping WDL which is where this WDL will eventually be integrated. This test only ensures that the WDL runs and doesn't compare to expected results (the same as the other WDL tests in this repo).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7932
https://github.com/broadinstitute/gatk/issues/7933:4330,Availability,error,error,4330,"9 Jul 2022 14:35:24,720 DEBUG: 		at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$null$1(VariantLocusWalker.java:161); 09 Jul 2022 14:35:24,726 DEBUG: 		at java.util.Iterator.forEachRemaining(Iterator.java:116); 09 Jul 2022 14:35:24,731 DEBUG: 		at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 09 Jul 2022 14:35:24,738 DEBUG: 		at java.util.stream.ReferencePipeline$Head.forEachOrdered(ReferencePipeline.java:590); 09 Jul 2022 14:35:24,743 DEBUG: 		at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$2(VariantLocusWalker.java:151); 09 Jul 2022 14:35:24,749 DEBUG: 		at java.util.Iterator.forEachRemaining(Iterator.java:116); 09 Jul 2022 14:35:24,755 DEBUG: 		at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 09 Jul 2022 14:35:24,761 DEBUG: 		at java.util.stream.ReferencePipeline$Head.forEachOrdered(ReferencePipeline.java:590); 09 Jul 2022 14:35:24,767 DEBUG: 		at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:148); 09 Jul 2022 14:35:24,773 DEBUG: 		at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 09 Jul 2022 14:35:24,780 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 09 Jul 2022 14:35:24,786 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 09 Jul 2022 14:35:24,792 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 09 Jul 2022 14:35:24,798 DEBUG: 		at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 09 Jul 2022 14:35:24,804 DEBUG: 		at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 09 Jul 2022 14:35:24,813 DEBUG: 		at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I thought this was fixed in the prior version. Is this a new error or a regression?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7933
https://github.com/broadinstitute/gatk/issues/7933:296,Performance,cache,cachedData,296,"I think we're having a very similar issue as #7639, though we're using v4.2.6.1. We're running GenotypeGVCFs with a command similar to:; ```; java8 -Xmx120g -Xms120g -Xss2m \; -jar GenomeAnalysisTK4.jar GenotypeGVCFs \; -R 128_Mmul_10.fasta \; --variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/92a51eab-e10f-103a-8ff9-f8f3fc866871/WGS_v2_db03_500.gdb \; -O /home/exacloud/gscratch/prime-seq/workDir/4570d7bc-e1e5-103a-8ff9-f8f3fc866871/Job493.work/WGS_v2_db03_500.temp.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --genomicsdb-max-alternate-alleles 9 \; --force-output-intervals WGS-WXS.whitelist.v2.3.sort.merge.bed \; -L X:117601765-123544428 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```. the exception is:; ```; 09 Jul 2022 14:35:24,661 DEBUG: 	java.lang.IllegalStateException: Genotype has no likelihoods: [29446 T*|C GQ 60 DP 10 AD 8,0 {PGT=0|1, PID=117659384_CAGATCGATGGATCT_C, PS=117659384, SB=[1, 7, 0, 2]}]; 09 Jul 2022 14:35:24,667 DEBUG: 		at org.broadinstitute.hellbender.utils.GenotypeUtils.computeDiploidGenotypeCounts(GenotypeUtils.java:89); 09 Jul 2022 14:35:24,673 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.calculateEH(ExcessHet.java:96); 09 Jul 2022 14:35:24,679 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.annotate(ExcessHet.java:84); 09 Jul 2022 14:35:24,684 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.addInfoAnnotations(VariantAnnotatorEngine.java:355); 09 Jul 2022 14:35:24,690 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:334); 09 Jul 2022 14:35:24,696 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:306); 09 Jul 2022 14:35:24,701 DEBUG: 		at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7933
https://github.com/broadinstitute/gatk/issues/7933:793,Performance,optimiz,optimizations,793,"I think we're having a very similar issue as #7639, though we're using v4.2.6.1. We're running GenotypeGVCFs with a command similar to:; ```; java8 -Xmx120g -Xms120g -Xss2m \; -jar GenomeAnalysisTK4.jar GenotypeGVCFs \; -R 128_Mmul_10.fasta \; --variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/92a51eab-e10f-103a-8ff9-f8f3fc866871/WGS_v2_db03_500.gdb \; -O /home/exacloud/gscratch/prime-seq/workDir/4570d7bc-e1e5-103a-8ff9-f8f3fc866871/Job493.work/WGS_v2_db03_500.temp.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --genomicsdb-max-alternate-alleles 9 \; --force-output-intervals WGS-WXS.whitelist.v2.3.sort.merge.bed \; -L X:117601765-123544428 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```. the exception is:; ```; 09 Jul 2022 14:35:24,661 DEBUG: 	java.lang.IllegalStateException: Genotype has no likelihoods: [29446 T*|C GQ 60 DP 10 AD 8,0 {PGT=0|1, PID=117659384_CAGATCGATGGATCT_C, PS=117659384, SB=[1, 7, 0, 2]}]; 09 Jul 2022 14:35:24,667 DEBUG: 		at org.broadinstitute.hellbender.utils.GenotypeUtils.computeDiploidGenotypeCounts(GenotypeUtils.java:89); 09 Jul 2022 14:35:24,673 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.calculateEH(ExcessHet.java:96); 09 Jul 2022 14:35:24,679 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.annotate(ExcessHet.java:84); 09 Jul 2022 14:35:24,684 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.addInfoAnnotations(VariantAnnotatorEngine.java:355); 09 Jul 2022 14:35:24,690 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:334); 09 Jul 2022 14:35:24,696 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:306); 09 Jul 2022 14:35:24,701 DEBUG: 		at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7933
https://github.com/broadinstitute/gatk/pull/7934:5,Deployability,update,updated,5,Have updated the gatk override jar with a fresh build (ah_var_store on 7/8/2022); And updated the docker similarly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7934
https://github.com/broadinstitute/gatk/pull/7934:86,Deployability,update,updated,86,Have updated the gatk override jar with a fresh build (ah_var_store on 7/8/2022); And updated the docker similarly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7934
https://github.com/broadinstitute/gatk/issues/7935:743,Availability,error,error,743,"## Bug Report; Hi, we are using the dockstore version of the GATK variant calling pipeline that leverages mutect 2:; [github.com/broadinstitute/gatk/mutect2:4.1.8.1](https://dockstore.org/workflows/github.com/broadinstitute/gatk/mutect2:4.1.8.1). We're processing human glioma data, and currently we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935
https://github.com/broadinstitute/gatk/issues/7935:771,Availability,ERROR,ERROR,771,"## Bug Report; Hi, we are using the dockstore version of the GATK variant calling pipeline that leverages mutect 2:; [github.com/broadinstitute/gatk/mutect2:4.1.8.1](https://dockstore.org/workflows/github.com/broadinstitute/gatk/mutect2:4.1.8.1). We're processing human glioma data, and currently we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935
https://github.com/broadinstitute/gatk/issues/7935:82,Deployability,pipeline,pipeline,82,"## Bug Report; Hi, we are using the dockstore version of the GATK variant calling pipeline that leverages mutect 2:; [github.com/broadinstitute/gatk/mutect2:4.1.8.1](https://dockstore.org/workflows/github.com/broadinstitute/gatk/mutect2:4.1.8.1). We're processing human glioma data, and currently we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935
https://github.com/broadinstitute/gatk/issues/7935:334,Deployability,pipeline,pipeline,334,"## Bug Report; Hi, we are using the dockstore version of the GATK variant calling pipeline that leverages mutect 2:; [github.com/broadinstitute/gatk/mutect2:4.1.8.1](https://dockstore.org/workflows/github.com/broadinstitute/gatk/mutect2:4.1.8.1). We're processing human glioma data, and currently we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935
https://github.com/broadinstitute/gatk/issues/7935:749,Integrability,message,message,749,"## Bug Report; Hi, we are using the dockstore version of the GATK variant calling pipeline that leverages mutect 2:; [github.com/broadinstitute/gatk/mutect2:4.1.8.1](https://dockstore.org/workflows/github.com/broadinstitute/gatk/mutect2:4.1.8.1). We're processing human glioma data, and currently we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935
https://github.com/broadinstitute/gatk/issues/7935:1237,Modifiability,variab,variable,1237,"y we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION -L gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/81583498-648e-4e70-8452-80509b626927/Mutect2/dbb6ef96-ea07-4cfe-9e85-3b133c6d89ea/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0030-scattered.interval_list \; -V gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935
https://github.com/broadinstitute/gatk/issues/7935:2151,Performance,cache,cacheCopy,2151,"OR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION -L gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/81583498-648e-4e70-8452-80509b626927/Mutect2/dbb6ef96-ea07-4cfe-9e85-3b133c6d89ea/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0030-scattered.interval_list \; -V gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz -L gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz -O tumor-pileups.table; ``` . Have you encountered this issue before? Is there a problem with the .gz file, or does IndexFeatureFile need to be a required step for variant contamination filtering, or does there need to be a supported option to pass the path to the index? Any help would be greatly appreciated, thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935
https://github.com/broadinstitute/gatk/issues/7938:4952,Availability,ERROR,ERROR,4952,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938
https://github.com/broadinstitute/gatk/issues/7938:5163,Availability,error,error,5163,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938
https://github.com/broadinstitute/gatk/issues/7938:5235,Availability,down,down,5235,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938
https://github.com/broadinstitute/gatk/issues/7938:5278,Availability,error,error,5278,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938
https://github.com/broadinstitute/gatk/issues/7938:5342,Availability,error,error,5342,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938
https://github.com/broadinstitute/gatk/issues/7938:5409,Availability,error,error,5409,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938
https://github.com/broadinstitute/gatk/issues/7938:5450,Deployability,update,update,5450,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938
https://github.com/broadinstitute/gatk/issues/7938:5348,Integrability,message,message,5348,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938
https://github.com/broadinstitute/gatk/pull/7939:36,Availability,error,error,36,"In the VAT validation, give clearer error msg about which clinvar classification values are missing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7939
https://github.com/broadinstitute/gatk/pull/7939:11,Security,validat,validation,11,"In the VAT validation, give clearer error msg about which clinvar classification values are missing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7939
https://github.com/broadinstitute/gatk/pull/7939:28,Usability,clear,clearer,28,"In the VAT validation, give clearer error msg about which clinvar classification values are missing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7939
https://github.com/broadinstitute/gatk/pull/7940:0,Usability,Simpl,Simple,0,Simple WDL with the sole purpose of getting the cost for a callset based on the callset identifier,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7940
https://github.com/broadinstitute/gatk/issues/7941:553,Availability,echo,echo,553,"## Feature request. ### Tool(s) or class(es) involved; Docker image. ### Description; We've been using this function in the GATK-SV WDLs to estimate the appropriate `-XmX` Java parameter for GATK tools:; ```; function getJavaMem() {; # get JVM memory in MiB by getting total memory from /proc/meminfo; # and multiplying by java_mem_fraction; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%dM"", f[MEM_FIELD] * ~{default=""0.85"" java_mem_fraction} / 1024; }'; }; JVM_MAX_MEM=$(getJavaMem MemTotal); echo ""JVM memory: $JVM_MAX_MEM""; gatk --java-options ""-Xmx${JVM_MAX_MEM}"" …; ```; We've found this to be a reliable method because it uses the VM's actual memory rather than what was requested, which can be less than what is actually provided. This would be a generally useful utility function to build into the Docker for use across GATK WDLs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7941
https://github.com/broadinstitute/gatk/issues/7941:660,Availability,reliab,reliable,660,"## Feature request. ### Tool(s) or class(es) involved; Docker image. ### Description; We've been using this function in the GATK-SV WDLs to estimate the appropriate `-XmX` Java parameter for GATK tools:; ```; function getJavaMem() {; # get JVM memory in MiB by getting total memory from /proc/meminfo; # and multiplying by java_mem_fraction; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%dM"", f[MEM_FIELD] * ~{default=""0.85"" java_mem_fraction} / 1024; }'; }; JVM_MAX_MEM=$(getJavaMem MemTotal); echo ""JVM memory: $JVM_MAX_MEM""; gatk --java-options ""-Xmx${JVM_MAX_MEM}"" …; ```; We've found this to be a reliable method because it uses the VM's actual memory rather than what was requested, which can be less than what is actually provided. This would be a generally useful utility function to build into the Docker for use across GATK WDLs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7941
https://github.com/broadinstitute/gatk/pull/7942:646,Integrability,message,message,646,"```; % python3 workflow_compute_costs.py \; --workspace_namespace 'not' \ ; --workspace_name 'there' ; Traceback (most recent call last):; File ""workflow_compute_costs.py"", line 135, in <module>; costs = compute_costs(args.workspace_namespace, args.workspace_name, args.exclude); File ""workflow_compute_costs.py"", line 32, in compute_costs; submissions = list_submissions(workspace_namespace, workspace_name); File ""workflow_compute_costs.py"", line 14, in fapi_list_submissions; return fapi_error_check(fapi.list_submissions(workspace_namespace, workspace_name)); File ""workflow_compute_costs.py"", line 8, in fapi_error_check; raise Exception(j['message']); Exception: workspace not/there does not exist or you do not have permission to use it; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7942
https://github.com/broadinstitute/gatk/issues/7944:450,Safety,sanity check,sanity check,450,I have noticed after looking at the HaplotypeCaller command line in some recent forum posts (https://gatk.broadinstitute.org/hc/en-us/community/posts/7293912288795-Haploytpe-caller-shows-me-that-0-read-s-were-filtered-by-MappingQualityAvailableReadFilter-etc) that the output of the filtering summary can be confusing if a lot of reads were processed. It can be very useful to know that a lot of reads are lost to a particular filter as an important sanity check for processing but unfortunately that information can be very confusing and not helpful without some indication of the total number of reads that were processed to begin with. I propose that we add to the `CountingReadFilter` code additional logic to keep track of the unfiltered reads as well so we can report both numbers to the user and clear up potential confusion.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7944
https://github.com/broadinstitute/gatk/issues/7944:705,Testability,log,logic,705,I have noticed after looking at the HaplotypeCaller command line in some recent forum posts (https://gatk.broadinstitute.org/hc/en-us/community/posts/7293912288795-Haploytpe-caller-shows-me-that-0-read-s-were-filtered-by-MappingQualityAvailableReadFilter-etc) that the output of the filtering summary can be confusing if a lot of reads were processed. It can be very useful to know that a lot of reads are lost to a particular filter as an important sanity check for processing but unfortunately that information can be very confusing and not helpful without some indication of the total number of reads that were processed to begin with. I propose that we add to the `CountingReadFilter` code additional logic to keep track of the unfiltered reads as well so we can report both numbers to the user and clear up potential confusion.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7944
https://github.com/broadinstitute/gatk/issues/7944:803,Usability,clear,clear,803,I have noticed after looking at the HaplotypeCaller command line in some recent forum posts (https://gatk.broadinstitute.org/hc/en-us/community/posts/7293912288795-Haploytpe-caller-shows-me-that-0-read-s-were-filtered-by-MappingQualityAvailableReadFilter-etc) that the output of the filtering summary can be confusing if a lot of reads were processed. It can be very useful to know that a lot of reads are lost to a particular filter as an important sanity check for processing but unfortunately that information can be very confusing and not helpful without some indication of the total number of reads that were processed to begin with. I propose that we add to the `CountingReadFilter` code additional logic to keep track of the unfiltered reads as well so we can report both numbers to the user and clear up potential confusion.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7944
https://github.com/broadinstitute/gatk/pull/7945:0,Deployability,Integrat,Integration,0,"Integration tests are not passing as the gold files have not been updated. Understanding what ""correct"" looks like for spanning deletions beyond just unit tests is very complex, and fraught with bad data (ie multiple overlapping reference blocks, etc). . Putting this work on the shelf until the value is worth the effort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7945
https://github.com/broadinstitute/gatk/pull/7945:66,Deployability,update,updated,66,"Integration tests are not passing as the gold files have not been updated. Understanding what ""correct"" looks like for spanning deletions beyond just unit tests is very complex, and fraught with bad data (ie multiple overlapping reference blocks, etc). . Putting this work on the shelf until the value is worth the effort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7945
https://github.com/broadinstitute/gatk/pull/7945:0,Integrability,Integrat,Integration,0,"Integration tests are not passing as the gold files have not been updated. Understanding what ""correct"" looks like for spanning deletions beyond just unit tests is very complex, and fraught with bad data (ie multiple overlapping reference blocks, etc). . Putting this work on the shelf until the value is worth the effort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7945
https://github.com/broadinstitute/gatk/pull/7945:12,Testability,test,tests,12,"Integration tests are not passing as the gold files have not been updated. Understanding what ""correct"" looks like for spanning deletions beyond just unit tests is very complex, and fraught with bad data (ie multiple overlapping reference blocks, etc). . Putting this work on the shelf until the value is worth the effort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7945
https://github.com/broadinstitute/gatk/pull/7945:155,Testability,test,tests,155,"Integration tests are not passing as the gold files have not been updated. Understanding what ""correct"" looks like for spanning deletions beyond just unit tests is very complex, and fraught with bad data (ie multiple overlapping reference blocks, etc). . Putting this work on the shelf until the value is worth the effort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7945
https://github.com/broadinstitute/gatk/pull/7946:294,Deployability,integrat,integration,294,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:294,Integrability,integrat,integration,294,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:5,Modifiability,refactor,refactoring,5,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:106,Modifiability,Refactor,Refactoring,106,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:28,Testability,test,testability,28,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:168,Testability,test,testing,168,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:274,Testability,test,test,274,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:317,Testability,test,test,317,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:497,Testability,test,tests,497,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/pull/7946:898,Testability,test,tests,898,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946
https://github.com/broadinstitute/gatk/issues/7948:920,Energy Efficiency,efficient,efficient,920,"I am doing somatic variant calling for whole genome sequencing. This tool could be improved to have reasonable RAM demands. ```; #PBS -l walltime=05:00:00,ncpus=26,mem=750GB; #PBS -q hugemem; ```. Fails after almost two hours. ```; NCPUs Requested: 26 NCPUs Used: 26; CPU Time Used: 28:09:38; Memory Requested: 750.0GB Memory Used: 749.95GB; Walltime requested: 05:00:00 Walltime Used: 01:54:14; ```. The reason is that the RAM allowance was exceeded. ```; Job 51320644 has exceeded memory allocation on node gadi-hmem-clx-0005.gadi.nci.org.au; Process ""bash"", pid 742028, rss 3346432, vmem 21680128; Process ""51320644.gadi-p"", pid 742086, rss 3059712, vmem 10174464; Process ""mpirun"", pid 742112, rss 6406144, vmem 220180480; Process ""nci-parallel"", pid 742118, rss 266031104, vmem 1423237120; ... ...; ```. Normals about 30 times coverage, tumours about 60 times coverage, 26 patients in total. It should be made more efficient.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7948
https://github.com/broadinstitute/gatk/issues/7949:294,Safety,timeout,timeout,294,"We need to be able to build docker images for gatk remotely for people who cant run our current build script. For carrot we have already gotten the build working with GoogleCloudBuild using a command like this:; `gcloud builds submit --tag {whatever you want to tag the image and push it as} --timeout=24h --machine-type e2_highcpu_8`; This script should function like our current docker script, it should optionally check out a fresh remote gatk branch/image clone it locally, use that context to do the build, then remove the directory that was cloned when it is done (similar to how `build_docker.sh` works currently)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7949
https://github.com/broadinstitute/gatk/pull/7950:173,Integrability,depend,dependency,173,"I'm never revisiting this effort (even if we somehow go back to the spark SV tool, I think there are better ways to approach this), so I wanted to get the xgboost predictor dependency out of GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7950
https://github.com/broadinstitute/gatk/pull/7950:163,Safety,predict,predictor,163,"I'm never revisiting this effort (even if we somehow go back to the spark SV tool, I think there are better ways to approach this), so I wanted to get the xgboost predictor dependency out of GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7950
https://github.com/broadinstitute/gatk/issues/7955:105,Testability,test,tests,105,Coverage analysis indicates that the new --assembly-complexity-reference-mode argument is not covered by tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7955
https://github.com/broadinstitute/gatk/issues/7956:57,Testability,test,tests,57,Codecov indicates that this new method is not covered by tests -- needs a test in IOUtilsUnitTes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7956
https://github.com/broadinstitute/gatk/issues/7956:74,Testability,test,test,74,Codecov indicates that this new method is not covered by tests -- needs a test in IOUtilsUnitTes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7956
https://github.com/broadinstitute/gatk/issues/7957:30,Testability,test,test,30,Currently:. // TODO NOTE this test is disabled. That is because currently it is difficult to generate ArtificialFlowBasedReads from scratch...; @Test (enabled = false),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7957
https://github.com/broadinstitute/gatk/issues/7957:145,Testability,Test,Test,145,Currently:. // TODO NOTE this test is disabled. That is because currently it is difficult to generate ArtificialFlowBasedReads from scratch...; @Test (enabled = false),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7957
https://github.com/broadinstitute/gatk/issues/7958:72,Testability,test,test,72,"This method and a couple others in this class don't appear to have good test coverage,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7958
https://github.com/broadinstitute/gatk/issues/7961:261,Availability,error,error,261,## Bug Report. ### Affected tool(s) or class(es); GATK ReblockGVCF. ### Affected version(s); - 4.2.5.0 and 4.2.6.1. ### Description ; I am running ReblockGVCF on GVCF's that are haplotyped on version 4.0.1.4. About 1 out of 500 samples crash with the following error:; `ReblockGVCF fails by an exception:No shortest ALT at 464564654 across alleles: [*]`. Complete error message:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr4::464564654[VC /bug.g.vcf.gz @ ; ```; redacted; ```. ] filters=; at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:145); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7961
https://github.com/broadinstitute/gatk/issues/7961:364,Availability,error,error,364,## Bug Report. ### Affected tool(s) or class(es); GATK ReblockGVCF. ### Affected version(s); - 4.2.5.0 and 4.2.6.1. ### Description ; I am running ReblockGVCF on GVCF's that are haplotyped on version 4.0.1.4. About 1 out of 500 samples crash with the following error:; `ReblockGVCF fails by an exception:No shortest ALT at 464564654 across alleles: [*]`. Complete error message:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr4::464564654[VC /bug.g.vcf.gz @ ; ```; redacted; ```. ] filters=; at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:145); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7961
https://github.com/broadinstitute/gatk/issues/7961:370,Integrability,message,message,370,## Bug Report. ### Affected tool(s) or class(es); GATK ReblockGVCF. ### Affected version(s); - 4.2.5.0 and 4.2.6.1. ### Description ; I am running ReblockGVCF on GVCF's that are haplotyped on version 4.0.1.4. About 1 out of 500 samples crash with the following error:; `ReblockGVCF fails by an exception:No shortest ALT at 464564654 across alleles: [*]`. Complete error message:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr4::464564654[VC /bug.g.vcf.gz @ ; ```; redacted; ```. ] filters=; at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:145); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7961
https://github.com/broadinstitute/gatk/issues/7961:1285,Integrability,wrap,wrapAndCopyInto,1285,on:No shortest ALT at 464564654 across alleles: [*]`. Complete error message:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr4::464564654[VC /bug.g.vcf.gz @ ; ```; redacted; ```. ] filters=; at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:145); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7961
https://github.com/broadinstitute/gatk/pull/7962:99,Testability,test,tests,99,"This is a possible workaround for #7938. This isnt a finished PR, but I'm opening this to kick off tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7962
https://github.com/broadinstitute/gatk/issues/7964:787,Modifiability,variab,variable,787,"Hello,. I am running GermlineCNVCaller and PostprocessGermlineCNVCalls (GATK v4.2.5) for CNV analysis on our targeted capture. . My output segment vcfs have no SVLEN or SVTYPE values although those are described in their headers. . Info from header includes:; ```; ##INFO=<ID=AC_Orig,Number=A,Type=Integer,Description=""Original AC"">; ##INFO=<ID=AF_Orig,Number=A,Type=Float,Description=""Original AF"">; ##INFO=<ID=AN_Orig,Number=1,Type=Integer,Description=""Original AN"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##INFO=<ID=SVLEN,Number=.,Type=Integer,Description=""Difference in length between REF and ALT alleles"">; ##INFO=<ID=SVTYPE,Number=1,Type=String,Description=""Type of structural variant"">; ```; But the actual vcf output only has the END variable. Example output:. ```; 13	32839931	CNV_13_32839931_32945267	N	.	3076.53	.	END=32945267	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:63:169:3077:523:342; 13	32950659	CNV_13_32950659_32954345	N	<DEL>	3076.53	.	END=32954345	GT:CN:NP:QA:QS:QSE:QSS	0/1:1:7:709:3077:709:831; 13	32968699	CNV_13_32968699_73961012	N	.	3076.53	.	END=73961012	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:14:210:3077:295:630; 14	24883828	CNV_14_24883828_94854954	N	.	3076.53	.	END=94854954	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:72:100:3077:287:299; 15	32992921	CNV_15_32992921_91535389	N	.	3076.53	.	END=91535389	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:35:102:3077:198:331; ```. Commands running are below:. ```; docker run -v /home/dnanexus/inputs:/data $GATK_image gatk GermlineCNVCaller \; -L /data/beds/filtered.interval_list -imr OVERLAPPING_ONLY \; --annotated-intervals /data/beds/annotated_intervals.tsv \; --run-mode COHORT \; $batch_input \; --contig-ploidy-calls /data/ploidy-dir/ploidy-calls/ \; --output-prefix CNV \; -O /data/gCNV-dir. parallel --jobs 8 '/usr/bin/time -v docker run -v /home/dnanexus/inputs:/data $GATK_image \; gatk PostprocessGermlineCNVCalls \; --sample-index {} \; --autosomal-ref-copy-number 2 \; --allosomal-contig X \; --allosomal-contig Y \; --",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7964
https://github.com/broadinstitute/gatk/pull/7965:0,Deployability,Update,Update,0,"Update GvsCreateVAT.wdl to build the subpopulation-specific files from the input ancestry file.; Have GvsCreateVAT.wdl only pull fields from the VCF for the selected subpopulations.; Update create_variant_annotation_table.py to set empty population-specific AC/AN/AF, etc. values if population is not present.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7965
https://github.com/broadinstitute/gatk/pull/7965:183,Deployability,Update,Update,183,"Update GvsCreateVAT.wdl to build the subpopulation-specific files from the input ancestry file.; Have GvsCreateVAT.wdl only pull fields from the VCF for the selected subpopulations.; Update create_variant_annotation_table.py to set empty population-specific AC/AN/AF, etc. values if population is not present.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7965
https://github.com/broadinstitute/gatk/issues/7966:709,Availability,error,error,709,"Discussed in office hours. Seems to be a bug with --force-output-intervals in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:781,Availability,down,down,781,"Discussed in office hours. Seems to be a bug with --force-output-intervals in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:824,Availability,error,error,824,"Discussed in office hours. Seems to be a bug with --force-output-intervals in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:888,Availability,error,error,888,"Discussed in office hours. Seems to be a bug with --force-output-intervals in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:955,Availability,error,error,955,"Discussed in office hours. Seems to be a bug with --force-output-intervals in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:1061,Availability,error,errors,1061," in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) Entire program log:. Using GATK jar MySoftwares/gatk-4.2.6.1/gatk-package-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:1985,Availability,ERROR,ERROR,1985,"on to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) Entire program log:. Using GATK jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar. Running:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR. 15:30:47.303 info Nativ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:2897,Availability,ERROR,ERROR,2897,tervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) Entire program log:. Using GATK jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar. Running:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF records. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field HaplotypeScore - the field will NOT be part of INFO fields in the generated VCF records. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO f,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:996,Deployability,update,update,996,"Discussed in office hours. Seems to be a bug with --force-output-intervals in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:894,Integrability,message,message,894,"Discussed in office hours. Seems to be a bug with --force-output-intervals in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/issues/7966:2016,Testability,log,log,2016,t it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) Entire program log:. Using GATK jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar. Running:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for IN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966
https://github.com/broadinstitute/gatk/pull/7967:287,Deployability,integrat,integration,287,The sample name map file accepted by GenomicsDBImport can now optionally contain a third; column giving an explicit path to an index for the corresponding GVCF. It is allowed to; specify an explicit index in some lines of the sample name map and not others. Added comprehensive unit and integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7967
https://github.com/broadinstitute/gatk/pull/7967:287,Integrability,integrat,integration,287,The sample name map file accepted by GenomicsDBImport can now optionally contain a third; column giving an explicit path to an index for the corresponding GVCF. It is allowed to; specify an explicit index in some lines of the sample name map and not others. Added comprehensive unit and integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7967
https://github.com/broadinstitute/gatk/pull/7967:299,Testability,test,tests,299,The sample name map file accepted by GenomicsDBImport can now optionally contain a third; column giving an explicit path to an index for the corresponding GVCF. It is allowed to; specify an explicit index in some lines of the sample name map and not others. Added comprehensive unit and integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7967
https://github.com/broadinstitute/gatk/issues/7968:609,Availability,error,error,609,"I am running GATK GenotypeGVCFs, v4.2.6.1. I am trying to call Genotypes on a GenomicsDB workspace with about 500 WGS samples. Note, this is the macaque MMul10 genome, so it has 2,939 contigs (including unplaced). We've run commands like this quite a lot before, though we periodically do have issues like this. We can consolidate on this workspace prior to running this (using a standalone tool @nalinigans provided on #7674). As you can see we ran java with relatively low RAM, but left ~150G for the C++ layer. I'm surprised this isnt good enough. . I'm going to try to interactively inspect this, but the error is from slurm killing my job, not a java memory error, which I believe means the -Xmx 92G isnt getting exceeded. I could be mistaken though. You'll also see: 1) I'm using --force-output-intervals, 2) I'm giving it -XL to excluded repetitive regions (and therefore also skipping some of the more gnarly and memory-intensive sites), and 3) I'm giving it a fairly small -L interval list (this is split into 750 jobs/genome). . ```; java8 \; -Djava.io.tmpdir=<folder> \; -Xmx92g -Xms92g -Xss2m \; -jar GenomeAnalysisTK4.jar \; GenotypeGVCFs \; -R 128_Mmul_10.fasta \; --variant gendb:///<path>/WGS_v2_db03_500.gdb \; -O WGS_v2_db03_500.temp.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; -XL NCBI_Mmul_10.softmask.bed \; --max-alternate-alleles 6 \; --genomicsdb-max-alternate-alleles 9 \; --force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; -L 14:37234750-41196525 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```. Each job gets about 250K to 800K variants into the data, and then they pretty consistently start to exceed memory and get killed. . Does anyone have suggestions on debugging or troubleshooting steps? Thanks in advance for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968
https://github.com/broadinstitute/gatk/issues/7968:663,Availability,error,error,663,"I am running GATK GenotypeGVCFs, v4.2.6.1. I am trying to call Genotypes on a GenomicsDB workspace with about 500 WGS samples. Note, this is the macaque MMul10 genome, so it has 2,939 contigs (including unplaced). We've run commands like this quite a lot before, though we periodically do have issues like this. We can consolidate on this workspace prior to running this (using a standalone tool @nalinigans provided on #7674). As you can see we ran java with relatively low RAM, but left ~150G for the C++ layer. I'm surprised this isnt good enough. . I'm going to try to interactively inspect this, but the error is from slurm killing my job, not a java memory error, which I believe means the -Xmx 92G isnt getting exceeded. I could be mistaken though. You'll also see: 1) I'm using --force-output-intervals, 2) I'm giving it -XL to excluded repetitive regions (and therefore also skipping some of the more gnarly and memory-intensive sites), and 3) I'm giving it a fairly small -L interval list (this is split into 750 jobs/genome). . ```; java8 \; -Djava.io.tmpdir=<folder> \; -Xmx92g -Xms92g -Xss2m \; -jar GenomeAnalysisTK4.jar \; GenotypeGVCFs \; -R 128_Mmul_10.fasta \; --variant gendb:///<path>/WGS_v2_db03_500.gdb \; -O WGS_v2_db03_500.temp.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; -XL NCBI_Mmul_10.softmask.bed \; --max-alternate-alleles 6 \; --genomicsdb-max-alternate-alleles 9 \; --force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; -L 14:37234750-41196525 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```. Each job gets about 250K to 800K variants into the data, and then they pretty consistently start to exceed memory and get killed. . Does anyone have suggestions on debugging or troubleshooting steps? Thanks in advance for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968
https://github.com/broadinstitute/gatk/issues/7968:1602,Performance,optimiz,optimizations,1602,"I am running GATK GenotypeGVCFs, v4.2.6.1. I am trying to call Genotypes on a GenomicsDB workspace with about 500 WGS samples. Note, this is the macaque MMul10 genome, so it has 2,939 contigs (including unplaced). We've run commands like this quite a lot before, though we periodically do have issues like this. We can consolidate on this workspace prior to running this (using a standalone tool @nalinigans provided on #7674). As you can see we ran java with relatively low RAM, but left ~150G for the C++ layer. I'm surprised this isnt good enough. . I'm going to try to interactively inspect this, but the error is from slurm killing my job, not a java memory error, which I believe means the -Xmx 92G isnt getting exceeded. I could be mistaken though. You'll also see: 1) I'm using --force-output-intervals, 2) I'm giving it -XL to excluded repetitive regions (and therefore also skipping some of the more gnarly and memory-intensive sites), and 3) I'm giving it a fairly small -L interval list (this is split into 750 jobs/genome). . ```; java8 \; -Djava.io.tmpdir=<folder> \; -Xmx92g -Xms92g -Xss2m \; -jar GenomeAnalysisTK4.jar \; GenotypeGVCFs \; -R 128_Mmul_10.fasta \; --variant gendb:///<path>/WGS_v2_db03_500.gdb \; -O WGS_v2_db03_500.temp.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; -XL NCBI_Mmul_10.softmask.bed \; --max-alternate-alleles 6 \; --genomicsdb-max-alternate-alleles 9 \; --force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; -L 14:37234750-41196525 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```. Each job gets about 250K to 800K variants into the data, and then they pretty consistently start to exceed memory and get killed. . Does anyone have suggestions on debugging or troubleshooting steps? Thanks in advance for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968
https://github.com/broadinstitute/gatk/pull/7972:32,Availability,down,downloads,32,"Reverts VS-569 now that Temurin downloads are working again, leaves in the Corretto breadcrumbs and minor improvements like `-o xtrace`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7972
https://github.com/broadinstitute/gatk/issues/7976:93,Availability,Error,Error,93,"### Affected tool(s) or class(es); GenotypeGVCFs is reporting:; ```. [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow; ```. ### Affected version(s); ```; Using GATK jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar --version; The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1. ```. ### Description ; Hi, I developed AnchorWave to call long indels(could be a couple of Mb). We are trying to connect the AnchorWave variant calling result with GATK to generate VCF files. We generated whole genome alignments for 26 maize accession via AnchorWave. And we wrote out own code to generate GVCF files from the outputs of AnchorWave. Those GVCF files works well with GATK GenomicsDBImport. While, the `GenotypeGVCFs` function is reporting `Buffer overflow` errors and could generate the complete VCF files. . Here is the command we used:. ```; gatk --java-options ""-Xmx100g"" GenotypeGVCFs -R Zm-B73-REFERENCE-NAM-5.0.fa -stand-call-conf 0 -ploidy 1 -V gendb:///home/xuql/NAM_anchorwave_song/NAM_out_gatk9 -O gatk9.vcf.gz --cloud-prefetch-buffer 10000 --cloud-index-prefetch-buffer 10000 --genomicsdb-max-alternate-alleles 110 --max-alternate-alleles 100 --tmp-dir /home/xuql/NAM_anchorwave_song/temp9 --gcs-max-retries 1000; ```; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976
https://github.com/broadinstitute/gatk/issues/7976:325,Availability,error,error,325,"### Affected tool(s) or class(es); GenotypeGVCFs is reporting:; ```. [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow; ```. ### Affected version(s); ```; Using GATK jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar --version; The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1. ```. ### Description ; Hi, I developed AnchorWave to call long indels(could be a couple of Mb). We are trying to connect the AnchorWave variant calling result with GATK to generate VCF files. We generated whole genome alignments for 26 maize accession via AnchorWave. And we wrote out own code to generate GVCF files from the outputs of AnchorWave. Those GVCF files works well with GATK GenomicsDBImport. While, the `GenotypeGVCFs` function is reporting `Buffer overflow` errors and could generate the complete VCF files. . Here is the command we used:. ```; gatk --java-options ""-Xmx100g"" GenotypeGVCFs -R Zm-B73-REFERENCE-NAM-5.0.fa -stand-call-conf 0 -ploidy 1 -V gendb:///home/xuql/NAM_anchorwave_song/NAM_out_gatk9 -O gatk9.vcf.gz --cloud-prefetch-buffer 10000 --cloud-index-prefetch-buffer 10000 --genomicsdb-max-alternate-alleles 110 --max-alternate-alleles 100 --tmp-dir /home/xuql/NAM_anchorwave_song/temp9 --gcs-max-retries 1000; ```; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976
https://github.com/broadinstitute/gatk/issues/7976:365,Availability,Error,Error,365,"### Affected tool(s) or class(es); GenotypeGVCFs is reporting:; ```. [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow; ```. ### Affected version(s); ```; Using GATK jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar --version; The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1. ```. ### Description ; Hi, I developed AnchorWave to call long indels(could be a couple of Mb). We are trying to connect the AnchorWave variant calling result with GATK to generate VCF files. We generated whole genome alignments for 26 maize accession via AnchorWave. And we wrote out own code to generate GVCF files from the outputs of AnchorWave. Those GVCF files works well with GATK GenomicsDBImport. While, the `GenotypeGVCFs` function is reporting `Buffer overflow` errors and could generate the complete VCF files. . Here is the command we used:. ```; gatk --java-options ""-Xmx100g"" GenotypeGVCFs -R Zm-B73-REFERENCE-NAM-5.0.fa -stand-call-conf 0 -ploidy 1 -V gendb:///home/xuql/NAM_anchorwave_song/NAM_out_gatk9 -O gatk9.vcf.gz --cloud-prefetch-buffer 10000 --cloud-index-prefetch-buffer 10000 --genomicsdb-max-alternate-alleles 110 --max-alternate-alleles 100 --tmp-dir /home/xuql/NAM_anchorwave_song/temp9 --gcs-max-retries 1000; ```; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976
https://github.com/broadinstitute/gatk/issues/7976:1366,Availability,error,errors,1366,"### Affected tool(s) or class(es); GenotypeGVCFs is reporting:; ```. [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow; ```. ### Affected version(s); ```; Using GATK jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar --version; The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1. ```. ### Description ; Hi, I developed AnchorWave to call long indels(could be a couple of Mb). We are trying to connect the AnchorWave variant calling result with GATK to generate VCF files. We generated whole genome alignments for 26 maize accession via AnchorWave. And we wrote out own code to generate GVCF files from the outputs of AnchorWave. Those GVCF files works well with GATK GenomicsDBImport. While, the `GenotypeGVCFs` function is reporting `Buffer overflow` errors and could generate the complete VCF files. . Here is the command we used:. ```; gatk --java-options ""-Xmx100g"" GenotypeGVCFs -R Zm-B73-REFERENCE-NAM-5.0.fa -stand-call-conf 0 -ploidy 1 -V gendb:///home/xuql/NAM_anchorwave_song/NAM_out_gatk9 -O gatk9.vcf.gz --cloud-prefetch-buffer 10000 --cloud-index-prefetch-buffer 10000 --genomicsdb-max-alternate-alleles 110 --max-alternate-alleles 100 --tmp-dir /home/xuql/NAM_anchorwave_song/temp9 --gcs-max-retries 1000; ```; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976
https://github.com/broadinstitute/gatk/issues/7976:331,Integrability,message,message,331,"### Affected tool(s) or class(es); GenotypeGVCFs is reporting:; ```. [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow; ```. ### Affected version(s); ```; Using GATK jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar --version; The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1. ```. ### Description ; Hi, I developed AnchorWave to call long indels(could be a couple of Mb). We are trying to connect the AnchorWave variant calling result with GATK to generate VCF files. We generated whole genome alignments for 26 maize accession via AnchorWave. And we wrote out own code to generate GVCF files from the outputs of AnchorWave. Those GVCF files works well with GATK GenomicsDBImport. While, the `GenotypeGVCFs` function is reporting `Buffer overflow` errors and could generate the complete VCF files. . Here is the command we used:. ```; gatk --java-options ""-Xmx100g"" GenotypeGVCFs -R Zm-B73-REFERENCE-NAM-5.0.fa -stand-call-conf 0 -ploidy 1 -V gendb:///home/xuql/NAM_anchorwave_song/NAM_out_gatk9 -O gatk9.vcf.gz --cloud-prefetch-buffer 10000 --cloud-index-prefetch-buffer 10000 --genomicsdb-max-alternate-alleles 110 --max-alternate-alleles 100 --tmp-dir /home/xuql/NAM_anchorwave_song/temp9 --gcs-max-retries 1000; ```; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976
https://github.com/broadinstitute/gatk/issues/7976:1136,Security,access,accession,1136,"### Affected tool(s) or class(es); GenotypeGVCFs is reporting:; ```. [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow; ```. ### Affected version(s); ```; Using GATK jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar --version; The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1. ```. ### Description ; Hi, I developed AnchorWave to call long indels(could be a couple of Mb). We are trying to connect the AnchorWave variant calling result with GATK to generate VCF files. We generated whole genome alignments for 26 maize accession via AnchorWave. And we wrote out own code to generate GVCF files from the outputs of AnchorWave. Those GVCF files works well with GATK GenomicsDBImport. While, the `GenotypeGVCFs` function is reporting `Buffer overflow` errors and could generate the complete VCF files. . Here is the command we used:. ```; gatk --java-options ""-Xmx100g"" GenotypeGVCFs -R Zm-B73-REFERENCE-NAM-5.0.fa -stand-call-conf 0 -ploidy 1 -V gendb:///home/xuql/NAM_anchorwave_song/NAM_out_gatk9 -O gatk9.vcf.gz --cloud-prefetch-buffer 10000 --cloud-index-prefetch-buffer 10000 --genomicsdb-max-alternate-alleles 110 --max-alternate-alleles 100 --tmp-dir /home/xuql/NAM_anchorwave_song/temp9 --gcs-max-retries 1000; ```; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976
https://github.com/broadinstitute/gatk/issues/7980:228,Availability,ERROR,ERROR,228,"- version; GATK4.2.6.1. Hi, ; When I use the following command to genotype a single g vcf file, the output **alt depth**(ref depth is ok) in the FORMAT column ; are missing. ```; gatk GenotypeGVCFs -R ucsc.hg19.fasta -verbosity ERROR -all-sites true -stand-call-conf 0 --dbsnp dbsnp_138.hg19.vcf -V 0003.g.vcf -O 0003.4.2.6.1.vcf ; ```; For example, `AD:DP 2,1013:1116`, `1116` is for the total depth(DP), and `2,1013` is for the ref depth, however, the **alt allele depth** is missing. is this a bug? Or did I miss something?. ```; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">. #input ; chr19	45923653	.	A	G,<NON_REF>	31988.06	.	BaseQRankSum=-1.361;DP=1149;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;MQRankSum=0.000;RAW_MQandDP=4136400,1149;ReadPosRankSum=-1.250	GT:AD:DP:GQ:PL:SB	1/1:2,1013,101:1116:99:32002,2966,0,32008,3040,32082:1,1,538,576. #output ; chr19	45923653	rs11615	A	G	31988.06	.	AC=2;AF=1.00;AN=2;BaseQRankSum=-1.361e+00;DB;DP=1149;ExcessHet=0.0000;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQRankSum=0.00;QD=31.52;ReadPosRankSum=-1.250e+00;SOR=0.764	GT:AD:DP:GQ:PL	1/1:2,1013:1116:99:32002,2966,0. ```; Best,; xiucz",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7980
https://github.com/broadinstitute/gatk/issues/7982:18,Energy Efficiency,battery,battery,18,"We already have a battery of carrot tests that we would like to run to evaluate the HaplotypeCaller. However those tests do not currently cover any of the new use cases in the FlowBasedGATK code. The ask here is that we should create and design a set of longer running, multi-sample evaluation code for the FlowBasedGATK on a reasonable set of non-sensitive input bams that we can call and then subsequently run evaluation metrics on so we can have a better evaluation of how future improvements impact the FlowBasedCode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7982
https://github.com/broadinstitute/gatk/issues/7982:36,Testability,test,tests,36,"We already have a battery of carrot tests that we would like to run to evaluate the HaplotypeCaller. However those tests do not currently cover any of the new use cases in the FlowBasedGATK code. The ask here is that we should create and design a set of longer running, multi-sample evaluation code for the FlowBasedGATK on a reasonable set of non-sensitive input bams that we can call and then subsequently run evaluation metrics on so we can have a better evaluation of how future improvements impact the FlowBasedCode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7982
https://github.com/broadinstitute/gatk/issues/7982:115,Testability,test,tests,115,"We already have a battery of carrot tests that we would like to run to evaluate the HaplotypeCaller. However those tests do not currently cover any of the new use cases in the FlowBasedGATK code. The ask here is that we should create and design a set of longer running, multi-sample evaluation code for the FlowBasedGATK on a reasonable set of non-sensitive input bams that we can call and then subsequently run evaluation metrics on so we can have a better evaluation of how future improvements impact the FlowBasedCode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7982
https://github.com/broadinstitute/gatk/issues/7983:5,Availability,error,error,5,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:137,Availability,error,error,137,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:271,Availability,ERROR,ERROR,271,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:382,Availability,failure,failure,382,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1615,Availability,failure,failure,1615,"tConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ```. This request was created from a contribution made by Pryce Turner on July 29, 2022 03:44 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078378372--Could-not-determine-local-host-name-#community\_comment\_7692552841755](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078378372--",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:2816,Availability,error,error,2816," org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ```. This request was created from a contribution made by Pryce Turner on July 29, 2022 03:44 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078378372--Could-not-determine-local-host-name-#community\_comment\_7692552841755](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078378372--Could-not-determine-local-host-name-#community_comment_7692552841755). \--. Hey 417227834892,. Were you ever able to solve this? I'm running into the same issue with HaplotypeCaller. It still runs, but the error output is potentially disruptive. Thanks!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/292152'>Zendesk ticket #292152</a>)<br> gz#292152</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:11,Integrability,message,message,11,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:143,Integrability,message,message,143,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:665,Performance,concurren,concurrent,665,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:676,Performance,Concurren,ConcurrentHashMap,676,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:710,Performance,Concurren,ConcurrentHashMap,710,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:485,Testability,log,logging,485,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:568,Testability,log,logging,568,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:587,Testability,Log,LoggerContext,587,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:627,Testability,Log,LoggerContext,627,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:754,Testability,log,logging,754,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:773,Testability,Log,LoggerContext,773,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:804,Testability,Log,LoggerContext,804,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:843,Testability,log,logging,843,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:862,Testability,Log,LoggerContext,862,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:888,Testability,Log,LoggerContext,888,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:927,Testability,log,logging,927,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:946,Testability,Log,LoggerContext,946,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:972,Testability,Log,LoggerContext,972,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1011,Testability,log,logging,1011,"age occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1030,Testability,Log,LoggerContext,1030,"ng the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1050,Testability,Log,LoggerContext,1050,"en running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1089,Testability,log,logging,1089,"GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.u",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1189,Testability,log,logging,1189,"-network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_writ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1288,Testability,log,logging,1288,mine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ``,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1302,Testability,Log,LogManager,1302, name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ```. This request ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1324,Testability,Log,LogManager,1324,knownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ```. This request was created from,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1360,Testability,log,logging,1360,c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ```. This request was created from a contribution made by Pryce Turner on ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1374,Testability,Log,LogManager,1374,"ry failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ```. This request was created from a contribution made by Pryce Turner on July 29, 2022 03",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7983:1395,Testability,Log,LogManager,1395,"me resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ```. This request was created from a contribution made by Pryce Turner on July 29, 2022 03:44 UTC. Link: [",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983
https://github.com/broadinstitute/gatk/issues/7984:658,Availability,FAILURE,FAILURE,658,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:842,Availability,down,download,842,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:1056,Availability,down,download,1056,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:1079,Availability,down,download,1079,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:102,Deployability,release,release,102,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:363,Deployability,release,release,363,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:457,Deployability,continuous,continuous,457,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:577,Deployability,release,release-notes,577,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:503,Integrability,depend,dependency,503,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:815,Modifiability,Plugin,Plugin,815,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:933,Modifiability,Plugin,Plugins,933,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:942,Modifiability,plugin,plugin,942,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:986,Modifiability,Plugin,Plugin,986,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:1025,Modifiability,plugin,plugin,1025,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:1095,Modifiability,plugin,plugin,1095,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:1167,Modifiability,Plugin,Plugin,1167,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:172,Testability,test,test,172,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7984:446,Usability,responsiv,responsive,446,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984
https://github.com/broadinstitute/gatk/issues/7986:426,Availability,error,errors,426,"Hello - we're trying to run Funcotator with a custom data source, where that source is a locatableXsv (i.e. simple tab-delimited file with columns for contig, start, and end). I believe I understand how to make this TSV and the config file. The issue is that I dont see a way to create the index (i.e. tsv.idx), and GATK fails when I try to run against a data source without the index. Not that surprisingly, IndexFeatureFile errors when trying to index a TSV saying ""no suitable codecs found"". Is there another tool that's able to make indexes on simple TSVs?. FWIW, the only example LocatableXsv source I could find in the default data sources is Oreganno. The majority of TSV-based sources are simpleXSV and just map using Gene symbol (so apparently no index is required). When I try to index the existing oreganno.tsv file, I get the same problem. I dont know how that original index was created. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7986
https://github.com/broadinstitute/gatk/issues/7986:228,Modifiability,config,config,228,"Hello - we're trying to run Funcotator with a custom data source, where that source is a locatableXsv (i.e. simple tab-delimited file with columns for contig, start, and end). I believe I understand how to make this TSV and the config file. The issue is that I dont see a way to create the index (i.e. tsv.idx), and GATK fails when I try to run against a data source without the index. Not that surprisingly, IndexFeatureFile errors when trying to index a TSV saying ""no suitable codecs found"". Is there another tool that's able to make indexes on simple TSVs?. FWIW, the only example LocatableXsv source I could find in the default data sources is Oreganno. The majority of TSV-based sources are simpleXSV and just map using Gene symbol (so apparently no index is required). When I try to index the existing oreganno.tsv file, I get the same problem. I dont know how that original index was created. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7986
https://github.com/broadinstitute/gatk/issues/7986:108,Usability,simpl,simple,108,"Hello - we're trying to run Funcotator with a custom data source, where that source is a locatableXsv (i.e. simple tab-delimited file with columns for contig, start, and end). I believe I understand how to make this TSV and the config file. The issue is that I dont see a way to create the index (i.e. tsv.idx), and GATK fails when I try to run against a data source without the index. Not that surprisingly, IndexFeatureFile errors when trying to index a TSV saying ""no suitable codecs found"". Is there another tool that's able to make indexes on simple TSVs?. FWIW, the only example LocatableXsv source I could find in the default data sources is Oreganno. The majority of TSV-based sources are simpleXSV and just map using Gene symbol (so apparently no index is required). When I try to index the existing oreganno.tsv file, I get the same problem. I dont know how that original index was created. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7986
https://github.com/broadinstitute/gatk/issues/7986:548,Usability,simpl,simple,548,"Hello - we're trying to run Funcotator with a custom data source, where that source is a locatableXsv (i.e. simple tab-delimited file with columns for contig, start, and end). I believe I understand how to make this TSV and the config file. The issue is that I dont see a way to create the index (i.e. tsv.idx), and GATK fails when I try to run against a data source without the index. Not that surprisingly, IndexFeatureFile errors when trying to index a TSV saying ""no suitable codecs found"". Is there another tool that's able to make indexes on simple TSVs?. FWIW, the only example LocatableXsv source I could find in the default data sources is Oreganno. The majority of TSV-based sources are simpleXSV and just map using Gene symbol (so apparently no index is required). When I try to index the existing oreganno.tsv file, I get the same problem. I dont know how that original index was created. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7986
https://github.com/broadinstitute/gatk/issues/7986:697,Usability,simpl,simpleXSV,697,"Hello - we're trying to run Funcotator with a custom data source, where that source is a locatableXsv (i.e. simple tab-delimited file with columns for contig, start, and end). I believe I understand how to make this TSV and the config file. The issue is that I dont see a way to create the index (i.e. tsv.idx), and GATK fails when I try to run against a data source without the index. Not that surprisingly, IndexFeatureFile errors when trying to index a TSV saying ""no suitable codecs found"". Is there another tool that's able to make indexes on simple TSVs?. FWIW, the only example LocatableXsv source I could find in the default data sources is Oreganno. The majority of TSV-based sources are simpleXSV and just map using Gene symbol (so apparently no index is required). When I try to index the existing oreganno.tsv file, I get the same problem. I dont know how that original index was created. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7986
https://github.com/broadinstitute/gatk/issues/7988:171,Safety,detect,detected,171,"To-do's for CompareReferences and CheckReferenceCompatibility:. CompareReferences; * add functionality to run base-level comparison modes on specified sequences (not just detected mismatching sequences); * add option to ignore case-level differences in FIND_SNPs mode ; * squash contiguous SNPs in FIND_SNPS mode . CheckReferenceCompatibility; * change wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7988
https://github.com/broadinstitute/gatk/pull/7989:384,Deployability,pipeline,pipeline,384,"Documents steps to clean up GVS assets in the following use case:; - the samples in the callset are going to be joint called once (no sub-cohorts, no re-using the model); - the outputs (VCFs, indexes, interval lists, manifest, sample name list) have all been copied to an independent location (not in the workspace bucket); - no need to maintain provenance; - the workspace where the pipeline was run is only being used for this GVS callset",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7989
https://github.com/broadinstitute/gatk/issues/7991:208,Deployability,integrat,integration,208,"This came up on PR #6351 after a rebase. The branch passes all tests locally (verified on my laptop and on @cmnbroad's) but fails `DocumentationGenerationIntegrationTest.documentationSmokeTest` in the docker integration CI job on Java 8 but not Java 11. The same test passed locally for me even after verifying Java 8 was enabled, refreshing the gradle project, rebasing again etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991
https://github.com/broadinstitute/gatk/issues/7991:208,Integrability,integrat,integration,208,"This came up on PR #6351 after a rebase. The branch passes all tests locally (verified on my laptop and on @cmnbroad's) but fails `DocumentationGenerationIntegrationTest.documentationSmokeTest` in the docker integration CI job on Java 8 but not Java 11. The same test passed locally for me even after verifying Java 8 was enabled, refreshing the gradle project, rebasing again etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991
https://github.com/broadinstitute/gatk/issues/7991:63,Testability,test,tests,63,"This came up on PR #6351 after a rebase. The branch passes all tests locally (verified on my laptop and on @cmnbroad's) but fails `DocumentationGenerationIntegrationTest.documentationSmokeTest` in the docker integration CI job on Java 8 but not Java 11. The same test passed locally for me even after verifying Java 8 was enabled, refreshing the gradle project, rebasing again etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991
https://github.com/broadinstitute/gatk/issues/7991:263,Testability,test,test,263,"This came up on PR #6351 after a rebase. The branch passes all tests locally (verified on my laptop and on @cmnbroad's) but fails `DocumentationGenerationIntegrationTest.documentationSmokeTest` in the docker integration CI job on Java 8 but not Java 11. The same test passed locally for me even after verifying Java 8 was enabled, refreshing the gradle project, rebasing again etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991
https://github.com/broadinstitute/gatk/pull/7994:6,Deployability,update,update,6,Minor update to AoU Documentation; Update to include link to Firecloud support script which can be used to upload sample sets without using the UI.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7994
https://github.com/broadinstitute/gatk/pull/7994:35,Deployability,Update,Update,35,Minor update to AoU Documentation; Update to include link to Firecloud support script which can be used to upload sample sets without using the UI.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7994
https://github.com/broadinstitute/gatk/pull/7995:45,Availability,error,error,45,Change backticks to single quotes in several error messages - causing shell to attempt to execute. ```; /cromwell_root/script: line 26: load_data_batch_size: command not found; Importing 25000 samples but not explicitly specified; limit for auto batch-sizing is 20000 samples.; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7995
https://github.com/broadinstitute/gatk/pull/7995:51,Integrability,message,messages,51,Change backticks to single quotes in several error messages - causing shell to attempt to execute. ```; /cromwell_root/script: line 26: load_data_batch_size: command not found; Importing 25000 samples but not explicitly specified; limit for auto batch-sizing is 20000 samples.; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7995
https://github.com/broadinstitute/gatk/pull/7996:80,Deployability,pipeline,pipeline,80,"Currently, there is an argument to keep all raw annotations, but the flow based pipeline has a use case that needs to keep `RAW_GT_COUNT` without keeping the other raw annotations. This new argument allows the user to keep `RAW_GT_COUNT` while still cleaning up the other raw annotations in GenotypeGVCFs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7996
https://github.com/broadinstitute/gatk/pull/7997:44,Testability,test,tests,44,"In debugging Java 17 issues with the docker tests, I noticed some lines in the Docker file were missing the append i/o redirection that I assume was intended to be included. Not sure what impact this will have so running the build with these changes as a test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7997
https://github.com/broadinstitute/gatk/pull/7997:255,Testability,test,test,255,"In debugging Java 17 issues with the docker tests, I noticed some lines in the Docker file were missing the append i/o redirection that I assume was intended to be included. Not sure what impact this will have so running the build with these changes as a test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7997
https://github.com/broadinstitute/gatk/pull/7998:69,Deployability,update,updated,69,"I tested this by:; - running the `GvsAssignIds` workflow; - manually updated the `sample_ids` to span from 1 to 15535; - manually created the `vet_002`, `vet_003`, `vet_004`, `ref_ranges_002`, `ref_ranges_003`, `ref_ranged_004` tables; - running the `GvsImportGenomes` workflow; - running the `GvsPopulateAltAllele` workflow with `max_alt_allele_shards` to 3 so that it would divide the vet tables into (at most) 3 files; see https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/b1b319bc-a2aa-44cb-ad9a-079b7c1c33de for `GvsPopulateAltAllele` run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7998
https://github.com/broadinstitute/gatk/pull/7998:2,Testability,test,tested,2,"I tested this by:; - running the `GvsAssignIds` workflow; - manually updated the `sample_ids` to span from 1 to 15535; - manually created the `vet_002`, `vet_003`, `vet_004`, `ref_ranges_002`, `ref_ranges_003`, `ref_ranged_004` tables; - running the `GvsImportGenomes` workflow; - running the `GvsPopulateAltAllele` workflow with `max_alt_allele_shards` to 3 so that it would divide the vet tables into (at most) 3 files; see https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/b1b319bc-a2aa-44cb-ad9a-079b7c1c33de for `GvsPopulateAltAllele` run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7998
https://github.com/broadinstitute/gatk/pull/8004:168,Deployability,integrat,integration,168,"Minimal GATK port of nvscorevariants from https://github.com/NVIDIA-Genomics-Research/nvscorevariants. The tool runs successfully in both 1D and 2D modes, and a strict integration test passes for the 1D model. However, this PR has a number of outstanding issues that need to be resolved before it can be merged and replace the legacy CNNScoreVariants tool:. - The conda environment in scripts/nvscorevariants_environment.yml needs to be incorporated into the main GATK conda environment. - The integration test for the 2D model does not currently pass, despite using a much higher epsilon than the 1D test. Some of the scores differ by significant amounts vs. the CNNScoreVariants 2D output. We need to investigate why this is. - There is currently no training tool to train a new model, like there is for the legacy CNN tool. @samuelklee and @mwalker174 , could you please comment on what it would take to incorporate the `scripts/nvscorevariants_environment.yml` conda environment into the main GATK conda environment, assuming we are free to remove/retire the CNN tool?. @lbergelson and @zamirai, please do a general code review when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8004
https://github.com/broadinstitute/gatk/pull/8004:494,Deployability,integrat,integration,494,"Minimal GATK port of nvscorevariants from https://github.com/NVIDIA-Genomics-Research/nvscorevariants. The tool runs successfully in both 1D and 2D modes, and a strict integration test passes for the 1D model. However, this PR has a number of outstanding issues that need to be resolved before it can be merged and replace the legacy CNNScoreVariants tool:. - The conda environment in scripts/nvscorevariants_environment.yml needs to be incorporated into the main GATK conda environment. - The integration test for the 2D model does not currently pass, despite using a much higher epsilon than the 1D test. Some of the scores differ by significant amounts vs. the CNNScoreVariants 2D output. We need to investigate why this is. - There is currently no training tool to train a new model, like there is for the legacy CNN tool. @samuelklee and @mwalker174 , could you please comment on what it would take to incorporate the `scripts/nvscorevariants_environment.yml` conda environment into the main GATK conda environment, assuming we are free to remove/retire the CNN tool?. @lbergelson and @zamirai, please do a general code review when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8004
https://github.com/broadinstitute/gatk/pull/8004:168,Integrability,integrat,integration,168,"Minimal GATK port of nvscorevariants from https://github.com/NVIDIA-Genomics-Research/nvscorevariants. The tool runs successfully in both 1D and 2D modes, and a strict integration test passes for the 1D model. However, this PR has a number of outstanding issues that need to be resolved before it can be merged and replace the legacy CNNScoreVariants tool:. - The conda environment in scripts/nvscorevariants_environment.yml needs to be incorporated into the main GATK conda environment. - The integration test for the 2D model does not currently pass, despite using a much higher epsilon than the 1D test. Some of the scores differ by significant amounts vs. the CNNScoreVariants 2D output. We need to investigate why this is. - There is currently no training tool to train a new model, like there is for the legacy CNN tool. @samuelklee and @mwalker174 , could you please comment on what it would take to incorporate the `scripts/nvscorevariants_environment.yml` conda environment into the main GATK conda environment, assuming we are free to remove/retire the CNN tool?. @lbergelson and @zamirai, please do a general code review when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8004
https://github.com/broadinstitute/gatk/pull/8004:494,Integrability,integrat,integration,494,"Minimal GATK port of nvscorevariants from https://github.com/NVIDIA-Genomics-Research/nvscorevariants. The tool runs successfully in both 1D and 2D modes, and a strict integration test passes for the 1D model. However, this PR has a number of outstanding issues that need to be resolved before it can be merged and replace the legacy CNNScoreVariants tool:. - The conda environment in scripts/nvscorevariants_environment.yml needs to be incorporated into the main GATK conda environment. - The integration test for the 2D model does not currently pass, despite using a much higher epsilon than the 1D test. Some of the scores differ by significant amounts vs. the CNNScoreVariants 2D output. We need to investigate why this is. - There is currently no training tool to train a new model, like there is for the legacy CNN tool. @samuelklee and @mwalker174 , could you please comment on what it would take to incorporate the `scripts/nvscorevariants_environment.yml` conda environment into the main GATK conda environment, assuming we are free to remove/retire the CNN tool?. @lbergelson and @zamirai, please do a general code review when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8004
https://github.com/broadinstitute/gatk/pull/8004:180,Testability,test,test,180,"Minimal GATK port of nvscorevariants from https://github.com/NVIDIA-Genomics-Research/nvscorevariants. The tool runs successfully in both 1D and 2D modes, and a strict integration test passes for the 1D model. However, this PR has a number of outstanding issues that need to be resolved before it can be merged and replace the legacy CNNScoreVariants tool:. - The conda environment in scripts/nvscorevariants_environment.yml needs to be incorporated into the main GATK conda environment. - The integration test for the 2D model does not currently pass, despite using a much higher epsilon than the 1D test. Some of the scores differ by significant amounts vs. the CNNScoreVariants 2D output. We need to investigate why this is. - There is currently no training tool to train a new model, like there is for the legacy CNN tool. @samuelklee and @mwalker174 , could you please comment on what it would take to incorporate the `scripts/nvscorevariants_environment.yml` conda environment into the main GATK conda environment, assuming we are free to remove/retire the CNN tool?. @lbergelson and @zamirai, please do a general code review when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8004
https://github.com/broadinstitute/gatk/pull/8004:506,Testability,test,test,506,"Minimal GATK port of nvscorevariants from https://github.com/NVIDIA-Genomics-Research/nvscorevariants. The tool runs successfully in both 1D and 2D modes, and a strict integration test passes for the 1D model. However, this PR has a number of outstanding issues that need to be resolved before it can be merged and replace the legacy CNNScoreVariants tool:. - The conda environment in scripts/nvscorevariants_environment.yml needs to be incorporated into the main GATK conda environment. - The integration test for the 2D model does not currently pass, despite using a much higher epsilon than the 1D test. Some of the scores differ by significant amounts vs. the CNNScoreVariants 2D output. We need to investigate why this is. - There is currently no training tool to train a new model, like there is for the legacy CNN tool. @samuelklee and @mwalker174 , could you please comment on what it would take to incorporate the `scripts/nvscorevariants_environment.yml` conda environment into the main GATK conda environment, assuming we are free to remove/retire the CNN tool?. @lbergelson and @zamirai, please do a general code review when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8004
https://github.com/broadinstitute/gatk/pull/8004:601,Testability,test,test,601,"Minimal GATK port of nvscorevariants from https://github.com/NVIDIA-Genomics-Research/nvscorevariants. The tool runs successfully in both 1D and 2D modes, and a strict integration test passes for the 1D model. However, this PR has a number of outstanding issues that need to be resolved before it can be merged and replace the legacy CNNScoreVariants tool:. - The conda environment in scripts/nvscorevariants_environment.yml needs to be incorporated into the main GATK conda environment. - The integration test for the 2D model does not currently pass, despite using a much higher epsilon than the 1D test. Some of the scores differ by significant amounts vs. the CNNScoreVariants 2D output. We need to investigate why this is. - There is currently no training tool to train a new model, like there is for the legacy CNN tool. @samuelklee and @mwalker174 , could you please comment on what it would take to incorporate the `scripts/nvscorevariants_environment.yml` conda environment into the main GATK conda environment, assuming we are free to remove/retire the CNN tool?. @lbergelson and @zamirai, please do a general code review when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8004
https://github.com/broadinstitute/gatk/issues/8005:337,Availability,error,errors,337,"This request was created from a contribution made by Duo Xie on August 20, 2022 16:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --know",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:544,Availability,error,error,544,"This request was created from a contribution made by Duo Xie on August 20, 2022 16:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --know",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:5164,Availability,down,down,5164,"1 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 00:09:41.681 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 00:09:41.681 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 00:09:41.682 INFO  BaseRecalibrator - Deflater: JdkDeflater ; ; 00:09:41.682 INFO  BaseRecalibrator - Inflater: JdkInflater ; ; 00:09:41.682 INFO  BaseRecalibrator - GCS max retries/reopens: 20 ; ; 00:09:41.682 INFO  BaseRecalibrator - Requester pays: disabled ; ; 00:09:41.682 INFO  BaseRecalibrator - Initializing engine ; ; 00:09:41.884 WARN  IntelInflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater ; ; 00:09:41.888 WARN  IntelInflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater ; ; 00:09:42.030 WARN  IntelInflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater ; ; 00:09:42.036 INFO  BaseRecalibrator - Shutting down engine ; ; \[August 21, 2022 at 12:09:42 AM CST\] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.01 minutes. ; ; Runtime.totalMemory()=1140850688 ; ; org.broadinstitute.hellbender.exceptions.GATKException: Unable to automatically instantiate codec org.broadinstitute.hellbender.utils.codecs.AnnotatedIntervalCodec ; ;     at org.broadinstitute.hellbender.engine.FeatureManager.getCandidateCodecsForFile(FeatureManager.java:535) ; ;     at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:482) ; ;     at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:397) ; ;     at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:373) ; ;     at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319) ; ;     at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291) ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:7345,Availability,error,error,7345,"eSources(FeatureManager.java:208) ; ;     at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:155) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.initializeFeatures(ReadWalker.java:72) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:726) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). And I will get the same error when I assign the temp directory in another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:11942,Availability,down,down,11942,"4 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 00:11:11.814 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 00:11:11.814 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 00:11:11.814 INFO  BaseRecalibrator - Deflater: JdkDeflater ; ; 00:11:11.815 INFO  BaseRecalibrator - Inflater: JdkInflater ; ; 00:11:11.815 INFO  BaseRecalibrator - GCS max retries/reopens: 20 ; ; 00:11:11.815 INFO  BaseRecalibrator - Requester pays: disabled ; ; 00:11:11.815 INFO  BaseRecalibrator - Initializing engine ; ; 00:11:12.005 WARN  IntelInflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater ; ; 00:11:12.009 WARN  IntelInflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater ; ; 00:11:12.127 WARN  IntelInflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater ; ; 00:11:12.134 INFO  BaseRecalibrator - Shutting down engine ; ; \[August 21, 2022 at 12:11:12 AM CST\] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.01 minutes. ; ; Runtime.totalMemory()=285212672 ; ; org.broadinstitute.hellbender.exceptions.GATKException: Unable to automatically instantiate codec org.broadinstitute.hellbender.utils.codecs.AnnotatedIntervalCodec ; ;     at org.broadinstitute.hellbender.engine.FeatureManager.getCandidateCodecsForFile(FeatureManager.java:535) ; ;     at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:482) ; ;     at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:397) ; ;     at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:373) ; ;     at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319) ; ;     at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291) ; ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:610,Deployability,pipeline,pipeline,610,"This request was created from a contribution made by Duo Xie on August 20, 2022 16:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --know",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:948,Deployability,pipeline,pipeline,948,"This request was created from a contribution made by Duo Xie on August 20, 2022 16:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --know",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:1322,Deployability,pipeline,pipeline,1322,"ator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  Native",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:1691,Deployability,pipeline,pipeline,1691,"tions ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:1948,Deployability,pipeline,pipeline,1948,pe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:2392,Deployability,pipeline,pipeline,2392, Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:09:41.679 INFO  Base,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:2753,Deployability,pipeline,pipeline,2753,".jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:09:41.679 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:09:41.679 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:09:41.680 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:09:41 AM CST ; ; 00:09:41.680 INFO  BaseRecalibrator - --------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:7424,Deployability,pipeline,pipeline,7424,"e.hellbender.engine.FeatureManager.<init>(FeatureManager.java:155) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.initializeFeatures(ReadWalker.java:72) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:726) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). And I will get the same error when I assign the temp directory in another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:7680,Deployability,pipeline,pipeline,7680,"    at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). And I will get the same error when I assign the temp directory in another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:8118,Deployability,pipeline,pipeline,8118,"n(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). And I will get the same error when I assign the temp directory in another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:8405,Deployability,pipeline,pipeline,8405," another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:8662,Deployability,pipeline,pipeline,8662,ata/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  Nat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:9170,Deployability,pipeline,pipeline,9170,.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO  Base,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:9531,Deployability,pipeline,pipeline,9531,"piens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:11:11.813 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:11:11.813 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:11:11 AM CST ; ; 00:11:11.813 INFO  BaseRecalibrator - --------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:14196,Deployability,pipeline,pipeline,14196,"titute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:155) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.initializeFeatures(ReadWalker.java:72) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:726) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /dat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:14452,Deployability,pipeline,pipeline,14452," ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:14826,Deployability,pipeline,pipeline,14826,"ine.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pip",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:15113,Deployability,pipeline,pipeline,15113,"9). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:12:21.140 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.141 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:15370,Deployability,pipeline,pipeline,15370,data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:12:21.140 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.141 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:12:21.141 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:12:21.141 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:15814,Deployability,pipeline,pipeline,15814,"data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:12:21.140 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.141 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:12:21.141 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:12:21.141 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:12:21.141 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:12:21.142 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:12:20 AM CST ; ; 00:12:21.142 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.142 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:17815,Deployability,pipeline,pipeline,17815,"-- ; ; 00:12:21.142 INFO  BaseRecalibrator - HTSJDK Version: 2.24.1 ; ; 00:12:21.143 INFO  BaseRecalibrator - Picard Version: 2.27.1 ; ; 00:12:21.143 INFO  BaseRecalibrator - Built for Spark Version: 2.4.5 ; ; 00:12:21.143 INFO  BaseRecalibrator - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 00:12:21.143 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 00:12:21.143 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 00:12:21.143 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 00:12:21.143 INFO  BaseRecalibrator - Deflater: IntelDeflater ; ; 00:12:21.144 INFO  BaseRecalibrator - Inflater: IntelInflater ; ; 00:12:21.144 INFO  BaseRecalibrator - GCS max retries/reopens: 20 ; ; 00:12:21.144 INFO  BaseRecalibrator - Requester pays: disabled ; ; 00:12:21.144 INFO  BaseRecalibrator - Initializing engine ; ; 00:12:21.485 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz ; ; 00:12:21.565 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz ; ; 00:12:21.688 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz ; ; 00:12:21.797 WARN  IndexUtils - Feature file ""file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file ; ; 00:12:21.895 WARN  IntelInflater - Zero Bytes Written : 0 ; ; 00:12:21.966 INFO  BaseRecalibrator - Done initializing engine ; ; 00:12:21.969 INFO  BaseRecalibrationEngine - The covariates being used here: ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     ReadGroupCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     QualityScoreCovari",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:18267,Deployability,pipeline,pipeline,18267,"RITE\_FOR\_SAMTOOLS : true ; ; 00:12:21.143 INFO  BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 00:12:21.143 INFO  BaseRecalibrator - Deflater: IntelDeflater ; ; 00:12:21.144 INFO  BaseRecalibrator - Inflater: IntelInflater ; ; 00:12:21.144 INFO  BaseRecalibrator - GCS max retries/reopens: 20 ; ; 00:12:21.144 INFO  BaseRecalibrator - Requester pays: disabled ; ; 00:12:21.144 INFO  BaseRecalibrator - Initializing engine ; ; 00:12:21.485 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz ; ; 00:12:21.565 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz ; ; 00:12:21.688 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz ; ; 00:12:21.797 WARN  IndexUtils - Feature file ""file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file ; ; 00:12:21.895 WARN  IntelInflater - Zero Bytes Written : 0 ; ; 00:12:21.966 INFO  BaseRecalibrator - Done initializing engine ; ; 00:12:21.969 INFO  BaseRecalibrationEngine - The covariates being used here: ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     ReadGroupCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     QualityScoreCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     ContextCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     CycleCovariate ; ; 00:12:22.016 INFO  ProgressMeter - Starting traversal ; ; 00:12:22.017 INFO  ProgressMeter -        Current Locus  Elapsed Minutes       Reads Processed     Reads/Minute. **How can I assign a temp directory and won't get the bug?**. I set the gatk environment using conda:. /data/xieduo/WES\_pipe/pipeline/bin/Minicond",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:19230,Deployability,pipeline,pipeline,19230,"seRecalibrator - Inflater: IntelInflater ; ; 00:12:21.144 INFO  BaseRecalibrator - GCS max retries/reopens: 20 ; ; 00:12:21.144 INFO  BaseRecalibrator - Requester pays: disabled ; ; 00:12:21.144 INFO  BaseRecalibrator - Initializing engine ; ; 00:12:21.485 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz ; ; 00:12:21.565 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz ; ; 00:12:21.688 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz ; ; 00:12:21.797 WARN  IndexUtils - Feature file ""file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file ; ; 00:12:21.895 WARN  IntelInflater - Zero Bytes Written : 0 ; ; 00:12:21.966 INFO  BaseRecalibrator - Done initializing engine ; ; 00:12:21.969 INFO  BaseRecalibrationEngine - The covariates being used here: ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     ReadGroupCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     QualityScoreCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     ContextCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     CycleCovariate ; ; 00:12:22.016 INFO  ProgressMeter - Starting traversal ; ; 00:12:22.017 INFO  ProgressMeter -        Current Locus  Elapsed Minutes       Reads Processed     Reads/Minute. **How can I assign a temp directory and won't get the bug?**. I set the gatk environment using conda:. /data/xieduo/WES\_pipe/pipeline/bin/Miniconda3/bin/conda env create -n gatk\_4.2.6.1 -f gatkcondaenv.yml. Thank you!. Best,. Duo<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/293634'>Zendesk ticket #293634</a>)<br> gz#293634</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:2324,Performance,Load,Loading,2324,pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://so,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:2549,Performance,load,load,2549,se -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:09:41.679 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:09:41.679 INFO  BaseRecalibrator - Java runtime: Java HotSpot,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:2685,Performance,Load,Loading,2685,"S\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:09:41.679 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:09:41.679 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:09:41.680 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:09:41 AM CST ; ; 00:09:41.6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:2910,Performance,load,load,2910,"rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:09:41.679 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:09:41.679 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:09:41.680 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:09:41 AM CST ; ; 00:09:41.680 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.680 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.681 INFO  Base",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:9102,Performance,Load,Loading,9102,/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://so,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:9327,Performance,load,load,9327,write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:11:11.813 INFO  BaseRecalibrator - Java runtime: Java HotSpot,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:9463,Performance,Load,Loading,9463," BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:11:11.813 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:11:11.813 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:11:11 AM CST ; ; 00:11:11.8",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:9688,Performance,load,load,9688,"source/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:11:11.813 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:11:11.813 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:11:11 AM CST ; ; 00:11:11.813 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.814 INFO  Base",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:15746,Performance,Load,Loading,15746,"ture/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:12:21.140 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.141 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:12:21.141 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:12:21.141 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:12:21.141 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:12:21.142 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:12:20 AM CST ; ; 00:12:21.142 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.142 INFO  BaseRecalibrator - ---------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:450,Testability,log,log,450,"This request was created from a contribution made by Duo Xie on August 20, 2022 16:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --know",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:1269,Testability,test,test,1269,"14427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:2268,Testability,test,test,2268,est.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO  BaseRecalibrator - For support and documentation go to [ht,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:14773,Testability,test,test,14773,"at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/issues/8005:15690,Testability,test,test,15690,"O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:12:21.140 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.141 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:12:21.141 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:12:21.141 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:12:21.141 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:12:21.142 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:12:20 AM CST ; ; 00:12:21.142 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005
https://github.com/broadinstitute/gatk/pull/8006:58,Testability,test,test,58,"Putting this out for early review as I slowly continue to test the changes. 'Hide whitespace' highly recommended when viewing the diffs due to PEP8-fueled horizontal whitespace fixes in a couple of Python scripts. <img width=""873"" alt=""Screen Shot 2022-08-30 at 8 31 37 AM"" src=""https://user-images.githubusercontent.com/10790523/187438182-d1e0fc8e-88bd-44b8-a286-37efb8aa9933.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8006
https://github.com/broadinstitute/gatk/issues/8007:833,Availability,error,error,833,"## Bug Report. ### Affected tool(s) or class(es); gatk MarkDuplicatesSpark. ### Affected version(s); - GATK 4.2.6.1; - Spark 3.2.1. ### Description ; File sizes are different between MarkDuplicates and MarkDuplicatesSpark (run locally). file sizes:; input cram: 1094584927; output bam (MarkDuplicates): 2839215419; output bam (MarkDuplicatesSpark): 3536690732. #### Steps to reproduce; command:. `java -Xmx200G -jar /opt/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar MarkDuplicatesSpark \; -I file.cram \; -O file_sorted_markduplicates.bam \; -M file_markduplicates_metrics.txt \; -R homo_sapiens.fa`. #### Expected behavior; output bam should be the same size (or very similar) between MarkDuplicates and MarkDuplicatesSpark. Note: this is when using the local version of the gatk package, using the spark version I get the following error:. `Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Partitioner; 	at java.lang.Class.getDeclaredConstructors0(Native Method); 	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671); 	at java.lang.Class.getConstructors(Class.java:1651); 	at org.broadinstitute.hellbender.utils.ClassUtils.canMakeInstances(ClassUtils.java:31); 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:319); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Partitioner; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:387); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 8 more`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8007
https://github.com/broadinstitute/gatk/issues/8007:1659,Performance,load,loadClass,1659,"## Bug Report. ### Affected tool(s) or class(es); gatk MarkDuplicatesSpark. ### Affected version(s); - GATK 4.2.6.1; - Spark 3.2.1. ### Description ; File sizes are different between MarkDuplicates and MarkDuplicatesSpark (run locally). file sizes:; input cram: 1094584927; output bam (MarkDuplicates): 2839215419; output bam (MarkDuplicatesSpark): 3536690732. #### Steps to reproduce; command:. `java -Xmx200G -jar /opt/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar MarkDuplicatesSpark \; -I file.cram \; -O file_sorted_markduplicates.bam \; -M file_markduplicates_metrics.txt \; -R homo_sapiens.fa`. #### Expected behavior; output bam should be the same size (or very similar) between MarkDuplicates and MarkDuplicatesSpark. Note: this is when using the local version of the gatk package, using the spark version I get the following error:. `Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Partitioner; 	at java.lang.Class.getDeclaredConstructors0(Native Method); 	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671); 	at java.lang.Class.getConstructors(Class.java:1651); 	at org.broadinstitute.hellbender.utils.ClassUtils.canMakeInstances(ClassUtils.java:31); 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:319); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Partitioner; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:387); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 8 more`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8007
https://github.com/broadinstitute/gatk/issues/8007:1729,Performance,load,loadClass,1729,"## Bug Report. ### Affected tool(s) or class(es); gatk MarkDuplicatesSpark. ### Affected version(s); - GATK 4.2.6.1; - Spark 3.2.1. ### Description ; File sizes are different between MarkDuplicates and MarkDuplicatesSpark (run locally). file sizes:; input cram: 1094584927; output bam (MarkDuplicates): 2839215419; output bam (MarkDuplicatesSpark): 3536690732. #### Steps to reproduce; command:. `java -Xmx200G -jar /opt/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar MarkDuplicatesSpark \; -I file.cram \; -O file_sorted_markduplicates.bam \; -M file_markduplicates_metrics.txt \; -R homo_sapiens.fa`. #### Expected behavior; output bam should be the same size (or very similar) between MarkDuplicates and MarkDuplicatesSpark. Note: this is when using the local version of the gatk package, using the spark version I get the following error:. `Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Partitioner; 	at java.lang.Class.getDeclaredConstructors0(Native Method); 	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671); 	at java.lang.Class.getConstructors(Class.java:1651); 	at org.broadinstitute.hellbender.utils.ClassUtils.canMakeInstances(ClassUtils.java:31); 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:319); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Partitioner; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:387); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 8 more`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8007
https://github.com/broadinstitute/gatk/issues/8007:1785,Performance,load,loadClass,1785,"## Bug Report. ### Affected tool(s) or class(es); gatk MarkDuplicatesSpark. ### Affected version(s); - GATK 4.2.6.1; - Spark 3.2.1. ### Description ; File sizes are different between MarkDuplicates and MarkDuplicatesSpark (run locally). file sizes:; input cram: 1094584927; output bam (MarkDuplicates): 2839215419; output bam (MarkDuplicatesSpark): 3536690732. #### Steps to reproduce; command:. `java -Xmx200G -jar /opt/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar MarkDuplicatesSpark \; -I file.cram \; -O file_sorted_markduplicates.bam \; -M file_markduplicates_metrics.txt \; -R homo_sapiens.fa`. #### Expected behavior; output bam should be the same size (or very similar) between MarkDuplicates and MarkDuplicatesSpark. Note: this is when using the local version of the gatk package, using the spark version I get the following error:. `Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Partitioner; 	at java.lang.Class.getDeclaredConstructors0(Native Method); 	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671); 	at java.lang.Class.getConstructors(Class.java:1651); 	at org.broadinstitute.hellbender.utils.ClassUtils.canMakeInstances(ClassUtils.java:31); 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:319); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Partitioner; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:387); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 8 more`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8007
https://github.com/broadinstitute/gatk/pull/8009:171,Testability,Test,Test,171,"Given a list of samples for a callset, withdraw samples from GVS that are not included by marking them as 'withdrawn' in the sample_info table with a passed in timestamp. Test run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/568919e6-3c51-4ce9-978c-75ffc5ff9997",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8009
https://github.com/broadinstitute/gatk/pull/8010:45,Security,expose,exposes,45,"This PR does most of the work for VS-565. It exposes the interval list and the sample list to whole way up the nested WDLs to GvsJointVariantCallng.wdl. Two minor things of note:; 1. sample_name_list is a File option and not a File because it's only computed inside of a branch of GvsExtractCallset where control_samples is false. If there is other behavior we want in the condition where it isn't computed, just let me know. This isn't an issue when it's run inside of GvsJointVariantCalling for the beta workflow though, and making it work there was the ultimate purpose of the ticket. 2. This PR does not fully complete the ticket. It will also require changes to the actual beta work space to add the necessary columns to the data table sample_set and change the outputs for the GvsJointVariantCalling workflow to map the new outputs to those columns. I have made these changes and test thems in my copy of the beta workflow, and can make the required changes in the one in gvs-prod once this PR is verified and merged.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8010
https://github.com/broadinstitute/gatk/pull/8010:886,Testability,test,test,886,"This PR does most of the work for VS-565. It exposes the interval list and the sample list to whole way up the nested WDLs to GvsJointVariantCallng.wdl. Two minor things of note:; 1. sample_name_list is a File option and not a File because it's only computed inside of a branch of GvsExtractCallset where control_samples is false. If there is other behavior we want in the condition where it isn't computed, just let me know. This isn't an issue when it's run inside of GvsJointVariantCalling for the beta workflow though, and making it work there was the ultimate purpose of the ticket. 2. This PR does not fully complete the ticket. It will also require changes to the actual beta work space to add the necessary columns to the data table sample_set and change the outputs for the GvsJointVariantCalling workflow to map the new outputs to those columns. I have made these changes and test thems in my copy of the beta workflow, and can make the required changes in the one in gvs-prod once this PR is verified and merged.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8010
https://github.com/broadinstitute/gatk/pull/8011:0,Testability,Test,Test,0,Test run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/7f2307e7-684d-45d7-a4e7-852a666ec440,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8011
https://github.com/broadinstitute/gatk/pull/8020:11,Performance,scalab,scalability,11,To address scalability failings with unbatched Avro exports.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8020
https://github.com/broadinstitute/gatk/pull/8022:0,Deployability,Integrat,Integration,0,Integration run here (failed because of known issue cost/output checks): https://job-manager.dsde-prod.broadinstitute.org/jobs/e904302b-e338-4bc7-bf7d-3ef1bfcd3fd9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8022
https://github.com/broadinstitute/gatk/pull/8022:0,Integrability,Integrat,Integration,0,Integration run here (failed because of known issue cost/output checks): https://job-manager.dsde-prod.broadinstitute.org/jobs/e904302b-e338-4bc7-bf7d-3ef1bfcd3fd9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8022
https://github.com/broadinstitute/gatk/pull/8023:56,Testability,test,tested,56,Gonna merge WDLs one at a time as they get changed then tested,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8023
https://github.com/broadinstitute/gatk/pull/8024:487,Safety,predict,predicted,487,"Simply replaced a custom version of the task with a standard one we are using elsewhere. Original run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20hatcher/job_history/193c7c2b-2d29-4bab-8abc-6ab85a2f5270. Run from the modified branch:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20hatcher/job_history/575121f1-07e9-4821-bb20-35b6ed430560. Both ran within my quickstart workspace, but were pointed at George's dataset. Both failed the same two predicted tasks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8024
https://github.com/broadinstitute/gatk/pull/8024:0,Usability,Simpl,Simply,0,"Simply replaced a custom version of the task with a standard one we are using elsewhere. Original run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20hatcher/job_history/193c7c2b-2d29-4bab-8abc-6ab85a2f5270. Run from the modified branch:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20hatcher/job_history/575121f1-07e9-4821-bb20-35b6ed430560. Both ran within my quickstart workspace, but were pointed at George's dataset. Both failed the same two predicted tasks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8024
https://github.com/broadinstitute/gatk/pull/8025:5,Deployability,upgrade,upgrades,5,This upgrades htsjdk to v3.0.0 which was attempted in #7867 and then reverted in #7960 in order to unblock the jukebox merge.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8025
https://github.com/broadinstitute/gatk/pull/8028:1421,Deployability,integrat,integration,1421,"Closes #7884. The bug was caused by two position-only checks that should have been contig+position checks. I added a minimal regression test that uses a 2-record snippet of the test data discussed in the first post of the issue. Also discussed there is expected behavior that this fix does not induce. It seems the expectation is that a QUAL = 0 variant block should be turned into a GQ0 reference block. However, I wonder if this is not actually representative of the current (pre-fix) behavior of the tool. For example, if the test data is run only over chr13 using master, the position is not dropped (since then the position-only checks are valid). In that case, we do not see a GQ0 reference block; we instead see a GQ40 reference block, since the original record had GQ61. With the fix, we reproduce this reference block. So although we do not induce the expected GQ0 behavior, I would say the bug is fixed. Whether or not we should issue an additional fix to induce the expected GQ0 behavior is another question entirely. I'm not completely sure what the expected behavior should be from the tool or code documentation alone. Someone more familiar with this tool (@droazen perhaps you can suggest?) may have to chime in. They should probably also check that the contig+position checks are all that need to be added to address the original bug; I'm not 100% sure about behavior there either, but at least all other integration tests seem to still pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8028
https://github.com/broadinstitute/gatk/pull/8028:1421,Integrability,integrat,integration,1421,"Closes #7884. The bug was caused by two position-only checks that should have been contig+position checks. I added a minimal regression test that uses a 2-record snippet of the test data discussed in the first post of the issue. Also discussed there is expected behavior that this fix does not induce. It seems the expectation is that a QUAL = 0 variant block should be turned into a GQ0 reference block. However, I wonder if this is not actually representative of the current (pre-fix) behavior of the tool. For example, if the test data is run only over chr13 using master, the position is not dropped (since then the position-only checks are valid). In that case, we do not see a GQ0 reference block; we instead see a GQ40 reference block, since the original record had GQ61. With the fix, we reproduce this reference block. So although we do not induce the expected GQ0 behavior, I would say the bug is fixed. Whether or not we should issue an additional fix to induce the expected GQ0 behavior is another question entirely. I'm not completely sure what the expected behavior should be from the tool or code documentation alone. Someone more familiar with this tool (@droazen perhaps you can suggest?) may have to chime in. They should probably also check that the contig+position checks are all that need to be added to address the original bug; I'm not 100% sure about behavior there either, but at least all other integration tests seem to still pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8028
https://github.com/broadinstitute/gatk/pull/8028:136,Testability,test,test,136,"Closes #7884. The bug was caused by two position-only checks that should have been contig+position checks. I added a minimal regression test that uses a 2-record snippet of the test data discussed in the first post of the issue. Also discussed there is expected behavior that this fix does not induce. It seems the expectation is that a QUAL = 0 variant block should be turned into a GQ0 reference block. However, I wonder if this is not actually representative of the current (pre-fix) behavior of the tool. For example, if the test data is run only over chr13 using master, the position is not dropped (since then the position-only checks are valid). In that case, we do not see a GQ0 reference block; we instead see a GQ40 reference block, since the original record had GQ61. With the fix, we reproduce this reference block. So although we do not induce the expected GQ0 behavior, I would say the bug is fixed. Whether or not we should issue an additional fix to induce the expected GQ0 behavior is another question entirely. I'm not completely sure what the expected behavior should be from the tool or code documentation alone. Someone more familiar with this tool (@droazen perhaps you can suggest?) may have to chime in. They should probably also check that the contig+position checks are all that need to be added to address the original bug; I'm not 100% sure about behavior there either, but at least all other integration tests seem to still pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8028
https://github.com/broadinstitute/gatk/pull/8028:177,Testability,test,test,177,"Closes #7884. The bug was caused by two position-only checks that should have been contig+position checks. I added a minimal regression test that uses a 2-record snippet of the test data discussed in the first post of the issue. Also discussed there is expected behavior that this fix does not induce. It seems the expectation is that a QUAL = 0 variant block should be turned into a GQ0 reference block. However, I wonder if this is not actually representative of the current (pre-fix) behavior of the tool. For example, if the test data is run only over chr13 using master, the position is not dropped (since then the position-only checks are valid). In that case, we do not see a GQ0 reference block; we instead see a GQ40 reference block, since the original record had GQ61. With the fix, we reproduce this reference block. So although we do not induce the expected GQ0 behavior, I would say the bug is fixed. Whether or not we should issue an additional fix to induce the expected GQ0 behavior is another question entirely. I'm not completely sure what the expected behavior should be from the tool or code documentation alone. Someone more familiar with this tool (@droazen perhaps you can suggest?) may have to chime in. They should probably also check that the contig+position checks are all that need to be added to address the original bug; I'm not 100% sure about behavior there either, but at least all other integration tests seem to still pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8028
https://github.com/broadinstitute/gatk/pull/8028:529,Testability,test,test,529,"Closes #7884. The bug was caused by two position-only checks that should have been contig+position checks. I added a minimal regression test that uses a 2-record snippet of the test data discussed in the first post of the issue. Also discussed there is expected behavior that this fix does not induce. It seems the expectation is that a QUAL = 0 variant block should be turned into a GQ0 reference block. However, I wonder if this is not actually representative of the current (pre-fix) behavior of the tool. For example, if the test data is run only over chr13 using master, the position is not dropped (since then the position-only checks are valid). In that case, we do not see a GQ0 reference block; we instead see a GQ40 reference block, since the original record had GQ61. With the fix, we reproduce this reference block. So although we do not induce the expected GQ0 behavior, I would say the bug is fixed. Whether or not we should issue an additional fix to induce the expected GQ0 behavior is another question entirely. I'm not completely sure what the expected behavior should be from the tool or code documentation alone. Someone more familiar with this tool (@droazen perhaps you can suggest?) may have to chime in. They should probably also check that the contig+position checks are all that need to be added to address the original bug; I'm not 100% sure about behavior there either, but at least all other integration tests seem to still pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8028
https://github.com/broadinstitute/gatk/pull/8028:1433,Testability,test,tests,1433,"Closes #7884. The bug was caused by two position-only checks that should have been contig+position checks. I added a minimal regression test that uses a 2-record snippet of the test data discussed in the first post of the issue. Also discussed there is expected behavior that this fix does not induce. It seems the expectation is that a QUAL = 0 variant block should be turned into a GQ0 reference block. However, I wonder if this is not actually representative of the current (pre-fix) behavior of the tool. For example, if the test data is run only over chr13 using master, the position is not dropped (since then the position-only checks are valid). In that case, we do not see a GQ0 reference block; we instead see a GQ40 reference block, since the original record had GQ61. With the fix, we reproduce this reference block. So although we do not induce the expected GQ0 behavior, I would say the bug is fixed. Whether or not we should issue an additional fix to induce the expected GQ0 behavior is another question entirely. I'm not completely sure what the expected behavior should be from the tool or code documentation alone. Someone more familiar with this tool (@droazen perhaps you can suggest?) may have to chime in. They should probably also check that the contig+position checks are all that need to be added to address the original bug; I'm not 100% sure about behavior there either, but at least all other integration tests seem to still pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8028
https://github.com/broadinstitute/gatk/pull/8029:0,Security,Expose,Expose,0,Expose the maximum-training-variants parameter for both INDEL and SNP model creation in the `GvsCreateFilterSet` WDL. Closes https://broadworkbench.atlassian.net/browse/VS-634,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8029
https://github.com/broadinstitute/gatk/issues/8030:1183,Availability,down,down,1183,"s-include-non-variant-sites](https://gatk.broadinstitute.org/hc/en-us/community/posts/6808932798363-Monomorphic-sites-after-GenotypeGVCFs-include-non-variant-sites). \--. Hello,. I am using GATKv4.2.6.1 and GATK best practices. I performed joint genotyping of a multi-sample GVCF with GenotypeGVCFs. Because I am doing a population genetic analysis I am very interested in obtaining high confidence monomorphic sites, so I included the option --include-non-variant-sites. In the output VCF, however, I find that there are 3 types of monomorphic sites, for example:. #CHROM                POS  ID   REF               ALT   QUAL     FILTER. HiC\_scaffold\_493    961    .    A                     .        .              . ; ; HiC\_scaffold\_493    962    .    ATCTCCCC    .        7.65        LowQual ; ; HiC\_scaffold\_493    963    .    T                     .        180.56    . I am not sure what the differences between those 3 types of monomorphic sites are. I tracked down those positions in the input GVCF and they look like this:. #CHROM                     POS  ID     REF                    ALT                     QUAL     FILTER. HiC\_scaffold\_493        961     .       A                        <NON\\\_REF>        .            .  ; ; HiC\_scaffold\_493        962     .       ATCTCCCC       A,<NON\\\_REF>     .            . ; ; HiC\_scaffold\_493        963     .       T                         \*,<NON\\\_REF>     .            . I assume that in the GVCF, position 962 had some evidence of the presence of an alternative allele (A) but it was so poor (QUAL < 30) that it was discarded and the position was deemed as monomorphic in the VCF (LowQual). But what about position 963? There was some evidence of a deletion (\*) as alternative allele in the GVCF but it got discarded in the VCF despite QUAL = 180.56?. Also, why does position 961 has no QUAL score at all? In fact, these are results from a small scaffold with 1,000 bp, of which 789 monomorphic sites have no QUAL score at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8030
https://github.com/broadinstitute/gatk/issues/8030:439,Performance,perform,performed,439,"This request was created from a contribution made by Diana Robledo on June 23, 2022 10:56 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6808932798363-Monomorphic-sites-after-GenotypeGVCFs-include-non-variant-sites](https://gatk.broadinstitute.org/hc/en-us/community/posts/6808932798363-Monomorphic-sites-after-GenotypeGVCFs-include-non-variant-sites). \--. Hello,. I am using GATKv4.2.6.1 and GATK best practices. I performed joint genotyping of a multi-sample GVCF with GenotypeGVCFs. Because I am doing a population genetic analysis I am very interested in obtaining high confidence monomorphic sites, so I included the option --include-non-variant-sites. In the output VCF, however, I find that there are 3 types of monomorphic sites, for example:. #CHROM                POS  ID   REF               ALT   QUAL     FILTER. HiC\_scaffold\_493    961    .    A                     .        .              . ; ; HiC\_scaffold\_493    962    .    ATCTCCCC    .        7.65        LowQual ; ; HiC\_scaffold\_493    963    .    T                     .        180.56    . I am not sure what the differences between those 3 types of monomorphic sites are. I tracked down those positions in the input GVCF and they look like this:. #CHROM                     POS  ID     REF                    ALT                     QUAL     FILTER. HiC\_scaffold\_493        961     .       A                        <NON\\\_REF>        .            .  ; ; HiC\_scaffold\_493        962     .       ATCTCCCC       A,<NON\\\_REF>     .            . ; ; HiC\_scaffold\_493        963     .       T                         \*,<NON\\\_REF>     .            . I assume that in the GVCF, position 962 had some evidence of the presence of an alternative allele (A) but it was so poor (QUAL < 30) that it was discarded and the position was deemed as monomorphic in the VCF (LowQual). But what about position 963? There was some evidence of a deletion (\*) as alternative allele in the GVCF but it got discard",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8030
https://github.com/broadinstitute/gatk/issues/8033:412,Usability,simpl,simply,412,"Recently added SV tools including SVConcordance and SVCluster make use of VariantContext's `getStructuralVariantType()` method, which returns a `StructuralVariantType` enum. This causes the tools to crash with some VCFs, such as those produced by gatk-sv where some SVTYPE values are `CPX` or `CTX`, which are non-standard. In practice, it will be useful allow these types to be ingested, which could be done by simply using the standard attribute getter rather than than `getStructuralVariantType()`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8033
https://github.com/broadinstitute/gatk/pull/8036:2,Deployability,upgrade,upgrade,2,* upgrade protobuf-java 3.19.4 -> 3.21.6,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8036
https://github.com/broadinstitute/gatk/issues/8040:522,Testability,test,test,522,"## Bug Report. getBasesAndBaseQualitiesAlignedOneToOne behaves differently, in respect to the existence of SOFT_CLIP, on reads that contains and do not contain INDELS (due to optimised code path). When no INDELS present, it returns arrays of clipped length; When INDELS present, it returns arrays of unclipped length (with NUL bytes past actual unclipped content). ### Affected tool(s) or class(es); AlignmentUtils.getBasesAndBaseQualitiesAlignedOneToOne . ### Affected version(s); Latest. ### Description ; When adding a test case (see steps to reproduce):. expected [CGATCG] but found [ATCGAT\0\0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8040
https://github.com/broadinstitute/gatk/issues/8040:643,Testability,test,test,643,"## Bug Report. getBasesAndBaseQualitiesAlignedOneToOne behaves differently, in respect to the existence of SOFT_CLIP, on reads that contains and do not contain INDELS (due to optimised code path). When no INDELS present, it returns arrays of clipped length; When INDELS present, it returns arrays of unclipped length (with NUL bytes past actual unclipped content). ### Affected tool(s) or class(es); AlignmentUtils.getBasesAndBaseQualitiesAlignedOneToOne . ### Affected version(s); Latest. ### Description ; When adding a test case (see steps to reproduce):. expected [CGATCG] but found [ATCGAT\0\0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8040
https://github.com/broadinstitute/gatk/issues/8040:794,Testability,test,tests,794,"## Bug Report. getBasesAndBaseQualitiesAlignedOneToOne behaves differently, in respect to the existence of SOFT_CLIP, on reads that contains and do not contain INDELS (due to optimised code path). When no INDELS present, it returns arrays of clipped length; When INDELS present, it returns arrays of unclipped length (with NUL bytes past actual unclipped content). ### Affected tool(s) or class(es); AlignmentUtils.getBasesAndBaseQualitiesAlignedOneToOne . ### Affected version(s); Latest. ### Description ; When adding a test case (see steps to reproduce):. expected [CGATCG] but found [ATCGAT\0\0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8040
https://github.com/broadinstitute/gatk/issues/8040:895,Testability,test,tests,895,"## Bug Report. getBasesAndBaseQualitiesAlignedOneToOne behaves differently, in respect to the existence of SOFT_CLIP, on reads that contains and do not contain INDELS (due to optimised code path). When no INDELS present, it returns arrays of clipped length; When INDELS present, it returns arrays of unclipped length (with NUL bytes past actual unclipped content). ### Affected tool(s) or class(es); AlignmentUtils.getBasesAndBaseQualitiesAlignedOneToOne . ### Affected version(s); Latest. ### Description ; When adding a test case (see steps to reproduce):. expected [CGATCG] but found [ATCGAT\0\0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8040
https://github.com/broadinstitute/gatk/issues/8040:993,Testability,test,test,993,"## Bug Report. getBasesAndBaseQualitiesAlignedOneToOne behaves differently, in respect to the existence of SOFT_CLIP, on reads that contains and do not contain INDELS (due to optimised code path). When no INDELS present, it returns arrays of clipped length; When INDELS present, it returns arrays of unclipped length (with NUL bytes past actual unclipped content). ### Affected tool(s) or class(es); AlignmentUtils.getBasesAndBaseQualitiesAlignedOneToOne . ### Affected version(s); Latest. ### Description ; When adding a test case (see steps to reproduce):. expected [CGATCG] but found [ATCGAT\0\0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8040
https://github.com/broadinstitute/gatk/issues/8043:42,Availability,avail,available,42,"Hi team,. GATK have a GRCh38 version that available in Resource bundle. It is not clear if this version have a masked duplicates.; Could you provide a GRCH38 version, ready to use, with masked duplicates? that can deal with this issue that affect on variants recall in these regions, such as CBS gene. B.W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8043
https://github.com/broadinstitute/gatk/issues/8043:111,Availability,mask,masked,111,"Hi team,. GATK have a GRCh38 version that available in Resource bundle. It is not clear if this version have a masked duplicates.; Could you provide a GRCH38 version, ready to use, with masked duplicates? that can deal with this issue that affect on variants recall in these regions, such as CBS gene. B.W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8043
https://github.com/broadinstitute/gatk/issues/8043:186,Availability,mask,masked,186,"Hi team,. GATK have a GRCh38 version that available in Resource bundle. It is not clear if this version have a masked duplicates.; Could you provide a GRCH38 version, ready to use, with masked duplicates? that can deal with this issue that affect on variants recall in these regions, such as CBS gene. B.W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8043
https://github.com/broadinstitute/gatk/issues/8043:82,Usability,clear,clear,82,"Hi team,. GATK have a GRCh38 version that available in Resource bundle. It is not clear if this version have a masked duplicates.; Could you provide a GRCH38 version, ready to use, with masked duplicates? that can deal with this issue that affect on variants recall in these regions, such as CBS gene. B.W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8043
https://github.com/broadinstitute/gatk/pull/8044:11,Deployability,integrat,integration,11,Successful integration run https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/c711a4cf-ac33-4c93-a4b7-b46b2796f090,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8044
https://github.com/broadinstitute/gatk/pull/8044:11,Integrability,integrat,integration,11,Successful integration run https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/c711a4cf-ac33-4c93-a4b7-b46b2796f090,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8044
https://github.com/broadinstitute/gatk/pull/8045:98,Deployability,Integrat,Integration,98,Trailing SNP sites and depth intervals without read coverage were being omitted from the output.; Integration tests have been updated to test that this revision solves that problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8045
https://github.com/broadinstitute/gatk/pull/8045:126,Deployability,update,updated,126,Trailing SNP sites and depth intervals without read coverage were being omitted from the output.; Integration tests have been updated to test that this revision solves that problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8045
https://github.com/broadinstitute/gatk/pull/8045:98,Integrability,Integrat,Integration,98,Trailing SNP sites and depth intervals without read coverage were being omitted from the output.; Integration tests have been updated to test that this revision solves that problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8045
https://github.com/broadinstitute/gatk/pull/8045:110,Testability,test,tests,110,Trailing SNP sites and depth intervals without read coverage were being omitted from the output.; Integration tests have been updated to test that this revision solves that problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8045
https://github.com/broadinstitute/gatk/pull/8045:137,Testability,test,test,137,Trailing SNP sites and depth intervals without read coverage were being omitted from the output.; Integration tests have been updated to test that this revision solves that problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8045
https://github.com/broadinstitute/gatk/pull/8047:47,Availability,error,errors,47,hardening standard quickstart pipeline against errors when a location is specified,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8047
https://github.com/broadinstitute/gatk/pull/8047:30,Deployability,pipeline,pipeline,30,hardening standard quickstart pipeline against errors when a location is specified,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8047
https://github.com/broadinstitute/gatk/pull/8048:12,Availability,avail,available,12,this is now available on cran again,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8048
https://github.com/broadinstitute/gatk/pull/8049:154,Deployability,release,release,154,"Ticks off a few straggler issues noted in #7724. @meganshand mind reviewing? Hopefully should be quick and we can get it in before @droazen cuts the next release. Note that this shouldn't change behavior in the Ultima pipeline, as the default toggle is still the same start-position resource-matching strategy inherited from VQSR, but we might want to explore the effect of choosing another strategy there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8049
https://github.com/broadinstitute/gatk/pull/8049:218,Deployability,pipeline,pipeline,218,"Ticks off a few straggler issues noted in #7724. @meganshand mind reviewing? Hopefully should be quick and we can get it in before @droazen cuts the next release. Note that this shouldn't change behavior in the Ultima pipeline, as the default toggle is still the same start-position resource-matching strategy inherited from VQSR, but we might want to explore the effect of choosing another strategy there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8049
https://github.com/broadinstitute/gatk/pull/8049:243,Deployability,toggle,toggle,243,"Ticks off a few straggler issues noted in #7724. @meganshand mind reviewing? Hopefully should be quick and we can get it in before @droazen cuts the next release. Note that this shouldn't change behavior in the Ultima pipeline, as the default toggle is still the same start-position resource-matching strategy inherited from VQSR, but we might want to explore the effect of choosing another strategy there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8049
https://github.com/broadinstitute/gatk/pull/8049:310,Modifiability,inherit,inherited,310,"Ticks off a few straggler issues noted in #7724. @meganshand mind reviewing? Hopefully should be quick and we can get it in before @droazen cuts the next release. Note that this shouldn't change behavior in the Ultima pipeline, as the default toggle is still the same start-position resource-matching strategy inherited from VQSR, but we might want to explore the effect of choosing another strategy there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8049
https://github.com/broadinstitute/gatk/pull/8050:101,Safety,avoid,avoid,101,"- Added minValue and maxValue to the pileup arguments; - fixed documentation for pileup arguments to avoid using ""percentage""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8050
https://github.com/broadinstitute/gatk/pull/8052:519,Deployability,update,update,519,"This PR addresses some issues with tracking of sample loading in GVSImportGenomes.; 1) Due to a bug (unresolved as yet) where the status 'STARTED' can be written to sample_load_status multiple times, the method LoadStatus.getSampleLoadState would NEVER return COMPLETE. This PR fixes the logic to allow for the (unexpected) case of multiple 'STARTED' statuses.; 2) I have modified the SetIsLoadedColumn task in GVSImportGenomes to explicitly look for sample_load_status records with both STARTED and FINISHED to use to update sample_info.is_loaded to true.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8052
https://github.com/broadinstitute/gatk/pull/8052:54,Performance,load,loading,54,"This PR addresses some issues with tracking of sample loading in GVSImportGenomes.; 1) Due to a bug (unresolved as yet) where the status 'STARTED' can be written to sample_load_status multiple times, the method LoadStatus.getSampleLoadState would NEVER return COMPLETE. This PR fixes the logic to allow for the (unexpected) case of multiple 'STARTED' statuses.; 2) I have modified the SetIsLoadedColumn task in GVSImportGenomes to explicitly look for sample_load_status records with both STARTED and FINISHED to use to update sample_info.is_loaded to true.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8052
https://github.com/broadinstitute/gatk/pull/8052:211,Performance,Load,LoadStatus,211,"This PR addresses some issues with tracking of sample loading in GVSImportGenomes.; 1) Due to a bug (unresolved as yet) where the status 'STARTED' can be written to sample_load_status multiple times, the method LoadStatus.getSampleLoadState would NEVER return COMPLETE. This PR fixes the logic to allow for the (unexpected) case of multiple 'STARTED' statuses.; 2) I have modified the SetIsLoadedColumn task in GVSImportGenomes to explicitly look for sample_load_status records with both STARTED and FINISHED to use to update sample_info.is_loaded to true.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8052
https://github.com/broadinstitute/gatk/pull/8052:288,Testability,log,logic,288,"This PR addresses some issues with tracking of sample loading in GVSImportGenomes.; 1) Due to a bug (unresolved as yet) where the status 'STARTED' can be written to sample_load_status multiple times, the method LoadStatus.getSampleLoadState would NEVER return COMPLETE. This PR fixes the logic to allow for the (unexpected) case of multiple 'STARTED' statuses.; 2) I have modified the SetIsLoadedColumn task in GVSImportGenomes to explicitly look for sample_load_status records with both STARTED and FINISHED to use to update sample_info.is_loaded to true.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8052
https://github.com/broadinstitute/gatk/issues/8054:141,Availability,error,error,141,"## Bug Report. ### Affected tool(s) or class(es); ApplyVQSR . ### Affected version(s); gatk-4.2.6.1. ### Description . Getting the following error. However, the chr6.raw.excessHet.vcf.gz does not contain that multiallelic variant site. This variant is the nearest at that location but the error is for a variant at chr6:26914009 with different alleles =[G*, GTGTA, GTGTATA, GTGTGTA] . . `chr6 26914005 . A ATG,G 15390.7 PASS AC=278,2;AF=0.04,0.0002879;AN=6948;AS_BaseQRankSum=-0.2,0.4;AS_FS=0.522,0;AS_InbreedingCoeff=-0.0012,0.1805;AS_MQ=58.75,59.2;AS_MQRankSum=0,-3.2;AS_QD=2.56,0.02;AS_ReadPosRankSum=0.1,0.3;AS_SOR=0.652,0.724;BaseQRankSum=-0.152;DP=118313;ExcessHet=2.9774;FS=0.518;InbreedingCoeff=0.0016;MLEAC=278,2;MLEAF=0.04,0.0002879;MQ=56.9;MQRankSum=-0.962;QD=2.57;ReadPosRankSum=0.193;SOR=0.712; `. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, ; #### Steps to reproduce; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054
https://github.com/broadinstitute/gatk/issues/8054:289,Availability,error,error,289,"## Bug Report. ### Affected tool(s) or class(es); ApplyVQSR . ### Affected version(s); gatk-4.2.6.1. ### Description . Getting the following error. However, the chr6.raw.excessHet.vcf.gz does not contain that multiallelic variant site. This variant is the nearest at that location but the error is for a variant at chr6:26914009 with different alleles =[G*, GTGTA, GTGTATA, GTGTGTA] . . `chr6 26914005 . A ATG,G 15390.7 PASS AC=278,2;AF=0.04,0.0002879;AN=6948;AS_BaseQRankSum=-0.2,0.4;AS_FS=0.522,0;AS_InbreedingCoeff=-0.0012,0.1805;AS_MQ=58.75,59.2;AS_MQRankSum=0,-3.2;AS_QD=2.56,0.02;AS_ReadPosRankSum=0.1,0.3;AS_SOR=0.652,0.724;BaseQRankSum=-0.152;DP=118313;ExcessHet=2.9774;FS=0.518;InbreedingCoeff=0.0016;MLEAC=278,2;MLEAF=0.04,0.0002879;MQ=56.9;MQRankSum=-0.962;QD=2.57;ReadPosRankSum=0.193;SOR=0.712; `. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, ; #### Steps to reproduce; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054
https://github.com/broadinstitute/gatk/issues/8054:1656,Availability,error,error,1656,".652,0.724;BaseQRankSum=-0.152;DP=118313;ExcessHet=2.9774;FS=0.518;InbreedingCoeff=0.0016;MLEAC=278,2;MLEAF=0.04,0.0002879;MQ=56.9;MQRankSum=-0.962;QD=2.57;ReadPosRankSum=0.193;SOR=0.712; `. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, ; #### Steps to reproduce; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL. Command exit status:; 3. Command output:; (empty). Command error:; 23:21:52.354 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:22:02.735 INFO ProgressMeter - chr6:1162012 0.2 25000 144494.8; 23:22:12.789 INFO ProgressMeter - chr6:2449556 0.3 53000 155623.0; 23:22:23.019 INFO ProgressMeter - chr6:3663394 0.5 82000 160448.7; 23:22:33.257 INFO ProgressMeter - chr6:4991347 0.7 112000 164291.1; 23:22:43.683 INFO ProgressMeter - chr6:6325045 0.9 141000",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054
https://github.com/broadinstitute/gatk/issues/8054:2186,Availability,error,error,2186,ted.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL. Command exit status:; 3. Command output:; (empty). Command error:; 23:21:52.354 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:22:02.735 INFO ProgressMeter - chr6:1162012 0.2 25000 144494.8; 23:22:12.789 INFO ProgressMeter - chr6:2449556 0.3 53000 155623.0; 23:22:23.019 INFO ProgressMeter - chr6:3663394 0.5 82000 160448.7; 23:22:33.257 INFO ProgressMeter - chr6:4991347 0.7 112000 164291.1; 23:22:43.683 INFO ProgressMeter - chr6:6325045 0.9 141000 164832.0; 23:22:53.824 INFO ProgressMeter - chr6:7646289 1.0 171000 166913.4; 23:23:03.973 INFO ProgressMeter - chr6:9029926 1.2 200000 167553.3; 23:23:14.220 INFO ProgressMeter - chr6:10374988 1.4 229000 167835.2; 23:23:24.322 INFO ProgressMeter - chr6:11782077 1.5 259000 168971.8; 23:23:34.465 INFO ProgressMeter - chr6:13360174 1.7 290000 170404.5; 23:23:44.556 INFO ProgressMeter - chr6:14757971 1.9 319000 170585.2; 23:23:54.657 INFO ProgressMeter - chr6:16217652 2.0 350000 171704.7; 23:24:04.905 INFO ProgressMeter - chr6:17737681 2.2 381000 172461.9; 23:24:15.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054
https://github.com/broadinstitute/gatk/issues/8054:3634,Availability,down,down,3634,"NFO ProgressMeter - chr6:7646289 1.0 171000 166913.4; 23:23:03.973 INFO ProgressMeter - chr6:9029926 1.2 200000 167553.3; 23:23:14.220 INFO ProgressMeter - chr6:10374988 1.4 229000 167835.2; 23:23:24.322 INFO ProgressMeter - chr6:11782077 1.5 259000 168971.8; 23:23:34.465 INFO ProgressMeter - chr6:13360174 1.7 290000 170404.5; 23:23:44.556 INFO ProgressMeter - chr6:14757971 1.9 319000 170585.2; 23:23:54.657 INFO ProgressMeter - chr6:16217652 2.0 350000 171704.7; 23:24:04.905 INFO ProgressMeter - chr6:17737681 2.2 381000 172461.9; 23:24:15.102 INFO ProgressMeter - chr6:19070725 2.4 409000 171911.3; 23:24:25.321 INFO ProgressMeter - chr6:20580950 2.5 441000 172978.5; 23:24:36.315 INFO ProgressMeter - chr6:22100346 2.7 472000 172724.0; 23:24:46.346 INFO ProgressMeter - chr6:23531348 2.9 502000 173112.4; 23:24:56.432 INFO ProgressMeter - chr6:24734131 3.1 531000 173078.8; 23:25:06.662 INFO ProgressMeter - chr6:26183595 3.2 559000 172612.6; 23:25:11.087 INFO ApplyVQSR - Shutting down engine; [October 12, 2022 11:25:11 PM EDT] org.broadinstitute.hellbender.tools.walkers.vqsr.ApplyVQSR done. Elapsed time: 3.33 minutes.; Runtime.totalMemory()=8242331648; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, 7.257e-04], AN=6890, AS_BaseQRankSum=[0.500, 0.500, 0.500], AS_FS=[0.544, 0.000, 0.000], AS_InbreedingCoeff=[0.0312, 0.0151, 0.0858], AS_MQ=[59.29, 57.89, 58.81], AS_MQRankSum=[0.000, -3.800, -3.800], AS_QD=[3.94, 0.05, 0.01], AS_ReadPosRankSum=[0.200, 0.300, 0.300], AS_SOR=[0.747, 0.705, 0.739], BaseQRankSum=0.515, DP=121924, ExcessHet=0.1315, FS=0.552, InbreedingCoeff=0.0304, MLEAC=[4273, 28, 5], MLEAF=[0.620, 4.064e-03, 7.257e-04], MQ=57.54, MQRankSum=-1.059e+00, QD=3.98, ReadPosRankSum=0.244, SOR=0.780} GT=GT:AD:DP:GQ:PGT:PID:PL:PS31/1:0,15,0,0:18:45:.:.:106,45,0,569,45,106,569,4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054
https://github.com/broadinstitute/gatk/issues/8054:1114,Deployability,install,install,1114,"etting the following error. However, the chr6.raw.excessHet.vcf.gz does not contain that multiallelic variant site. This variant is the nearest at that location but the error is for a variant at chr6:26914009 with different alleles =[G*, GTGTA, GTGTATA, GTGTGTA] . . `chr6 26914005 . A ATG,G 15390.7 PASS AC=278,2;AF=0.04,0.0002879;AN=6948;AS_BaseQRankSum=-0.2,0.4;AS_FS=0.522,0;AS_InbreedingCoeff=-0.0012,0.1805;AS_MQ=58.75,59.2;AS_MQRankSum=0,-3.2;AS_QD=2.56,0.02;AS_ReadPosRankSum=0.1,0.3;AS_SOR=0.652,0.724;BaseQRankSum=-0.152;DP=118313;ExcessHet=2.9774;FS=0.518;InbreedingCoeff=0.0016;MLEAC=278,2;MLEAF=0.04,0.0002879;MQ=56.9;MQRankSum=-0.962;QD=2.57;ReadPosRankSum=0.193;SOR=0.712; `. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, ; #### Steps to reproduce; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054
https://github.com/broadinstitute/gatk/issues/8054:1737,Deployability,install,install,1737,"MQ=56.9;MQRankSum=-0.962;QD=2.57;ReadPosRankSum=0.193;SOR=0.712; `. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, ; #### Steps to reproduce; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL. Command exit status:; 3. Command output:; (empty). Command error:; 23:21:52.354 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:22:02.735 INFO ProgressMeter - chr6:1162012 0.2 25000 144494.8; 23:22:12.789 INFO ProgressMeter - chr6:2449556 0.3 53000 155623.0; 23:22:23.019 INFO ProgressMeter - chr6:3663394 0.5 82000 160448.7; 23:22:33.257 INFO ProgressMeter - chr6:4991347 0.7 112000 164291.1; 23:22:43.683 INFO ProgressMeter - chr6:6325045 0.9 141000 164832.0; 23:22:53.824 INFO ProgressMeter - chr6:7646289 1.0 171000 166913.4; 23:23:03.973 INFO ProgressMeter - chr6:902992",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054
https://github.com/broadinstitute/gatk/issues/8054:6525,Integrability,wrap,wrapAndCopyInto,6525,":117,56,0,457,73,134,457,73,134,134 0/1:13,11,0,0:24:40:.:.:62,0,40,369,546,409,369,546,409,409 0/1:12,7,0,0:19:40:.:.:47,0,40,281,489,321,281,489,321,321 0/1:17,7,0,0:25:40:.:.:54,0,40,280,678,320,280,678,320,320 0/0:34,0,0,0:34:60 1/1:0,18,0,0:19:55:.:.:118,55,0,800,55,118,800,55,118,118 1/1:0,; ```; ```. at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:145); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054
https://github.com/broadinstitute/gatk/issues/8059:186,Availability,error,error,186,## Bug Report. ### Affected tool(s) or class(es); GetSampleName. ### Affected version(s); 4.2.6.1. ### Description ; when running gatk GetSampleName on a cram file you get the following error:. > A USER ERROR has occurred: A reference file is required when using CRAM files. A reference is specified with the -R command line argument. however the command does work and it does output the sample name without requiring a reference genome. #### Steps to reproduce; gatk GetSampleName -I $input_cram_path -O GetSampleName.txt. #### Expected behavior; no USER ERROR. #### Actual behavior; USER ERROR outputted to STDOUT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8059
https://github.com/broadinstitute/gatk/issues/8059:203,Availability,ERROR,ERROR,203,## Bug Report. ### Affected tool(s) or class(es); GetSampleName. ### Affected version(s); 4.2.6.1. ### Description ; when running gatk GetSampleName on a cram file you get the following error:. > A USER ERROR has occurred: A reference file is required when using CRAM files. A reference is specified with the -R command line argument. however the command does work and it does output the sample name without requiring a reference genome. #### Steps to reproduce; gatk GetSampleName -I $input_cram_path -O GetSampleName.txt. #### Expected behavior; no USER ERROR. #### Actual behavior; USER ERROR outputted to STDOUT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8059
https://github.com/broadinstitute/gatk/issues/8059:556,Availability,ERROR,ERROR,556,## Bug Report. ### Affected tool(s) or class(es); GetSampleName. ### Affected version(s); 4.2.6.1. ### Description ; when running gatk GetSampleName on a cram file you get the following error:. > A USER ERROR has occurred: A reference file is required when using CRAM files. A reference is specified with the -R command line argument. however the command does work and it does output the sample name without requiring a reference genome. #### Steps to reproduce; gatk GetSampleName -I $input_cram_path -O GetSampleName.txt. #### Expected behavior; no USER ERROR. #### Actual behavior; USER ERROR outputted to STDOUT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8059
https://github.com/broadinstitute/gatk/issues/8059:590,Availability,ERROR,ERROR,590,## Bug Report. ### Affected tool(s) or class(es); GetSampleName. ### Affected version(s); 4.2.6.1. ### Description ; when running gatk GetSampleName on a cram file you get the following error:. > A USER ERROR has occurred: A reference file is required when using CRAM files. A reference is specified with the -R command line argument. however the command does work and it does output the sample name without requiring a reference genome. #### Steps to reproduce; gatk GetSampleName -I $input_cram_path -O GetSampleName.txt. #### Expected behavior; no USER ERROR. #### Actual behavior; USER ERROR outputted to STDOUT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8059
https://github.com/broadinstitute/gatk/issues/8060:29,Deployability,release,released,29,There is a new vulnerability released https://nvd.nist.gov/vuln/detail/CVE-2022-42889#vulnCurrentDescriptionTitle and exploitable as documented in the NIST DB. . A quick scan of the GATK jar shows it ships Apache Commons 1.6 (class location in the jar: /org/apache/commons/text/lookup/). Could you please confirm if the GATK is impacted by this issue and a plan to fix this? . Thanks,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8060
https://github.com/broadinstitute/gatk/pull/8062:83,Testability,test,tested,83,"Updating the beta workflow to use the latest jar, representing the version of GATK tested against the quickstart workflow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8062
https://github.com/broadinstitute/gatk/issues/8063:1201,Integrability,inject,inject,1201,"When running GATK with specific interval(s), the default behavior is to include any variant spanning those interval(s). When running scatter/gather jobs, this behavior is generally not what one wants, since this would result in variants spanning the job intervals getting included twice. In a handful of GATK tools, there is support for something like --ignore-variants-starting-outside-interval, which is probably designed to solve this problem. GenotypeGVCFs supports this. However, the implementation/support is generally tool-level and I dont believe all tools support this. For example, SelectVariants does not appear to. If one wants to run a scatter/gather task that doesnt start with a GATK tool that supports --ignore-variants-starting-outside-interval, you're out of luck. My questions are:. 1) Am I completely missing some existing capability?. 2) There is already some low-level support in the engine for control over intervals. Would you be receptive to a PR that pushes support for ""--ignore-variants-starting-outside-intervals"" lower into GATK? Perhaps into VariantWalkerBase? One possibility would be to create a StartsWithinIntervalsVariantFilter, and override makeVariantFilter() to inject it. I dont think this would be particularly invasive, and could be pretty useful across many tools. As part of this, MultiVariantWalkerGroupedOnStart's argument would get merged with this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8063
https://github.com/broadinstitute/gatk/issues/8063:1201,Security,inject,inject,1201,"When running GATK with specific interval(s), the default behavior is to include any variant spanning those interval(s). When running scatter/gather jobs, this behavior is generally not what one wants, since this would result in variants spanning the job intervals getting included twice. In a handful of GATK tools, there is support for something like --ignore-variants-starting-outside-interval, which is probably designed to solve this problem. GenotypeGVCFs supports this. However, the implementation/support is generally tool-level and I dont believe all tools support this. For example, SelectVariants does not appear to. If one wants to run a scatter/gather task that doesnt start with a GATK tool that supports --ignore-variants-starting-outside-interval, you're out of luck. My questions are:. 1) Am I completely missing some existing capability?. 2) There is already some low-level support in the engine for control over intervals. Would you be receptive to a PR that pushes support for ""--ignore-variants-starting-outside-intervals"" lower into GATK? Perhaps into VariantWalkerBase? One possibility would be to create a StartsWithinIntervalsVariantFilter, and override makeVariantFilter() to inject it. I dont think this would be particularly invasive, and could be pretty useful across many tools. As part of this, MultiVariantWalkerGroupedOnStart's argument would get merged with this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8063
https://github.com/broadinstitute/gatk/pull/8064:6,Usability,guid,guidelines,6,Added guidelines for resource usage that we use for running gCNV to GermlineCNVCaller doc. This addressed #6166.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8064
https://github.com/broadinstitute/gatk/pull/8065:327,Availability,down,download,327,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/76c46310-3c0d-43a8-9fce-072ef7750651). As written the task requires `apt-get`. Converting this to Alpine would be non-trivial and not really worthwhile as it might even take longer to build all the extra things into the `alpine` image that we simply download with the `slim` image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8065
https://github.com/broadinstitute/gatk/pull/8065:0,Deployability,Integrat,Integration,0,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/76c46310-3c0d-43a8-9fce-072ef7750651). As written the task requires `apt-get`. Converting this to Alpine would be non-trivial and not really worthwhile as it might even take longer to build all the extra things into the `alpine` image that we simply download with the `slim` image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8065
https://github.com/broadinstitute/gatk/pull/8065:0,Integrability,Integrat,Integration,0,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/76c46310-3c0d-43a8-9fce-072ef7750651). As written the task requires `apt-get`. Converting this to Alpine would be non-trivial and not really worthwhile as it might even take longer to build all the extra things into the `alpine` image that we simply download with the `slim` image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8065
https://github.com/broadinstitute/gatk/pull/8065:320,Usability,simpl,simply,320,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/76c46310-3c0d-43a8-9fce-072ef7750651). As written the task requires `apt-get`. Converting this to Alpine would be non-trivial and not really worthwhile as it might even take longer to build all the extra things into the `alpine` image that we simply download with the `slim` image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8065
https://github.com/broadinstitute/gatk/pull/8066:0,Deployability,Integrat,Integration,0,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/5bcd1072-f9c4-4885-805b-a29091e27791).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8066
https://github.com/broadinstitute/gatk/pull/8066:0,Integrability,Integrat,Integration,0,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/5bcd1072-f9c4-4885-805b-a29091e27791).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8066
https://github.com/broadinstitute/gatk/pull/8068:23,Deployability,upgrade,upgraded,23,ubuntu 18 -> 22; conda upgraded,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8068
https://github.com/broadinstitute/gatk/pull/8070:271,Availability,error,errors,271,@ahaessly Could you please take a look at this? I'd appreciate it if you would also look at the expected output and make sure those intervals make sense since I don't think that aspect of the code was previously tested and I want to be sure there are no off by one style errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8070
https://github.com/broadinstitute/gatk/pull/8070:212,Testability,test,tested,212,@ahaessly Could you please take a look at this? I'd appreciate it if you would also look at the expected output and make sure those intervals make sense since I don't think that aspect of the code was previously tested and I want to be sure there are no off by one style errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8070
https://github.com/broadinstitute/gatk/pull/8071:7,Deployability,update,updates,7,* This updates us from commons-text:1.6.0 -> 1.10.0 to fix a vulnerability; * fixes https://github.com/broadinstitute/gatk/issues/8060,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8071
https://github.com/broadinstitute/gatk/pull/8072:117,Deployability,Integrat,Integration,117,Apparently I broke the inclusion of VAT schema JSONs in our Docker image when I was redoing the Dockerfile recently. Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/b270ddee-1bc8-4488-bc60-79d0cb82797e).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8072
https://github.com/broadinstitute/gatk/pull/8072:117,Integrability,Integrat,Integration,117,Apparently I broke the inclusion of VAT schema JSONs in our Docker image when I was redoing the Dockerfile recently. Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/b270ddee-1bc8-4488-bc60-79d0cb82797e).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8072
https://github.com/broadinstitute/gatk/pull/8074:642,Availability,down,down,642,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:318,Deployability,update,updated,318,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:1250,Integrability,depend,depending,1250,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:585,Testability,Test,Test,585,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:650,Testability,test,test,650,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:708,Testability,test,tests,708,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:1042,Testability,test,test,1042,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:97,Usability,simpl,simplified,97,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:430,Usability,simpl,simply,430,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:895,Usability,simpl,simplified,895,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/pull/8074:1140,Usability,simpl,simplified,1140,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but I’m not sure it’s worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074
https://github.com/broadinstitute/gatk/issues/8076:439,Availability,error,error,439,"This request was created from a contribution made by Elizabeth Lee on October 19, 2022 17:47 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/9761457082907-JointGenotyping-ImportGvcfs-terminates-without-an-active-exception](https://gatk.broadinstitute.org/hc/en-us/community/posts/9761457082907-JointGenotyping-ImportGvcfs-terminates-without-an-active-exception). \--. JointGenotyping fails in ImportGvcfs with the c++ error ""terminate called without an active exception"", which occurs when a thread goes out of scope without calling join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the wh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1392,Availability,error,error,1392,"minates-without-an-active-exception). \--. JointGenotyping fails in ImportGvcfs with the c++ error ""terminate called without an active exception"", which occurs when a thread goes out of scope without calling join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1427,Availability,error,errors,1427,"or ""terminate called without an active exception"", which occurs when a thread goes out of scope without calling join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1800,Availability,echo,echo,1800,"intGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:4917,Availability,echo,echo,4917,"preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,18\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:8:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,18\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:7:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,19\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:1:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:43,39\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:4:1\]: set -euo pipefail. rm -rf genomicsdb . .... (and so on). singularity exec --containall docker://us.gcr.io/broad-gatk/gatk@sha256:21c3cb43b7d11891ed4b63cc7274f20187f00387cfaa0433b3e7991b5be34dbe \\.   echo ""successfully pulled us.gcr.io/broad-gatk/gatk@sha256:21c3cb43b7d11891ed4b63cc7274f20187f00387cfaa0433b3e7991b5be34dbe!"". singularity exec --containall --bind /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-0:/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-0 docker://us.gcr.io/broad-gatk/gatk@sha256:21c3cb43b7d11891ed4b63cc7274f20187f00387cfaa0433b3e7991b5be34dbe /bin/bash /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-0/execution/script. \[2022-10-18 15:38:47,76\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:5:1\]: job id: 698507. \[2022-10-18 15:38:47,76\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:2:1\]: job id: 698509. \[2022-10-18 15:38:47,76\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:4:1\]: j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:13934,Availability,error,error,13934,ortGVCFs/shard-5/inputs/1422537242/000006KQ0748.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0757.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0775.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0784.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0793.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ1479.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/00000. ...(all the shards fail in the same way). (this is stderr.background for one shard; all 10 shards log the same error). lee04110@ln0005 \[/scratch.global/lee04110/batch\] % cat /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.background . INFO:    Using cached SIF image. INFO:    Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:.     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\] --reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:21965,Availability,down,down,21965,"mport - Importing batch 5 with 50 samples. 01:50:11.697 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 02:11:57.531 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 02:20:02.171 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 02:35:13.654 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 03:04:30.490 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 03:05:06.171 INFO  GenomicsDBImport - Done importing batch 5/7. 03:05:14.150 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 03:08:31.080 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 03:23:52.054 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 03:30:37.049 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 03:43:46.119 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 04:09:27.761 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 04:10:10.953 INFO  GenomicsDBImport - Done importing batch 6/7. 04:10:18.233 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 04:13:55.022 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 04:27:28.342 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 04:33:32.781 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 04:44:09.752 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 05:04:33.112 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 05:05:02.272 INFO  GenomicsDBImport - Done importing batch 7/7. 05:05:02.299 INFO  GenomicsDBImport - Import of all batches to GenomicsDB completed!. 05:05:02.300 INFO  GenomicsDBImport - Shutting down engine. \[October 19, 2022 5:05:02 AM GMT\] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 506.11 minutes. Runtime.totalMemory()=8653373440. pure virtual method called. terminate called without an active exception<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/297458'>Zendesk ticket #297458</a>)<br> gz#297458</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1722,Deployability,pipeline,pipelines,1722,"e node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1550,Modifiability,config,config,1550,"ng join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1658,Modifiability,config,config,1658,"ineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1562,Performance,cache,cache,1562,"ng join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:14167,Performance,cache,cached,14167,19-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0775.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0784.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0793.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ1479.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/00000. ...(all the shards fail in the same way). (this is stderr.background for one shard; all 10 shards log the same error). lee04110@ln0005 \[/scratch.global/lee04110/batch\] % cat /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.background . INFO:    Using cached SIF image. INFO:    Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:.     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\] --reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/tmp.bd1b0bc7. 20:38:55.819 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:14200,Performance,cache,cached,14200,portGVCFs/shard-5/inputs/1422537242/000006KQ0775.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0784.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0793.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ1479.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/00000. ...(all the shards fail in the same way). (this is stderr.background for one shard; all 10 shards log the same error). lee04110@ln0005 \[/scratch.global/lee04110/batch\] % cat /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.background . INFO:    Using cached SIF image. INFO:    Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:.     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\] --reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/tmp.bd1b0bc7. 20:38:55.819 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compres,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:15082,Performance,Load,Loading,15082,"598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.background . INFO:    Using cached SIF image. INFO:    Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:.     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\] --reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/tmp.bd1b0bc7. 20:38:55.819 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. 20:38:56.233 INFO  GenomicsDBImport - ------------------------------------------------------------. 20:38:56.233 INFO  GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.6.1. 20:38:56.234 INFO  GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/). 20:38:56.237 INFO  GenomicsDBImport - Executing as [lee04110@cn2006.mesabi.msi.umn.edu](mailto:lee04110@cn2006.mesabi.msi.umn.edu) on Linux v3.10.0-1160.76.1.el7.x86\_64 amd64. 20:38:56.237 INFO  GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08. 20:38:56.238 INFO  GenomicsDBImport - Start Date/Time: October 18, 2022 8:38:55 PM GMT. 20:38:56.238 INFO  GenomicsDBImport - ------------------------------------------------------------. 20:38:56.238 INFO  GenomicsDBImport - ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1926,Testability,log,log-temporary,1926,"n this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:2:1\]: Unrecognized runtime attribute keys: p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:1970,Testability,log,log,1970,"'t seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:2:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,15\] \[warn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/issues/8076:13921,Testability,log,log,13921,ortGVCFs/shard-5/inputs/1422537242/000006KQ0748.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0757.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0775.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0784.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0793.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ1479.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/00000. ...(all the shards fail in the same way). (this is stderr.background for one shard; all 10 shards log the same error). lee04110@ln0005 \[/scratch.global/lee04110/batch\] % cat /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.background . INFO:    Using cached SIF image. INFO:    Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:.     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\] --reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076
https://github.com/broadinstitute/gatk/pull/8079:214,Testability,Test,Test,214,"Since we might need to run the GvsPrepareRangesCallset.wdl in the future just to help to get the callset stats, add an optional input to not create the __REF_DATA and __SAMPLES tables, since those won't be needed. Test runs: ; - [with](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/1ee2bc37-abd4-4185-a17d-e1f6b47a6914) `skip_ref_ranges_tables ` set to `true`:; - [with](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/2849dfd1-fe41-4757-83f3-fc0f5451eaef) `skip_ref_ranges_tables ` set to `false`; - [with](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/5e8c5c4d-6ae5-4117-a5ea-89a05d379cfd) `skip_ref_ranges_tables ` not set (default is false); - [GvsCallsetStatistics run](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/3e14b26f-125b-4742-b7a2-17cab2ac7f6f); ... after changes ... ; - [with](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/3560a8a6-71ef-46b4-be64-fd35ea5303c6) `only_output_vet_tables` set to `true`; - [with](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/ac3a2126-c3f1-4241-aad5-55ece4178a56) `only_output_vet_tables` set to `false`; - [GvsCreateCohortFromSampleNames](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/35379d1b-b54a-4f5e-990a-9650c92d1ead) run (failed because of other issue)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8079
https://github.com/broadinstitute/gatk/issues/8080:2145,Availability,error,error,2145,"p [often](https://gatk.broadinstitute.org/hc/en-us/community/posts/4566282375835-Mutect2-AF-does-not-match-AD-and-DP) in the GATK forum and it feels like having clear documentation around this would be helpful. . My impression is that Mutect2 might be using an AD ""1-read-per-allele"" prior and incorporating that into its reported AF. From the [article on informative reads](https://gatk.broadinstitute.org/hc/en-us/articles/360035532252-Allele-Depth-AD-is-lower-than-expected), once you're at the sample level (FORMAT field), both DP and AD appear to include only informative alleles. It is tempting to think that AF would be computed from them directly (e.g., `AD_alt / DP`, which is equivalent to `AD_alt/[AD_alt+AD_ref]` in the biallelic case since only informative reads are retained). However, as noted in those linked forum posts, Mutect2 (in my case, version 4.2.5.0) does not produce AF values that can be computed from the AD values in that way. Rather, the AF value appears to incorporate a prior. . I investigated this across a range of allele depths in real calls. Here are some examples. The format is:; |AlleleDepthRef,AlleleDepthAlt | DP | AF[provided by Mutect2] | AF[if I calculate it myself]|; | ------- | ------- | ----- | ------- |; | 0,1|1|0.667|1.000 |; | 23,4|27|0.170|0.148 |; | 39,125|164|0.758|0.762 | . The intuition here is that there is a huge discrepancy between the Mutect2 AF and the AF I calculate when AD (or DP) is small (first row), and the error gets smaller as DP increases. The formula that Mutect2 seems to use to compute AF is:. ```py; # Formula that Mutect2 seems to use to calculate AF of the alternative allele in a biallelic scenario; Mutect2_AF = (ADalt+1) / (ADalt+1 + ADref+1). # Which is equivalent to:; Mutect2_AF = (ADalt + 1) / (DP + 2); ```. 1. Is my inference about a prior weight being added by Mutect2 prior to computing AF accurate?; 2. If so, is it intended behavior?; 3. If so, can the VCF header field be a bit more informative about this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8080
https://github.com/broadinstitute/gatk/issues/8080:828,Usability,clear,clear,828,"## Documentation request. Mutect2 seems to compute allele fraction (AF) from the allele depth (AD) data *after adding a prior* (which seems to be that it adds 1 read to each of the alleles). However, as of at least GATK 4.2.5.0, Mutect2 produces a header that does not indicate that this is happening: (`##FORMAT=<ID=AF,Number=A,Type=Float,Description=""Allele fractions of alternate alleles in the tumor"">`). ### Tools involved; * Mutect2. ### Background. There is a question about the meaning of the AF field that seems to [come](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077993111-Tumor-and-Normal-Read-Frequency-vs-tumor-f-value-in-MAF-?page=1) up [often](https://gatk.broadinstitute.org/hc/en-us/community/posts/4566282375835-Mutect2-AF-does-not-match-AD-and-DP) in the GATK forum and it feels like having clear documentation around this would be helpful. . My impression is that Mutect2 might be using an AD ""1-read-per-allele"" prior and incorporating that into its reported AF. From the [article on informative reads](https://gatk.broadinstitute.org/hc/en-us/articles/360035532252-Allele-Depth-AD-is-lower-than-expected), once you're at the sample level (FORMAT field), both DP and AD appear to include only informative alleles. It is tempting to think that AF would be computed from them directly (e.g., `AD_alt / DP`, which is equivalent to `AD_alt/[AD_alt+AD_ref]` in the biallelic case since only informative reads are retained). However, as noted in those linked forum posts, Mutect2 (in my case, version 4.2.5.0) does not produce AF values that can be computed from the AD values in that way. Rather, the AF value appears to incorporate a prior. . I investigated this across a range of allele depths in real calls. Here are some examples. The format is:; |AlleleDepthRef,AlleleDepthAlt | DP | AF[provided by Mutect2] | AF[if I calculate it myself]|; | ------- | ------- | ----- | ------- |; | 0,1|1|0.667|1.000 |; | 23,4|27|0.170|0.148 |; | 39,125|164|0.758|0.762 | . The",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8080
https://github.com/broadinstitute/gatk/issues/8080:2002,Usability,intuit,intuition,2002,"p [often](https://gatk.broadinstitute.org/hc/en-us/community/posts/4566282375835-Mutect2-AF-does-not-match-AD-and-DP) in the GATK forum and it feels like having clear documentation around this would be helpful. . My impression is that Mutect2 might be using an AD ""1-read-per-allele"" prior and incorporating that into its reported AF. From the [article on informative reads](https://gatk.broadinstitute.org/hc/en-us/articles/360035532252-Allele-Depth-AD-is-lower-than-expected), once you're at the sample level (FORMAT field), both DP and AD appear to include only informative alleles. It is tempting to think that AF would be computed from them directly (e.g., `AD_alt / DP`, which is equivalent to `AD_alt/[AD_alt+AD_ref]` in the biallelic case since only informative reads are retained). However, as noted in those linked forum posts, Mutect2 (in my case, version 4.2.5.0) does not produce AF values that can be computed from the AD values in that way. Rather, the AF value appears to incorporate a prior. . I investigated this across a range of allele depths in real calls. Here are some examples. The format is:; |AlleleDepthRef,AlleleDepthAlt | DP | AF[provided by Mutect2] | AF[if I calculate it myself]|; | ------- | ------- | ----- | ------- |; | 0,1|1|0.667|1.000 |; | 23,4|27|0.170|0.148 |; | 39,125|164|0.758|0.762 | . The intuition here is that there is a huge discrepancy between the Mutect2 AF and the AF I calculate when AD (or DP) is small (first row), and the error gets smaller as DP increases. The formula that Mutect2 seems to use to compute AF is:. ```py; # Formula that Mutect2 seems to use to calculate AF of the alternative allele in a biallelic scenario; Mutect2_AF = (ADalt+1) / (ADalt+1 + ADref+1). # Which is equivalent to:; Mutect2_AF = (ADalt + 1) / (DP + 2); ```. 1. Is my inference about a prior weight being added by Mutect2 prior to computing AF accurate?; 2. If so, is it intended behavior?; 3. If so, can the VCF header field be a bit more informative about this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8080
https://github.com/broadinstitute/gatk/pull/8084:0,Deployability,Update,Updated,0,"Updated the carrot github action workflow to the most recent version, which supports using #carrot_pr to trigger branch vs master comparison runs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8084
https://github.com/broadinstitute/gatk/pull/8085:182,Deployability,integrat,integration,182,"Creates a new ""build-base"" Docker image for the expensive and less frequently changed layers of the build image allowing for much improved variantstore image build times. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/ff13e48c-a9dc-48d7-8056-63d4f2028dc0). Other improvements:. * Bumps version of Google Cloud SDK base Docker image to latest `408.0.1-alpine`; * Bumps Arrow library version from 8.0.0 to 10.0.0; * Simplifies Arrow build to use `ninja`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8085
https://github.com/broadinstitute/gatk/pull/8085:182,Integrability,integrat,integration,182,"Creates a new ""build-base"" Docker image for the expensive and less frequently changed layers of the build image allowing for much improved variantstore image build times. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/ff13e48c-a9dc-48d7-8056-63d4f2028dc0). Other improvements:. * Bumps version of Google Cloud SDK base Docker image to latest `408.0.1-alpine`; * Bumps Arrow library version from 8.0.0 to 10.0.0; * Simplifies Arrow build to use `ninja`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8085
https://github.com/broadinstitute/gatk/pull/8085:86,Modifiability,layers,layers,86,"Creates a new ""build-base"" Docker image for the expensive and less frequently changed layers of the build image allowing for much improved variantstore image build times. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/ff13e48c-a9dc-48d7-8056-63d4f2028dc0). Other improvements:. * Bumps version of Google Cloud SDK base Docker image to latest `408.0.1-alpine`; * Bumps Arrow library version from 8.0.0 to 10.0.0; * Simplifies Arrow build to use `ninja`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8085
https://github.com/broadinstitute/gatk/pull/8085:483,Usability,Simpl,Simplifies,483,"Creates a new ""build-base"" Docker image for the expensive and less frequently changed layers of the build image allowing for much improved variantstore image build times. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/ff13e48c-a9dc-48d7-8056-63d4f2028dc0). Other improvements:. * Bumps version of Google Cloud SDK base Docker image to latest `408.0.1-alpine`; * Bumps Arrow library version from 8.0.0 to 10.0.0; * Simplifies Arrow build to use `ninja`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8085
https://github.com/broadinstitute/gatk/pull/8086:103,Deployability,integrat,integration,103,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:131,Deployability,integrat,integration,131,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:255,Deployability,integrat,integration,255,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:409,Deployability,integrat,integration,409,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:472,Deployability,integrat,integration,472,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:508,Deployability,toggle,toggled,508,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:103,Integrability,integrat,integration,103,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:131,Integrability,integrat,integration,131,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:255,Integrability,integrat,integration,255,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:409,Integrability,integrat,integration,409,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:472,Integrability,integrat,integration,472,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:115,Testability,test,test,115,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:143,Testability,test,test,143,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:267,Testability,test,test,267,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/pull/8086:484,Testability,test,tests,484,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086
https://github.com/broadinstitute/gatk/issues/8087:185,Availability,error,error,185,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:254,Availability,failure,failure,254,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:360,Availability,failure,failures,360,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:412,Availability,failure,failures,412,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:523,Availability,down,down,523,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:742,Deployability,update,updated,742,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:947,Deployability,integrat,integration,947,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:486,Integrability,depend,dependency,486,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:546,Integrability,rout,routinely,546,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:947,Integrability,integrat,integration,947,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1107,Integrability,wrap,wrapper,1107,"t annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test group it is told to run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:141,Testability,Log,Log,141,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:191,Testability,log,log,191,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:214,Testability,log,logs,214,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:275,Testability,test,test,275,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:280,Testability,log,logs,280,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:458,Testability,log,logs,458,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:720,Testability,log,logs,720,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:758,Testability,test,test,758,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:828,Testability,test,test,828,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1033,Testability,log,logs,1033,"nch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1057,Testability,test,test,1057,"nch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1097,Testability,test,test,1097,"t annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test group it is told to run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1404,Testability,test,testNG,1404,"t annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test group it is told to run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1429,Testability,test,tests,1429,"t annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test group it is told to run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1453,Testability,test,test,1453,"t annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test group it is told to run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1983,Testability,test,tests,1983,"t annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test group it is told to run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:2010,Testability,test,test,2010,"t annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test group it is told to run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/issues/8087:1790,Usability,intuit,intuitive,1790,"t annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather whatever test group it is told to run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087
https://github.com/broadinstitute/gatk/pull/8090:0,Deployability,Update,Update,0,Update the VAT pipeline readme to know about the VDS inputs. This will need to further be edited once George's VDS VAT pipeline changes have been completed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8090
https://github.com/broadinstitute/gatk/pull/8090:15,Deployability,pipeline,pipeline,15,Update the VAT pipeline readme to know about the VDS inputs. This will need to further be edited once George's VDS VAT pipeline changes have been completed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8090
https://github.com/broadinstitute/gatk/pull/8090:119,Deployability,pipeline,pipeline,119,Update the VAT pipeline readme to know about the VDS inputs. This will need to further be edited once George's VDS VAT pipeline changes have been completed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8090
https://github.com/broadinstitute/gatk/issues/8091:229,Availability,error,error,229,"Hi,. I am on GATK v4.3.0.0 (CentOS) and would like to implement GermlineCNVCaller in my work. In an attempt to set up the conda environment (gcnvkernel) from zip using the command `conda env create -f gatkcondaenv.yml`, I got an error like this: `Found conflicts! Looking for incompatible packages.` . I also tried installing with tar, but I couldn't find gatkPythonPackageArchive.zip as required in the yml. Any help on this issue is much appreciated!. Java version: 1.8.0_201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8091
https://github.com/broadinstitute/gatk/issues/8091:315,Deployability,install,installing,315,"Hi,. I am on GATK v4.3.0.0 (CentOS) and would like to implement GermlineCNVCaller in my work. In an attempt to set up the conda environment (gcnvkernel) from zip using the command `conda env create -f gatkcondaenv.yml`, I got an error like this: `Found conflicts! Looking for incompatible packages.` . I also tried installing with tar, but I couldn't find gatkPythonPackageArchive.zip as required in the yml. Any help on this issue is much appreciated!. Java version: 1.8.0_201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8091
https://github.com/broadinstitute/gatk/pull/8092:206,Performance,perform,performance,206,"This PR is an attempt at improving SelectVariants by. - Rewriting unclear code;; - Adding documentation where needed; and; - Adding tests. The initial motivation for this code change (#7497) was to improve performance of SelectVariants by adding an option to do the ""INFO-level filtering"" before doing ""genotype filtering."" Our assumption was that this would improve performance because then we would avoid the expensive genotype ""fully-decode"" operation, which turns string format fields into appropriate object/types (int, array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:367,Performance,perform,performance,367,"This PR is an attempt at improving SelectVariants by. - Rewriting unclear code;; - Adding documentation where needed; and; - Adding tests. The initial motivation for this code change (#7497) was to improve performance of SelectVariants by adding an option to do the ""INFO-level filtering"" before doing ""genotype filtering."" Our assumption was that this would improve performance because then we would avoid the expensive genotype ""fully-decode"" operation, which turns string format fields into appropriate object/types (int, array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:1564,Performance,perform,performance,1564," array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:1619,Performance,perform,performance,1619,"hly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] F",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:2365,Performance,perform,performance,2365,"or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:3363,Performance,perform,performance,3363,"mand line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ; - [x] Add a test for `select-random-fraction`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:401,Safety,avoid,avoid,401,"This PR is an attempt at improving SelectVariants by. - Rewriting unclear code;; - Adding documentation where needed; and; - Adding tests. The initial motivation for this code change (#7497) was to improve performance of SelectVariants by adding an option to do the ""INFO-level filtering"" before doing ""genotype filtering."" Our assumption was that this would improve performance because then we would avoid the expensive genotype ""fully-decode"" operation, which turns string format fields into appropriate object/types (int, array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:825,Security,access,accessing,825,"This PR is an attempt at improving SelectVariants by. - Rewriting unclear code;; - Adding documentation where needed; and; - Adding tests. The initial motivation for this code change (#7497) was to improve performance of SelectVariants by adding an option to do the ""INFO-level filtering"" before doing ""genotype filtering."" Our assumption was that this would improve performance because then we would avoid the expensive genotype ""fully-decode"" operation, which turns string format fields into appropriate object/types (int, array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:3012,Security,Access,Access,3012,"mand line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ; - [x] Add a test for `select-random-fraction`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:132,Testability,test,tests,132,"This PR is an attempt at improving SelectVariants by. - Rewriting unclear code;; - Adding documentation where needed; and; - Adding tests. The initial motivation for this code change (#7497) was to improve performance of SelectVariants by adding an option to do the ""INFO-level filtering"" before doing ""genotype filtering."" Our assumption was that this would improve performance because then we would avoid the expensive genotype ""fully-decode"" operation, which turns string format fields into appropriate object/types (int, array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:2358,Testability,test,tests,2358,"or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:2660,Testability,Test,Tests,2660,"mand line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ; - [x] Add a test for `select-random-fraction`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:2869,Testability,log,logical-or,2869,"mand line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ; - [x] Add a test for `select-random-fraction`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:2887,Testability,Test,Test,2887,"mand line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ; - [x] Add a test for `select-random-fraction`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:2966,Testability,log,logical-and,2966,"mand line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ; - [x] Add a test for `select-random-fraction`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:3400,Testability,test,test,3400,"mand line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - [x] Code path that uses ""fully-decode""; - [x] Failing cases (reference genotype fields in --select and vice versa); - [x] `--applyJexlFiltersBeforeFilteringGenotypes.` Does this actually give us performance advantage? ; - [x] Add a test for `select-random-fraction`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:1671,Usability,simpl,simply,1671,"hly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] F",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8092:2181,Usability,simpl,simple,2181,"Decode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy…) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run time) than the default. I kept it in the PR because I thought some user might find it useful, but I wouldn't be opposed to removing it. Tests needed:; - [x] Filter by genotypes with a new flag --genotype-select, with the default behavior being 'passes if at least one sample passes' ; - [x] Multiple --select expressions should be combined with logical-or; - [x] Test string annotations (e.g. ALGORITHM == 'depth'); - [x] Jexl involving with logical-and (e.g. AC > 0 && AF > 0.01); - [x] Access genotypes directly e.g. vc.getsample('NA12878'); - [x] DP > 0 as --genotype-select and as --select; - [x] Combine --select and --select-genotypes; - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092
https://github.com/broadinstitute/gatk/pull/8094:18,Deployability,update,updates,18,"This pull request updates the environment to include `pytorch` to gatk conda environment. This required an update to numpy and consequently updates of PyMC3 and its dependencies, as well as parts of gCNV code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8094
https://github.com/broadinstitute/gatk/pull/8094:107,Deployability,update,update,107,"This pull request updates the environment to include `pytorch` to gatk conda environment. This required an update to numpy and consequently updates of PyMC3 and its dependencies, as well as parts of gCNV code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8094
https://github.com/broadinstitute/gatk/pull/8094:140,Deployability,update,updates,140,"This pull request updates the environment to include `pytorch` to gatk conda environment. This required an update to numpy and consequently updates of PyMC3 and its dependencies, as well as parts of gCNV code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8094
https://github.com/broadinstitute/gatk/pull/8094:165,Integrability,depend,dependencies,165,"This pull request updates the environment to include `pytorch` to gatk conda environment. This required an update to numpy and consequently updates of PyMC3 and its dependencies, as well as parts of gCNV code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8094
https://github.com/broadinstitute/gatk/issues/8097:1211,Availability,error,error,1211,"stitute/hellbender/gcnvkernel/structs/metadata.py#L177); [gcnvkernel metadata.py SampleMetadataCollection class](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, Elemwise{mul,no_inplace}.0, Subtensor{int64::}.0, Elemwise{sub,no",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097
https://github.com/broadinstitute/gatk/issues/8097:1955,Availability,error,error,1955,".py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, Elemwise{mul,no_inplace}.0, Subtensor{int64::}.0, Elemwise{sub,no_inplace}.0), '\n', ""Inconsistency in the inner graph of scan 'scan_fn' : an input and an output are associated with the same recurrent state and should have the same type but have type 'TensorType(float64, row)' and 'TensorType(float64, matrix)' respectively."")```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097
https://github.com/broadinstitute/gatk/issues/8097:1093,Deployability,release,release,1093,"ps://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L177); [gcnvkernel metadata.py SampleMetadataCollection class](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097
https://github.com/broadinstitute/gatk/issues/8097:1163,Testability,test,test,1163,"ps://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L177); [gcnvkernel metadata.py SampleMetadataCollection class](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097
https://github.com/broadinstitute/gatk/issues/8097:1259,Testability,test,test,1259,"stitute/hellbender/gcnvkernel/structs/metadata.py#L177); [gcnvkernel metadata.py SampleMetadataCollection class](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, Elemwise{mul,no_inplace}.0, Subtensor{int64::}.0, Elemwise{sub,no",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097
https://github.com/broadinstitute/gatk/issues/8097:1276,Testability,test,test,1276,"MetadataCollection class](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, Elemwise{mul,no_inplace}.0, Subtensor{int64::}.0, Elemwise{sub,no_inplace}.0), '\n', ""Inconsistency in the inner graph of scan 'scan_fn' : an input and a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097
https://github.com/broadinstitute/gatk/issues/8097:1329,Testability,test,testing,1329,"MetadataCollection class](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, Elemwise{mul,no_inplace}.0, Subtensor{int64::}.0, Elemwise{sub,no_inplace}.0), '\n', ""Inconsistency in the inner graph of scan 'scan_fn' : an input and a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097
https://github.com/broadinstitute/gatk/pull/8100:0,Deployability,Upgrade,Upgrades,0,"Upgrades Barclay to 4.1.0. Adds an `@DeprecatedFeature` annotation that can be applied to any `@DocumentedFeature` or `@Argument`, and updates the handful of such features that are already tagged with the standard Java annotation `@Deprecated` to use the new annotation. Mutually exclusive with `@Beta` and `@Experimental`. The output for `--use-new-qual-calculator`:; <img width=""783"" alt=""Screen Shot 2022-11-21 at 5 04 44 PM"" src=""https://user-images.githubusercontent.com/10062863/203168110-381424c7-e0e2-4f93-b51b-41f74212b669.png"">; <img width=""918"" alt=""Screen Shot 2022-11-21 at 5 14 16 PM"" src=""https://user-images.githubusercontent.com/10062863/203168311-cfc7b1b1-79d8-48c8-b965-766e424135ab.png"">; <img width=""851"" alt=""Screen Shot 2022-11-21 at 6 39 03 PM"" src=""https://user-images.githubusercontent.com/10062863/203179466-b817bba3-312b-4718-a758-9c063f41d755.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8100
https://github.com/broadinstitute/gatk/pull/8100:135,Deployability,update,updates,135,"Upgrades Barclay to 4.1.0. Adds an `@DeprecatedFeature` annotation that can be applied to any `@DocumentedFeature` or `@Argument`, and updates the handful of such features that are already tagged with the standard Java annotation `@Deprecated` to use the new annotation. Mutually exclusive with `@Beta` and `@Experimental`. The output for `--use-new-qual-calculator`:; <img width=""783"" alt=""Screen Shot 2022-11-21 at 5 04 44 PM"" src=""https://user-images.githubusercontent.com/10062863/203168110-381424c7-e0e2-4f93-b51b-41f74212b669.png"">; <img width=""918"" alt=""Screen Shot 2022-11-21 at 5 14 16 PM"" src=""https://user-images.githubusercontent.com/10062863/203168311-cfc7b1b1-79d8-48c8-b965-766e424135ab.png"">; <img width=""851"" alt=""Screen Shot 2022-11-21 at 6 39 03 PM"" src=""https://user-images.githubusercontent.com/10062863/203179466-b817bba3-312b-4718-a758-9c063f41d755.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8100
https://github.com/broadinstitute/gatk/issues/8103:414,Availability,ERROR,ERROR,414,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:575,Availability,ERROR,ERROR,575,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:790,Availability,error,error,790,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:882,Availability,error,error,882,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:1797,Availability,error,error,1797,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:2219,Availability,error,error,2219,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:2307,Availability,error,error,2307,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:156,Deployability,release,release,156,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:1029,Deployability,continuous,continuously,1029,"s). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/ou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:1897,Deployability,release,release,1897,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:888,Integrability,message,message,888,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:2225,Integrability,message,message,2225,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:2313,Integrability,message,message,2313,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:50,Security,Validat,ValidateVariants,50,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:358,Security,Validat,ValidateVariants,358,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:1399,Security,Validat,ValidateVariants,1399," blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:1633,Security,Validat,ValidateVariants,1633,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:1696,Security,Validat,ValidateVariants,1696,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:1984,Security,Validat,ValidateVariants,1984,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:2101,Security,validat,validation-type-to-exclude,2101,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8103:226,Testability,test,test,226,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103
https://github.com/broadinstitute/gatk/issues/8106:4949,Availability,Avail,Available,4949,- Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f4[0/1667]a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.374 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.385 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.403 INFO IntervalArgumentCollection - Processing 1028 bp from intervals ; 18:15:23.411 INFO HaplotypeCaller - Done initializing engine ; 18:15:23.430 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so ; 18:15:23.475 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 18:15:23.651 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 18:15:23.651 INFO IntelPairHmm - Available threads: 4 ; 18:15:23.651 INFO IntelPairHmm - Requested threads: 4 ; 18:15:23.651 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 INFO ProgressMeter - chr1:32128426 0.2 40 202.1 ; 18:15:48.416 INFO ProgressMeter - chr1:36398656 0.4 80 194.0 ; 18:15:51.025 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.012874514 ; 18:15:51.026 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 5.818477527000001; 18:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106
https://github.com/broadinstitute/gatk/issues/8106:6090,Availability,down,down,6090,"rated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 INFO ProgressMeter - chr1:32128426 0.2 40 202.1 ; 18:15:48.416 INFO ProgressMeter - chr1:36398656 0.4 80 194.0 ; 18:15:51.025 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.012874514 ; 18:15:51.026 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 5.818477527000001; 18:15:51.026 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.35 sec ; 18:15:51.027 INFO HaplotypeCaller - Shutting down engine ; [November 24, 2022 6:15:51 PM CET] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.53 minutes.; Runtime.totalMemory()=3212836864 ; java.lang.ArrayIndexOutOfBoundsException ; at java.util.Arrays.copyOfRange(Arrays.java:3521) ; at org.broadinstitute.hellbender.tools.walkers.annotator.TandemRepeat.getNumTandemRepeatUnits(TandemRepeat.java:54) ; at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer.trim(AssemblyRegionTrimmer.java:189); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:655); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:271); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200) ; at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173) ; at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106
https://github.com/broadinstitute/gatk/issues/8106:253,Deployability,release,release,253,"### Instructions. ## Bug Report. Hi GATK team , I'm afraid I found an exception in gatk HC related to https://github.com/broadinstitute/gatk/issues/6516. ### Affected tool(s) or class(es). GATK HC v4.3.0.0 . ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description . ```; + gatk --java-options '-Xmx5g -Djava.io.tmpdir=TMP' HaplotypeCaller -R /LAB-DATA/BiRD/resources/species/human/cng.fr/hs38me/hs38me_all_chr.fasta --minimum-mapping-quality 10 --sample-ploidy 2 --do-not-run-physical-phasing --alleles TMP/jeter.vcf.gz -L TMP/jeter.vcf.gz -I /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz Using GATK jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5g -Dj; ava.io.tmpdir=TMP -jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R /LAB-DATA/BiRD/resources/species/human/cng.fr/hs38me/hs38me_all_chr.fasta --minimum-mapping-quality 10 --sample-ploidy 2 --do-not-run-physical-phasing --alleles TMP/jeter.vcf.gz -L TMP/jeter.vcf.gz -I /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz; 18:15:19.107 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.j; ar!/com/intel/gkl/native/libgkl_compression.so ; 18:15:21.727 INFO HaplotypeCaller - ------------------------------------------------------------ ; 18:15:21.728 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0 ; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106
https://github.com/broadinstitute/gatk/issues/8106:1635,Performance,Load,Loading,1635,"H-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz Using GATK jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5g -Dj; ava.io.tmpdir=TMP -jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R /LAB-DATA/BiRD/resources/species/human/cng.fr/hs38me/hs38me_all_chr.fasta --minimum-mapping-quality 10 --sample-ploidy 2 --do-not-run-physical-phasing --alleles TMP/jeter.vcf.gz -L TMP/jeter.vcf.gz -I /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz; 18:15:19.107 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.j; ar!/com/intel/gkl/native/libgkl_compression.so ; 18:15:21.727 INFO HaplotypeCaller - ------------------------------------------------------------ ; 18:15:21.728 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0 ; 18:15:21.728 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:15:21.728 INFO HaplotypeCaller - Executing as lindenbaum-p@bigmem002 on Linux v3.10.0-1160.66.1.el7.x86_64 amd64; 18:15:21.728 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11 ; 18:15:21.728 INFO HaplotypeCaller - Start Date/Time: November 24, 2022 6:15:19 PM CET ; 18:15:21.728 INFO HaplotypeCaller - ------------------------------------------------------------; 18:15:21.728 INFO HaplotypeCaller - ------------------------------------------------------------ ; 18:15:21.728 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106
https://github.com/broadinstitute/gatk/issues/8106:4437,Performance,Load,Loading,4437,aplotypeCaller - Initializing engine ; 18:15:23.068 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f410396038; a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.077 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.361 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f410396038; 18:15:23.361 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f4[0/1667]a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.374 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.385 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.403 INFO IntervalArgumentCollection - Processing 1028 bp from intervals ; 18:15:23.411 INFO HaplotypeCaller - Done initializing engine ; 18:15:23.430 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so ; 18:15:23.475 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 18:15:23.651 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 18:15:23.651 INFO IntelPairHmm - Available threads: 4 ; 18:15:23.651 INFO IntelPairHmm - Requested threads: 4 ; 18:15:23.651 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106
https://github.com/broadinstitute/gatk/issues/8106:4647,Performance,Load,Loading,4647, a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.077 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.361 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f410396038; 18:15:23.361 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f4[0/1667]a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.374 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.385 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.403 INFO IntervalArgumentCollection - Processing 1028 bp from intervals ; 18:15:23.411 INFO HaplotypeCaller - Done initializing engine ; 18:15:23.430 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so ; 18:15:23.475 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 18:15:23.651 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 18:15:23.651 INFO IntelPairHmm - Available threads: 4 ; 18:15:23.651 INFO IntelPairHmm - Requested threads: 4 ; 18:15:23.651 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 IN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106
https://github.com/broadinstitute/gatk/issues/8106:5073,Performance,multi-thread,multi-threaded,5073,7]a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.374 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.385 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.403 INFO IntervalArgumentCollection - Processing 1028 bp from intervals ; 18:15:23.411 INFO HaplotypeCaller - Done initializing engine ; 18:15:23.430 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so ; 18:15:23.475 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 18:15:23.651 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 18:15:23.651 INFO IntelPairHmm - Available threads: 4 ; 18:15:23.651 INFO IntelPairHmm - Requested threads: 4 ; 18:15:23.651 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 INFO ProgressMeter - chr1:32128426 0.2 40 202.1 ; 18:15:48.416 INFO ProgressMeter - chr1:36398656 0.4 80 194.0 ; 18:15:51.025 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.012874514 ; 18:15:51.026 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 5.818477527000001; 18:15:51.026 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.35 sec ; 18:15:51.027 INFO HaplotypeCaller - Shutting d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106
https://github.com/broadinstitute/gatk/issues/8106:323,Testability,test,test,323,"### Instructions. ## Bug Report. Hi GATK team , I'm afraid I found an exception in gatk HC related to https://github.com/broadinstitute/gatk/issues/6516. ### Affected tool(s) or class(es). GATK HC v4.3.0.0 . ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description . ```; + gatk --java-options '-Xmx5g -Djava.io.tmpdir=TMP' HaplotypeCaller -R /LAB-DATA/BiRD/resources/species/human/cng.fr/hs38me/hs38me_all_chr.fasta --minimum-mapping-quality 10 --sample-ploidy 2 --do-not-run-physical-phasing --alleles TMP/jeter.vcf.gz -L TMP/jeter.vcf.gz -I /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz Using GATK jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5g -Dj; ava.io.tmpdir=TMP -jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R /LAB-DATA/BiRD/resources/species/human/cng.fr/hs38me/hs38me_all_chr.fasta --minimum-mapping-quality 10 --sample-ploidy 2 --do-not-run-physical-phasing --alleles TMP/jeter.vcf.gz -L TMP/jeter.vcf.gz -I /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz; 18:15:19.107 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.j; ar!/com/intel/gkl/native/libgkl_compression.so ; 18:15:21.727 INFO HaplotypeCaller - ------------------------------------------------------------ ; 18:15:21.728 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0 ; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106
https://github.com/broadinstitute/gatk/issues/8107:126,Availability,failure,failures,126,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107
https://github.com/broadinstitute/gatk/issues/8107:216,Availability,failure,failures,216,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107
https://github.com/broadinstitute/gatk/issues/8107:581,Availability,failure,failure,581,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107
https://github.com/broadinstitute/gatk/issues/8107:845,Availability,failure,failures,845,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107
https://github.com/broadinstitute/gatk/issues/8107:257,Testability,test,tests,257,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107
https://github.com/broadinstitute/gatk/issues/8107:387,Testability,test,tests,387,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107
https://github.com/broadinstitute/gatk/issues/8107:876,Testability,test,tests,876,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107
https://github.com/broadinstitute/gatk/pull/8110:208,Testability,test,test,208,(also fixed a broken link in [Removing Samples from a VDS.md](https://github.com/broadinstitute/gatk/compare/ah_var_store...rsa_vs_749#diff-74ddd9f71eec0cc8080c093e76fbecdb3fa8b9d443e7071591b635e3d47afca5)). test run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/d2957947-0249-4197-9531-0b4203f2ea1c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8110
https://github.com/broadinstitute/gatk/issues/8112:348,Availability,error,error,348,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [4.3.0.0 ] Latest public release version . ### Description ; I'm attempting to run HaplotypeCaller on Ultima flow based bams with the ""--flow-mode STANDARD"" and ""--likelihood-calculation-engine FlowBasedHMM"" arguments. However, I'm getting the following error ""java.lang.IllegalArgumentException: read must be flow based: 180652-BC94-0022826568 chr1:14585-14703"". The bams were created from fastqs provided directly from Ultima, so they are definitely flow-based. One question I have: how does HaplotypeCaller determine if a read is flow-based or not? Is this specified in the Read Groups?. #### Steps to reproduce; gatk --java-options ""-Xmx4g"" HaplotypeCaller -R hg38.fa --flow-mode STANDARD --likelihood-calculation-engine FlowBasedHMM -I [bam] -O [bam%.BQSRapplied.bam].GATK.vcf.gz. #### Expected behavior; HaplotypeCaller should complete successfully. #### Actual behavior; HaplotypeCaller fails, expecting flow-based reads",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8112
https://github.com/broadinstitute/gatk/issues/8112:119,Deployability,release,release,119,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [4.3.0.0 ] Latest public release version . ### Description ; I'm attempting to run HaplotypeCaller on Ultima flow based bams with the ""--flow-mode STANDARD"" and ""--likelihood-calculation-engine FlowBasedHMM"" arguments. However, I'm getting the following error ""java.lang.IllegalArgumentException: read must be flow based: 180652-BC94-0022826568 chr1:14585-14703"". The bams were created from fastqs provided directly from Ultima, so they are definitely flow-based. One question I have: how does HaplotypeCaller determine if a read is flow-based or not? Is this specified in the Read Groups?. #### Steps to reproduce; gatk --java-options ""-Xmx4g"" HaplotypeCaller -R hg38.fa --flow-mode STANDARD --likelihood-calculation-engine FlowBasedHMM -I [bam] -O [bam%.BQSRapplied.bam].GATK.vcf.gz. #### Expected behavior; HaplotypeCaller should complete successfully. #### Actual behavior; HaplotypeCaller fails, expecting flow-based reads",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8112
https://github.com/broadinstitute/gatk/pull/8113:368,Availability,failure,failure,368,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113
https://github.com/broadinstitute/gatk/pull/8113:754,Availability,error,error,754,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113
https://github.com/broadinstitute/gatk/pull/8113:920,Deployability,patch,patched,920,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113
https://github.com/broadinstitute/gatk/pull/8113:456,Integrability,depend,depending,456,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113
https://github.com/broadinstitute/gatk/pull/8113:681,Modifiability,rewrite,rewrite,681,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113
https://github.com/broadinstitute/gatk/pull/8113:1438,Modifiability,refactor,refactoring,1438,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113
https://github.com/broadinstitute/gatk/pull/8113:901,Testability,log,logic,901,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113
https://github.com/broadinstitute/gatk/issues/8115:942,Energy Efficiency,reduce,reduce,942,"## Documentation request. ### Tool(s) or class(es) involved; _online documentation_. ### Description ; _First of all, thank you for a great tool with amazing documentation. The issue that I noticed recently is that all that great documentation on the website became almost impossible to read. The text is light grey on white and parameters are shown in pale blue on white. Surely I cannot be the only one who gets a headache after looking at it for 5 minutes._; ![image](https://user-images.githubusercontent.com/22867431/204846210-c6d03c9c-f91b-4b3f-88b3-6927768b4946.png). Ideally a dark theme would be amazing, but anything with a little more contrast would be a big help. ----. P.S.: Before you say so, I did try to make a post about this on the forum. Any sign in request bounces back to the main page without doing anything.; P.P.S: I do think that the issue here is appropriate considering the fact that the influx of new issues might reduce significantly if people can actually read the documentation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8115
https://github.com/broadinstitute/gatk/pull/8117:306,Deployability,pipeline,pipeline,306,for the tieout of the VCF origin VAT and the VDS origin VAT we want to compare apples to apples (or at least run them both on the same nirvana version with the same nirvana annotations). TODO:; This is the branch I will want to tag so that future users can start here if they want to run VCFs thru the VAT pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8117
https://github.com/broadinstitute/gatk/issues/8118:126,Deployability,release,release,126,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller/ActiveProfile. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; In function findEndOfRegion (line 355 in src/main/java/org/broadinstitute/hellbender/utils/activityprofile/ActivityProfile.java), it tries to determine the end of an active region. . The problem happens here, (at line 356); ![activeregion](https://user-images.githubusercontent.com/34263164/205565469-84900a73-1180-48e1-ba9f-f96c23d91e11.PNG); There could be an edge case where stateList.size() = maxRegionSize + getMaxProbPropagationDistance(), the function processes forward for further calculation. Hence the end of active region is determined immediately. However, the end of region is determined earlier than we expected. If by coincidence location at maxRegionSize is determined as minimum, region end is determined here. IBut wait a sec... If location at maxRegionSize+50 (which is NOT involved in current code in the ""if"" judgement at line 356) has an active score larger than 0, it rises the probability value at location maxRegionSize. . Now you should understand what I said. The state of location at maxRegionSize+50 is not updated when you accessed it. Let's assume ; maxRegionSize = 300 and point at location 350 has active value > 0. We trasverse the region to find the minimum point where we could cut the region and we found location at 300 in current logic. However, location 350 can acturally increase the probability at point 300 but this is not considered (or not updated) when making region end decision. #### Expected behavior; Simply use less or equal to at line 356 in the above image would fix this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8118
https://github.com/broadinstitute/gatk/issues/8118:1207,Deployability,update,updated,1207,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller/ActiveProfile. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; In function findEndOfRegion (line 355 in src/main/java/org/broadinstitute/hellbender/utils/activityprofile/ActivityProfile.java), it tries to determine the end of an active region. . The problem happens here, (at line 356); ![activeregion](https://user-images.githubusercontent.com/34263164/205565469-84900a73-1180-48e1-ba9f-f96c23d91e11.PNG); There could be an edge case where stateList.size() = maxRegionSize + getMaxProbPropagationDistance(), the function processes forward for further calculation. Hence the end of active region is determined immediately. However, the end of region is determined earlier than we expected. If by coincidence location at maxRegionSize is determined as minimum, region end is determined here. IBut wait a sec... If location at maxRegionSize+50 (which is NOT involved in current code in the ""if"" judgement at line 356) has an active score larger than 0, it rises the probability value at location maxRegionSize. . Now you should understand what I said. The state of location at maxRegionSize+50 is not updated when you accessed it. Let's assume ; maxRegionSize = 300 and point at location 350 has active value > 0. We trasverse the region to find the minimum point where we could cut the region and we found location at 300 in current logic. However, location 350 can acturally increase the probability at point 300 but this is not considered (or not updated) when making region end decision. #### Expected behavior; Simply use less or equal to at line 356 in the above image would fix this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8118
https://github.com/broadinstitute/gatk/issues/8118:1556,Deployability,update,updated,1556,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller/ActiveProfile. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; In function findEndOfRegion (line 355 in src/main/java/org/broadinstitute/hellbender/utils/activityprofile/ActivityProfile.java), it tries to determine the end of an active region. . The problem happens here, (at line 356); ![activeregion](https://user-images.githubusercontent.com/34263164/205565469-84900a73-1180-48e1-ba9f-f96c23d91e11.PNG); There could be an edge case where stateList.size() = maxRegionSize + getMaxProbPropagationDistance(), the function processes forward for further calculation. Hence the end of active region is determined immediately. However, the end of region is determined earlier than we expected. If by coincidence location at maxRegionSize is determined as minimum, region end is determined here. IBut wait a sec... If location at maxRegionSize+50 (which is NOT involved in current code in the ""if"" judgement at line 356) has an active score larger than 0, it rises the probability value at location maxRegionSize. . Now you should understand what I said. The state of location at maxRegionSize+50 is not updated when you accessed it. Let's assume ; maxRegionSize = 300 and point at location 350 has active value > 0. We trasverse the region to find the minimum point where we could cut the region and we found location at 300 in current logic. However, location 350 can acturally increase the probability at point 300 but this is not considered (or not updated) when making region end decision. #### Expected behavior; Simply use less or equal to at line 356 in the above image would fix this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8118
https://github.com/broadinstitute/gatk/issues/8118:1224,Security,access,accessed,1224,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller/ActiveProfile. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; In function findEndOfRegion (line 355 in src/main/java/org/broadinstitute/hellbender/utils/activityprofile/ActivityProfile.java), it tries to determine the end of an active region. . The problem happens here, (at line 356); ![activeregion](https://user-images.githubusercontent.com/34263164/205565469-84900a73-1180-48e1-ba9f-f96c23d91e11.PNG); There could be an edge case where stateList.size() = maxRegionSize + getMaxProbPropagationDistance(), the function processes forward for further calculation. Hence the end of active region is determined immediately. However, the end of region is determined earlier than we expected. If by coincidence location at maxRegionSize is determined as minimum, region end is determined here. IBut wait a sec... If location at maxRegionSize+50 (which is NOT involved in current code in the ""if"" judgement at line 356) has an active score larger than 0, it rises the probability value at location maxRegionSize. . Now you should understand what I said. The state of location at maxRegionSize+50 is not updated when you accessed it. Let's assume ; maxRegionSize = 300 and point at location 350 has active value > 0. We trasverse the region to find the minimum point where we could cut the region and we found location at 300 in current logic. However, location 350 can acturally increase the probability at point 300 but this is not considered (or not updated) when making region end decision. #### Expected behavior; Simply use less or equal to at line 356 in the above image would fix this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8118
https://github.com/broadinstitute/gatk/issues/8118:1440,Testability,log,logic,1440,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller/ActiveProfile. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; In function findEndOfRegion (line 355 in src/main/java/org/broadinstitute/hellbender/utils/activityprofile/ActivityProfile.java), it tries to determine the end of an active region. . The problem happens here, (at line 356); ![activeregion](https://user-images.githubusercontent.com/34263164/205565469-84900a73-1180-48e1-ba9f-f96c23d91e11.PNG); There could be an edge case where stateList.size() = maxRegionSize + getMaxProbPropagationDistance(), the function processes forward for further calculation. Hence the end of active region is determined immediately. However, the end of region is determined earlier than we expected. If by coincidence location at maxRegionSize is determined as minimum, region end is determined here. IBut wait a sec... If location at maxRegionSize+50 (which is NOT involved in current code in the ""if"" judgement at line 356) has an active score larger than 0, it rises the probability value at location maxRegionSize. . Now you should understand what I said. The state of location at maxRegionSize+50 is not updated when you accessed it. Let's assume ; maxRegionSize = 300 and point at location 350 has active value > 0. We trasverse the region to find the minimum point where we could cut the region and we found location at 300 in current logic. However, location 350 can acturally increase the probability at point 300 but this is not considered (or not updated) when making region end decision. #### Expected behavior; Simply use less or equal to at line 356 in the above image would fix this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8118
https://github.com/broadinstitute/gatk/issues/8118:1622,Usability,Simpl,Simply,1622,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller/ActiveProfile. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; In function findEndOfRegion (line 355 in src/main/java/org/broadinstitute/hellbender/utils/activityprofile/ActivityProfile.java), it tries to determine the end of an active region. . The problem happens here, (at line 356); ![activeregion](https://user-images.githubusercontent.com/34263164/205565469-84900a73-1180-48e1-ba9f-f96c23d91e11.PNG); There could be an edge case where stateList.size() = maxRegionSize + getMaxProbPropagationDistance(), the function processes forward for further calculation. Hence the end of active region is determined immediately. However, the end of region is determined earlier than we expected. If by coincidence location at maxRegionSize is determined as minimum, region end is determined here. IBut wait a sec... If location at maxRegionSize+50 (which is NOT involved in current code in the ""if"" judgement at line 356) has an active score larger than 0, it rises the probability value at location maxRegionSize. . Now you should understand what I said. The state of location at maxRegionSize+50 is not updated when you accessed it. Let's assume ; maxRegionSize = 300 and point at location 350 has active value > 0. We trasverse the region to find the minimum point where we could cut the region and we found location at 300 in current logic. However, location 350 can acturally increase the probability at point 300 but this is not considered (or not updated) when making region end decision. #### Expected behavior; Simply use less or equal to at line 356 in the above image would fix this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8118
https://github.com/broadinstitute/gatk/issues/8120:89,Availability,down,downloaded,89,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/issues/8120:246,Availability,down,downloaded,246,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/issues/8120:144,Deployability,release,releases,144,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/issues/8120:199,Deployability,release,release,199,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/issues/8120:318,Deployability,release,releases,318,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/issues/8120:361,Deployability,install,installed,361,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/issues/8120:454,Deployability,install,installing,454,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/issues/8120:485,Deployability,install,installed,485,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/issues/8120:551,Deployability,install,installed,551,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120
https://github.com/broadinstitute/gatk/pull/8122:30,Deployability,pipeline,pipeline,30,"This PR modifies the VDS->VAT pipeline to scatter. We take the input sites only VCF, and scatter it.; Successful run at: https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%2010K%20Callset/job_history/468ab890-a49e-4487-ad12-7813296d9f04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8122
https://github.com/broadinstitute/gatk/pull/8124:224,Modifiability,extend,extend,224,Hello - we're interested in creating a custom extension of Funcotator with different output formats. This PR should be quite low risk - it just converts a handful of privates fields/methods to protected to make it easier to extend this tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8124
https://github.com/broadinstitute/gatk/pull/8124:129,Safety,risk,risk,129,Hello - we're interested in creating a custom extension of Funcotator with different output formats. This PR should be quite low risk - it just converts a handful of privates fields/methods to protected to make it easier to extend this tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8124
https://github.com/broadinstitute/gatk/pull/8125:4,Deployability,Update,Updates,4,### Updates; Some GATK-SV VCFs contain MEI deletions with ALT in the format <DEL:ME:ALU> or <DEL:ME>. This change will allow SVAnnotate to recognize and annotate those records as deletions. ### Testing; * Added unit test with MEI DEL; * Ran all unit and integration tests for SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8125
https://github.com/broadinstitute/gatk/pull/8125:254,Deployability,integrat,integration,254,### Updates; Some GATK-SV VCFs contain MEI deletions with ALT in the format <DEL:ME:ALU> or <DEL:ME>. This change will allow SVAnnotate to recognize and annotate those records as deletions. ### Testing; * Added unit test with MEI DEL; * Ran all unit and integration tests for SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8125
https://github.com/broadinstitute/gatk/pull/8125:254,Integrability,integrat,integration,254,### Updates; Some GATK-SV VCFs contain MEI deletions with ALT in the format <DEL:ME:ALU> or <DEL:ME>. This change will allow SVAnnotate to recognize and annotate those records as deletions. ### Testing; * Added unit test with MEI DEL; * Ran all unit and integration tests for SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8125
https://github.com/broadinstitute/gatk/pull/8125:194,Testability,Test,Testing,194,### Updates; Some GATK-SV VCFs contain MEI deletions with ALT in the format <DEL:ME:ALU> or <DEL:ME>. This change will allow SVAnnotate to recognize and annotate those records as deletions. ### Testing; * Added unit test with MEI DEL; * Ran all unit and integration tests for SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8125
https://github.com/broadinstitute/gatk/pull/8125:216,Testability,test,test,216,### Updates; Some GATK-SV VCFs contain MEI deletions with ALT in the format <DEL:ME:ALU> or <DEL:ME>. This change will allow SVAnnotate to recognize and annotate those records as deletions. ### Testing; * Added unit test with MEI DEL; * Ran all unit and integration tests for SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8125
https://github.com/broadinstitute/gatk/pull/8125:266,Testability,test,tests,266,### Updates; Some GATK-SV VCFs contain MEI deletions with ALT in the format <DEL:ME:ALU> or <DEL:ME>. This change will allow SVAnnotate to recognize and annotate those records as deletions. ### Testing; * Added unit test with MEI DEL; * Ran all unit and integration tests for SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8125
https://github.com/broadinstitute/gatk/issues/8126:80,Testability,log,log,80,### Instructions; it‘s my first time to run the pipline for RNA-seq data. ; the log of SplitNCigarReads makes me confused which said '00:39:03.278 WARN IntelInflater - Zero Bytes Written : 0' at below.; i don't know weather this step works will or just return the files same as last step？; maybe someone know what's happened，I'm very grateful for someone to help me answer this question！. 00:35:09.127 INFO ProgressMeter - 23:27354557 12.4 29918000 2404017.7; 00:35:19.176 INFO ProgressMeter - 23:39821286 12.6 30308000 2403016.1; 00:35:29.182 INFO ProgressMeter - unmapped 12.8 30712000 2403270.9; 00:35:39.184 INFO ProgressMeter - 25:18185455 12.9 31112000 2403222.6; 00:35:49.196 INFO ProgressMeter - 25:35485074 13.1 31463000 2399411.3; 00:35:59.205 INFO ProgressMeter - 26:15194951 13.3 31932000 2404584.5; 00:36:09.210 INFO ProgressMeter - 26:39796753 13.4 32325000 2403995.1; 00:36:19.234 INFO ProgressMeter - 27:19938980 13.6 33035000 2426647.2; 00:36:29.429 INFO ProgressMeter - 28:32806413 13.8 33417000 2424444.0; 00:36:39.586 INFO ProgressMeter - unmapped 14.0 33821000 2423984.0; 00:36:49.602 INFO ProgressMeter - 29:43615735 14.1 34294000 2428825.2; 00:36:59.630 INFO ProgressMeter - X:20297509 14.3 34746000 2432052.2; 00:37:09.635 INFO ProgressMeter - X:79368355 14.5 35002000 2421702.8; 00:37:19.744 INFO ProgressMeter - unmapped 14.6 35364000 2418555.7; 00:37:29.986 INFO ProgressMeter - MT:4752 14.8 36632000 2476365.0; 00:37:39.989 INFO ProgressMeter - MT:6105 15.0 37831000 2528917.2; 00:37:50.172 INFO ProgressMeter - MT:6537 15.1 38943000 2574051.7; 00:38:00.172 INFO ProgressMeter - MT:7097 15.3 40049000 2618311.9; 00:38:10.174 INFO ProgressMeter - MT:8432 15.5 41100000 2658052.2; 00:38:20.250 INFO ProgressMeter - MT:9258 15.6 42088000 2692704.3; 00:38:30.365 INFO ProgressMeter - MT:11181 15.8 43055000 2725181.1; 00:38:40.367 INFO ProgressMeter - MT:13120 16.0 44100000 2762177.2; 00:38:50.424 INFO ProgressMeter - MT:14886 16.1 45149000 2798500.4; 00:39:00.640 INFO Progr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8126
https://github.com/broadinstitute/gatk/issues/8127:48,Deployability,update,update,48,gatkcondaenv.yml ; - uses python 3.6.10. how to update the python version?. I did try with:; conda update --all. but it does not work. . Thankyou,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8127
https://github.com/broadinstitute/gatk/issues/8127:99,Deployability,update,update,99,gatkcondaenv.yml ; - uses python 3.6.10. how to update the python version?. I did try with:; conda update --all. but it does not work. . Thankyou,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8127
https://github.com/broadinstitute/gatk/pull/8128:223,Availability,avail,available,223,"The CNN tools launch python immediately even when they're just instantiated, rather than waiting until the tool actually starts executing. This can cause some build tasks (gatkDoc, gatkWDLGen, etc.) to fail if python isn't available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8128
https://github.com/broadinstitute/gatk/issues/8129:2550,Availability,ERROR,ERROR,2550," your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I conducted joint-call with GATK GenotypeGVCfs for two samples (proband, and mother). I have identified the maternal variant information filled with ""."" in jointcall.vcf which is the output file of GTAK GenotypeGVCfs (see, figure 1). For chromosome MT, all variant information field values were fileld with ""."", instead of mother's g.vcf. . ; ![image](https://user-images.githubusercontent.com/45510932/207542098-cd4af866-c209-405f-9553-3810275f7d8e.png); Figure 1. Jointcall.vcf of proband, and mother. redbox refers to maternal variant information. . . Except for chromosomes X, Y, and MT, these issues did not occur. In addition, I suspected the false positive variants filtering which is the advantage of GATK jointcall. however, the variants which have ""."" values do not seem to be false positive variants considering AD, DP, and variant allele frequency (VAF). #### Steps to reproduce; GATK version used: 3.8.1; ```; java -jar GenomeAnalysisTK-3.8-1-0-gf15c1c3ef/GenomeAnalysisTK.jar \ ; -T GenotypeGVCFs \; -R Homo_sapiens_assembly38.main_chr.fasta \; --variant proband.hg38.g.vcf.gz \; --variant mother.hg38.g.vcf.gz \; -o output_jointcall.vcf \; --logging_level ERROR; ```. #### Expected behavior; variant info (field_value ) not ""."" . #### Actual behavior; filled with "".""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8129
https://github.com/broadinstitute/gatk/issues/8129:1283,Deployability,release,release,1283,"existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I conducted joint-call with GATK GenotypeGVCfs for two samples (proband, and mother). I have identified the maternal variant information filled with ""."" in jointcall.vcf which is the output file of GTAK GenotypeGVCfs (see, figure 1). For chromosome MT, all variant information field values were fileld with ""."", instead of mother's g.vcf. . ; ![image](https://user-images.githubusercontent.com/45510932/207542098-cd4af866-c209-405f-9553-3810275f7d8e.png); Figure 1. Jointcall.vcf of proband, and mother. redbox refers to maternal variant information. . . Except for chromosomes X, Y, and MT, these issues did not occur. In addition, I suspected the false positive variants filtering which is the advantage of GATK jointcall. however, the variants which have ""."" values do not seem to be false positive variants considering AD, DP, and variant allele frequency (VAF). #### Steps to reproduce; GATK version used: 3.8.1; ``",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8129
https://github.com/broadinstitute/gatk/issues/8129:1353,Testability,test,test,1353,"existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I conducted joint-call with GATK GenotypeGVCfs for two samples (proband, and mother). I have identified the maternal variant information filled with ""."" in jointcall.vcf which is the output file of GTAK GenotypeGVCfs (see, figure 1). For chromosome MT, all variant information field values were fileld with ""."", instead of mother's g.vcf. . ; ![image](https://user-images.githubusercontent.com/45510932/207542098-cd4af866-c209-405f-9553-3810275f7d8e.png); Figure 1. Jointcall.vcf of proband, and mother. redbox refers to maternal variant information. . . Except for chromosomes X, Y, and MT, these issues did not occur. In addition, I suspected the false positive variants filtering which is the advantage of GATK jointcall. however, the variants which have ""."" values do not seem to be false positive variants considering AD, DP, and variant allele frequency (VAF). #### Steps to reproduce; GATK version used: 3.8.1; ``",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8129
https://github.com/broadinstitute/gatk/pull/8131:93,Deployability,integrat,integration,93,- Removed positive-negative training from TrainVariantAnnotationsModel along with associated integration and WDL tests.; - Added ability to run positive-unlabeled training by passing unlabeled annotations to a custom python backend (although no example backend or tests were added).; - Cleaned up some WDL arguments to allow distinct training and scoring python scripts.; - Removed the `useAlleleSpecificAnnotations` argument; we instead infer whether to run in allele-specific mode from the VCF header.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131
https://github.com/broadinstitute/gatk/pull/8131:93,Integrability,integrat,integration,93,- Removed positive-negative training from TrainVariantAnnotationsModel along with associated integration and WDL tests.; - Added ability to run positive-unlabeled training by passing unlabeled annotations to a custom python backend (although no example backend or tests were added).; - Cleaned up some WDL arguments to allow distinct training and scoring python scripts.; - Removed the `useAlleleSpecificAnnotations` argument; we instead infer whether to run in allele-specific mode from the VCF header.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131
https://github.com/broadinstitute/gatk/pull/8131:113,Testability,test,tests,113,- Removed positive-negative training from TrainVariantAnnotationsModel along with associated integration and WDL tests.; - Added ability to run positive-unlabeled training by passing unlabeled annotations to a custom python backend (although no example backend or tests were added).; - Cleaned up some WDL arguments to allow distinct training and scoring python scripts.; - Removed the `useAlleleSpecificAnnotations` argument; we instead infer whether to run in allele-specific mode from the VCF header.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131
https://github.com/broadinstitute/gatk/pull/8131:264,Testability,test,tests,264,- Removed positive-negative training from TrainVariantAnnotationsModel along with associated integration and WDL tests.; - Added ability to run positive-unlabeled training by passing unlabeled annotations to a custom python backend (although no example backend or tests were added).; - Cleaned up some WDL arguments to allow distinct training and scoring python scripts.; - Removed the `useAlleleSpecificAnnotations` argument; we instead infer whether to run in allele-specific mode from the VCF header.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131
https://github.com/broadinstitute/gatk/pull/8133:182,Performance,perform,performance,182,This PR uses Nirvana's JASIX parser to break the annotated output of Nirvana into a genes and positions json which are then parsed by the corresponding python methods. This improves performance significantly. Successful workflow here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/14d2d1cf-3a2a-42e0-8542-c70edd2133e5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8133
https://github.com/broadinstitute/gatk/issues/8134:907,Integrability,Wrap,WrappedArray,907,"## Bug Report. ### Affected tool(s) or class(es); _MarkDuplicatesSpark_. ### Affected version(s); - [ ] GATK version 4.1.9.0. ### Description ; Headers with another `@` character fail to create a valid bam using MarkDuplicatesSpark. The bam file is empty. But the header will work when using samtools markdup instead. The following example was found in one of many samples we found in ICGC datasets. Example header:; `@HWI-ST700660_163:1:1101:1243:1870#1@0/1`. Log:<br> (removed some content since it was too long); ```; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; ...; 01:22 DEBUG: [kryo] Read: CompressedMapStatus; 01:22 DEBUG: [kryo] Write: CompressedMapStatus; ...; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: WrappedArray([]); 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: scala.Tuple3[]; ...; 02:42 DEBUG: [kryo] Write object reference 1941: HLA-A*24:152; 02:42 DEBUG: [kryo] Write object reference 1945: chrUn_JTFH01001224v1_decoy; 02:42 DEBUG: [kryo] Write object reference 1949: HLA-B*14:01:01; 02:42 DEBUG: [kryo] Write object reference 1953: chr5_GL949742v1_alt; ...; 02:42 DEBUG: [kryo] Write object reference 1942: SAMSequenceRecord(name=HLA-A*24:152,length=3176,dict_index=2919,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1946: SAMSequenceRecord(name=chrUn_JTFH01001224v1_decoy,length=1051,dict_index=2066,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1950: SAMSequenceRecord(name=HLA-B*14:01:01,length=3312,dict_index=2999,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1954: SAMSequenceRecord(name",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134
https://github.com/broadinstitute/gatk/issues/8134:952,Integrability,Wrap,WrappedArray,952,"## Bug Report. ### Affected tool(s) or class(es); _MarkDuplicatesSpark_. ### Affected version(s); - [ ] GATK version 4.1.9.0. ### Description ; Headers with another `@` character fail to create a valid bam using MarkDuplicatesSpark. The bam file is empty. But the header will work when using samtools markdup instead. The following example was found in one of many samples we found in ICGC datasets. Example header:; `@HWI-ST700660_163:1:1101:1243:1870#1@0/1`. Log:<br> (removed some content since it was too long); ```; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; ...; 01:22 DEBUG: [kryo] Read: CompressedMapStatus; 01:22 DEBUG: [kryo] Write: CompressedMapStatus; ...; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: WrappedArray([]); 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: scala.Tuple3[]; ...; 02:42 DEBUG: [kryo] Write object reference 1941: HLA-A*24:152; 02:42 DEBUG: [kryo] Write object reference 1945: chrUn_JTFH01001224v1_decoy; 02:42 DEBUG: [kryo] Write object reference 1949: HLA-B*14:01:01; 02:42 DEBUG: [kryo] Write object reference 1953: chr5_GL949742v1_alt; ...; 02:42 DEBUG: [kryo] Write object reference 1942: SAMSequenceRecord(name=HLA-A*24:152,length=3176,dict_index=2919,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1946: SAMSequenceRecord(name=chrUn_JTFH01001224v1_decoy,length=1051,dict_index=2066,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1950: SAMSequenceRecord(name=HLA-B*14:01:01,length=3312,dict_index=2999,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1954: SAMSequenceRecord(name",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134
https://github.com/broadinstitute/gatk/issues/8134:1080,Integrability,Wrap,WrappedArray,1080,"version 4.1.9.0. ### Description ; Headers with another `@` character fail to create a valid bam using MarkDuplicatesSpark. The bam file is empty. But the header will work when using samtools markdup instead. The following example was found in one of many samples we found in ICGC datasets. Example header:; `@HWI-ST700660_163:1:1101:1243:1870#1@0/1`. Log:<br> (removed some content since it was too long); ```; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; ...; 01:22 DEBUG: [kryo] Read: CompressedMapStatus; 01:22 DEBUG: [kryo] Write: CompressedMapStatus; ...; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: WrappedArray([]); 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: scala.Tuple3[]; ...; 02:42 DEBUG: [kryo] Write object reference 1941: HLA-A*24:152; 02:42 DEBUG: [kryo] Write object reference 1945: chrUn_JTFH01001224v1_decoy; 02:42 DEBUG: [kryo] Write object reference 1949: HLA-B*14:01:01; 02:42 DEBUG: [kryo] Write object reference 1953: chr5_GL949742v1_alt; ...; 02:42 DEBUG: [kryo] Write object reference 1942: SAMSequenceRecord(name=HLA-A*24:152,length=3176,dict_index=2919,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1946: SAMSequenceRecord(name=chrUn_JTFH01001224v1_decoy,length=1051,dict_index=2066,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1950: SAMSequenceRecord(name=HLA-B*14:01:01,length=3312,dict_index=2999,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1954: SAMSequenceRecord(name=chr5_GL949742v1_alt,length=226852,dict_index=241,assembly=null,alternate_names=[]); ...; 02:42 DEBUG: [kryo]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134
https://github.com/broadinstitute/gatk/issues/8134:1124,Integrability,Wrap,WrappedArray,1124,"version 4.1.9.0. ### Description ; Headers with another `@` character fail to create a valid bam using MarkDuplicatesSpark. The bam file is empty. But the header will work when using samtools markdup instead. The following example was found in one of many samples we found in ICGC datasets. Example header:; `@HWI-ST700660_163:1:1101:1243:1870#1@0/1`. Log:<br> (removed some content since it was too long); ```; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; ...; 01:22 DEBUG: [kryo] Read: CompressedMapStatus; 01:22 DEBUG: [kryo] Write: CompressedMapStatus; ...; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: WrappedArray([]); 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: scala.Tuple3[]; ...; 02:42 DEBUG: [kryo] Write object reference 1941: HLA-A*24:152; 02:42 DEBUG: [kryo] Write object reference 1945: chrUn_JTFH01001224v1_decoy; 02:42 DEBUG: [kryo] Write object reference 1949: HLA-B*14:01:01; 02:42 DEBUG: [kryo] Write object reference 1953: chr5_GL949742v1_alt; ...; 02:42 DEBUG: [kryo] Write object reference 1942: SAMSequenceRecord(name=HLA-A*24:152,length=3176,dict_index=2919,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1946: SAMSequenceRecord(name=chrUn_JTFH01001224v1_decoy,length=1051,dict_index=2066,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1950: SAMSequenceRecord(name=HLA-B*14:01:01,length=3312,dict_index=2999,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1954: SAMSequenceRecord(name=chr5_GL949742v1_alt,length=226852,dict_index=241,assembly=null,alternate_names=[]); ...; 02:42 DEBUG: [kryo]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134
https://github.com/broadinstitute/gatk/issues/8134:2242,Integrability,Wrap,WrappedArray,2242,"G: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: scala.Tuple3[]; ...; 02:42 DEBUG: [kryo] Write object reference 1941: HLA-A*24:152; 02:42 DEBUG: [kryo] Write object reference 1945: chrUn_JTFH01001224v1_decoy; 02:42 DEBUG: [kryo] Write object reference 1949: HLA-B*14:01:01; 02:42 DEBUG: [kryo] Write object reference 1953: chr5_GL949742v1_alt; ...; 02:42 DEBUG: [kryo] Write object reference 1942: SAMSequenceRecord(name=HLA-A*24:152,length=3176,dict_index=2919,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1946: SAMSequenceRecord(name=chrUn_JTFH01001224v1_decoy,length=1051,dict_index=2066,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1950: SAMSequenceRecord(name=HLA-B*14:01:01,length=3312,dict_index=2999,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1954: SAMSequenceRecord(name=chr5_GL949742v1_alt,length=226852,dict_index=241,assembly=null,alternate_names=[]); ...; 02:42 DEBUG: [kryo] Write: Array[java.lang.Object]; 02:42 DEBUG: [kryo] Write: Object[]; 02:42 DEBUG: [kryo] Write: byte[]; 02:42 DEBUG: [kryo] Write: WrappedArray([]); ...; 03:20 DEBUG: [kryo] Read: CompressedMapStatus; 03:20 DEBUG: [kryo] Write: Array[java.lang.Object]; 03:20 DEBUG: [kryo] Write: Object[]; 03:20 DEBUG: [kryo] Write: byte[]; 03:20 DEBUG: [kryo] Read: Array[java.lang.Object]; 03:20 DEBUG: [kryo] Read: Object[]; 03:21 DEBUG: [kryo] Write: TaskCommitMessage; 03:21 DEBUG: [kryo] Read: TaskCommitMessage; ```. #### Steps to reproduce; **Command:**; ```; gatk --java-options ""-Djava.io.tmpdir=/scratch"" MarkDuplicatesSpark --input C19CUACXX.1.1-1.sorted.bam --output /scratch/C19CUACXX.1.1.sorted.Spark.Strigency-strict.bam --conf 'spark.local.dir=/scratch' --tmp-dir /scratch --read-validation-stringency STRICT; ```. #### Expected behavior; Finish MarkDuplicatesSpark successfully and output a valid bam file. #### Actual behavior; The bam file is empty.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134
https://github.com/broadinstitute/gatk/issues/8134:2892,Security,validat,validation-stringency,2892,"G: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: scala.Tuple3[]; ...; 02:42 DEBUG: [kryo] Write object reference 1941: HLA-A*24:152; 02:42 DEBUG: [kryo] Write object reference 1945: chrUn_JTFH01001224v1_decoy; 02:42 DEBUG: [kryo] Write object reference 1949: HLA-B*14:01:01; 02:42 DEBUG: [kryo] Write object reference 1953: chr5_GL949742v1_alt; ...; 02:42 DEBUG: [kryo] Write object reference 1942: SAMSequenceRecord(name=HLA-A*24:152,length=3176,dict_index=2919,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1946: SAMSequenceRecord(name=chrUn_JTFH01001224v1_decoy,length=1051,dict_index=2066,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1950: SAMSequenceRecord(name=HLA-B*14:01:01,length=3312,dict_index=2999,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1954: SAMSequenceRecord(name=chr5_GL949742v1_alt,length=226852,dict_index=241,assembly=null,alternate_names=[]); ...; 02:42 DEBUG: [kryo] Write: Array[java.lang.Object]; 02:42 DEBUG: [kryo] Write: Object[]; 02:42 DEBUG: [kryo] Write: byte[]; 02:42 DEBUG: [kryo] Write: WrappedArray([]); ...; 03:20 DEBUG: [kryo] Read: CompressedMapStatus; 03:20 DEBUG: [kryo] Write: Array[java.lang.Object]; 03:20 DEBUG: [kryo] Write: Object[]; 03:20 DEBUG: [kryo] Write: byte[]; 03:20 DEBUG: [kryo] Read: Array[java.lang.Object]; 03:20 DEBUG: [kryo] Read: Object[]; 03:21 DEBUG: [kryo] Write: TaskCommitMessage; 03:21 DEBUG: [kryo] Read: TaskCommitMessage; ```. #### Steps to reproduce; **Command:**; ```; gatk --java-options ""-Djava.io.tmpdir=/scratch"" MarkDuplicatesSpark --input C19CUACXX.1.1-1.sorted.bam --output /scratch/C19CUACXX.1.1.sorted.Spark.Strigency-strict.bam --conf 'spark.local.dir=/scratch' --tmp-dir /scratch --read-validation-stringency STRICT; ```. #### Expected behavior; Finish MarkDuplicatesSpark successfully and output a valid bam file. #### Actual behavior; The bam file is empty.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134
https://github.com/broadinstitute/gatk/issues/8134:461,Testability,Log,Log,461,"## Bug Report. ### Affected tool(s) or class(es); _MarkDuplicatesSpark_. ### Affected version(s); - [ ] GATK version 4.1.9.0. ### Description ; Headers with another `@` character fail to create a valid bam using MarkDuplicatesSpark. The bam file is empty. But the header will work when using samtools markdup instead. The following example was found in one of many samples we found in ICGC datasets. Example header:; `@HWI-ST700660_163:1:1101:1243:1870#1@0/1`. Log:<br> (removed some content since it was too long); ```; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Read: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; 00:05 DEBUG: [kryo] Write: Object[]; ...; 01:22 DEBUG: [kryo] Read: CompressedMapStatus; 01:22 DEBUG: [kryo] Write: CompressedMapStatus; ...; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: WrappedArray([]); 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: scala.Tuple3[]; 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: scala.Tuple3[]; ...; 02:42 DEBUG: [kryo] Write object reference 1941: HLA-A*24:152; 02:42 DEBUG: [kryo] Write object reference 1945: chrUn_JTFH01001224v1_decoy; 02:42 DEBUG: [kryo] Write object reference 1949: HLA-B*14:01:01; 02:42 DEBUG: [kryo] Write object reference 1953: chr5_GL949742v1_alt; ...; 02:42 DEBUG: [kryo] Write object reference 1942: SAMSequenceRecord(name=HLA-A*24:152,length=3176,dict_index=2919,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1946: SAMSequenceRecord(name=chrUn_JTFH01001224v1_decoy,length=1051,dict_index=2066,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1950: SAMSequenceRecord(name=HLA-B*14:01:01,length=3312,dict_index=2999,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1954: SAMSequenceRecord(name",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134
https://github.com/broadinstitute/gatk/pull/8137:9,Testability,test,testing,9,Add more testing for create_genes_bqloadjson_from_annotations and create_vt_bqloadjson_from_annotations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8137
https://github.com/broadinstitute/gatk/issues/8139:1509,Availability,error,error,1509,"nquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK CalibrateDragstrModel. ### Affected version(s); - [x] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When running CalibrateDragstrModel in parallel mode, the supplied reference isn't detected correctly causing the following error stack trace:. ```bash; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx72g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CalibrateDragstrModel --input input.cram --output input.txt --reference hg38.fa --str-table-path hg38.zip --threads 12 --intervals fasta_bed.bed --tmp-dir .; 10:24:21.117 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:21.289 INFO CalibrateDragstrModel - ------------------------------------------------------------; 10:24:21.289 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:24:21.289 INFO CalibrateDragstrModel -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:5281,Availability,down,down,5281,"sted number of threads: 12; 10:24:22.008 INFO CalibrateDragstrModel - Done initializing engine; 10:24:22.008 INFO ProgressMeter - Starting traversal; 10:24:22.008 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 10:24:32.859 INFO ProgressMeter - chr1:26000000 0.2 59038 326477.4; 10:24:42.867 INFO ProgressMeter - chr1:83000000 0.3 184245 529998.1; 10:24:52.965 INFO ProgressMeter - chr1:137193529 0.5 306766 594565.4; 10:25:03.307 INFO ProgressMeter - chr1:193193529 0.7 428759 622924.6; 10:25:13.318 INFO ProgressMeter - chr2:3237107 0.9 564835 660497.0; 10:25:23.358 INFO ProgressMeter - chr2:57237107 1.0 681209 666219.1; 10:25:33.392 INFO ProgressMeter - chr2:109237107 1.2 799610 672091.8; 10:25:44.527 INFO ProgressMeter - chr2:177512416 1.4 930822 676805.6; 10:25:54.821 INFO ProgressMeter - chr2:237512416 1.5 1069999 691712.8; 10:26:04.863 INFO ProgressMeter - chr3:54999378 1.7 1197525 698570.8; 10:26:09.642 INFO CalibrateDragstrModel - Shutting down engine; [January 2, 2023 at 10:26:09 AM GMT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 1.81 minutes.; Runtime.totalMemory()=47647293440; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006); at org.broadinstitute.hellbender.utils.Utils.runInParallel(Uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:1291,Deployability,release,release,1291,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK CalibrateDragstrModel. ### Affected version(s); - [x] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When running CalibrateDragstrModel in parallel mode, the supplied reference isn't detected correctly causing the following error stack trace:. ```bash; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx72g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CalibrateDragstrModel --input input.cram --output input.txt --reference hg38.fa --str-table-path hg38.zip --threads 12 --intervals fasta_bed.bed --tmp-dir .; 10:24:21.117 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:21.289 INFO CalibrateDragstrModel - --------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:7985,Energy Efficiency,Reduce,ReduceOps,7985,Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:7995,Energy Efficiency,Reduce,ReduceOp,7995,Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8021,Energy Efficiency,Reduce,ReduceOps,8021,broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8174,Energy Efficiency,reduce,reduce,8174,ang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8394,Energy Efficiency,Adapt,AdaptedCallable,8394,.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11356,Energy Efficiency,Reduce,ReduceOps,11356,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11366,Energy Efficiency,Reduce,ReduceTask,11366,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11384,Energy Efficiency,Reduce,ReduceOps,11384,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11435,Energy Efficiency,Reduce,ReduceOps,11435,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11445,Energy Efficiency,Reduce,ReduceTask,11445,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11463,Energy Efficiency,Reduce,ReduceOps,11463,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11282,Integrability,wrap,wrapAndCopyInto,11282,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8394,Modifiability,Adapt,AdaptedCallable,8394,.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:2063,Performance,Load,Loading,2063,"t in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK CalibrateDragstrModel. ### Affected version(s); - [x] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When running CalibrateDragstrModel in parallel mode, the supplied reference isn't detected correctly causing the following error stack trace:. ```bash; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx72g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CalibrateDragstrModel --input input.cram --output input.txt --reference hg38.fa --str-table-path hg38.zip --threads 12 --intervals fasta_bed.bed --tmp-dir .; 10:24:21.117 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:21.289 INFO CalibrateDragstrModel - ------------------------------------------------------------; 10:24:21.289 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:24:21.289 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:21.289 INFO CalibrateDragstrModel - Executing as nvnieuwk on Linux v4.18.0-372.36.1.el8_6.x86_64 amd64; 10:24:21.289 INFO CalibrateDragstrModel - Java runtime: OpenJDK 64-Bit Server VM v11.0.15-internal+0-adhoc..src; 10:24:21.289 INFO CalibrateDragstrModel - Start Date/Time: January 2, 2023 at 10:24:21 AM GMT; 10:24:21.289 INFO CalibrateDragstrModel - ------------------------------------------------------------; 10:24:21.289 INFO CalibrateDragstrModel - ------------------------------------------------------------; 10:24:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:6079,Performance,concurren,concurrent,6079,"5.6; 10:25:54.821 INFO ProgressMeter - chr2:237512416 1.5 1069999 691712.8; 10:26:04.863 INFO ProgressMeter - chr3:54999378 1.7 1197525 698570.8; 10:26:09.642 INFO CalibrateDragstrModel - Shutting down engine; [January 2, 2023 at 10:26:09 AM GMT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 1.81 minutes.; Runtime.totalMemory()=47647293440; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006); at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:551); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:202); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:6172,Performance,concurren,concurrent,6172," ProgressMeter - chr3:54999378 1.7 1197525 698570.8; 10:26:09.642 INFO CalibrateDragstrModel - Shutting down engine; [January 2, 2023 at 10:26:09 AM GMT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 1.81 minutes.; Runtime.totalMemory()=47647293440; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006); at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:551); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:202); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:7720,Performance,concurren,concurrent,7720,adinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.Fo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:7813,Performance,concurren,concurrent,7813,m.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorke,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:7900,Performance,concurren,concurrent,7900,ommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentExcepti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8370,Performance,concurren,concurrent,8370,d); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.n,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8463,Performance,concurren,concurrent,8463,ctorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8541,Performance,concurren,concurrent,8541,ctorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextReco,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8636,Performance,concurren,concurrent,8636,.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:589); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initial,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8713,Performance,concurren,concurrent,8713,l.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:589); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initializeIterator(CRAMFileReader.java:562); at htsjdk.samtools.CRAMFileReader$CRAMI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:8795,Performance,concurren,concurrent,8795,se/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:589); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initializeIterator(CRAMFileReader.java:562); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:620); at htsjdk.samtools.CRAMFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:10194,Performance,load,loadNextIterator,10194,ture.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:589); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initializeIterator(CRAMFileReader.java:562); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:620); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:615); at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:487); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11582,Performance,concurren,concurrent,11582,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:1468,Safety,detect,detected,1468,"nquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK CalibrateDragstrModel. ### Affected version(s); - [x] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When running CalibrateDragstrModel in parallel mode, the supplied reference isn't detected correctly causing the following error stack trace:. ```bash; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx72g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CalibrateDragstrModel --input input.cram --output input.txt --reference hg38.fa --str-table-path hg38.zip --threads 12 --intervals fasta_bed.bed --tmp-dir .; 10:24:21.117 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:21.289 INFO CalibrateDragstrModel - ------------------------------------------------------------; 10:24:21.289 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:24:21.289 INFO CalibrateDragstrModel -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:1360,Testability,test,test,1360,"e (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK CalibrateDragstrModel. ### Affected version(s); - [x] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When running CalibrateDragstrModel in parallel mode, the supplied reference isn't detected correctly causing the following error stack trace:. ```bash; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx72g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CalibrateDragstrModel --input input.cram --output input.txt --reference hg38.fa --str-table-path hg38.zip --threads 12 --intervals fasta_bed.bed --tmp-dir .; 10:24:21.117 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:21.289 INFO CalibrateDragstrModel - ------------------------------------------------------------; 10:24:21.289 INFO Cal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8139:11926,Testability,test,tests,11926,der$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:915); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$11(CalibrateDragstrModel.java:556); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at org.broadinstitute.hellbender.tools.dragstr.InterleavingListSpliterator.forEachRemaining(InterleavingListSpliterator.java:87); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952); at java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926); at java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327); at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); ... 5 more; ```. However it does work when running the tool single threaded with the exact same options. . #### Steps to reproduce; I've sadly been unable to create a reproducible example. I've only encountered this with non-public data which I can't share here. I'd be happy to run tests for you though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139
https://github.com/broadinstitute/gatk/issues/8141:2027,Deployability,release,release,2027,"-O output.bam. Mostly, I just get an identical 9GB bam over and over again (as confirmed by md5). However, sometimes (~10% of the time it seems), I get a MUCH larger “bam”, more like ~45GB. In runs where I get these larger output files, they are not always the same size, sometimes 45GB, sometimes 47GB (still always with the same input file, same commandline, same wdl task, etc). The runs that produce these larger bam also take much longer, with slower “reads per minute rate). They report exactly the same number of reads processed in the logs as the “normal” runs. Looking inside the large output “bams” with gsutil cat, I see the header suddenly transitioning from compressed looking jibberish to a plaintext header, and then after a bit back to compressed looking jibberish again. Additionally, if I run these large bams through samtools view to get samtools to write them as a bam (ie samtools view big.bam -o samtools_out.bam) the resulting bam is much smaller ~6GB. It kind of seems like sometimes gatk will just stop compressing the output, and then start back up again, seemingly randomly??. I suspect this may be an issue with all gatk tools, I first encountered this recently with PostProcessReadsForRSEM, and then confirmed the behavior in PrintReads as a minimal example. Maybe it’s something to do with google hardware, I’ve only seen this in Terra so far (not that I’ve tried to reproduce it anywhere else).; seeing this in 4.2.6.0. Summary of investigative results:; * reproducible on very small files (at about same rate of ~10%); * appears to be related to intel deflater. when running with jdk deflater (--use-jdk-deflater) all 100/100 runs result in same sized bam. I’ve run a version sweep, and it looks like the behavior begins in 4.2.1.0, but does not occur in earlier versions. Looking at the 4.2.1.0 release notes, **it seems highly likely that the issue was introduced by the upgrade from gkl 0.8.6 to gkl 0.8.8 in https://github.com/broadinstitute/gatk/pull/7203/files**",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8141
https://github.com/broadinstitute/gatk/issues/8141:2104,Deployability,upgrade,upgrade,2104,"-O output.bam. Mostly, I just get an identical 9GB bam over and over again (as confirmed by md5). However, sometimes (~10% of the time it seems), I get a MUCH larger “bam”, more like ~45GB. In runs where I get these larger output files, they are not always the same size, sometimes 45GB, sometimes 47GB (still always with the same input file, same commandline, same wdl task, etc). The runs that produce these larger bam also take much longer, with slower “reads per minute rate). They report exactly the same number of reads processed in the logs as the “normal” runs. Looking inside the large output “bams” with gsutil cat, I see the header suddenly transitioning from compressed looking jibberish to a plaintext header, and then after a bit back to compressed looking jibberish again. Additionally, if I run these large bams through samtools view to get samtools to write them as a bam (ie samtools view big.bam -o samtools_out.bam) the resulting bam is much smaller ~6GB. It kind of seems like sometimes gatk will just stop compressing the output, and then start back up again, seemingly randomly??. I suspect this may be an issue with all gatk tools, I first encountered this recently with PostProcessReadsForRSEM, and then confirmed the behavior in PrintReads as a minimal example. Maybe it’s something to do with google hardware, I’ve only seen this in Terra so far (not that I’ve tried to reproduce it anywhere else).; seeing this in 4.2.6.0. Summary of investigative results:; * reproducible on very small files (at about same rate of ~10%); * appears to be related to intel deflater. when running with jdk deflater (--use-jdk-deflater) all 100/100 runs result in same sized bam. I’ve run a version sweep, and it looks like the behavior begins in 4.2.1.0, but does not occur in earlier versions. Looking at the 4.2.1.0 release notes, **it seems highly likely that the issue was introduced by the upgrade from gkl 0.8.6 to gkl 0.8.8 in https://github.com/broadinstitute/gatk/pull/7203/files**",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8141
https://github.com/broadinstitute/gatk/issues/8141:742,Testability,log,logs,742,"As reported by @kachulis and confirmed by @epiercehoffman :. I run PrintReads over and over again, on the same input data, not doing anything, just read in, write out, ie gatk PrintRead -I input.bam -O output.bam. Mostly, I just get an identical 9GB bam over and over again (as confirmed by md5). However, sometimes (~10% of the time it seems), I get a MUCH larger “bam”, more like ~45GB. In runs where I get these larger output files, they are not always the same size, sometimes 45GB, sometimes 47GB (still always with the same input file, same commandline, same wdl task, etc). The runs that produce these larger bam also take much longer, with slower “reads per minute rate). They report exactly the same number of reads processed in the logs as the “normal” runs. Looking inside the large output “bams” with gsutil cat, I see the header suddenly transitioning from compressed looking jibberish to a plaintext header, and then after a bit back to compressed looking jibberish again. Additionally, if I run these large bams through samtools view to get samtools to write them as a bam (ie samtools view big.bam -o samtools_out.bam) the resulting bam is much smaller ~6GB. It kind of seems like sometimes gatk will just stop compressing the output, and then start back up again, seemingly randomly??. I suspect this may be an issue with all gatk tools, I first encountered this recently with PostProcessReadsForRSEM, and then confirmed the behavior in PrintReads as a minimal example. Maybe it’s something to do with google hardware, I’ve only seen this in Terra so far (not that I’ve tried to reproduce it anywhere else).; seeing this in 4.2.6.0. Summary of investigative results:; * reproducible on very small files (at about same rate of ~10%); * appears to be related to intel deflater. when running with jdk deflater (--use-jdk-deflater) all 100/100 runs result in same sized bam. I’ve run a version sweep, and it looks like the behavior begins in 4.2.1.0, but does not occur in earlier version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8141
https://github.com/broadinstitute/gatk/issues/8143:104,Deployability,update,update,104,"## Bug Report. ### Affected tool(s) or class(es). VariantAnnotator ... but this is due to an old syntax update perhaps other docs in other tools are also affected. ### Affected version(s); - [X] Latest public release version [version?]; - [Presumptive] Latest master branch as of [date of test?]. ### Description . The argument ```--resource``` example(s) show a wrong syntax in regards to the location of the ""provider"" name ; ; #### Steps to reproduce; Google 'GATK VariantAnnotator'; the first or one of the first hits points to the current GATK doc on the tool. . #### Expected behavior. The example should read ```--resource:foo resource-file.vcf.gz```. #### Actual behavior. The example reads ```--resource foo:resource-file.vcf.gz```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8143
https://github.com/broadinstitute/gatk/issues/8143:209,Deployability,release,release,209,"## Bug Report. ### Affected tool(s) or class(es). VariantAnnotator ... but this is due to an old syntax update perhaps other docs in other tools are also affected. ### Affected version(s); - [X] Latest public release version [version?]; - [Presumptive] Latest master branch as of [date of test?]. ### Description . The argument ```--resource``` example(s) show a wrong syntax in regards to the location of the ""provider"" name ; ; #### Steps to reproduce; Google 'GATK VariantAnnotator'; the first or one of the first hits points to the current GATK doc on the tool. . #### Expected behavior. The example should read ```--resource:foo resource-file.vcf.gz```. #### Actual behavior. The example reads ```--resource foo:resource-file.vcf.gz```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8143
https://github.com/broadinstitute/gatk/issues/8143:289,Testability,test,test,289,"## Bug Report. ### Affected tool(s) or class(es). VariantAnnotator ... but this is due to an old syntax update perhaps other docs in other tools are also affected. ### Affected version(s); - [X] Latest public release version [version?]; - [Presumptive] Latest master branch as of [date of test?]. ### Description . The argument ```--resource``` example(s) show a wrong syntax in regards to the location of the ""provider"" name ; ; #### Steps to reproduce; Google 'GATK VariantAnnotator'; the first or one of the first hits points to the current GATK doc on the tool. . #### Expected behavior. The example should read ```--resource:foo resource-file.vcf.gz```. #### Actual behavior. The example reads ```--resource foo:resource-file.vcf.gz```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8143
https://github.com/broadinstitute/gatk/issues/8146:2822,Availability,avail,available,2822,"if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://user-images.githubusercontent.com/10062863/210841121-15a4d357-dbfa-47e2-808b-08cdeb6d42be.png"">. The rendering in gatkdoc (the variable name appears in the text, but it should be interpolated):; <img width=""1055"" alt=""Screen Shot 2023-01-05 at 12 12 46 PM"" src=""https://user-images.githubusercontent.com/10062863/210840538-9a42bf02-b968-4ac9-9591-90512e87ab50.png"">. Note that QuickDocumentation within IntelliJ seems to render them correctly. Additionally, I noticed that some {@link} targets are not rendering correctly in gatkdoc, i.e., these links in `ScoreVariantAnnotations`:; ```; * {@link VariantRecalibrator} workflow. Using a previously trained model produced by {@link TrainVariantAnnotationsModel},; ```; work in javadoc, but not gatkDoc, even though the target in this case IS included in the set of objects available to the gatkDoc process. The gatkdoc process is not translating these (and apparently its replacing them with the text). But generating the anchor tags will require translation because the javadoc output files are organized hierarchically whereas the gatkdoc files are flat. The links in html file generated by javadoc have anchor tags with proper hrefs, whereas the html generated by gatkDoc has only the plain text (no anchors - the links are removed and replaced with the text by gatkdoc). Even if we embedded the javadoc in with the gatkdoc, we still would have to translate some links for gatkdoc targets at doc generation time, because when a user views the gatkdoc for `ScoreVariants`, we want the link to `VariantRecalibrator` to go to the `VariantRecalibrator` page from gatkdoc, not the page from javadoc. Also, for any entities for which there is no gatkdoc (i.e., a method reference), the gatkdoc pa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146
https://github.com/broadinstitute/gatk/issues/8146:298,Modifiability,variab,variables,298,"Original report by @samuelklee (see https://github.com/broadinstitute/barclay/issues/189):; > I noticed that Javadoc @value tags are not being rendered correctly in e.g. https://gatk.broadinstitute.org/hc/en-us/articles/9570326304155-ScoreVariantAnnotations-BETA-. I used these tags to specify the variables corresponding to argument names (e.g., StandardArgumentDefinitions#INTERVALS_LONG_NAME instead of intervals , USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME instead of use-allele-specific-annotations, etc.), and while they show up correctly when rendering the Javadoc within IntelliJ, it seems the same is not true on the GATK website. Is there an easy fix in the code for generating these docs, or should I just avoid using this tag?. My original response:. > I tested this using the new Java 17 doclets in the hope that it would just work, but the result is the same. However, the new Java language model classes make it easy to interpolate these, so I’ll fix this in the barclay Java 17 branch. However, in looking more closely, it's not as easy to fix as I first thought, and the problem is a little deeper than I first realized. Although it's easy to detect these using the new Java 17 apis, it's more difficult to retrieve the actual values. And even then, because the gatkdoc process only consumes a subset of the classes consumed by the javadoc process (it only sees `@DocumentedFeature`s), it's quite easy to reference something in the javadoc comment that can be resolved by javdoc, but not by gatkdoc. But it appears that even the javadoc process isn't rendering these tags correctly. Here is the raw javadoc comment:; ```; * Input VCF file. Site-level annotations will be extracted from the contained variants (or alleles,; * if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146
https://github.com/broadinstitute/gatk/issues/8146:2128,Modifiability,variab,variable,2128," 17 apis, it's more difficult to retrieve the actual values. And even then, because the gatkdoc process only consumes a subset of the classes consumed by the javadoc process (it only sees `@DocumentedFeature`s), it's quite easy to reference something in the javadoc comment that can be resolved by javdoc, but not by gatkdoc. But it appears that even the javadoc process isn't rendering these tags correctly. Here is the raw javadoc comment:; ```; * Input VCF file. Site-level annotations will be extracted from the contained variants (or alleles,; * if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://user-images.githubusercontent.com/10062863/210841121-15a4d357-dbfa-47e2-808b-08cdeb6d42be.png"">. The rendering in gatkdoc (the variable name appears in the text, but it should be interpolated):; <img width=""1055"" alt=""Screen Shot 2023-01-05 at 12 12 46 PM"" src=""https://user-images.githubusercontent.com/10062863/210840538-9a42bf02-b968-4ac9-9591-90512e87ab50.png"">. Note that QuickDocumentation within IntelliJ seems to render them correctly. Additionally, I noticed that some {@link} targets are not rendering correctly in gatkdoc, i.e., these links in `ScoreVariantAnnotations`:; ```; * {@link VariantRecalibrator} workflow. Using a previously trained model produced by {@link TrainVariantAnnotationsModel},; ```; work in javadoc, but not gatkDoc, even though the target in this case IS included in the set of objects available to the gatkDoc process. The gatkdoc process is not translating these (and apparently its replacing them with the text). But generating the anchor tags will require translation because the javadoc output files are organized hierarchically whereas the gatkdoc files are flat. The links in html file generated by javadoc have anchor tags with proper hrefs, wher",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146
https://github.com/broadinstitute/gatk/issues/8146:716,Safety,avoid,avoid,716,"Original report by @samuelklee (see https://github.com/broadinstitute/barclay/issues/189):; > I noticed that Javadoc @value tags are not being rendered correctly in e.g. https://gatk.broadinstitute.org/hc/en-us/articles/9570326304155-ScoreVariantAnnotations-BETA-. I used these tags to specify the variables corresponding to argument names (e.g., StandardArgumentDefinitions#INTERVALS_LONG_NAME instead of intervals , USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME instead of use-allele-specific-annotations, etc.), and while they show up correctly when rendering the Javadoc within IntelliJ, it seems the same is not true on the GATK website. Is there an easy fix in the code for generating these docs, or should I just avoid using this tag?. My original response:. > I tested this using the new Java 17 doclets in the hope that it would just work, but the result is the same. However, the new Java language model classes make it easy to interpolate these, so I’ll fix this in the barclay Java 17 branch. However, in looking more closely, it's not as easy to fix as I first thought, and the problem is a little deeper than I first realized. Although it's easy to detect these using the new Java 17 apis, it's more difficult to retrieve the actual values. And even then, because the gatkdoc process only consumes a subset of the classes consumed by the javadoc process (it only sees `@DocumentedFeature`s), it's quite easy to reference something in the javadoc comment that can be resolved by javdoc, but not by gatkdoc. But it appears that even the javadoc process isn't rendering these tags correctly. Here is the raw javadoc comment:; ```; * Input VCF file. Site-level annotations will be extracted from the contained variants (or alleles,; * if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146
https://github.com/broadinstitute/gatk/issues/8146:1159,Safety,detect,detect,1159,"tute.org/hc/en-us/articles/9570326304155-ScoreVariantAnnotations-BETA-. I used these tags to specify the variables corresponding to argument names (e.g., StandardArgumentDefinitions#INTERVALS_LONG_NAME instead of intervals , USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME instead of use-allele-specific-annotations, etc.), and while they show up correctly when rendering the Javadoc within IntelliJ, it seems the same is not true on the GATK website. Is there an easy fix in the code for generating these docs, or should I just avoid using this tag?. My original response:. > I tested this using the new Java 17 doclets in the hope that it would just work, but the result is the same. However, the new Java language model classes make it easy to interpolate these, so I’ll fix this in the barclay Java 17 branch. However, in looking more closely, it's not as easy to fix as I first thought, and the problem is a little deeper than I first realized. Although it's easy to detect these using the new Java 17 apis, it's more difficult to retrieve the actual values. And even then, because the gatkdoc process only consumes a subset of the classes consumed by the javadoc process (it only sees `@DocumentedFeature`s), it's quite easy to reference something in the javadoc comment that can be resolved by javdoc, but not by gatkdoc. But it appears that even the javadoc process isn't rendering these tags correctly. Here is the raw javadoc comment:; ```; * Input VCF file. Site-level annotations will be extracted from the contained variants (or alleles,; * if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://user-images.githubusercontent.com/10062863/210841121-15a4d357-dbfa-47e2-808b-08cdeb6d42be.png"">. The rendering in gatkdoc (the variable name appears in the text, but it should be interpolated)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146
https://github.com/broadinstitute/gatk/issues/8146:766,Testability,test,tested,766,"Original report by @samuelklee (see https://github.com/broadinstitute/barclay/issues/189):; > I noticed that Javadoc @value tags are not being rendered correctly in e.g. https://gatk.broadinstitute.org/hc/en-us/articles/9570326304155-ScoreVariantAnnotations-BETA-. I used these tags to specify the variables corresponding to argument names (e.g., StandardArgumentDefinitions#INTERVALS_LONG_NAME instead of intervals , USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME instead of use-allele-specific-annotations, etc.), and while they show up correctly when rendering the Javadoc within IntelliJ, it seems the same is not true on the GATK website. Is there an easy fix in the code for generating these docs, or should I just avoid using this tag?. My original response:. > I tested this using the new Java 17 doclets in the hope that it would just work, but the result is the same. However, the new Java language model classes make it easy to interpolate these, so I’ll fix this in the barclay Java 17 branch. However, in looking more closely, it's not as easy to fix as I first thought, and the problem is a little deeper than I first realized. Although it's easy to detect these using the new Java 17 apis, it's more difficult to retrieve the actual values. And even then, because the gatkdoc process only consumes a subset of the classes consumed by the javadoc process (it only sees `@DocumentedFeature`s), it's quite easy to reference something in the javadoc comment that can be resolved by javdoc, but not by gatkdoc. But it appears that even the javadoc process isn't rendering these tags correctly. Here is the raw javadoc comment:; ```; * Input VCF file. Site-level annotations will be extracted from the contained variants (or alleles,; * if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146
https://github.com/broadinstitute/gatk/issues/8147:267,Availability,error,error,267,"## Bug Report. ### Affected tool(s) or class(es); TransferReadTags. ### Affected version(s); - [X] Latest public release version [gatk-4.3.0.0]. ### Description ; When traversing the reads in both aligned (target) and unaligned (the one with the desired tag) BAMs an error is thrown complaining about a read `found in the aligned bam is not found in the unmapped bam`. However the reads exists. It looks like the `traverse` function that uses the lexicographic order difference between both query names will find a _negative_ `diff` and assume that the read in the aligned BAM is missing in the uBAM. However, with Illumina read headers it seems almost guaranteed that this is going to be an issue since the y-coord (the last colon-separated field in the header) often has numbers with different number of digits. The lexicographical comparison will fail to adjust when comparing two read names where the length of the read in the target BAM is larger than the length of the read in the uBAM. . This is the `traverse` function that throws the error:; https://github.com/broadinstitute/gatk/blob/2b0a558fdb9fdf654e796d5d69a092e26345583b/src/main/java/org/broadinstitute/hellbender/tools/walkers/qc/TransferReadTags.java#L109-L145 . #### Steps to reproduce; Run `TransferReadTags` with an Illumina sequenced aligned BAM. I can provide dummy files if needed, but should be easy to reproduce. The following example should help illustrate the issue:. ```sh; $ /data/reddylab/software/gatk/gatk-4.3.0.0/gatk TransferReadTags \; --output /data/reddylab/Alex/tmp/TEST_BAM.with_umis.bam \; --read-tags RX \; --unmapped-sam /data/reddylab/Alex/tmp/TEST_BAM.umi.nsorted.ubam \; --input /data/reddylab/Alex/tmp/TEST_BAM.nsorted.bam; ```. Produces the following output:; ```; Using GATK jar /gpfs/fs1/data/reddylab/software/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8147
https://github.com/broadinstitute/gatk/issues/8147:1043,Availability,error,error,1043,"ol(s) or class(es); TransferReadTags. ### Affected version(s); - [X] Latest public release version [gatk-4.3.0.0]. ### Description ; When traversing the reads in both aligned (target) and unaligned (the one with the desired tag) BAMs an error is thrown complaining about a read `found in the aligned bam is not found in the unmapped bam`. However the reads exists. It looks like the `traverse` function that uses the lexicographic order difference between both query names will find a _negative_ `diff` and assume that the read in the aligned BAM is missing in the uBAM. However, with Illumina read headers it seems almost guaranteed that this is going to be an issue since the y-coord (the last colon-separated field in the header) often has numbers with different number of digits. The lexicographical comparison will fail to adjust when comparing two read names where the length of the read in the target BAM is larger than the length of the read in the uBAM. . This is the `traverse` function that throws the error:; https://github.com/broadinstitute/gatk/blob/2b0a558fdb9fdf654e796d5d69a092e26345583b/src/main/java/org/broadinstitute/hellbender/tools/walkers/qc/TransferReadTags.java#L109-L145 . #### Steps to reproduce; Run `TransferReadTags` with an Illumina sequenced aligned BAM. I can provide dummy files if needed, but should be easy to reproduce. The following example should help illustrate the issue:. ```sh; $ /data/reddylab/software/gatk/gatk-4.3.0.0/gatk TransferReadTags \; --output /data/reddylab/Alex/tmp/TEST_BAM.with_umis.bam \; --read-tags RX \; --unmapped-sam /data/reddylab/Alex/tmp/TEST_BAM.umi.nsorted.ubam \; --input /data/reddylab/Alex/tmp/TEST_BAM.nsorted.bam; ```. Produces the following output:; ```; Using GATK jar /gpfs/fs1/data/reddylab/software/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8147
https://github.com/broadinstitute/gatk/issues/8147:4877,Availability,down,down,4877,":16.216 INFO TransferReadTags - Deflater: IntelDeflater; 13:08:16.216 INFO TransferReadTags - Inflater: IntelInflater; 13:08:16.216 INFO TransferReadTags - GCS max retries/reopens: 20; 13:08:16.216 INFO TransferReadTags - Requester pays: disabled; 13:08:16.216 WARN TransferReadTags -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: TransferReadTags is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:08:16.217 INFO TransferReadTags - Initializing engine; 13:08:16.658 INFO TransferReadTags - Done initializing engine; 13:08:16.710 WARN ReadUtils - Skipping index file creation for: /data/reddylab/Alex/tmp/TEST_BAM.with_umis.bam. Index file creation requires reads in coordinate sorted order.; 13:08:16.739 INFO ProgressMeter - Starting traversal; 13:08:16.739 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:08:16.741 INFO TransferReadTags - Shutting down engine; [January 5, 2023 1:08:16 PM EST] org.broadinstitute.hellbender.tools.walkers.qc.TransferReadTags done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2612002816; java.lang.IllegalStateException: A read found in the aligned bam is not found in the unmapped bam. This tool assumes reads in both input files are query-name sorted lexicographically (i.e. by Picard SortSam but not by samtools sort): aligned read = A00257:933:HG2MVDSX5:1:1101:1045:11130, unmapped read = A00257:933:HG2MVDSX5:1:1101:1045:8531; at org.broadinstitute.hellbender.tools.walkers.qc.TransferReadTags.traverse(TransferReadTags.java:142); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8147
https://github.com/broadinstitute/gatk/issues/8147:7960,Availability,error,error,7960,"933:HG2MVDSX5:1:1101:1045:5306 A00257:933:HG2MVDSX5:1:1101:1045:5306; A00257:933:HG2MVDSX5:1:1101:1045:5306 A00257:933:HG2MVDSX5:1:1101:1045:6277; A00257:933:HG2MVDSX5:1:1101:1045:6277 A00257:933:HG2MVDSX5:1:1101:1045:7717; A00257:933:HG2MVDSX5:1:1101:1045:6277 A00257:933:HG2MVDSX5:1:1101:1045:8375; A00257:933:HG2MVDSX5:1:1101:1045:7717 A00257:933:HG2MVDSX5:1:1101:1045:8531; A00257:933:HG2MVDSX5:1:1101:1045:7717 A00257:933:HG2MVDSX5:1:1101:1045:9283; A00257:933:HG2MVDSX5:1:1101:1045:8531 A00257:933:HG2MVDSX5:1:1101:1045:10316; A00257:933:HG2MVDSX5:1:1101:1045:8531 A00257:933:HG2MVDSX5:1:1101:1045:11130; A00257:933:HG2MVDSX5:1:1101:1045:11130 A00257:933:HG2MVDSX5:1:1101:1045:11882; A00257:933:HG2MVDSX5:1:1101:1045:11130 A00257:933:HG2MVDSX5:1:1101:1045:12007; A00257:933:HG2MVDSX5:1:1101:1045:12007 A00257:933:HG2MVDSX5:1:1101:1045:12665; A00257:933:HG2MVDSX5:1:1101:1045:12007 A00257:933:HG2MVDSX5:1:1101:1045:12727; A00257:933:HG2MVDSX5:1:1101:1045:12665 A00257:933:HG2MVDSX5:1:1101:1045:13260; A00257:933:HG2MVDSX5:1:1101:1045:12665 A00257:933:HG2MVDSX5:1:1101:1045:13322; ```. The error above makes sense since the lexicographic difference between `A00257:933:HG2MVDSX5:1:1101:1045:11130` and `A00257:933:HG2MVDSX5:1:1101:1045:8531` is `-7`, the result of `ord(""1"") - ord(""8"")`. #### Expected behavior; The `traverse` function above should keep advancing through the uBAM. Assuming the BAMs to be queryname sorted, the `traverse` function could check the lengths of the names when the `diff` is not zero in https://github.com/broadinstitute/gatk/blob/2b0a558fdb9fdf654e796d5d69a092e26345583b/src/main/java/org/broadinstitute/hellbender/tools/walkers/qc/TransferReadTags.java#L121, advancing the `unmappedSamIterator` if the name of the `currentTargetRead` is larger than the name of `currentUnmappedRead`. . #### Actual behavior; An `java.lang.IllegalStateException: A read found in the aligned bam is not found in the unmapped bam.` exception is raised when in fact the read might exist",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8147
https://github.com/broadinstitute/gatk/issues/8147:113,Deployability,release,release,113,"## Bug Report. ### Affected tool(s) or class(es); TransferReadTags. ### Affected version(s); - [X] Latest public release version [gatk-4.3.0.0]. ### Description ; When traversing the reads in both aligned (target) and unaligned (the one with the desired tag) BAMs an error is thrown complaining about a read `found in the aligned bam is not found in the unmapped bam`. However the reads exists. It looks like the `traverse` function that uses the lexicographic order difference between both query names will find a _negative_ `diff` and assume that the read in the aligned BAM is missing in the uBAM. However, with Illumina read headers it seems almost guaranteed that this is going to be an issue since the y-coord (the last colon-separated field in the header) often has numbers with different number of digits. The lexicographical comparison will fail to adjust when comparing two read names where the length of the read in the target BAM is larger than the length of the read in the uBAM. . This is the `traverse` function that throws the error:; https://github.com/broadinstitute/gatk/blob/2b0a558fdb9fdf654e796d5d69a092e26345583b/src/main/java/org/broadinstitute/hellbender/tools/walkers/qc/TransferReadTags.java#L109-L145 . #### Steps to reproduce; Run `TransferReadTags` with an Illumina sequenced aligned BAM. I can provide dummy files if needed, but should be easy to reproduce. The following example should help illustrate the issue:. ```sh; $ /data/reddylab/software/gatk/gatk-4.3.0.0/gatk TransferReadTags \; --output /data/reddylab/Alex/tmp/TEST_BAM.with_umis.bam \; --read-tags RX \; --unmapped-sam /data/reddylab/Alex/tmp/TEST_BAM.umi.nsorted.ubam \; --input /data/reddylab/Alex/tmp/TEST_BAM.nsorted.bam; ```. Produces the following output:; ```; Using GATK jar /gpfs/fs1/data/reddylab/software/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8147
https://github.com/broadinstitute/gatk/issues/8147:2365,Performance,Load,Loading,2365,"uld be easy to reproduce. The following example should help illustrate the issue:. ```sh; $ /data/reddylab/software/gatk/gatk-4.3.0.0/gatk TransferReadTags \; --output /data/reddylab/Alex/tmp/TEST_BAM.with_umis.bam \; --read-tags RX \; --unmapped-sam /data/reddylab/Alex/tmp/TEST_BAM.umi.nsorted.ubam \; --input /data/reddylab/Alex/tmp/TEST_BAM.nsorted.bam; ```. Produces the following output:; ```; Using GATK jar /gpfs/fs1/data/reddylab/software/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs1/data/reddylab/software/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar TransferReadTags --output /data/reddylab/Alex/tmp/TEST_BAM.with_umis.bam --read-tags RX --unmapped-sam /data/reddylab/Alex/tmp/TEST_BAM.umi.nsorted.ubam --input /data/reddylab/Alex/tmp/TEST_BAM.nsorted.bam; 13:08:15.961 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/fs1/data/reddylab/software/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:08:16.213 INFO TransferReadTags - ------------------------------------------------------------; 13:08:16.213 INFO TransferReadTags - The Genome Analysis Toolkit (GATK) v4.3.0.0; 13:08:16.213 INFO TransferReadTags - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:08:16.214 INFO TransferReadTags - Executing as aeb84@x1-01-2.genome.duke.edu on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 13:08:16.214 INFO TransferReadTags - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_92-b15; 13:08:16.214 INFO TransferReadTags - Start Date/Time: January 5, 2023 1:08:15 PM EST; 13:08:16.214 INFO TransferReadTags - ------------------------------------------------------------; 13:08:16.214 INFO TransferReadTags - ------------------------------------------------------------; 13:08:16.215 INFO TransferRead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8147
https://github.com/broadinstitute/gatk/issues/8149:5331,Availability,Avail,Available,5331,":56:44.709 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:56:44.709 INFO HaplotypeCaller - Requester pays: disabled; 03:56:44.709 INFO HaplotypeCaller - Initializing engine; 03:56:45.204 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:56:45.276 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:56:45.305 INFO HaplotypeCaller - Done initializing engine; 03:56:45.324 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:56:45.349 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:56:45.351 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:56:45.373 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:56:45.375 INFO IntelPairHmm - Available threads: 8; 03:56:45.375 INFO IntelPairHmm - Requested threads: 4; 03:56:45.375 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:56:45.415 INFO ProgressMeter - Starting traversal; 03:56:45.416 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:56:46.180 WARN VariantAnnotatorEngine - Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given.; 03:56:46.210 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 13:32911888 and possibly subsequent; at least 10 samples must have called genotypes; 03:56:46.621 INFO HaplotypeCaller - 1 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:7010,Availability,down,down,7010,"er - 1 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 58 total reads filtered; 03:56:46.621 INFO ProgressMeter - 13:115070262 0.0 4029 200614.1; 03:56:46.621 INFO ProgressMeter - Traversal complete. Processed 4029 total regions in 0.0 minutes.; 03:56:46.646 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0011332; 03:56:46.646 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0031919; 03:56:46.647 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.01 sec; 03:56:46.647 INFO HaplotypeCaller - Shutting down engine; [January 6, 2023 3:56:46 AM GMT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=395837440; ```. Since OrientationBiasReadCounts replaced OxoGReadCounts in GATK 4.1.1.0, we tested this version as well. It delievered the expected results, with variants reporting F1R2/F2R1:; ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT sample; 13 32911888 . A G 177.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.086;DP=21;ExcessHet=3.0103;FS=1.719;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=8.46;ReadPosRankSum=0.475;SOR=0.368 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:13,8:21:6,6:7,2:99:185,0,339; 13 32913055 . A G 402.03 . AC=2;AF=1.00;AN=2;DP=15;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=26.80;SOR=1.112 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,15:15:0,12:0,2:45:416,45,0; 13 32915005 . G C 378.02 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:11643,Availability,Avail,Available,11643,":58:33.933 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:58:33.933 INFO HaplotypeCaller - Requester pays: disabled; 03:58:33.934 INFO HaplotypeCaller - Initializing engine; 03:58:34.384 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:58:34.461 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:58:34.491 INFO HaplotypeCaller - Done initializing engine; 03:58:34.509 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:58:34.532 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:58:34.536 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:58:34.580 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:58:34.582 INFO IntelPairHmm - Available threads: 8; 03:58:34.582 INFO IntelPairHmm - Requested threads: 4; 03:58:34.582 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:58:34.623 INFO ProgressMeter - Starting traversal; 03:58:34.623 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:58:35.812 INFO HaplotypeCaller - 58 read(s) filtered by: ((((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter) AND WellformedReadFilter); 58 read(s) filtered by: (((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter); 58 read(s) filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:14263,Availability,down,down,14263,"y: (((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter); 58 read(s) filtered by: ((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter); 1 read(s) filtered by: (((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter); 1 read(s) filtered by: ((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter); 1 read(s) filtered by: (MappingQualityReadFilter AND MappingQualityAvailableReadFilter); 1 read(s) filtered by: MappingQualityReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter . 03:58:35.812 INFO ProgressMeter - 13:115070262 0.0 4029 203313.7; 03:58:35.812 INFO ProgressMeter - Traversal complete. Processed 4029 total regions in 0.0 minutes.; 03:58:35.839 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 8.397000000000001E-4; 03:58:35.839 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0028144000000000003; 03:58:35.839 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 03:58:35.840 INFO HaplotypeCaller - Shutting down engine; ```. #### Steps to reproduce; Command used:; ```; gatk HaplotypeCaller \; --input sample.bam \; --annotation OrientationBiasReadCounts \; --intervals b37.chr13.bed \; --reference hs37d5.fa \; --output sample.vcf.gz; ```. The processings were executed locally with Docker images `broadinstitute/gatk:4.1.1.0`, `broadinstitute/gatk:4.2.2.0` and `broadinstitute/gatk:4.3.0.0`. Other versions apart from these were not tested. #### Expected behavior; F1R2 and F2R1 computed and specified for each variant in recent versions of GATK. #### Actual behavior; F1R2 and F2R1 are described in the header, but they are not calculated in recent versions. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:153,Deployability,release,release,153,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller `--annotation OrientationBiasReadCounts`. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]; - [ ] 4.2.2.0. ### Description; When specifying OrientationBiasReadCounts, HaplotypeCaller adds the description of F1R2 and F2R1 to the header, but does not calculate them. This was observed in GATK 4.2.2.0 and also in a test with 4.3.0.0 (the latest at the moment of this issue).; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=F1R2,Number=R,Type=Integer,Description=""Count of reads in F1R2 pair orientation supporting each allele"">; ##FORMAT=<ID=F2R1,Number=R,Type=Integer,Description=""Count of reads in F2R1 pair orientation supporting each allele"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT sample; 13 32911888 . A G 177.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.086;DP=21;ExcessHet=3.0103;FS=1.719;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=8.46;ReadPosRankSum=0.475;SOR=0.368 GT:AD:DP:GQ:PL 0/1:13,8:21:99:185,0,339; 13 32913055 . A G 402.06 . AC=2;AF=1.00;AN=2;DP=15;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=26.80;SOR=1.112 GT:AD:DP:GQ:PL 1/1:0,15:15:45:416,45,0; 13 32915005 . G C 378.06 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:GQ:PL 1/1:0,13:13:39:392,39,0; 13 32929232 . A G 168.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=16;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:GQ:PL 0/1:5,6:11:99:176,0,121; 13 32929387 . T C 209.02 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:2567,Performance,Load,Loading,2567,"6 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:GQ:PL 1/1:0,13:13:39:392,39,0; 13 32929232 . A G 168.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=16;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:GQ:PL 0/1:5,6:11:99:176,0,121; 13 32929387 . T C 209.02 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.86;SOR=1.609 GT:AD:DP:GQ:PL 1/1:0,7:7:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:56:44.380 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:56:44 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:56:44.703 INFO HaplotypeCaller - ------------------------------------------------------------; 03:56:44.704 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.2.0; 03:56:44.704 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:56:44.705 INFO HaplotypeCaller - Executing as root@d2b0ea7e4079 on Linux v5.10.76-linuxkit amd64; 03:56:44.705 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 03:56:44.705 INFO HaplotypeCaller - Start Date/Time: January 6, 2023 3:56:44 AM GMT; 03:56:44.705 INFO HaplotypeCaller - ------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:4932,Performance,Load,Loading,4932,"L : 2; 03:56:44.708 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 03:56:44.708 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 03:56:44.708 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:56:44.708 INFO HaplotypeCaller - Deflater: IntelDeflater; 03:56:44.708 INFO HaplotypeCaller - Inflater: IntelInflater; 03:56:44.709 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:56:44.709 INFO HaplotypeCaller - Requester pays: disabled; 03:56:44.709 INFO HaplotypeCaller - Initializing engine; 03:56:45.204 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:56:45.276 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:56:45.305 INFO HaplotypeCaller - Done initializing engine; 03:56:45.324 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:56:45.349 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:56:45.351 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:56:45.373 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:56:45.375 INFO IntelPairHmm - Available threads: 8; 03:56:45.375 INFO IntelPairHmm - Requested threads: 4; 03:56:45.375 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:56:45.415 INFO ProgressMeter - Starting traversal; 03:56:45.416 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:56:46.180 WARN VariantAnnotatorEngine - Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given.; 03:56:46.210 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 13:32911888 and possibly s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:5086,Performance,Load,Loading,5086,"YNC_IO_WRITE_FOR_SAMTOOLS : true; 03:56:44.708 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:56:44.708 INFO HaplotypeCaller - Deflater: IntelDeflater; 03:56:44.708 INFO HaplotypeCaller - Inflater: IntelInflater; 03:56:44.709 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:56:44.709 INFO HaplotypeCaller - Requester pays: disabled; 03:56:44.709 INFO HaplotypeCaller - Initializing engine; 03:56:45.204 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:56:45.276 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:56:45.305 INFO HaplotypeCaller - Done initializing engine; 03:56:45.324 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:56:45.349 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:56:45.351 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:56:45.373 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:56:45.375 INFO IntelPairHmm - Available threads: 8; 03:56:45.375 INFO IntelPairHmm - Requested threads: 4; 03:56:45.375 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:56:45.415 INFO ProgressMeter - Starting traversal; 03:56:45.416 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:56:46.180 WARN VariantAnnotatorEngine - Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given.; 03:56:46.210 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 13:32911888 and possibly subsequent; at least 10 samples must have called genotypes; 03:56:46.621 INFO HaplotypeCaller - 1 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:5453,Performance,multi-thread,multi-threaded,5453,"9 INFO HaplotypeCaller - Initializing engine; 03:56:45.204 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:56:45.276 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:56:45.305 INFO HaplotypeCaller - Done initializing engine; 03:56:45.324 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:56:45.349 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:56:45.351 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:56:45.373 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:56:45.375 INFO IntelPairHmm - Available threads: 8; 03:56:45.375 INFO IntelPairHmm - Requested threads: 4; 03:56:45.375 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:56:45.415 INFO ProgressMeter - Starting traversal; 03:56:45.416 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:56:46.180 WARN VariantAnnotatorEngine - Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given.; 03:56:46.210 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 13:32911888 and possibly subsequent; at least 10 samples must have called genotypes; 03:56:46.621 INFO HaplotypeCaller - 1 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:8945,Performance,Load,Loading,8945,"LEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,13:13:0,4:0,9:39:392,39,0; 13 32929232 . A G 168.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=11;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:5,6:11:3,3:2,3:99:176,0,121; 13 32929387 . T C 208.98 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.85;SOR=1.609 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,7:7:0,2:0,5:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:58:32.017 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:58:33 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:58:33.929 INFO HaplotypeCaller - ------------------------------------------------------------; 03:58:33.930 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.1.0; 03:58:33.930 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:58:33.930 INFO HaplotypeCaller - Executing as root@91e458b8c2fc on Linux v5.10.76-linuxkit amd64; 03:58:33.930 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 03:58:33.931 INFO HaplotypeCaller - Start Date/Time: January 6, 2023 3:58:31 AM UTC; 03:58:33.931 INFO HaplotypeCaller - ----------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:11244,Performance,Load,Loading,11244,"L : 2; 03:58:33.932 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 03:58:33.932 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 03:58:33.932 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:58:33.933 INFO HaplotypeCaller - Deflater: IntelDeflater; 03:58:33.933 INFO HaplotypeCaller - Inflater: IntelInflater; 03:58:33.933 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:58:33.933 INFO HaplotypeCaller - Requester pays: disabled; 03:58:33.934 INFO HaplotypeCaller - Initializing engine; 03:58:34.384 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:58:34.461 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:58:34.491 INFO HaplotypeCaller - Done initializing engine; 03:58:34.509 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:58:34.532 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:58:34.536 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:58:34.580 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:58:34.582 INFO IntelPairHmm - Available threads: 8; 03:58:34.582 INFO IntelPairHmm - Requested threads: 4; 03:58:34.582 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:58:34.623 INFO ProgressMeter - Starting traversal; 03:58:34.623 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:58:35.812 INFO HaplotypeCaller - 58 read(s) filtered by: ((((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:11398,Performance,Load,Loading,11398,"YNC_IO_WRITE_FOR_SAMTOOLS : true; 03:58:33.932 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:58:33.933 INFO HaplotypeCaller - Deflater: IntelDeflater; 03:58:33.933 INFO HaplotypeCaller - Inflater: IntelInflater; 03:58:33.933 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:58:33.933 INFO HaplotypeCaller - Requester pays: disabled; 03:58:33.934 INFO HaplotypeCaller - Initializing engine; 03:58:34.384 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:58:34.461 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:58:34.491 INFO HaplotypeCaller - Done initializing engine; 03:58:34.509 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:58:34.532 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:58:34.536 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:58:34.580 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:58:34.582 INFO IntelPairHmm - Available threads: 8; 03:58:34.582 INFO IntelPairHmm - Requested threads: 4; 03:58:34.582 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:58:34.623 INFO ProgressMeter - Starting traversal; 03:58:34.623 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:58:35.812 INFO HaplotypeCaller - 58 read(s) filtered by: ((((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter) AND WellformedReadFilter); 58 read(s) filtered by: (((((((MappingQualityReadFilter AND",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:11765,Performance,multi-thread,multi-threaded,11765,"4 INFO HaplotypeCaller - Initializing engine; 03:58:34.384 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:58:34.461 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:58:34.491 INFO HaplotypeCaller - Done initializing engine; 03:58:34.509 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:58:34.532 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:58:34.536 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:58:34.580 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:58:34.582 INFO IntelPairHmm - Available threads: 8; 03:58:34.582 INFO IntelPairHmm - Requested threads: 4; 03:58:34.582 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:58:34.623 INFO ProgressMeter - Starting traversal; 03:58:34.623 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:58:35.812 INFO HaplotypeCaller - 58 read(s) filtered by: ((((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter) AND WellformedReadFilter); 58 read(s) filtered by: (((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter); 58 read(s) filtered by: ((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:2822,Safety,detect,detect,2822,"0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:GQ:PL 0/1:5,6:11:99:176,0,121; 13 32929387 . T C 209.02 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.86;SOR=1.609 GT:AD:DP:GQ:PL 1/1:0,7:7:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:56:44.380 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:56:44 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:56:44.703 INFO HaplotypeCaller - ------------------------------------------------------------; 03:56:44.704 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.2.0; 03:56:44.704 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:56:44.705 INFO HaplotypeCaller - Executing as root@d2b0ea7e4079 on Linux v5.10.76-linuxkit amd64; 03:56:44.705 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 03:56:44.705 INFO HaplotypeCaller - Start Date/Time: January 6, 2023 3:56:44 AM GMT; 03:56:44.705 INFO HaplotypeCaller - ------------------------------------------------------------; 03:56:44.706 INFO HaplotypeCaller - ------------------------------------------------------------; 03:56:44.707 INFO HaplotypeCaller - HTSJDK Version: 2.24.1; 03:56:44.707 INFO HaplotypeCaller - Picard Version: 2.25.4; 03:56:44.707 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:9200,Safety,detect,detect,9200,"33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:5,6:11:3,3:2,3:99:176,0,121; 13 32929387 . T C 208.98 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.85;SOR=1.609 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,7:7:0,2:0,5:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:58:32.017 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:58:33 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:58:33.929 INFO HaplotypeCaller - ------------------------------------------------------------; 03:58:33.930 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.1.0; 03:58:33.930 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:58:33.930 INFO HaplotypeCaller - Executing as root@91e458b8c2fc on Linux v5.10.76-linuxkit amd64; 03:58:33.930 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 03:58:33.931 INFO HaplotypeCaller - Start Date/Time: January 6, 2023 3:58:31 AM UTC; 03:58:33.931 INFO HaplotypeCaller - ------------------------------------------------------------; 03:58:33.931 INFO HaplotypeCaller - ------------------------------------------------------------; 03:58:33.932 INFO HaplotypeCaller - HTSJDK Version: 2.19.0; 03:58:33.932 INFO HaplotypeCaller - Picard Version: 2.19.0; 03:58:33.932 INFO Haplot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:401,Testability,test,test,401,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller `--annotation OrientationBiasReadCounts`. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]; - [ ] 4.2.2.0. ### Description; When specifying OrientationBiasReadCounts, HaplotypeCaller adds the description of F1R2 and F2R1 to the header, but does not calculate them. This was observed in GATK 4.2.2.0 and also in a test with 4.3.0.0 (the latest at the moment of this issue).; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=F1R2,Number=R,Type=Integer,Description=""Count of reads in F1R2 pair orientation supporting each allele"">; ##FORMAT=<ID=F2R1,Number=R,Type=Integer,Description=""Count of reads in F2R1 pair orientation supporting each allele"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT sample; 13 32911888 . A G 177.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.086;DP=21;ExcessHet=3.0103;FS=1.719;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=8.46;ReadPosRankSum=0.475;SOR=0.368 GT:AD:DP:GQ:PL 0/1:13,8:21:99:185,0,339; 13 32913055 . A G 402.06 . AC=2;AF=1.00;AN=2;DP=15;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=26.80;SOR=1.112 GT:AD:DP:GQ:PL 1/1:0,15:15:45:416,45,0; 13 32915005 . G C 378.06 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:GQ:PL 1/1:0,13:13:39:392,39,0; 13 32929232 . A G 168.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=16;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:GQ:PL 0/1:5,6:11:99:176,0,121; 13 32929387 . T C 209.02 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:2105,Testability,log,log,2105," QUAL FILTER INFO FORMAT sample; 13 32911888 . A G 177.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.086;DP=21;ExcessHet=3.0103;FS=1.719;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=8.46;ReadPosRankSum=0.475;SOR=0.368 GT:AD:DP:GQ:PL 0/1:13,8:21:99:185,0,339; 13 32913055 . A G 402.06 . AC=2;AF=1.00;AN=2;DP=15;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=26.80;SOR=1.112 GT:AD:DP:GQ:PL 1/1:0,15:15:45:416,45,0; 13 32915005 . G C 378.06 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:GQ:PL 1/1:0,13:13:39:392,39,0; 13 32929232 . A G 168.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=16;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:GQ:PL 0/1:5,6:11:99:176,0,121; 13 32929387 . T C 209.02 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.86;SOR=1.609 GT:AD:DP:GQ:PL 1/1:0,7:7:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:56:44.380 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:56:44 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:56:44.703 INFO HaplotypeCaller - ------------------------------------------------------------; 03:56:44.704 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.2.0; 03:56:44.704 INFO HaplotypeCaller - For support and documentation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:7281,Testability,test,tested,7281,"tered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 58 total reads filtered; 03:56:46.621 INFO ProgressMeter - 13:115070262 0.0 4029 200614.1; 03:56:46.621 INFO ProgressMeter - Traversal complete. Processed 4029 total regions in 0.0 minutes.; 03:56:46.646 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0011332; 03:56:46.646 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0031919; 03:56:46.647 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.01 sec; 03:56:46.647 INFO HaplotypeCaller - Shutting down engine; [January 6, 2023 3:56:46 AM GMT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=395837440; ```. Since OrientationBiasReadCounts replaced OxoGReadCounts in GATK 4.1.1.0, we tested this version as well. It delievered the expected results, with variants reporting F1R2/F2R1:; ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT sample; 13 32911888 . A G 177.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.086;DP=21;ExcessHet=3.0103;FS=1.719;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=8.46;ReadPosRankSum=0.475;SOR=0.368 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:13,8:21:6,6:7,2:99:185,0,339; 13 32913055 . A G 402.03 . AC=2;AF=1.00;AN=2;DP=15;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=26.80;SOR=1.112 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,15:15:0,12:0,2:45:416,45,0; 13 32915005 . G C 378.02 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,13:13:0,4:0,9:39:392,39,0; 13 32929232 . A G 168.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=11;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:5,6:11:3,3:2,3:99:176,0,121; 13 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:8483,Testability,log,log,8483,".086;DP=21;ExcessHet=3.0103;FS=1.719;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=8.46;ReadPosRankSum=0.475;SOR=0.368 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:13,8:21:6,6:7,2:99:185,0,339; 13 32913055 . A G 402.03 . AC=2;AF=1.00;AN=2;DP=15;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=26.80;SOR=1.112 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,15:15:0,12:0,2:45:416,45,0; 13 32915005 . G C 378.02 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,13:13:0,4:0,9:39:392,39,0; 13 32929232 . A G 168.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=11;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:5,6:11:3,3:2,3:99:176,0,121; 13 32929387 . T C 208.98 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.85;SOR=1.609 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,7:7:0,2:0,5:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:58:32.017 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:58:33 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:58:33.929 INFO HaplotypeCaller - ------------------------------------------------------------; 03:58:33.930 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.1.0; 03:58:33.930 INFO HaplotypeCaller - For support and documentation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/issues/8149:14691,Testability,test,tested,14691,"y: (((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter); 58 read(s) filtered by: ((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter); 1 read(s) filtered by: (((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter); 1 read(s) filtered by: ((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter); 1 read(s) filtered by: (MappingQualityReadFilter AND MappingQualityAvailableReadFilter); 1 read(s) filtered by: MappingQualityReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter . 03:58:35.812 INFO ProgressMeter - 13:115070262 0.0 4029 203313.7; 03:58:35.812 INFO ProgressMeter - Traversal complete. Processed 4029 total regions in 0.0 minutes.; 03:58:35.839 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 8.397000000000001E-4; 03:58:35.839 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0028144000000000003; 03:58:35.839 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 03:58:35.840 INFO HaplotypeCaller - Shutting down engine; ```. #### Steps to reproduce; Command used:; ```; gatk HaplotypeCaller \; --input sample.bam \; --annotation OrientationBiasReadCounts \; --intervals b37.chr13.bed \; --reference hs37d5.fa \; --output sample.vcf.gz; ```. The processings were executed locally with Docker images `broadinstitute/gatk:4.1.1.0`, `broadinstitute/gatk:4.2.2.0` and `broadinstitute/gatk:4.3.0.0`. Other versions apart from these were not tested. #### Expected behavior; F1R2 and F2R1 computed and specified for each variant in recent versions of GATK. #### Actual behavior; F1R2 and F2R1 are described in the header, but they are not calculated in recent versions. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149
https://github.com/broadinstitute/gatk/pull/8151:51,Deployability,update,updates,51,This PR increases the memory for IndexVCF; It also updates SelectVariants to use symlinks so that the VCF and its index file can be in different paths.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8151
https://github.com/broadinstitute/gatk/issues/8152:745,Performance,perform,performance,745,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); I test this problem in two versions, V4.1.4.1 and V4.3.0.0.They all have this problem. ### Description ; Following the recommendations of the 'Best Practice Workflows', I run mutect2 in the following command. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${all_chrome_bed}; --bam-output ${bam_output} ; -O ${vcf_output}. To improve parallelism, I try to split my all chrome bed to 25 files.Parallel running the flowing command brings me signficient performance improvement. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${chr1_bed}; --bam-output ${chr1_bam_output} ; -O ${chr1_vcf_output}. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${chr2_bed}; --bam-output ${chr2_bam_output} ; -O ${chr2_vcf_output}. But when I examined the vcf results produced by both modes of operation, I found consistency issues. #### Expected behavior; Let's focus on chromosome 2.I expect 100% consistency between the following two runs.; 1. The vcf file is obtained using a bed file containing only chromosome 2.; 2. Use bed file with all chromosomes to get all calling results, then filter to get chromesome 2 calling result. #### Actual behavior; 1. The first method above gives one more result than the second.; 2. There are 168 vcf results inconsistent, out of 1247 total.One of the inconsistencie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152
https://github.com/broadinstitute/gatk/issues/8152:86,Testability,test,test,86,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); I test this problem in two versions, V4.1.4.1 and V4.3.0.0.They all have this problem. ### Description ; Following the recommendations of the 'Best Practice Workflows', I run mutect2 in the following command. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${all_chrome_bed}; --bam-output ${bam_output} ; -O ${vcf_output}. To improve parallelism, I try to split my all chrome bed to 25 files.Parallel running the flowing command brings me signficient performance improvement. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${chr1_bed}; --bam-output ${chr1_bam_output} ; -O ${chr1_vcf_output}. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${chr2_bed}; --bam-output ${chr2_bam_output} ; -O ${chr2_vcf_output}. But when I examined the vcf results produced by both modes of operation, I found consistency issues. #### Expected behavior; Let's focus on chromosome 2.I expect 100% consistency between the following two runs.; 1. The vcf file is obtained using a bed file containing only chromosome 2.; 2. Use bed file with all chromosomes to get all calling results, then filter to get chromesome 2 calling result. #### Actual behavior; 1. The first method above gives one more result than the second.; 2. There are 168 vcf results inconsistent, out of 1247 total.One of the inconsistencie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152
https://github.com/broadinstitute/gatk/issues/8154:440,Availability,mainten,maintenance,440,"## Bug Report. Not a bug, but a question. See **Description section** below. It could also be related to things not being labelled correctly in this repository. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - 4. ### Description . Looking around at https://github.com/broadinstitute/gatk/issues?q=label%3AFuncotator+is%3Aclosed I noticed that not much happened since April 2022. Is there no more active development/maintenance planned for Funcotator? Or are the labels misleading? I am planning on writing a generic parser for Funcotator output, but would like to know a bit more about this project's status before investing in this. Thanks. #### Steps to reproduce; See https://github.com/broadinstitute/gatk/issues?q=label%3AFuncotator+is%3Aclosed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8154
https://github.com/broadinstitute/gatk/issues/8158:1144,Availability,error,errors,1144,"tps://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L147. Casued because attribute ""oneShotLogger"" is uninitialized. See line (https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L42) and its missing initialization in the constructor method (https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L65). ### Affected version(s); - [ ] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [12/01/2023]. ### Description ; I work as support for a HPC cluster and this bug has affected one of our users, so I won't be able to provide the exact specifics. Long story short, the user reports that for a high enough value of ploidy (20-50), they start getting null pointer exception errors. Here we can see an example of how they launch the program:. ```; gatk --java-options ""-Xmx4g"" HaplotypeCaller \; -I ${bamfile} \; -R ${reference} \; -O ${outpath}/${sample_id}.ploidy_${SLURM_ARRAY_TASK_ID}.output.g.vcf.gz \; -RF MappingQualityReadFilter \; --minimum-mapping-quality 10 \; --max-alternate-alleles 10 \; --max-genotype-count 75000 \; --dont-use-soft-clipped-bases true \; -ploidy ${SLURM_ARRAY_TASK_ID} \; -ERC GVCF; ```. And this is the stack trace obtained when it fails:. ```; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.logging.OneShotLogger.warn(String)"" because ""this.oneShotLogger"" is null; at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:147); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:21",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:796,Deployability,release,release,796,"## Bug Report. ### Affected tool(s) or class(es); Bug when this line is executed: https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L147. Casued because attribute ""oneShotLogger"" is uninitialized. See line (https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L42) and its missing initialization in the constructor method (https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L65). ### Affected version(s); - [ ] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [12/01/2023]. ### Description ; I work as support for a HPC cluster and this bug has affected one of our users, so I won't be able to provide the exact specifics. Long story short, the user reports that for a high enough value of ploidy (20-50), they start getting null pointer exception errors. Here we can see an example of how they launch the program:. ```; gatk --java-options ""-Xmx4g"" HaplotypeCaller \; -I ${bamfile} \; -R ${reference} \; -O ${outpath}/${sample_id}.ploidy_${SLURM_ARRAY_TASK_ID}.output.g.vcf.gz \; -RF MappingQualityReadFilter \; --minimum-mapping-quality 10 \; --max-alternate-alleles 10 \; --max-genotype-count 75000 \; --dont-use-soft-clipped-bases true \; -ploidy ${SLURM_ARRAY_TASK_ID} \; -ERC GVCF; ```. And this is the stack trace obtained when it fails:. ```; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.logging.OneShotLogger.warn(String)"" because ""this.oneShotLogger"" is null; at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:147); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCall",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3686,Deployability,configurat,configuration,3686,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3774,Deployability,configurat,configuration,3774,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3804,Deployability,configurat,configuration,3804,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3824,Deployability,configurat,configuration,3824,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:4202,Deployability,configurat,configuration,4202,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:4313,Deployability,configurat,configuration,4313,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3679,Modifiability,Config,Config,3679,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3686,Modifiability,config,configuration,3686,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3774,Modifiability,config,configuration,3774,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3804,Modifiability,config,configuration,3804,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3824,Modifiability,config,configuration,3824,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:4202,Modifiability,config,configuration,4202,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:4313,Modifiability,config,configuration,4313,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3863,Security,validat,validate,3863,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:1730,Testability,log,logging,1730,"ngine.java#L65). ### Affected version(s); - [ ] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [12/01/2023]. ### Description ; I work as support for a HPC cluster and this bug has affected one of our users, so I won't be able to provide the exact specifics. Long story short, the user reports that for a high enough value of ploidy (20-50), they start getting null pointer exception errors. Here we can see an example of how they launch the program:. ```; gatk --java-options ""-Xmx4g"" HaplotypeCaller \; -I ${bamfile} \; -R ${reference} \; -O ${outpath}/${sample_id}.ploidy_${SLURM_ARRAY_TASK_ID}.output.g.vcf.gz \; -RF MappingQualityReadFilter \; --minimum-mapping-quality 10 \; --max-alternate-alleles 10 \; --max-genotype-count 75000 \; --dont-use-soft-clipped-bases true \; -ploidy ${SLURM_ARRAY_TASK_ID} \; -ERC GVCF; ```. And this is the stack trace obtained when it fails:. ```; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.logging.OneShotLogger.warn(String)"" because ""this.oneShotLogger"" is null; at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:147); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:219); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:700); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:273); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:4033,Testability,log,logger,4033,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:4042,Testability,Log,LogManager,4042,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:4116,Testability,log,logger,4116,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8158:3480,Usability,simpl,simple,3480,"averse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158
https://github.com/broadinstitute/gatk/issues/8161:59,Availability,mask,mask,59,"Currently `VariantFiltration` does not have the ability to mask a VCF file with multiple bed files. The files must be run individually in sequence to aggregate all the annotations in the `FILTER` column. . To fix this, the `--mask` and `--mask-name` options should be changed to `List` inputs with a 1:1 mapping implied between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8161
https://github.com/broadinstitute/gatk/issues/8161:226,Availability,mask,mask,226,"Currently `VariantFiltration` does not have the ability to mask a VCF file with multiple bed files. The files must be run individually in sequence to aggregate all the annotations in the `FILTER` column. . To fix this, the `--mask` and `--mask-name` options should be changed to `List` inputs with a 1:1 mapping implied between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8161
https://github.com/broadinstitute/gatk/issues/8161:239,Availability,mask,mask-name,239,"Currently `VariantFiltration` does not have the ability to mask a VCF file with multiple bed files. The files must be run individually in sequence to aggregate all the annotations in the `FILTER` column. . To fix this, the `--mask` and `--mask-name` options should be changed to `List` inputs with a 1:1 mapping implied between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8161
https://github.com/broadinstitute/gatk/pull/8163:42,Deployability,integrat,integration,42,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/pull/8163:466,Deployability,integrat,integration,466,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/pull/8163:42,Integrability,integrat,integration,42,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/pull/8163:466,Integrability,integrat,integration,466,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/pull/8163:633,Modifiability,refactor,refactoring,633,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/pull/8163:179,Safety,detect,detected,179,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/pull/8163:54,Testability,test,tests,54,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/pull/8163:478,Testability,test,test,478,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/pull/8163:600,Testability,test,tested,600,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163
https://github.com/broadinstitute/gatk/issues/8164:4707,Availability,error,error,4707,"m.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. `gatk JointGermlineCNVSegmentation --reference hs37d5.fa --variant index.vcf.gz --variant father.vcf.gz --variant mother.vcf.gz --model-call-intervals gcnv_preprocess_intervals.Agilent_SureSelect_Human_All_Exon_V6.interval_list --pedigree family.ped --output out.vcf.gz`. The input VCF lines look as follows:. ```; ## index; Y 2654827 CNV_Y_2654827_24461230 N . 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS .:0:220:94:3077:472:1358; ## father; Y 2654827 CNV_Y_2654827_24461230 N . 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS 0:1:220:58:3077:105:376; ## mother; Y 2654827 CNV_Y_2654827_24461230 N <DEL> 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS 1:0:220:29:3077:357:640; ```. The call looks like an artifact in the BAM alignments. However, the contig ploidy for the mother looks ... interesting. ```; ## index (sex assigned at birth: female); CONTIG PLOIDY PLOIDY_GQ; X 2 123.51003746478007; Y 0 9.176757618621913; ## father (sex assigned at birth: male); CONTIG PLOIDY PLOIDY_GQ; X 1 123.5100374633715; Y 1 17.498503426830368; ## mother (sex assigned at birth: female); CONTIG PLOIDY PLOIDY_GQ; X 2 123.51003745758246; Y 1 0.09888866060944837; ```. The sample of the mother has a slightly increased fraction of chrY reads when compared to other female samples but is far below the fraction of chrY reads that male samples have that were sequenced with the same kit. There is an increase in the variance of alternate allele balance for het. sites in this sample as well. I assume that this sample has been contaminated with male DNA. #### Expected behavior; I would like to be able to deactivate the hard error on the command line and replace it with a warning in the output logs. #### Actual behavior; There is a hard crash that cannot be circumvented.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/issues/8164:125,Deployability,release,release,125,"## Bug Report. ### Affected tool(s) or class(es); JointGermlineCNVSegmentation. ### Affected version(s); - [x] Latest public release version [v4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; I get the following exception when running JointGermlineCNVSegmentation on an exome trio dataset:. ```; [January 19, 2023 at 6:59:29 AM CET] org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation done. Elapsed time: 0.82 minutes.; Runtime.totalMemory()=300941312; java.lang.IllegalStateException: Encountered genotype with ploidy 0 but 1 alleles.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/issues/8164:1577,Energy Efficiency,Reduce,ReduceOps,1577,s.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.apply(JointGermlineCNVSegmentation.java:280); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.afterTraverse(MultiVariantWalkerGroupedOnStart.java:193); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:166); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/issues/8164:1587,Energy Efficiency,Reduce,ReduceOp,1587,s.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.apply(JointGermlineCNVSegmentation.java:280); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.afterTraverse(MultiVariantWalkerGroupedOnStart.java:193); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:166); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/issues/8164:1615,Energy Efficiency,Reduce,ReduceOps,1615,e.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.apply(JointGermlineCNVSegmentation.java:280); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.afterTraverse(MultiVariantWalkerGroupedOnStart.java:193); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:166); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/issues/8164:1503,Integrability,wrap,wrapAndCopyInto,1503,egalStateException: Encountered genotype with ploidy 0 but 1 alleles.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.apply(JointGermlineCNVSegmentation.java:280); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.afterTraverse(MultiVariantWalkerGroupedOnStart.java:193); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:166); at org.broadinstitute.hellbender.engine.GATKTo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/issues/8164:635,Security,validat,validate,635,"## Bug Report. ### Affected tool(s) or class(es); JointGermlineCNVSegmentation. ### Affected version(s); - [x] Latest public release version [v4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; I get the following exception when running JointGermlineCNVSegmentation on an exome trio dataset:. ```; [January 19, 2023 at 6:59:29 AM CET] org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation done. Elapsed time: 0.82 minutes.; Runtime.totalMemory()=300941312; java.lang.IllegalStateException: Encountered genotype with ploidy 0 but 1 alleles.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/issues/8164:195,Testability,test,test,195,"## Bug Report. ### Affected tool(s) or class(es); JointGermlineCNVSegmentation. ### Affected version(s); - [x] Latest public release version [v4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; I get the following exception when running JointGermlineCNVSegmentation on an exome trio dataset:. ```; [January 19, 2023 at 6:59:29 AM CET] org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation done. Elapsed time: 0.82 minutes.; Runtime.totalMemory()=300941312; java.lang.IllegalStateException: Encountered genotype with ploidy 0 but 1 alleles.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/issues/8164:4777,Testability,log,logs,4777,"m.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. `gatk JointGermlineCNVSegmentation --reference hs37d5.fa --variant index.vcf.gz --variant father.vcf.gz --variant mother.vcf.gz --model-call-intervals gcnv_preprocess_intervals.Agilent_SureSelect_Human_All_Exon_V6.interval_list --pedigree family.ped --output out.vcf.gz`. The input VCF lines look as follows:. ```; ## index; Y 2654827 CNV_Y_2654827_24461230 N . 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS .:0:220:94:3077:472:1358; ## father; Y 2654827 CNV_Y_2654827_24461230 N . 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS 0:1:220:58:3077:105:376; ## mother; Y 2654827 CNV_Y_2654827_24461230 N <DEL> 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS 1:0:220:29:3077:357:640; ```. The call looks like an artifact in the BAM alignments. However, the contig ploidy for the mother looks ... interesting. ```; ## index (sex assigned at birth: female); CONTIG PLOIDY PLOIDY_GQ; X 2 123.51003746478007; Y 0 9.176757618621913; ## father (sex assigned at birth: male); CONTIG PLOIDY PLOIDY_GQ; X 1 123.5100374633715; Y 1 17.498503426830368; ## mother (sex assigned at birth: female); CONTIG PLOIDY PLOIDY_GQ; X 2 123.51003745758246; Y 1 0.09888866060944837; ```. The sample of the mother has a slightly increased fraction of chrY reads when compared to other female samples but is far below the fraction of chrY reads that male samples have that were sequenced with the same kit. There is an increase in the variance of alternate allele balance for het. sites in this sample as well. I assume that this sample has been contaminated with male DNA. #### Expected behavior; I would like to be able to deactivate the hard error on the command line and replace it with a warning in the output logs. #### Actual behavior; There is a hard crash that cannot be circumvented.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164
https://github.com/broadinstitute/gatk/pull/8166:73,Modifiability,extend,extend,73,Make several Funcotator methods and fields protected so it is easiest to extend the tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8166
https://github.com/broadinstitute/gatk/pull/8169:21,Deployability,update,update,21,We now want to fully update the AoU documentation so that a VDS can be created (no VCFs!),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8169
https://github.com/broadinstitute/gatk/pull/8175:262,Availability,failure,failure,262,"Add additional validation around duplicated rows in the VAT; <img width=""1418"" alt=""duplicate_AN_or_AC_values"" src=""https://user-images.githubusercontent.com/6863459/220667710-a416ab64-4f9b-475b-9268-ef7b86bfa81e.png"">. This has a successful run (except for one failure that is because it's being run on way less data); https://job-manager.dsde-prod.broadinstitute.org/jobs/07ddde58-ac0d-4229-9f96-d093f5c11682; The failed test is:; SpotCheckForAAChangeAndExonNumberConsistency. Perhaps we want to update this to not run this test if there are less than 10k samples?; Yes we do:; Here's the ticket for that:; https://broadworkbench.atlassian.net/browse/VS-878",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8175
https://github.com/broadinstitute/gatk/pull/8175:498,Deployability,update,update,498,"Add additional validation around duplicated rows in the VAT; <img width=""1418"" alt=""duplicate_AN_or_AC_values"" src=""https://user-images.githubusercontent.com/6863459/220667710-a416ab64-4f9b-475b-9268-ef7b86bfa81e.png"">. This has a successful run (except for one failure that is because it's being run on way less data); https://job-manager.dsde-prod.broadinstitute.org/jobs/07ddde58-ac0d-4229-9f96-d093f5c11682; The failed test is:; SpotCheckForAAChangeAndExonNumberConsistency. Perhaps we want to update this to not run this test if there are less than 10k samples?; Yes we do:; Here's the ticket for that:; https://broadworkbench.atlassian.net/browse/VS-878",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8175
https://github.com/broadinstitute/gatk/pull/8175:15,Security,validat,validation,15,"Add additional validation around duplicated rows in the VAT; <img width=""1418"" alt=""duplicate_AN_or_AC_values"" src=""https://user-images.githubusercontent.com/6863459/220667710-a416ab64-4f9b-475b-9268-ef7b86bfa81e.png"">. This has a successful run (except for one failure that is because it's being run on way less data); https://job-manager.dsde-prod.broadinstitute.org/jobs/07ddde58-ac0d-4229-9f96-d093f5c11682; The failed test is:; SpotCheckForAAChangeAndExonNumberConsistency. Perhaps we want to update this to not run this test if there are less than 10k samples?; Yes we do:; Here's the ticket for that:; https://broadworkbench.atlassian.net/browse/VS-878",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8175
https://github.com/broadinstitute/gatk/pull/8175:423,Testability,test,test,423,"Add additional validation around duplicated rows in the VAT; <img width=""1418"" alt=""duplicate_AN_or_AC_values"" src=""https://user-images.githubusercontent.com/6863459/220667710-a416ab64-4f9b-475b-9268-ef7b86bfa81e.png"">. This has a successful run (except for one failure that is because it's being run on way less data); https://job-manager.dsde-prod.broadinstitute.org/jobs/07ddde58-ac0d-4229-9f96-d093f5c11682; The failed test is:; SpotCheckForAAChangeAndExonNumberConsistency. Perhaps we want to update this to not run this test if there are less than 10k samples?; Yes we do:; Here's the ticket for that:; https://broadworkbench.atlassian.net/browse/VS-878",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8175
https://github.com/broadinstitute/gatk/pull/8175:526,Testability,test,test,526,"Add additional validation around duplicated rows in the VAT; <img width=""1418"" alt=""duplicate_AN_or_AC_values"" src=""https://user-images.githubusercontent.com/6863459/220667710-a416ab64-4f9b-475b-9268-ef7b86bfa81e.png"">. This has a successful run (except for one failure that is because it's being run on way less data); https://job-manager.dsde-prod.broadinstitute.org/jobs/07ddde58-ac0d-4229-9f96-d093f5c11682; The failed test is:; SpotCheckForAAChangeAndExonNumberConsistency. Perhaps we want to update this to not run this test if there are less than 10k samples?; Yes we do:; Here's the ticket for that:; https://broadworkbench.atlassian.net/browse/VS-878",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8175
https://github.com/broadinstitute/gatk/issues/8179:999,Availability,error,error,999,"## Bug Report; HaplotypeCaller. ### Affected version(s); 4.3.0. ### Description ; A plot of the frequency distribution of GQ values associated with variants reported by HaplotypeCaller demonstrates ""periodicity"". The following counts GQ values for TP variant calls (data from HG002, aligned to GRCh38 with three different read aligners, chr14 only):. ![GQdist HC](https://user-images.githubusercontent.com/8249753/215591505-06b76118-cdbf-4b04-ae70-55acaaf8fce9.png). Most of the distribution is periodic on GQ values that are even multiples of 3. This is seen in the data for this plot: [GCdist.xlsx](https://github.com/broadinstitute/gatk/files/10540627/GCdist.xlsx). In addition, about 80% of the reported variants were associated with GQ=99 (not plotted here). This kind of thing might be an artifact of the algorithm used to compute GQ. For example, underlying data such as MAPQ might be manifesting the same periodicity, which is then ""passed through"" to GQ. It might also be an implementation error. For example, premature rounding or the use of an integer variable instead of a floating point variable might lead to inadvertent quantization of a result. But this is just speculation, given only that the distribution would be expected to be smooth, not periodic. #### Steps to reproduce; A little bit of awk should suffice to pull GQ values from a plain-text VCF file. #### Expected behavior; No periodicity in the frequency distribution of GQ values. For example, here is the distribution of GQ values for the same three sets of read mappings but with variants called with DeepVariant:. ![GQdist DV](https://user-images.githubusercontent.com/8249753/215611788-9372cec8-7841-4d90-b137-b3f950902fba.png). In addition, about 15% of the reported variants were associated with GQ=99 (not plotted here). #### Actual behavior; (As above.). Thanks for any insight you can provide on this!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8179
https://github.com/broadinstitute/gatk/issues/8179:1063,Modifiability,variab,variable,1063,"## Bug Report; HaplotypeCaller. ### Affected version(s); 4.3.0. ### Description ; A plot of the frequency distribution of GQ values associated with variants reported by HaplotypeCaller demonstrates ""periodicity"". The following counts GQ values for TP variant calls (data from HG002, aligned to GRCh38 with three different read aligners, chr14 only):. ![GQdist HC](https://user-images.githubusercontent.com/8249753/215591505-06b76118-cdbf-4b04-ae70-55acaaf8fce9.png). Most of the distribution is periodic on GQ values that are even multiples of 3. This is seen in the data for this plot: [GCdist.xlsx](https://github.com/broadinstitute/gatk/files/10540627/GCdist.xlsx). In addition, about 80% of the reported variants were associated with GQ=99 (not plotted here). This kind of thing might be an artifact of the algorithm used to compute GQ. For example, underlying data such as MAPQ might be manifesting the same periodicity, which is then ""passed through"" to GQ. It might also be an implementation error. For example, premature rounding or the use of an integer variable instead of a floating point variable might lead to inadvertent quantization of a result. But this is just speculation, given only that the distribution would be expected to be smooth, not periodic. #### Steps to reproduce; A little bit of awk should suffice to pull GQ values from a plain-text VCF file. #### Expected behavior; No periodicity in the frequency distribution of GQ values. For example, here is the distribution of GQ values for the same three sets of read mappings but with variants called with DeepVariant:. ![GQdist DV](https://user-images.githubusercontent.com/8249753/215611788-9372cec8-7841-4d90-b137-b3f950902fba.png). In addition, about 15% of the reported variants were associated with GQ=99 (not plotted here). #### Actual behavior; (As above.). Thanks for any insight you can provide on this!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8179
https://github.com/broadinstitute/gatk/issues/8179:1100,Modifiability,variab,variable,1100,"## Bug Report; HaplotypeCaller. ### Affected version(s); 4.3.0. ### Description ; A plot of the frequency distribution of GQ values associated with variants reported by HaplotypeCaller demonstrates ""periodicity"". The following counts GQ values for TP variant calls (data from HG002, aligned to GRCh38 with three different read aligners, chr14 only):. ![GQdist HC](https://user-images.githubusercontent.com/8249753/215591505-06b76118-cdbf-4b04-ae70-55acaaf8fce9.png). Most of the distribution is periodic on GQ values that are even multiples of 3. This is seen in the data for this plot: [GCdist.xlsx](https://github.com/broadinstitute/gatk/files/10540627/GCdist.xlsx). In addition, about 80% of the reported variants were associated with GQ=99 (not plotted here). This kind of thing might be an artifact of the algorithm used to compute GQ. For example, underlying data such as MAPQ might be manifesting the same periodicity, which is then ""passed through"" to GQ. It might also be an implementation error. For example, premature rounding or the use of an integer variable instead of a floating point variable might lead to inadvertent quantization of a result. But this is just speculation, given only that the distribution would be expected to be smooth, not periodic. #### Steps to reproduce; A little bit of awk should suffice to pull GQ values from a plain-text VCF file. #### Expected behavior; No periodicity in the frequency distribution of GQ values. For example, here is the distribution of GQ values for the same three sets of read mappings but with variants called with DeepVariant:. ![GQdist DV](https://user-images.githubusercontent.com/8249753/215611788-9372cec8-7841-4d90-b137-b3f950902fba.png). In addition, about 15% of the reported variants were associated with GQ=99 (not plotted here). #### Actual behavior; (As above.). Thanks for any insight you can provide on this!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8179
https://github.com/broadinstitute/gatk/pull/8181:72,Deployability,update,update,72,* com.intel.gkl:gkl:0.8.8 -> 0.8.10. @droazen @kachulis Maybe we should update to the newest version and rerun the tests?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8181
https://github.com/broadinstitute/gatk/pull/8181:115,Testability,test,tests,115,* com.intel.gkl:gkl:0.8.8 -> 0.8.10. @droazen @kachulis Maybe we should update to the newest version and rerun the tests?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8181
https://github.com/broadinstitute/gatk/issues/8183:5061,Availability,down,down,5061," shard 4 / 8...; 14:27:24.879 INFO PostprocessGermlineCNVCalls - Analyzing shard 5 / 8...; 14:27:26.062 INFO PostprocessGermlineCNVCalls - Analyzing shard 6 / 8...; 14:27:26.849 INFO PostprocessGermlineCNVCalls - Analyzing shard 7 / 8...; 14:27:27.893 INFO PostprocessGermlineCNVCalls - Analyzing shard 8 / 8...; 14:27:28.412 INFO PostprocessGermlineCNVCalls - Generating segments...; 14:29:52.532 INFO PostprocessGermlineCNVCalls - Parsing Python output...; 14:29:52.537 INFO PostprocessGermlineCNVCalls - Writing segments VCF file to /bettik/tintest/CNV_Hyperexome/segments/genotyped-segments-SAMPLE_6.vcf.gz...; 14:29:52.703 INFO PostprocessGermlineCNVCalls - Generating denoised copy ratios...; 14:29:53.592 INFO PostprocessGermlineCNVCalls - Writing denoised copy ratios to /bettik/tintest/CNV_Hyperexome/ratios/denoised-copy-ratios-SAMPLE_6.tsv...; 14:29:55.274 INFO PostprocessGermlineCNVCalls - PostprocessGermlineCNVCalls complete.; 14:29:55.275 INFO PostprocessGermlineCNVCalls - Shutting down engine; [December 2, 2022 2:29:55 PM GMT] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 6.03 minutes.; Runtime.totalMemory()=2820145152; Using GATK jar /gatk/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path GermlineCNVCalle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:11131,Availability,down,down,11131,"132 INFO PostprocessGermlineCNVCalls - Analyzing shard 5 / 8...; 23:46:11.901 INFO PostprocessGermlineCNVCalls - Analyzing shard 6 / 8...; 23:46:12.730 INFO PostprocessGermlineCNVCalls - Analyzing shard 7 / 8...; 23:46:14.288 INFO PostprocessGermlineCNVCalls - Analyzing shard 8 / 8...; 23:46:15.617 INFO PostprocessGermlineCNVCalls - Generating segments...; 01:48:30.792 INFO PostprocessGermlineCNVCalls - Parsing Python output...; 01:48:30.875 INFO PostprocessGermlineCNVCalls - Writing segments VCF file to /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz...; 01:48:46.860 INFO PostprocessGermlineCNVCalls - Generating denoised copy ratios...; 01:48:47.487 INFO PostprocessGermlineCNVCalls - Writing denoised copy ratios to /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv...; 01:48:47.773 INFO PostprocessGermlineCNVCalls - PostprocessGermlineCNVCalls complete.; 01:48:47.773 INFO PostprocessGermlineCNVCalls - Shutting down engine; [December 6, 2022 1:48:47 AM GMT] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 123.29 minutes.; Runtime.totalMemory()=7257194496; Using GATK jar /gatk/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4g -Djava.io.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:126,Deployability,release,release,126,"## Bug Report. ### Affected tool(s) or class(es); PostprocessGermlineCNVCalls . ### Affected version(s); - [X ] Latest public release version [4.3.0.0]. ### Description ; Hello, I am a regular user of the gCNV pipeline of GATK4. Since version GATK 4.2.0.0, you have introduced the germline CNV calling joint which I wanted to try and I encountered several problems. So I used, in order, the DetermineGermlineContigPloidy and GermlineCNVCaller tools (cutting my target into 8 bins) version 4.3.0.0 on a cohort of 540 patients. Then I used the PostProcessGermlineCaller tool to produce the VCF files for these patients. Next, I used the JointGermlineCNVSegmentation beta tool to produce a multisample VCF which I reused with PostProcessGermlineCaller to produce joined VCFs. The problem is that the time needed to produce each VCF file has been multiplied by 20 (on average 120 minutes compared to 6), which makes it difficult to use on large cohorts. Here is an extract of the logs, from a sample without, then with the --clustered-breakpoints option: ; #PostprocessGermlineCNVCalls. 14:23:53.500 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:23:54.242 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:23:54.242 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.3.0.0; 14:23:54.242 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Executing as [tintest@dahu132.u-ga.fr](mailto:tintest@dahu132.u-ga.fr) on Linux v5.10.0-18-amd64 amd64; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:23:54.263 INFO PostprocessGermlineCNVCalls - Start Date/Time: December 2, 2022 2:23:53 PM GMT; 14:23:54.263 INFO PostprocessGermlineCNVCall",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:210,Deployability,pipeline,pipeline,210,"## Bug Report. ### Affected tool(s) or class(es); PostprocessGermlineCNVCalls . ### Affected version(s); - [X ] Latest public release version [4.3.0.0]. ### Description ; Hello, I am a regular user of the gCNV pipeline of GATK4. Since version GATK 4.2.0.0, you have introduced the germline CNV calling joint which I wanted to try and I encountered several problems. So I used, in order, the DetermineGermlineContigPloidy and GermlineCNVCaller tools (cutting my target into 8 bins) version 4.3.0.0 on a cohort of 540 patients. Then I used the PostProcessGermlineCaller tool to produce the VCF files for these patients. Next, I used the JointGermlineCNVSegmentation beta tool to produce a multisample VCF which I reused with PostProcessGermlineCaller to produce joined VCFs. The problem is that the time needed to produce each VCF file has been multiplied by 20 (on average 120 minutes compared to 6), which makes it difficult to use on large cohorts. Here is an extract of the logs, from a sample without, then with the --clustered-breakpoints option: ; #PostprocessGermlineCNVCalls. 14:23:53.500 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:23:54.242 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:23:54.242 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.3.0.0; 14:23:54.242 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Executing as [tintest@dahu132.u-ga.fr](mailto:tintest@dahu132.u-ga.fr) on Linux v5.10.0-18-amd64 amd64; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:23:54.263 INFO PostprocessGermlineCNVCalls - Start Date/Time: December 2, 2022 2:23:53 PM GMT; 14:23:54.263 INFO PostprocessGermlineCNVCall",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:14273,Energy Efficiency,reduce,reduce,14273,"aller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-intervals-SAMPLE_6.vcf.gz --allosomal-contig chrX --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls /srv/scratch/testardqu/CNV_Hyperexome/DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary /srv/scratch/testardqu/CNV_Hyperexome/hg19_min_oldM.dict. Is this normal ? Is there a way to reduce the calculation time?. In addition, I noticed that an abnormal number of most likely artifactual CNVs were called on the sex chromosomes in the joined vcfs, no CNVs are operable there, while some CNVs were (supposedly) called correctly in the VCFs produced by the first iteration of PostProcessGermlineCNVCalls. Here are commands that were run on the VCF segments produced by the 2nd iteration (with --clustered-breakpoints) that show a large number of artifactual CNVs on the sex chromosomes in my data (for the autosomal chromosomes, everything looks normal) :. zgrep -v ""#"" *.gz | grep chrY | sort | uniq | cut -f 3 | sort -V | uniq -c; 540 CNV_chrY_7042509_7064541; 540 CNV_chrY_9357472_9360034; ...; 540 CNV_chrY_24795591_24796548; 540 CNV_chrY_24795591_24893824; zgrep -v ""#"" *.gz | grep chrY | sort | uniq | cut -f 3 | sort -V | uniq -c | wc -l; 27; zgrep -v ""#"" *.gz | grep chrY | sort | uniq | grep PASS | cut -f 3 | sort -V | uniq -c; 540 CNV_chrY_7042509_7064541; 288 CNV_chrY_93",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:1123,Performance,Load,Loading,1123,"blic release version [4.3.0.0]. ### Description ; Hello, I am a regular user of the gCNV pipeline of GATK4. Since version GATK 4.2.0.0, you have introduced the germline CNV calling joint which I wanted to try and I encountered several problems. So I used, in order, the DetermineGermlineContigPloidy and GermlineCNVCaller tools (cutting my target into 8 bins) version 4.3.0.0 on a cohort of 540 patients. Then I used the PostProcessGermlineCaller tool to produce the VCF files for these patients. Next, I used the JointGermlineCNVSegmentation beta tool to produce a multisample VCF which I reused with PostProcessGermlineCaller to produce joined VCFs. The problem is that the time needed to produce each VCF file has been multiplied by 20 (on average 120 minutes compared to 6), which makes it difficult to use on large cohorts. Here is an extract of the logs, from a sample without, then with the --clustered-breakpoints option: ; #PostprocessGermlineCNVCalls. 14:23:53.500 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:23:54.242 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:23:54.242 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.3.0.0; 14:23:54.242 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Executing as [tintest@dahu132.u-ga.fr](mailto:tintest@dahu132.u-ga.fr) on Linux v5.10.0-18-amd64 amd64; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:23:54.263 INFO PostprocessGermlineCNVCalls - Start Date/Time: December 2, 2022 2:23:53 PM GMT; 14:23:54.263 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:23:54.263 INFO PostprocessGermlineCNVCalls - ------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:7172,Performance,Load,Loading,7172,"lls-shard-path GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --allosomal-contig chrX --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals intervals/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments segments/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios ratios/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary hg19_min_oldM.fa.dict. #PostprocessGermlineCNVCalls_joint. 23:45:30.659 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:45:31.000 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 23:45:31.001 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.3.0.0; 23:45:31.001 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:45:31.002 INFO PostprocessGermlineCNVCalls - Executing as testardqu@chu-lyon.fr@ge95142-vm1 on Linux v5.18.0-0.bpo.1-amd64 amd64; 23:45:31.002 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 23:45:31.002 INFO PostprocessGermlineCNVCalls - Start Date/Time: December 5, 2022 11:45:30 PM GMT; 23:45:31.002 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 23:45:31.002 INFO PostprocessGermlineCNVCalls - ------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:976,Testability,log,logs,976,"### Affected tool(s) or class(es); PostprocessGermlineCNVCalls . ### Affected version(s); - [X ] Latest public release version [4.3.0.0]. ### Description ; Hello, I am a regular user of the gCNV pipeline of GATK4. Since version GATK 4.2.0.0, you have introduced the germline CNV calling joint which I wanted to try and I encountered several problems. So I used, in order, the DetermineGermlineContigPloidy and GermlineCNVCaller tools (cutting my target into 8 bins) version 4.3.0.0 on a cohort of 540 patients. Then I used the PostProcessGermlineCaller tool to produce the VCF files for these patients. Next, I used the JointGermlineCNVSegmentation beta tool to produce a multisample VCF which I reused with PostProcessGermlineCaller to produce joined VCFs. The problem is that the time needed to produce each VCF file has been multiplied by 20 (on average 120 minutes compared to 6), which makes it difficult to use on large cohorts. Here is an extract of the logs, from a sample without, then with the --clustered-breakpoints option: ; #PostprocessGermlineCNVCalls. 14:23:53.500 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:23:54.242 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:23:54.242 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.3.0.0; 14:23:54.242 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Executing as [tintest@dahu132.u-ga.fr](mailto:tintest@dahu132.u-ga.fr) on Linux v5.10.0-18-amd64 amd64; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:23:54.263 INFO PostprocessGermlineCNVCalls - Start Date/Time: December 2, 2022 2:23:53 PM GMT; 14:23:54.263 INFO PostprocessGermlineCNVCalls - -----------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:7689,Testability,test,testardqu,7689," --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals intervals/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments segments/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios ratios/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary hg19_min_oldM.fa.dict. #PostprocessGermlineCNVCalls_joint. 23:45:30.659 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:45:31.000 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 23:45:31.001 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.3.0.0; 23:45:31.001 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:45:31.002 INFO PostprocessGermlineCNVCalls - Executing as testardqu@chu-lyon.fr@ge95142-vm1 on Linux v5.18.0-0.bpo.1-amd64 amd64; 23:45:31.002 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 23:45:31.002 INFO PostprocessGermlineCNVCalls - Start Date/Time: December 5, 2022 11:45:30 PM GMT; 23:45:31.002 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 23:45:31.002 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 23:45:31.003 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 3.0.1; 23:45:31.003 INFO PostprocessGermlineCNVCalls - Picard Version: 2.27.5; 23:45:31.003 INFO PostprocessGermlineCNVCalls - Built for Spark Version: 2.4.5; 23:45:31.003 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:45:31.003 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:45:31.003 INFO PostprocessGermlineCNVCalls - HTS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:9746,Testability,test,testardqu,9746,NFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:45:31.004 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 23:45:31.004 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 23:45:31.004 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 23:45:31.004 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 23:45:31.004 INFO PostprocessGermlineCNVCalls - Initializing engine; 23:46:06.321 INFO PostprocessGermlineCNVCalls - Done initializing engine; 23:46:07.433 INFO ProgressMeter - Starting traversal; 23:46:07.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 23:46:07.434 INFO ProgressMeter - unmapped 0.0 0 NaN; 23:46:07.434 INFO ProgressMeter - Traversal complete. Processed 0 total records in 0.0 minutes.; 23:46:07.434 INFO PostprocessGermlineCNVCalls - Generating intervals VCF file...; 23:46:07.460 INFO PostprocessGermlineCNVCalls - Writing intervals VCF file to /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz...; 23:46:07.460 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 8...; 23:46:08.946 INFO PostprocessGermlineCNVCalls - Analyzing shard 2 / 8...; 23:46:09.725 INFO PostprocessGermlineCNVCalls - Analyzing shard 3 / 8...; 23:46:10.380 INFO PostprocessGermlineCNVCalls - Analyzing shard 4 / 8...; 23:46:11.132 INFO PostprocessGermlineCNVCalls - Analyzing shard 5 / 8...; 23:46:11.901 INFO PostprocessGermlineCNVCalls - Analyzing shard 6 / 8...; 23:46:12.730 INFO PostprocessGermlineCNVCalls - Analyzing shard 7 / 8...; 23:46:14.288 INFO PostprocessGermlineCNVCalls - Analyzing shard 8 / 8...; 23:46:15.617 INFO PostprocessGermlineCNVCalls - Generating segments...; 01:48:30.792 INFO PostprocessGermlineCNVCalls - Parsing Python output...; 01:48:30.875 INFO PostprocessGermlineCNVCalls - Writing segments VCF file to /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz...; 01:48:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:10655,Testability,test,testardqu,10655,"le...; 23:46:07.460 INFO PostprocessGermlineCNVCalls - Writing intervals VCF file to /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz...; 23:46:07.460 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 8...; 23:46:08.946 INFO PostprocessGermlineCNVCalls - Analyzing shard 2 / 8...; 23:46:09.725 INFO PostprocessGermlineCNVCalls - Analyzing shard 3 / 8...; 23:46:10.380 INFO PostprocessGermlineCNVCalls - Analyzing shard 4 / 8...; 23:46:11.132 INFO PostprocessGermlineCNVCalls - Analyzing shard 5 / 8...; 23:46:11.901 INFO PostprocessGermlineCNVCalls - Analyzing shard 6 / 8...; 23:46:12.730 INFO PostprocessGermlineCNVCalls - Analyzing shard 7 / 8...; 23:46:14.288 INFO PostprocessGermlineCNVCalls - Analyzing shard 8 / 8...; 23:46:15.617 INFO PostprocessGermlineCNVCalls - Generating segments...; 01:48:30.792 INFO PostprocessGermlineCNVCalls - Parsing Python output...; 01:48:30.875 INFO PostprocessGermlineCNVCalls - Writing segments VCF file to /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz...; 01:48:46.860 INFO PostprocessGermlineCNVCalls - Generating denoised copy ratios...; 01:48:47.487 INFO PostprocessGermlineCNVCalls - Writing denoised copy ratios to /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv...; 01:48:47.773 INFO PostprocessGermlineCNVCalls - PostprocessGermlineCNVCalls complete.; 01:48:47.773 INFO PostprocessGermlineCNVCalls - Shutting down engine; [December 6, 2022 1:48:47 AM GMT] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 123.29 minutes.; Runtime.totalMemory()=7257194496; Using GATK jar /gatk/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4g -Djava.io.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-loc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:10911,Testability,test,testardqu,10911,":46:08.946 INFO PostprocessGermlineCNVCalls - Analyzing shard 2 / 8...; 23:46:09.725 INFO PostprocessGermlineCNVCalls - Analyzing shard 3 / 8...; 23:46:10.380 INFO PostprocessGermlineCNVCalls - Analyzing shard 4 / 8...; 23:46:11.132 INFO PostprocessGermlineCNVCalls - Analyzing shard 5 / 8...; 23:46:11.901 INFO PostprocessGermlineCNVCalls - Analyzing shard 6 / 8...; 23:46:12.730 INFO PostprocessGermlineCNVCalls - Analyzing shard 7 / 8...; 23:46:14.288 INFO PostprocessGermlineCNVCalls - Analyzing shard 8 / 8...; 23:46:15.617 INFO PostprocessGermlineCNVCalls - Generating segments...; 01:48:30.792 INFO PostprocessGermlineCNVCalls - Parsing Python output...; 01:48:30.875 INFO PostprocessGermlineCNVCalls - Writing segments VCF file to /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz...; 01:48:46.860 INFO PostprocessGermlineCNVCalls - Generating denoised copy ratios...; 01:48:47.487 INFO PostprocessGermlineCNVCalls - Writing denoised copy ratios to /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv...; 01:48:47.773 INFO PostprocessGermlineCNVCalls - PostprocessGermlineCNVCalls complete.; 01:48:47.773 INFO PostprocessGermlineCNVCalls - Shutting down engine; [December 6, 2022 1:48:47 AM GMT] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 123.29 minutes.; Runtime.totalMemory()=7257194496; Using GATK jar /gatk/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4g -Djava.io.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --mod",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:11584,Testability,test,testardqu,11584,"CNVCalls - Writing segments VCF file to /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz...; 01:48:46.860 INFO PostprocessGermlineCNVCalls - Generating denoised copy ratios...; 01:48:47.487 INFO PostprocessGermlineCNVCalls - Writing denoised copy ratios to /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv...; 01:48:47.773 INFO PostprocessGermlineCNVCalls - PostprocessGermlineCNVCalls complete.; 01:48:47.773 INFO PostprocessGermlineCNVCalls - Shutting down engine; [December 6, 2022 1:48:47 AM GMT] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 123.29 minutes.; Runtime.totalMemory()=7257194496; Using GATK jar /gatk/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4g -Djava.io.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/Germl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:11716,Testability,test,testardqu,11716,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:11823,Testability,test,testardqu,11823,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:11930,Testability,test,testardqu,11930,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12037,Testability,test,testardqu,12037,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12144,Testability,test,testardqu,12144,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12251,Testability,test,testardqu,12251,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12358,Testability,test,testardqu,12358,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12465,Testability,test,testardqu,12465,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12572,Testability,test,testardqu,12572,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12679,Testability,test,testardqu,12679,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12786,Testability,test,testardqu,12786,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:12893,Testability,test,testardqu,12893,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13000,Testability,test,testardqu,13000,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13107,Testability,test,testardqu,13107,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13214,Testability,test,testardqu,13214,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13321,Testability,test,testardqu,13321,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13433,Testability,test,testardqu,13433,o.tmpdir=/srv/scratch/testardqu/CNV_Hyperexome/tmp/ -jar /gatk/gatk-package-4.3.0.0-local.jar PostprocessGermlineCNVCalls --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-model/ --model-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13515,Testability,test,testardqu,13515,"of_8-model/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_1_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_2_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-intervals-SAMPLE_6.vcf.gz --allosomal-contig chrX --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls /srv/scratch/testardqu/CNV_Hyperexome/DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary /srv/scratch/testardqu/CNV_Hyperexome/hg19_min_oldM.dict. Is this normal ? Is there a way to reduce the calculation time?. In addition, I noticed that an abnormal number of most likely artifactual CNVs were called on the sex chromosomes in the joined vcfs, no CNVs are operable there, while some CNVs were (supposedly) called correctly in the VCFs ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13699,Testability,test,testardqu,13699,"s/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-intervals-SAMPLE_6.vcf.gz --allosomal-contig chrX --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls /srv/scratch/testardqu/CNV_Hyperexome/DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary /srv/scratch/testardqu/CNV_Hyperexome/hg19_min_oldM.dict. Is this normal ? Is there a way to reduce the calculation time?. In addition, I noticed that an abnormal number of most likely artifactual CNVs were called on the sex chromosomes in the joined vcfs, no CNVs are operable there, while some CNVs were (supposedly) called correctly in the VCFs produced by the first iteration of PostProcessGermlineCNVCalls. Here are commands that were run on the VCF segments produced by the 2nd iteration (with --clustered-breakpoints) that show a large number of artifactual CNVs on",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13850,Testability,test,testardqu,13850,"s/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_3_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_4_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-intervals-SAMPLE_6.vcf.gz --allosomal-contig chrX --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls /srv/scratch/testardqu/CNV_Hyperexome/DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary /srv/scratch/testardqu/CNV_Hyperexome/hg19_min_oldM.dict. Is this normal ? Is there a way to reduce the calculation time?. In addition, I noticed that an abnormal number of most likely artifactual CNVs were called on the sex chromosomes in the joined vcfs, no CNVs are operable there, while some CNVs were (supposedly) called correctly in the VCFs produced by the first iteration of PostProcessGermlineCNVCalls. Here are commands that were run on the VCF segments produced by the 2nd iteration (with --clustered-breakpoints) that show a large number of artifactual CNVs on",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:13968,Testability,test,testardqu,13968,"d-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_5_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-intervals-SAMPLE_6.vcf.gz --allosomal-contig chrX --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls /srv/scratch/testardqu/CNV_Hyperexome/DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary /srv/scratch/testardqu/CNV_Hyperexome/hg19_min_oldM.dict. Is this normal ? Is there a way to reduce the calculation time?. In addition, I noticed that an abnormal number of most likely artifactual CNVs were called on the sex chromosomes in the joined vcfs, no CNVs are operable there, while some CNVs were (supposedly) called correctly in the VCFs produced by the first iteration of PostProcessGermlineCNVCalls. Here are commands that were run on the VCF segments produced by the 2nd iteration (with --clustered-breakpoints) that show a large number of artifactual CNVs on the sex chromosomes in my data (for the autosomal chromosomes, everything looks normal) :. zgrep -v ""#"" *.gz | grep chrY | sort | uniq | cut -f 3 | sort -V | uniq -c; 540 CNV_chrY_7042509_7064541; 540 CNV_chrY_9357472_9360034; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:14086,Testability,test,testardqu,14086,"v/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_6_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-intervals-SAMPLE_6.vcf.gz --allosomal-contig chrX --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls /srv/scratch/testardqu/CNV_Hyperexome/DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary /srv/scratch/testardqu/CNV_Hyperexome/hg19_min_oldM.dict. Is this normal ? Is there a way to reduce the calculation time?. In addition, I noticed that an abnormal number of most likely artifactual CNVs were called on the sex chromosomes in the joined vcfs, no CNVs are operable there, while some CNVs were (supposedly) called correctly in the VCFs produced by the first iteration of PostProcessGermlineCNVCalls. Here are commands that were run on the VCF segments produced by the 2nd iteration (with --clustered-breakpoints) that show a large number of artifactual CNVs on the sex chromosomes in my data (for the autosomal chromosomes, everything looks normal) :. zgrep -v ""#"" *.gz | grep chrY | sort | uniq | cut -f 3 | sort -V | uniq -c; 540 CNV_chrY_7042509_7064541; 540 CNV_chrY_9357472_9360034; ...; 540 CNV_chrY_24795591_24796548; 540 CNV_chrY_24795591_24893824; zgrep -v ""#"" *.gz | grep chrY | sort | uniq | cu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/issues/8183:14193,Testability,test,testardqu,14193,"rd-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_7_of_8-calls/ --calls-shard-path /srv/scratch/testardqu/CNV_Hyperexome/GermlineCNVCaller/GermlineCNVCaller_8_of_8-calls/ --clustered-breakpoints /srv/scratch/testardqu/CNV_Hyperexome/CNV_Hyperexome.vcf.gz --input-intervals-vcf /srv/scratch/testardqu/CNV_Hyperexome/intervals/genotyped-intervals-SAMPLE_6.vcf.gz --allosomal-contig chrX --allosomal-contig chrY --autosomal-ref-copy-number 2 --contig-ploidy-calls /srv/scratch/testardqu/CNV_Hyperexome/DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --sample-index 6 --output-genotyped-intervals /srv/scratch/testardqu/CNV_Hyperexome/intervals_joint/genotyped-intervals-SAMPLE_6.vcf.gz --output-genotyped-segments /srv/scratch/testardqu/CNV_Hyperexome/segments_joint/genotyped-segments-SAMPLE_6.vcf.gz --output-denoised-copy-ratios /srv/scratch/testardqu/CNV_Hyperexome/ratios_joint/denoised-copy-ratios-SAMPLE_6.tsv --sequence-dictionary /srv/scratch/testardqu/CNV_Hyperexome/hg19_min_oldM.dict. Is this normal ? Is there a way to reduce the calculation time?. In addition, I noticed that an abnormal number of most likely artifactual CNVs were called on the sex chromosomes in the joined vcfs, no CNVs are operable there, while some CNVs were (supposedly) called correctly in the VCFs produced by the first iteration of PostProcessGermlineCNVCalls. Here are commands that were run on the VCF segments produced by the 2nd iteration (with --clustered-breakpoints) that show a large number of artifactual CNVs on the sex chromosomes in my data (for the autosomal chromosomes, everything looks normal) :. zgrep -v ""#"" *.gz | grep chrY | sort | uniq | cut -f 3 | sort -V | uniq -c; 540 CNV_chrY_7042509_7064541; 540 CNV_chrY_9357472_9360034; ...; 540 CNV_chrY_24795591_24796548; 540 CNV_chrY_24795591_24893824; zgrep -v ""#"" *.gz | grep chrY | sort | uniq | cut -f 3 | sort -V | uniq -c | wc -l; 27; zgrep -v ""#"" *.gz | grep chrY | sort | uniq | grep PASS ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183
https://github.com/broadinstitute/gatk/pull/8188:12,Modifiability,config,configurable,12,Adding in a configurable delay after writing a batch of data to BigQuery,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8188
https://github.com/broadinstitute/gatk/issues/8192:135,Availability,error,errors,135,"Hello, When I implement ""HaplotypeCaller"" commands, the reference genome is about 15G , every chromosome is more then 600M, I get some errors, could you give me some advice?; the commands; ```; # the step is after marked duplication ; samtools index -c markdup.bam.gz; gatk --java-options ""-Xmx100G -Djava.io.tmpdir=./"" HaplotypeCaller -R Triticum_aestivum.IWGSC.dna.toplevel.fa -I rmarkdup.bam.gz -O SRR9851087.gvcf.gz -ERC GVCF -OVI >3gvcf.log 2>&1; ```; the bug:; ```; Using GATK jar /home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx100G -Djava.io.tmpdir=./ -jar /home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R Triticum_aestivum.IWGSC.dna.; toplevel.fa -I rmarkdup.bam.gz -O SRR9851087.gvcf.gz -ERC GVCF -OVI; 09:01:25.845 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.950 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 09:01:25.950 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:01:25.950 INFO HaplotypeCaller - Executing as ywt@ywt-Precision-5820-Tower on Linux v5.15.0-41-generic amd64; 09:01:25.950 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.5+8-Ubuntu-2ubuntu122.04; 09:01:25.950 INFO HaplotypeCaller - Start Date/Time: February 8, 2023 at 9:01:25 AM CST; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.951 INFO HaplotypeCaller - HTSJDK V",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192
https://github.com/broadinstitute/gatk/issues/8192:4014,Availability,Avail,Available,4014,"tations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:01:26.067 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 09:01:26.067 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:01:26.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:01:26.078 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:01:26.089 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:01:26.089 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:01:26.090 INFO IntelPairHmm - Available threads: 36; 09:01:26.090 INFO IntelPairHmm - Requested threads: 4; 09:01:26.090 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:01:26.121 INFO ProgressMeter - Starting traversal; 09:01:26.121 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:01:26.406 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 1A:145 and possibly subsequent; at least 10 samples must have called genotypes; 09:01:33.373 WARN DepthPerSampleHC - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:33.374 WARN StrandBiasBySample - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:36.316 INFO ProgressMeter - 1A:2054431 0.2 7310 43025.3; 09:01:46.831 INFO ProgressMeter - 1A:3580946 0.3 12960 37547.1; 09:01:56.858 INFO ProgressMeter - 1A:4888",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192
https://github.com/broadinstitute/gatk/issues/8192:5479,Availability,down,down,5479,"s must have called genotypes; 09:01:33.373 WARN DepthPerSampleHC - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:33.374 WARN StrandBiasBySample - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:36.316 INFO ProgressMeter - 1A:2054431 0.2 7310 43025.3; 09:01:46.831 INFO ProgressMeter - 1A:3580946 0.3 12960 37547.1; 09:01:56.858 INFO ProgressMeter - 1A:4888859 0.5 17840 34824.5; 09:02:07.416 INFO ProgressMeter - 1A:7184455 0.7 26090 37907.7; 09:02:17.850 INFO ProgressMeter - 1A:9469826 0.9 34580 40109.0; 09:02:28.162 INFO ProgressMeter - 1A:11632942 1.0 42480 41082.5; 09:02:38.391 INFO ProgressMeter - 1A:12861813 1.2 47220 39203.0; 09:02:48.460 INFO ProgressMeter - 1A:15373965 1.4 56590 41236.8. 09:20:43.520 INFO ProgressMeter - 1A:536836177 19.3 1951140 101147.8; 09:20:44.715 INFO HaplotypeCaller - Shutting down engine; [February 8, 2023 at 9:20:44 AM CST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 19.31 minutes.; Runtime.totalMemory()=1811939328; java.lang.ArrayIndexOutOfBoundsException: Index 32770 out of bounds for length 32770; at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:142); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:233); at org.broadinstitute.hellbender.utils.variant.writers.GVCFWriter.close(GVCFWriter.java:71); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.closeTool(HaplotypeCaller.java:277); at org.broadinstitute.hellbender.engine.GATKTool.doWork(G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192
https://github.com/broadinstitute/gatk/issues/8192:996,Performance,Load,Loading,996,"Hello, When I implement ""HaplotypeCaller"" commands, the reference genome is about 15G , every chromosome is more then 600M, I get some errors, could you give me some advice?; the commands; ```; # the step is after marked duplication ; samtools index -c markdup.bam.gz; gatk --java-options ""-Xmx100G -Djava.io.tmpdir=./"" HaplotypeCaller -R Triticum_aestivum.IWGSC.dna.toplevel.fa -I rmarkdup.bam.gz -O SRR9851087.gvcf.gz -ERC GVCF -OVI >3gvcf.log 2>&1; ```; the bug:; ```; Using GATK jar /home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx100G -Djava.io.tmpdir=./ -jar /home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R Triticum_aestivum.IWGSC.dna.; toplevel.fa -I rmarkdup.bam.gz -O SRR9851087.gvcf.gz -ERC GVCF -OVI; 09:01:25.845 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.950 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 09:01:25.950 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:01:25.950 INFO HaplotypeCaller - Executing as ywt@ywt-Precision-5820-Tower on Linux v5.15.0-41-generic amd64; 09:01:25.950 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.5+8-Ubuntu-2ubuntu122.04; 09:01:25.950 INFO HaplotypeCaller - Start Date/Time: February 8, 2023 at 9:01:25 AM CST; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.951 INFO HaplotypeCaller - HTSJDK V",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192
https://github.com/broadinstitute/gatk/issues/8192:3468,Performance,Load,Loading,3468,"C_IO_WRITE_FOR_TRIBBLE : false; 09:01:25.951 INFO HaplotypeCaller - Deflater: IntelDeflater; 09:01:25.951 INFO HaplotypeCaller - Inflater: IntelInflater; 09:01:25.951 INFO HaplotypeCaller - GCS max retries/reopens: 20; 09:01:25.951 INFO HaplotypeCaller - Requester pays: disabled; 09:01:25.952 INFO HaplotypeCaller - Initializing engine; 09:01:26.059 INFO HaplotypeCaller - Done initializing engine; 09:01:26.060 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:01:26.067 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 09:01:26.067 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:01:26.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:01:26.078 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:01:26.089 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:01:26.089 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:01:26.090 INFO IntelPairHmm - Available threads: 36; 09:01:26.090 INFO IntelPairHmm - Requested threads: 4; 09:01:26.090 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:01:26.121 INFO ProgressMeter - Starting traversal; 09:01:26.121 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:01:26.406 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 1A:145 and possibly subse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192
https://github.com/broadinstitute/gatk/issues/8192:3658,Performance,Load,Loading,3658," max retries/reopens: 20; 09:01:25.951 INFO HaplotypeCaller - Requester pays: disabled; 09:01:25.952 INFO HaplotypeCaller - Initializing engine; 09:01:26.059 INFO HaplotypeCaller - Done initializing engine; 09:01:26.060 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:01:26.067 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 09:01:26.067 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:01:26.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:01:26.078 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:01:26.089 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:01:26.089 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:01:26.090 INFO IntelPairHmm - Available threads: 36; 09:01:26.090 INFO IntelPairHmm - Requested threads: 4; 09:01:26.090 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:01:26.121 INFO ProgressMeter - Starting traversal; 09:01:26.121 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:01:26.406 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 1A:145 and possibly subsequent; at least 10 samples must have called genotypes; 09:01:33.373 WARN DepthPerSampleHC - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192
https://github.com/broadinstitute/gatk/issues/8192:4137,Performance,multi-thread,multi-threaded,4137,been disabled; 09:01:26.067 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 09:01:26.067 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:01:26.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:01:26.078 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:01:26.089 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:01:26.089 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:01:26.090 INFO IntelPairHmm - Available threads: 36; 09:01:26.090 INFO IntelPairHmm - Requested threads: 4; 09:01:26.090 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:01:26.121 INFO ProgressMeter - Starting traversal; 09:01:26.121 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:01:26.406 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 1A:145 and possibly subsequent; at least 10 samples must have called genotypes; 09:01:33.373 WARN DepthPerSampleHC - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:33.374 WARN StrandBiasBySample - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:36.316 INFO ProgressMeter - 1A:2054431 0.2 7310 43025.3; 09:01:46.831 INFO ProgressMeter - 1A:3580946 0.3 12960 37547.1; 09:01:56.858 INFO ProgressMeter - 1A:4888859 0.5 17840 34824.5; 09:02:07.416 INFO ProgressMeter - 1A:7184455 0.7 26090 37907.7; 09:02:17.850 INFO ProgressMeter - 1A:9469826 0.9 3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192
https://github.com/broadinstitute/gatk/issues/8192:442,Testability,log,log,442,"Hello, When I implement ""HaplotypeCaller"" commands, the reference genome is about 15G , every chromosome is more then 600M, I get some errors, could you give me some advice?; the commands; ```; # the step is after marked duplication ; samtools index -c markdup.bam.gz; gatk --java-options ""-Xmx100G -Djava.io.tmpdir=./"" HaplotypeCaller -R Triticum_aestivum.IWGSC.dna.toplevel.fa -I rmarkdup.bam.gz -O SRR9851087.gvcf.gz -ERC GVCF -OVI >3gvcf.log 2>&1; ```; the bug:; ```; Using GATK jar /home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx100G -Djava.io.tmpdir=./ -jar /home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R Triticum_aestivum.IWGSC.dna.; toplevel.fa -I rmarkdup.bam.gz -O SRR9851087.gvcf.gz -ERC GVCF -OVI; 09:01:25.845 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.950 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 09:01:25.950 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:01:25.950 INFO HaplotypeCaller - Executing as ywt@ywt-Precision-5820-Tower on Linux v5.15.0-41-generic amd64; 09:01:25.950 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.5+8-Ubuntu-2ubuntu122.04; 09:01:25.950 INFO HaplotypeCaller - Start Date/Time: February 8, 2023 at 9:01:25 AM CST; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.951 INFO HaplotypeCaller - HTSJDK V",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192
https://github.com/broadinstitute/gatk/pull/8193:442,Modifiability,refactor,refactoring,442,Limiting scattering size in ingest to keep beta customers under quota. Successful run on NHGRI AnVIL dataset: https://app.terra.bio/#workspaces/gvs-dev/NHGRI_AnVIL_3K%20hatcher/job_history/a9c2a81b-d81c-4f7a-a433-4096ccc7b579. Quota behavior during successful run:; ![Screenshot 2023-02-08 at 3 37 36 PM](https://user-images.githubusercontent.com/110987709/217656881-793e8a87-3e8c-40fc-90a6-6f640ff1c976.png). Next successful run after minor refactoring: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/8a34477f-af3b-4e19-8184-862ed1c2cba3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8193
https://github.com/broadinstitute/gatk/issues/8194:150,Performance,load,loaded,150,"I am running GATK4.3.0.0 in Conda environment on my Linux server. It seems that there are some trouble related to libgkl_utils.so, which could not be loaded and prevented PairHmm to be used. Curiously to me, the seemingly similar libgkl_compression.so could be normally loaded, though in the same tmp dir with the libgkl_utils.so. Can anyone help? Bug report is as follow:. ## Bug Report. ```; 12:00:40.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:00:43.082 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.082 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 12:00:43.083 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:00:43.083 INFO HaplotypeCaller - Executing as sysu_mhwang_1@zwei-ubuntu1804-9195440-86794746d6-89lcr on Linux v3.10.0-957.el7.x86_64 amd64; 12:00:43.083 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v16.0.1+9-24; 12:00:43.083 INFO HaplotypeCaller - Start Date/Time: February 9, 2023 at 12:00:40 PM GMT; 12:00:43.083 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.083 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Version: 3.0.1; 12:00:43.084 INFO HaplotypeCaller - Picard Version: 2.27.5; 12:00:43.084 INFO HaplotypeCaller - Built for Spark Version: 2.4.5; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:00:43.084 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:270,Performance,load,loaded,270,"I am running GATK4.3.0.0 in Conda environment on my Linux server. It seems that there are some trouble related to libgkl_utils.so, which could not be loaded and prevented PairHmm to be used. Curiously to me, the seemingly similar libgkl_compression.so could be normally loaded, though in the same tmp dir with the libgkl_utils.so. Can anyone help? Bug report is as follow:. ## Bug Report. ```; 12:00:40.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:00:43.082 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.082 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 12:00:43.083 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:00:43.083 INFO HaplotypeCaller - Executing as sysu_mhwang_1@zwei-ubuntu1804-9195440-86794746d6-89lcr on Linux v3.10.0-957.el7.x86_64 amd64; 12:00:43.083 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v16.0.1+9-24; 12:00:43.083 INFO HaplotypeCaller - Start Date/Time: February 9, 2023 at 12:00:40 PM GMT; 12:00:43.083 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.083 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Version: 3.0.1; 12:00:43.084 INFO HaplotypeCaller - Picard Version: 2.27.5; 12:00:43.084 INFO HaplotypeCaller - Built for Spark Version: 2.4.5; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:00:43.084 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:434,Performance,Load,Loading,434,"I am running GATK4.3.0.0 in Conda environment on my Linux server. It seems that there are some trouble related to libgkl_utils.so, which could not be loaded and prevented PairHmm to be used. Curiously to me, the seemingly similar libgkl_compression.so could be normally loaded, though in the same tmp dir with the libgkl_utils.so. Can anyone help? Bug report is as follow:. ## Bug Report. ```; 12:00:40.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:00:43.082 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.082 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 12:00:43.083 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:00:43.083 INFO HaplotypeCaller - Executing as sysu_mhwang_1@zwei-ubuntu1804-9195440-86794746d6-89lcr on Linux v3.10.0-957.el7.x86_64 amd64; 12:00:43.083 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v16.0.1+9-24; 12:00:43.083 INFO HaplotypeCaller - Start Date/Time: February 9, 2023 at 12:00:40 PM GMT; 12:00:43.083 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.083 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Version: 3.0.1; 12:00:43.084 INFO HaplotypeCaller - Picard Version: 2.27.5; 12:00:43.084 INFO HaplotypeCaller - Built for Spark Version: 2.4.5; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:00:43.084 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:2946,Performance,Load,Loading,2946,"C_IO_WRITE_FOR_TRIBBLE : false; 12:00:43.084 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:00:43.084 INFO HaplotypeCaller - Inflater: IntelInflater; 12:00:43.084 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:00:43.084 INFO HaplotypeCaller - Requester pays: disabled; 12:00:43.084 INFO HaplotypeCaller - Initializing engine; 12:00:43.217 INFO HaplotypeCaller - Done initializing engine; 12:00:43.218 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 12:00:43.230 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 12:00:43.230 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:00:43.239 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.240 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.240 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.240 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:00:43.240 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.241 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.241 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:3164,Performance,load,load,3164,"ns: 20; 12:00:43.084 INFO HaplotypeCaller - Requester pays: disabled; 12:00:43.084 INFO HaplotypeCaller - Initializing engine; 12:00:43.217 INFO HaplotypeCaller - Done initializing engine; 12:00:43.218 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 12:00:43.230 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 12:00:43.230 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:00:43.239 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.240 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.240 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.240 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:00:43.240 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.241 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slowe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:3383,Performance,load,loaded,3383,"allerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 12:00:43.230 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 12:00:43.230 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:00:43.239 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.240 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.240 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.240 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:00:43.240 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.241 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:3426,Performance,multi-thread,multi-threaded,3426,"nges will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 12:00:43.230 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 12:00:43.230 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:00:43.239 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.240 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.240 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.240 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:00:43.240 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.241 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subseq",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:3545,Performance,Load,Loading,3545,"d. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 12:00:43.230 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 12:00:43.230 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:00:43.239 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.240 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.240 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.240 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:00:43.240 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.241 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subsequent; at least 10 samples must have called genotypes; 12:00:51.132 WARN DepthPerSam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:3763,Performance,load,load,3763,ence-model confidence output; 12:00:43.230 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:00:43.239 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.240 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.240 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.240 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:00:43.240 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.241 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subsequent; at least 10 samples must have called genotypes; 12:00:51.132 WARN DepthPerSampleHC - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:51.133 WARN StrandBiasBySample - Annotation will not be calculated at pos,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:3981,Performance,load,loaded,3981,so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.240 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.240 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.240 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:00:43.240 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.241 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subsequent; at least 10 samples must have called genotypes; 12:00:51.132 WARN DepthPerSampleHC - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:51.133 WARN StrandBiasBySample - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:54.733 INFO ProgressMeter - LG01:112488 0.2 620 3245.2; 12:01:04.895 INFO ProgressMeter - LG01:258647 0.4 1440 3995.2; 12:01:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:5099,Performance,Load,Loading,5099,d; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subsequent; at least 10 samples must have called genotypes; 12:00:51.132 WARN DepthPerSampleHC - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:51.133 WARN StrandBiasBySample - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:54.733 INFO ProgressMeter - LG01:112488 0.2 620 3245.2; 12:01:04.895 INFO ProgressMeter - LG01:258647 0.4 1440 3995.2; 12:01:15.041 INFO ProgressMeter - LG01:425234 0.5 2410 4551.2; ```. ## Keywords of Log file. > 12:00:40.974 INFO NativeLibraryLoader - **Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so**. > 12:00:43.240 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; > 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:5335,Performance,load,load,5335,d; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subsequent; at least 10 samples must have called genotypes; 12:00:51.132 WARN DepthPerSampleHC - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:51.133 WARN StrandBiasBySample - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:54.733 INFO ProgressMeter - LG01:112488 0.2 620 3245.2; 12:01:04.895 INFO ProgressMeter - LG01:258647 0.4 1440 3995.2; 12:01:15.041 INFO ProgressMeter - LG01:425234 0.5 2410 4551.2; ```. ## Keywords of Log file. > 12:00:40.974 INFO NativeLibraryLoader - **Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so**. > 12:00:43.240 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; > 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:5557,Performance,load,load,5557,d; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subsequent; at least 10 samples must have called genotypes; 12:00:51.132 WARN DepthPerSampleHC - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:51.133 WARN StrandBiasBySample - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:54.733 INFO ProgressMeter - LG01:112488 0.2 620 3245.2; 12:01:04.895 INFO ProgressMeter - LG01:258647 0.4 1440 3995.2; 12:01:15.041 INFO ProgressMeter - LG01:425234 0.5 2410 4551.2; ```. ## Keywords of Log file. > 12:00:40.974 INFO NativeLibraryLoader - **Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so**. > 12:00:43.240 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; > 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:5779,Performance,load,loaded,5779,d; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subsequent; at least 10 samples must have called genotypes; 12:00:51.132 WARN DepthPerSampleHC - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:51.133 WARN StrandBiasBySample - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:54.733 INFO ProgressMeter - LG01:112488 0.2 620 3245.2; 12:01:04.895 INFO ProgressMeter - LG01:258647 0.4 1440 3995.2; 12:01:15.041 INFO ProgressMeter - LG01:425234 0.5 2410 4551.2; ```. ## Keywords of Log file. > 12:00:40.974 INFO NativeLibraryLoader - **Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so**. > 12:00:43.240 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; > 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/issues/8194:5045,Testability,Log,Log,5045,d; 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 12:00:43.269 INFO ProgressMeter - Starting traversal; 12:00:43.269 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:00:43.828 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position LG01:40057 and possibly subsequent; at least 10 samples must have called genotypes; 12:00:51.132 WARN DepthPerSampleHC - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:51.133 WARN StrandBiasBySample - Annotation will not be calculated at position LG01:69431 and possibly subsequent; genotype for sample F2012_7 is not called; 12:00:54.733 INFO ProgressMeter - LG01:112488 0.2 620 3245.2; 12:01:04.895 INFO ProgressMeter - LG01:258647 0.4 1440 3995.2; 12:01:15.041 INFO ProgressMeter - LG01:425234 0.5 2410 4551.2; ```. ## Keywords of Log file. > 12:00:40.974 INFO NativeLibraryLoader - **Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so**. > 12:00:43.240 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN NativeLibraryLoader - **Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory)**. > 12:00:43.241 WARN IntelPairHmm - Intel GKL Utils not loaded; > 12:00:43.241 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194
https://github.com/broadinstitute/gatk/pull/8196:45,Integrability,depend,dependency,45,"The GCC OpenMP library, libgomp1, a required dependency of GCC, needs to be present in order to run the GKL accelerated PairHMM in tools like HaplotypeCaller. We now mention this requirement in the GATK README. Resolves #6012",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8196
https://github.com/broadinstitute/gatk/pull/8197:232,Availability,down,downloaded,232,The goal of this PR is to adjust the ingest in two ways:; 1. To update the ingest to loop through all samples (not just the first 10k); 2. To update the ingest to be far more efficient in a few ways:; - To remove the files that are downloaded to each vm so that they do not carry around the extra weight; - To check that the samples in the fofns have not been ingested already so that additional work doesn't need to be done toward processing those samples. There is still work to do around making the bulk ingest process significantly more user-friendly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8197
https://github.com/broadinstitute/gatk/pull/8197:64,Deployability,update,update,64,The goal of this PR is to adjust the ingest in two ways:; 1. To update the ingest to loop through all samples (not just the first 10k); 2. To update the ingest to be far more efficient in a few ways:; - To remove the files that are downloaded to each vm so that they do not carry around the extra weight; - To check that the samples in the fofns have not been ingested already so that additional work doesn't need to be done toward processing those samples. There is still work to do around making the bulk ingest process significantly more user-friendly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8197
https://github.com/broadinstitute/gatk/pull/8197:142,Deployability,update,update,142,The goal of this PR is to adjust the ingest in two ways:; 1. To update the ingest to loop through all samples (not just the first 10k); 2. To update the ingest to be far more efficient in a few ways:; - To remove the files that are downloaded to each vm so that they do not carry around the extra weight; - To check that the samples in the fofns have not been ingested already so that additional work doesn't need to be done toward processing those samples. There is still work to do around making the bulk ingest process significantly more user-friendly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8197
https://github.com/broadinstitute/gatk/pull/8197:175,Energy Efficiency,efficient,efficient,175,The goal of this PR is to adjust the ingest in two ways:; 1. To update the ingest to loop through all samples (not just the first 10k); 2. To update the ingest to be far more efficient in a few ways:; - To remove the files that are downloaded to each vm so that they do not carry around the extra weight; - To check that the samples in the fofns have not been ingested already so that additional work doesn't need to be done toward processing those samples. There is still work to do around making the bulk ingest process significantly more user-friendly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8197
https://github.com/broadinstitute/gatk/pull/8197:541,Usability,user-friendly,user-friendly,541,The goal of this PR is to adjust the ingest in two ways:; 1. To update the ingest to loop through all samples (not just the first 10k); 2. To update the ingest to be far more efficient in a few ways:; - To remove the files that are downloaded to each vm so that they do not carry around the extra weight; - To check that the samples in the fofns have not been ingested already so that additional work doesn't need to be done toward processing those samples. There is still work to do around making the bulk ingest process significantly more user-friendly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8197
https://github.com/broadinstitute/gatk/pull/8200:64,Deployability,update,updated,64,"piped beta variables through to high-level beta workflow.; Also updated the gatk jar so it succeeds, as it didn't before. [Successful run of beta workflow on quickstart data](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/e9a1af96-8c1a-463a-8063-ae455d0ba6b3)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8200
https://github.com/broadinstitute/gatk/pull/8200:11,Modifiability,variab,variables,11,"piped beta variables through to high-level beta workflow.; Also updated the gatk jar so it succeeds, as it didn't before. [Successful run of beta workflow on quickstart data](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/e9a1af96-8c1a-463a-8063-ae455d0ba6b3)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8200
https://github.com/broadinstitute/gatk/issues/8203:365,Availability,error,error,365,"Based on this forum [post](https://gatk.broadinstitute.org/hc/en-us/community/posts/12230735621403-Missing-RawGtCount-annoation-from-GenotypeGVCF-output), when the argument `--tree-score-threshold-to-no-call` is used in `ReblockGVCFs` with input that doesn't contain the `TREE_SCORE` annotation, the tool gets very slow, since it tries to no-call every variant. An error message should be thrown before starting traversal of the VCF if the input is missing this annotation when that argument is set.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8203
https://github.com/broadinstitute/gatk/issues/8203:371,Integrability,message,message,371,"Based on this forum [post](https://gatk.broadinstitute.org/hc/en-us/community/posts/12230735621403-Missing-RawGtCount-annoation-from-GenotypeGVCF-output), when the argument `--tree-score-threshold-to-no-call` is used in `ReblockGVCFs` with input that doesn't contain the `TREE_SCORE` annotation, the tool gets very slow, since it tries to no-call every variant. An error message should be thrown before starting traversal of the VCF if the input is missing this annotation when that argument is set.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8203
https://github.com/broadinstitute/gatk/issues/8204:77,Availability,error,error,77,"Hello GATK team, I'm running the following command but getting the following error. Do you know how to solve it? Thank you very much!. ```; java -Xmx80g -Djava.io.tmpdir=/lustre/home/xyliu/02_tmp -jar /lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar PathSeqBuildKmers --reference pathseq_host.fa --output pathseq_host.hss --bloom-false-positive-probability 0.001 --kmer-mask 16 --kmer-size 31 &; ```. ```; $ 10:49:50.605 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2023 10:49:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:50.812 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.813 INFO PathSeqBuildKmers - The Genome Analysis Toolkit (GATK) v4.2.3.0; 10:49:50.813 INFO PathSeqBuildKmers - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:50.813 INFO PathSeqBuildKmers - Executing as xyliu@fat16 on Linux v3.10.0-862.el7.x86_64 amd64; 10:49:50.813 INFO PathSeqBuildKmers - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_161-b14; 10:49:50.813 INFO PathSeqBuildKmers - Start Date/Time: February 15, 2023 10:49:50 AM CST; 10:49:50.813 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Version: 2.24.1; 10:49:50.814 INFO PathSeqBuildKmers - Picard Version: 2.25.4; 10:49:50.814 INFO PathSeqBuildKmers - Built for Spark Version: 2.4.5; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:50.814 INFO PathSeqBuildKmer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204
https://github.com/broadinstitute/gatk/issues/8204:392,Availability,mask,mask,392,"Hello GATK team, I'm running the following command but getting the following error. Do you know how to solve it? Thank you very much!. ```; java -Xmx80g -Djava.io.tmpdir=/lustre/home/xyliu/02_tmp -jar /lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar PathSeqBuildKmers --reference pathseq_host.fa --output pathseq_host.hss --bloom-false-positive-probability 0.001 --kmer-mask 16 --kmer-size 31 &; ```. ```; $ 10:49:50.605 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2023 10:49:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:50.812 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.813 INFO PathSeqBuildKmers - The Genome Analysis Toolkit (GATK) v4.2.3.0; 10:49:50.813 INFO PathSeqBuildKmers - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:50.813 INFO PathSeqBuildKmers - Executing as xyliu@fat16 on Linux v3.10.0-862.el7.x86_64 amd64; 10:49:50.813 INFO PathSeqBuildKmers - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_161-b14; 10:49:50.813 INFO PathSeqBuildKmers - Start Date/Time: February 15, 2023 10:49:50 AM CST; 10:49:50.813 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Version: 2.24.1; 10:49:50.814 INFO PathSeqBuildKmers - Picard Version: 2.25.4; 10:49:50.814 INFO PathSeqBuildKmers - Built for Spark Version: 2.4.5; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:50.814 INFO PathSeqBuildKmer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204
https://github.com/broadinstitute/gatk/issues/8204:11656,Availability,down,down,11656,"emaining; 11:34:35.285 INFO PSKmerUtils - 86.4% complete - 3043.0 million kmers at 75.7 million kmers/min, 6.35 min remaining; 11:34:56.716 INFO PSKmerUtils - 87.4% complete - 3078.4 million kmers at 75.9 million kmers/min, 5.87 min remaining; 11:35:22.166 INFO PSKmerUtils - 88.4% complete - 3113.7 million kmers at 76.0 million kmers/min, 5.40 min remaining; 11:35:47.066 INFO PSKmerUtils - 89.4% complete - 3149.1 million kmers at 76.1 million kmers/min, 4.92 min remaining; 11:36:09.985 INFO PSKmerUtils - 90.4% complete - 3184.4 million kmers at 76.2 million kmers/min, 4.45 min remaining; 11:36:35.287 INFO PSKmerUtils - 91.4% complete - 3220.8 million kmers at 76.3 million kmers/min, 3.97 min remaining; 11:36:57.953 INFO PSKmerUtils - 92.4% complete - 3256.1 million kmers at 76.5 million kmers/min, 3.50 min remaining; 11:37:19.588 INFO PSKmerUtils - 93.4% complete - 3291.3 million kmers at 76.7 million kmers/min, 3.03 min remaining; 11:37:41.529 INFO PSKmerUtils - 94.4% complete - 3326.5 million kmers at 76.8 million kmers/min, 2.57 min remaining; 11:38:03.288 INFO PSKmerUtils - 95.4% complete - 3361.8 million kmers at 77.0 million kmers/min, 2.10 min remaining; 11:38:24.891 INFO PSKmerUtils - 96.4% complete - 3397.0 million kmers at 77.2 million kmers/min, 1.64 min remaining; 11:38:46.744 INFO PSKmerUtils - 97.4% complete - 3432.3 million kmers at 77.3 million kmers/min, 1.18 min remaining; 11:39:08.040 INFO PSKmerUtils - 98.4% complete - 3467.5 million kmers at 77.5 million kmers/min, 0.73 min remaining; 11:39:29.389 INFO PSKmerUtils - 99.4% complete - 3502.7 million kmers at 77.7 million kmers/min, 0.27 min remaining; 11:39:42.728 INFO PathSeqBuildKmers - Theoretical Bloom filter false positive probability: 1.0794569119941353E-4; 11:40:30.038 INFO PathSeqBuildKmers - Shutting down engine; [February 15, 2023 11:40:30 AM CST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqBuildKmers done. Elapsed time: 50.66 minutes.; Runtime.totalMemory()=58112606208; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204
https://github.com/broadinstitute/gatk/issues/8204:470,Performance,Load,Loading,470,"Hello GATK team, I'm running the following command but getting the following error. Do you know how to solve it? Thank you very much!. ```; java -Xmx80g -Djava.io.tmpdir=/lustre/home/xyliu/02_tmp -jar /lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar PathSeqBuildKmers --reference pathseq_host.fa --output pathseq_host.hss --bloom-false-positive-probability 0.001 --kmer-mask 16 --kmer-size 31 &; ```. ```; $ 10:49:50.605 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2023 10:49:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:50.812 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.813 INFO PathSeqBuildKmers - The Genome Analysis Toolkit (GATK) v4.2.3.0; 10:49:50.813 INFO PathSeqBuildKmers - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:50.813 INFO PathSeqBuildKmers - Executing as xyliu@fat16 on Linux v3.10.0-862.el7.x86_64 amd64; 10:49:50.813 INFO PathSeqBuildKmers - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_161-b14; 10:49:50.813 INFO PathSeqBuildKmers - Start Date/Time: February 15, 2023 10:49:50 AM CST; 10:49:50.813 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Version: 2.24.1; 10:49:50.814 INFO PathSeqBuildKmers - Picard Version: 2.25.4; 10:49:50.814 INFO PathSeqBuildKmers - Built for Spark Version: 2.4.5; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:50.814 INFO PathSeqBuildKmer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204
https://github.com/broadinstitute/gatk/issues/8204:2573,Performance,Load,Loading,2573,"------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Version: 2.24.1; 10:49:50.814 INFO PathSeqBuildKmers - Picard Version: 2.25.4; 10:49:50.814 INFO PathSeqBuildKmers - Built for Spark Version: 2.4.5; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:49:50.814 INFO PathSeqBuildKmers - Deflater: IntelDeflater; 10:49:50.814 INFO PathSeqBuildKmers - Inflater: IntelInflater; 10:49:50.815 INFO PathSeqBuildKmers - GCS max retries/reopens: 20; 10:49:50.815 INFO PathSeqBuildKmers - Requester pays: disabled; 10:49:50.815 INFO PathSeqBuildKmers - Initializing engine; 10:49:50.815 INFO PathSeqBuildKmers - Done initializing engine; 10:49:50.816 INFO PathSeqBuildKmers - Loading reference kmers...; 10:50:49.423 INFO PSKmerUtils - Generating kmers from 3713370968 bases in 412468 records...; 10:51:03.515 INFO PSKmerUtils - 6.7% complete - 249.0 Mbp at 1060.1 Mbp/min, 3.27 min remaining; 10:51:19.888 INFO PSKmerUtils - 13.2% complete - 491.1 Mbp at 967.3 Mbp/min, 3.33 min remaining; 10:51:35.575 INFO PSKmerUtils - 18.6% complete - 689.4 Mbp at 896.3 Mbp/min, 3.37 min remaining; 10:51:47.982 INFO PSKmerUtils - 23.7% complete - 879.7 Mbp at 901.3 Mbp/min, 3.14 min remaining; 10:52:01.291 INFO PSKmerUtils - 28.6% complete - 1061.2 Mbp at 886.0 Mbp/min, 2.99 min remaining; 10:52:10.127 INFO PSKmerUtils - 33.2% complete - 1232.0 Mbp at 916.0 Mbp/min, 2.71 min remaining; 10:52:21.475 INFO PSKmerUtils - 37.5% complete - 1391.4 Mbp at 906.9 Mbp/min, 2.56 min remaining; 10:52:28.848 INFO PSKmerUtils - 41.4% complete - 1536.5 Mbp at 927.2 Mbp/min, 2.35 min remaining; 10:52:34.246 INFO PSKmerUtils - 45.1% complete - 1674.9 Mbp at 958.7 Mbp/min, 2.13 min remaining;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204
https://github.com/broadinstitute/gatk/issues/8204:760,Safety,detect,detect,760,"Hello GATK team, I'm running the following command but getting the following error. Do you know how to solve it? Thank you very much!. ```; java -Xmx80g -Djava.io.tmpdir=/lustre/home/xyliu/02_tmp -jar /lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar PathSeqBuildKmers --reference pathseq_host.fa --output pathseq_host.hss --bloom-false-positive-probability 0.001 --kmer-mask 16 --kmer-size 31 &; ```. ```; $ 10:49:50.605 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2023 10:49:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:50.812 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.813 INFO PathSeqBuildKmers - The Genome Analysis Toolkit (GATK) v4.2.3.0; 10:49:50.813 INFO PathSeqBuildKmers - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:50.813 INFO PathSeqBuildKmers - Executing as xyliu@fat16 on Linux v3.10.0-862.el7.x86_64 amd64; 10:49:50.813 INFO PathSeqBuildKmers - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_161-b14; 10:49:50.813 INFO PathSeqBuildKmers - Start Date/Time: February 15, 2023 10:49:50 AM CST; 10:49:50.813 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Version: 2.24.1; 10:49:50.814 INFO PathSeqBuildKmers - Picard Version: 2.25.4; 10:49:50.814 INFO PathSeqBuildKmers - Built for Spark Version: 2.4.5; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:50.814 INFO PathSeqBuildKmer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204
https://github.com/broadinstitute/gatk/pull/8205:146,Security,validat,validation,146,"This includes:; * the GQ0 --> no call conversion; * the setting of the max ref block size (already 1000, but need to let the VDS know). Bonus:; a validation script for the VDS itself. <img width=""851"" alt=""valid"" src=""https://user-images.githubusercontent.com/6863459/220472873-184c7c51-7b1b-41e7-abca-55d05293e590.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8205
https://github.com/broadinstitute/gatk/issues/8208:1445,Availability,down,downstream,1445,"ssing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants — and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up someh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:2662,Availability,avail,available,2662,"eblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly available samples to demonstrate the issue. Let me know if this is needed. . #### Expected behavior; A large number of variants should be called as hom ref (`0/0`). #### Actual behavior; Many of these variants are left as not called / missing genotypes (`./.`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:164,Deployability,release,release,164,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF (& how it interacts with GenotypeGVCFs). ### Affected version(s); - [x] 4.2.6.1 ; - [ ] Latest public release version (haven't tried yet, but doesn't look like ReblockGVCFs has been changed substantially since 4.2.6.1); - [ ] Latest master branch (haven't tried yet, but doesn't look like ReblockGVCFs has been changed substantially since 4.2.6.1). ### Description . Our samples run with ReblockGVCF appear to have unusually high numbers of variants with missing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants — and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:766,Deployability,pipeline,pipeline,766,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF (& how it interacts with GenotypeGVCFs). ### Affected version(s); - [x] 4.2.6.1 ; - [ ] Latest public release version (haven't tried yet, but doesn't look like ReblockGVCFs has been changed substantially since 4.2.6.1); - [ ] Latest master branch (haven't tried yet, but doesn't look like ReblockGVCFs has been changed substantially since 4.2.6.1). ### Description . Our samples run with ReblockGVCF appear to have unusually high numbers of variants with missing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants — and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:859,Deployability,release,releases,859,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF (& how it interacts with GenotypeGVCFs). ### Affected version(s); - [x] 4.2.6.1 ; - [ ] Latest public release version (haven't tried yet, but doesn't look like ReblockGVCFs has been changed substantially since 4.2.6.1); - [ ] Latest master branch (haven't tried yet, but doesn't look like ReblockGVCFs has been changed substantially since 4.2.6.1). ### Description . Our samples run with ReblockGVCF appear to have unusually high numbers of variants with missing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants — and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:1469,Deployability,pipeline,pipeline,1469,"ssing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants — and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up someh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:2054,Deployability,release,releases,2054,"eblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly available samples to demonstrate the issue. Let me know if this is needed. . #### Expected behavior; A large number of variants should be called as hom ref (`0/0`). #### Actual behavior; Many of these variants are left as not called / missing genotypes (`./.`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:2101,Deployability,pipeline,pipeline,2101,"eblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly available samples to demonstrate the issue. Let me know if this is needed. . #### Expected behavior; A large number of variants should be called as hom ref (`0/0`). #### Actual behavior; Many of these variants are left as not called / missing genotypes (`./.`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:2264,Deployability,release,releases,2264,"eblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly available samples to demonstrate the issue. Let me know if this is needed. . #### Expected behavior; A large number of variants should be called as hom ref (`0/0`). #### Actual behavior; Many of these variants are left as not called / missing genotypes (`./.`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:2311,Deployability,pipeline,pipeline,2311,"eblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly available samples to demonstrate the issue. Let me know if this is needed. . #### Expected behavior; A large number of variants should be called as hom ref (`0/0`). #### Actual behavior; Many of these variants are left as not called / missing genotypes (`./.`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:1604,Testability,test,test,1604,"ssing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants — and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up someh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:1629,Testability,test,test,1629,"alled at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly availab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/issues/8208:2555,Testability,test,tested,2555,"eblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly available samples to demonstrate the issue. Let me know if this is needed. . #### Expected behavior; A large number of variants should be called as hom ref (`0/0`). #### Actual behavior; Many of these variants are left as not called / missing genotypes (`./.`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208
https://github.com/broadinstitute/gatk/pull/8210:37,Availability,failure,failure,37,updating the docker image to fix the failure in CreateFilterSet introduced by my older VSQR-Lite merge. [Successful run here.](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/95307258-02d0-4d33-b9bb-1ba1eaac6bff),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8210
https://github.com/broadinstitute/gatk/pull/8211:224,Availability,avail,available,224,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211
https://github.com/broadinstitute/gatk/pull/8211:280,Deployability,Integrat,Integration,280,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211
https://github.com/broadinstitute/gatk/pull/8211:491,Deployability,Integrat,Integration,491,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211
https://github.com/broadinstitute/gatk/pull/8211:759,Deployability,Update,Updated,759,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211
https://github.com/broadinstitute/gatk/pull/8211:280,Integrability,Integrat,Integration,280,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211
https://github.com/broadinstitute/gatk/pull/8211:491,Integrability,Integrat,Integration,491,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211
https://github.com/broadinstitute/gatk/pull/8211:292,Testability,test,tests,292,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211
https://github.com/broadinstitute/gatk/pull/8211:503,Testability,test,test,503,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211
https://github.com/broadinstitute/gatk/issues/8215:525,Availability,down,download,525,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215
https://github.com/broadinstitute/gatk/issues/8215:430,Deployability,deploy,deploy,430,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215
https://github.com/broadinstitute/gatk/issues/8215:441,Deployability,release,released,441,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215
https://github.com/broadinstitute/gatk/issues/8215:516,Deployability,release,releases,516,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215
https://github.com/broadinstitute/gatk/issues/8215:1013,Deployability,integrat,integrated,1013,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215
https://github.com/broadinstitute/gatk/issues/8215:1013,Integrability,integrat,integrated,1013,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215
https://github.com/broadinstitute/gatk/issues/8215:795,Security,secur,security,795,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215
https://github.com/broadinstitute/gatk/pull/8216:11,Testability,test,test,11,Successful test run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/379e28fa-7326-43e0-834a-6f8b846ef76f,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8216
https://github.com/broadinstitute/gatk/pull/8217:11,Performance,optimiz,optimized,11,"Adds cloud-optimized (scattered) version of gcnv case wdl to the dockstore file. Should be equivalent to the regular case workflow, just faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8217
https://github.com/broadinstitute/gatk/pull/8219:224,Deployability,update,updates,224,* add an new option to VariantsToTable to allow output VCF style numeric GT fields; previously it always output the actual bases of the Allele in the GT spot; * resolves https://github.com/broadinstitute/gatk/issues/8160; * updates htsjdk to 3.0.5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8219
https://github.com/broadinstitute/gatk/pull/8220:109,Deployability,deploy,deployment,109,"Say hello to Azure SQL Database from `sqlcmd`, Python and Java (via Ammonite) running in a Cromwell on Azure deployment. Since the Azure Batch VMs spun up by Cromwell on Azure appear to have no identity associated with them the workflow currently takes a database access token as a parameter which it passes to the three tasks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8220
https://github.com/broadinstitute/gatk/pull/8220:264,Security,access,access,264,"Say hello to Azure SQL Database from `sqlcmd`, Python and Java (via Ammonite) running in a Cromwell on Azure deployment. Since the Azure Batch VMs spun up by Cromwell on Azure appear to have no identity associated with them the workflow currently takes a database access token as a parameter which it passes to the three tasks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8220
https://github.com/broadinstitute/gatk/issues/8221:4295,Availability,Avail,Available,4295,1cb8661465e5c2b8/WGS-NA12878.filtered.vcf; 11:24:10.814 INFO FilterAlignmentArtifacts - Done initializing engine; 11:24:10.816 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:24:10.817 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 11:24:10.818 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 11:24:10.818 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 11:24:10.957 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:24:10.980 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:24:10.980 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:24:10.981 INFO IntelPairHmm - Available threads: 80; 11:24:10.981 INFO IntelPairHmm - Requested threads: 4; 11:24:10.981 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:24:10.981 INFO ProgressMeter - Starting traversal; 11:24:10.981 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:25:26.222 INFO ProgressMeter - chr1:32527418 1.3 1000 797.5; 11:26:14.235 INFO ProgressMeter - chr1:103944651 2.1 2000 973.6; 11:26:59.367 INFO ProgressMeter - chr1:121884881 2.8 3000 1069.0; 11:28:22.595 INFO ProgressMeter - chr1:124412677 4.2 4000 953.8; 11:30:27.936 INFO ProgressMeter - chr1:146326436 6.3 5000 795.9; 11:31:16.814 INFO ProgressMeter - chr1:151781328 7.1 6000 845.4; 11:31:47.039 INFO ProgressMeter - chr1:222591703 7.6 7000 920.9; 11:32:23.165 INFO ProgressMeter - chr2:33832294 8.2 8000 975.2; 11:32:57.177 INFO ProgressMeter - chr2:90283356 8.8 9000 1026.2; 11:34:06.535 INFO ProgressMeter - chr2:93744700 9.9 10000 1007.5; 11:34:46.020 IN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:9875,Availability,down,down,9875,"17.402 INFO ProgressMeter - chr20:26553705 51.1 64000 1252.3; 12:17:04.504 INFO ProgressMeter - chr20:29573718 52.9 65000 1228.9; 12:18:23.620 INFO ProgressMeter - chr20:30705782 54.2 66000 1217.5; 12:20:09.590 INFO ProgressMeter - chr21:5249184 56.0 67000 1196.9; 12:22:03.446 INFO ProgressMeter - chr21:10481328 57.9 68000 1175.0; 12:23:03.667 INFO ProgressMeter - chr22:11316842 58.9 69000 1171.9; 12:24:57.983 INFO ProgressMeter - chr22:16127710 60.8 70000 1151.6; 12:26:31.457 INFO ProgressMeter - chr22:25997978 62.3 71000 1138.9; 12:27:09.230 INFO ProgressMeter - chrX:29806474 63.0 72000 1143.4; 12:27:53.073 INFO ProgressMeter - chrX:67820874 63.7 73000 1146.0; 12:29:03.144 INFO ProgressMeter - chrX:86707912 64.9 74000 1140.8; 12:30:07.160 INFO ProgressMeter - chrX:114791793 65.9 75000 1137.5; 12:30:59.264 INFO ProgressMeter - chrX:141227354 66.8 76000 1137.6; 12:31:52.052 INFO ProgressMeter - chrX:153138333 67.7 77000 1137.6; 12:33:18.782 INFO FilterAlignmentArtifacts - Shutting down engine; [February 24, 2023 12:33:18 PM CST] org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts done. Elapsed time: 69.15 minutes.; Runtime.totalMemory()=12185501696; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr1_KI270708v1_random:2666 [VC /raid/tmp/82/68cd46b704bab21cb8661465e5c2b8/WGS-NA12878.filtered.vcf @ chr1_KI270708v1_random:2666 Q. of type=SNP alleles=[T*, A] attr={AS_FilterStatus=SITE, AS_SB_TABLE=[7, 9|3, 11], DP=30, ECNT=2, GERMQ=13, MBQ=[33, 37], MFRL=[317, 323], MMQ=[48, 47], MPOS=47, POPAF=7.30, ROQ=83, TLOD=47.80} GT=GT:AD:AF:DP:F1R2:F2R1:FAD:SB 0/1:16,14:0.486:30:8,9:5,4:15,14:7,9,3,11 filters=; at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:145); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:164,Deployability,release,release,164,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. FilterAlignmentArtifacts. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. 4.3.0.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. ```; Using GATK jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -XX:+UseNUMA -jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar FilterAlignmentArtifacts -R /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.gz -O WGS-NA12878.FilterAlignmentArtifacts.vcf --tmp-dir . -V WGS-NA12878.filtered.vcf -I WGS-NA12878.sorted.dedup.recal.bam --bwa-mem-index-image /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.img; 11:24:09.761 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:24:09.942 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.942 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.3.0.0; 11:24:09.943 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:24:09.943 INFO FilterAlignmentArtifacts - Executing as root@D52BV-2U on Linux v4.15.0-202-generic amd64; 11:24:09.943 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_352-8u352-ga-1~18.04-b08; 11:24:09.943 INFO FilterAlignmentArtifacts - Start Date/Time: February 24, 2023 11:24:09 AM CST; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO Filter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:11244,Integrability,wrap,wrapAndCopyInto,11244," chr1_KI270708v1_random:2666 Q. of type=SNP alleles=[T*, A] attr={AS_FilterStatus=SITE, AS_SB_TABLE=[7, 9|3, 11], DP=30, ECNT=2, GERMQ=13, MBQ=[33, 37], MFRL=[317, 323], MMQ=[48, 47], MPOS=47, POPAF=7.30, ROQ=83, TLOD=47.80} GT=GT:AD:AF:DP:F1R2:F2R1:FAD:SB 0/1:16,14:0.486:30:8,9:5,4:15,14:7,9,3,11 filters=; at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:145); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:165); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Mai",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:986,Performance,Load,Loading,986,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. FilterAlignmentArtifacts. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. 4.3.0.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. ```; Using GATK jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -XX:+UseNUMA -jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar FilterAlignmentArtifacts -R /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.gz -O WGS-NA12878.FilterAlignmentArtifacts.vcf --tmp-dir . -V WGS-NA12878.filtered.vcf -I WGS-NA12878.sorted.dedup.recal.bam --bwa-mem-index-image /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.img; 11:24:09.761 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:24:09.942 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.942 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.3.0.0; 11:24:09.943 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:24:09.943 INFO FilterAlignmentArtifacts - Executing as root@D52BV-2U on Linux v4.15.0-202-generic amd64; 11:24:09.943 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_352-8u352-ga-1~18.04-b08; 11:24:09.943 INFO FilterAlignmentArtifacts - Start Date/Time: February 24, 2023 11:24:09 AM CST; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO Filter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:3453,Performance,Load,Loading,3453,O_WRITE_FOR_SAMTOOLS : true; 11:24:09.944 INFO FilterAlignmentArtifacts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:24:09.944 INFO FilterAlignmentArtifacts - Deflater: IntelDeflater; 11:24:09.944 INFO FilterAlignmentArtifacts - Inflater: IntelInflater; 11:24:09.944 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 11:24:09.944 INFO FilterAlignmentArtifacts - Requester pays: disabled; 11:24:09.944 WARN FilterAlignmentArtifacts -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:24:09.944 INFO FilterAlignmentArtifacts - Initializing engine; 11:24:10.534 INFO FeatureManager - Using codec VCFCodec to read file file:///raid/tmp/82/68cd46b704bab21cb8661465e5c2b8/WGS-NA12878.filtered.vcf; 11:24:10.814 INFO FilterAlignmentArtifacts - Done initializing engine; 11:24:10.816 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:24:10.817 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 11:24:10.818 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 11:24:10.818 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 11:24:10.957 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:24:10.980 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:24:10.980 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:24:10.981 INFO IntelPairHmm - Available threads: 80; 11:24:10.981 INFO IntelPairHmm - Requested threads: 4; 11:24:10.981 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:3615,Performance,Load,Loading,3615,ifacts - Deflater: IntelDeflater; 11:24:09.944 INFO FilterAlignmentArtifacts - Inflater: IntelInflater; 11:24:09.944 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 11:24:09.944 INFO FilterAlignmentArtifacts - Requester pays: disabled; 11:24:09.944 WARN FilterAlignmentArtifacts -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:24:09.944 INFO FilterAlignmentArtifacts - Initializing engine; 11:24:10.534 INFO FeatureManager - Using codec VCFCodec to read file file:///raid/tmp/82/68cd46b704bab21cb8661465e5c2b8/WGS-NA12878.filtered.vcf; 11:24:10.814 INFO FilterAlignmentArtifacts - Done initializing engine; 11:24:10.816 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:24:10.817 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 11:24:10.818 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 11:24:10.818 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 11:24:10.957 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:24:10.980 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:24:10.980 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:24:10.981 INFO IntelPairHmm - Available threads: 80; 11:24:10.981 INFO IntelPairHmm - Requested threads: 4; 11:24:10.981 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:24:10.981 INFO ProgressMeter - Starting traversal; 11:24:10.981 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Process,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:3967,Performance,Load,Loading,3967,!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:24:09.944 INFO FilterAlignmentArtifacts - Initializing engine; 11:24:10.534 INFO FeatureManager - Using codec VCFCodec to read file file:///raid/tmp/82/68cd46b704bab21cb8661465e5c2b8/WGS-NA12878.filtered.vcf; 11:24:10.814 INFO FilterAlignmentArtifacts - Done initializing engine; 11:24:10.816 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:24:10.817 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 11:24:10.818 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 11:24:10.818 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 11:24:10.957 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:24:10.980 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:24:10.980 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:24:10.981 INFO IntelPairHmm - Available threads: 80; 11:24:10.981 INFO IntelPairHmm - Requested threads: 4; 11:24:10.981 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:24:10.981 INFO ProgressMeter - Starting traversal; 11:24:10.981 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:25:26.222 INFO ProgressMeter - chr1:32527418 1.3 1000 797.5; 11:26:14.235 INFO ProgressMeter - chr1:103944651 2.1 2000 973.6; 11:26:59.367 INFO ProgressMeter - chr1:121884881 2.8 3000 1069.0; 11:28:22.595 INFO ProgressMeter - chr1:124412677 4.2 4000 953.8; 11:30:27.936 INFO ProgressMeter - chr1:146326436 6.3 5000 795.9; 11:31:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:4418,Performance,multi-thread,multi-threaded,4418,veLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:24:10.817 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 11:24:10.818 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 11:24:10.818 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 11:24:10.957 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:24:10.980 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:24:10.980 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:24:10.981 INFO IntelPairHmm - Available threads: 80; 11:24:10.981 INFO IntelPairHmm - Requested threads: 4; 11:24:10.981 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:24:10.981 INFO ProgressMeter - Starting traversal; 11:24:10.981 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:25:26.222 INFO ProgressMeter - chr1:32527418 1.3 1000 797.5; 11:26:14.235 INFO ProgressMeter - chr1:103944651 2.1 2000 973.6; 11:26:59.367 INFO ProgressMeter - chr1:121884881 2.8 3000 1069.0; 11:28:22.595 INFO ProgressMeter - chr1:124412677 4.2 4000 953.8; 11:30:27.936 INFO ProgressMeter - chr1:146326436 6.3 5000 795.9; 11:31:16.814 INFO ProgressMeter - chr1:151781328 7.1 6000 845.4; 11:31:47.039 INFO ProgressMeter - chr1:222591703 7.6 7000 920.9; 11:32:23.165 INFO ProgressMeter - chr2:33832294 8.2 8000 975.2; 11:32:57.177 INFO ProgressMeter - chr2:90283356 8.8 9000 1026.2; 11:34:06.535 INFO ProgressMeter - chr2:93744700 9.9 10000 1007.5; 11:34:46.020 INFO ProgressMeter - chr2:146056068 10.6 11000 1039.3; 11:35:13.013 INFO ProgressMeter - chr2:223829124 11.0 12000 1087.6; 11:35:42.553 INF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:12552,Security,validat,validate,12552,cePipeline.forEach(ReferencePipeline.java:485); at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:136); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:165); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:109); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:85); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:120); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.makeAssemblyRegionFromVariantReads(FilterAlignmentArtifacts.java:280); at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.apply(FilterAlignmentArtifacts.java:212); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:133); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:108); at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:139); ... 21 more; ```. #### Steps to reproduce; _Tell us how to reproduce this,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:234,Testability,test,test,234,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. FilterAlignmentArtifacts. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. 4.3.0.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. ```; Using GATK jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -XX:+UseNUMA -jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar FilterAlignmentArtifacts -R /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.gz -O WGS-NA12878.FilterAlignmentArtifacts.vcf --tmp-dir . -V WGS-NA12878.filtered.vcf -I WGS-NA12878.sorted.dedup.recal.bam --bwa-mem-index-image /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.img; 11:24:09.761 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:24:09.942 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.942 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.3.0.0; 11:24:09.943 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:24:09.943 INFO FilterAlignmentArtifacts - Executing as root@D52BV-2U on Linux v4.15.0-202-generic amd64; 11:24:09.943 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_352-8u352-ga-1~18.04-b08; 11:24:09.943 INFO FilterAlignmentArtifacts - Start Date/Time: February 24, 2023 11:24:09 AM CST; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO Filter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8221:343,Testability,log,logs,343,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. FilterAlignmentArtifacts. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. 4.3.0.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. ```; Using GATK jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -XX:+UseNUMA -jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar FilterAlignmentArtifacts -R /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.gz -O WGS-NA12878.FilterAlignmentArtifacts.vcf --tmp-dir . -V WGS-NA12878.filtered.vcf -I WGS-NA12878.sorted.dedup.recal.bam --bwa-mem-index-image /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.img; 11:24:09.761 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:24:09.942 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.942 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.3.0.0; 11:24:09.943 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:24:09.943 INFO FilterAlignmentArtifacts - Executing as root@D52BV-2U on Linux v4.15.0-202-generic amd64; 11:24:09.943 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_352-8u352-ga-1~18.04-b08; 11:24:09.943 INFO FilterAlignmentArtifacts - Start Date/Time: February 24, 2023 11:24:09 AM CST; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO Filter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221
https://github.com/broadinstitute/gatk/issues/8224:56,Availability,error,error,56,"### Instructions. Dear GATK team,. I am encountering an error when using the mutect2 function in ""gatk-4.2.6.1"". Whenever I enable the ""--disable-tool-default-read-filters"" option, I receive a java.lang.ArrayIndexOutOfBoundsException error. Since I need to call SNVs for RNA-seq data, I first split the bam file by chromosome, and then perform markduplicate and splitNcigar in two steps. I found that when I use the bam file obtained after using the splitNcigar function to run mutect2, the same error still occurs, even if I don't disable the -read-filters. Therefore, I suspect that the error may be introduced by splitNcigar. Thus, I tried running mutect2 directly on the bam file after markduplicate. If the --disable-tool-default-read-filters option is not set, the command runs successfully. However, once it is set, the same error occurs. ----. ## Bug Report; [February 28, 2023 10:46:30 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2394947584; java.lang.ArrayIndexOutOfBoundsException; at java.lang.System.arraycopy(Native Method); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHardClipBases(ClippingOp.java:216); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:69); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:142); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipLowQualEnds(ReadClipper.java:251); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:255); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:263); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:132); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:270); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224
https://github.com/broadinstitute/gatk/issues/8224:234,Availability,error,error,234,"### Instructions. Dear GATK team,. I am encountering an error when using the mutect2 function in ""gatk-4.2.6.1"". Whenever I enable the ""--disable-tool-default-read-filters"" option, I receive a java.lang.ArrayIndexOutOfBoundsException error. Since I need to call SNVs for RNA-seq data, I first split the bam file by chromosome, and then perform markduplicate and splitNcigar in two steps. I found that when I use the bam file obtained after using the splitNcigar function to run mutect2, the same error still occurs, even if I don't disable the -read-filters. Therefore, I suspect that the error may be introduced by splitNcigar. Thus, I tried running mutect2 directly on the bam file after markduplicate. If the --disable-tool-default-read-filters option is not set, the command runs successfully. However, once it is set, the same error occurs. ----. ## Bug Report; [February 28, 2023 10:46:30 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2394947584; java.lang.ArrayIndexOutOfBoundsException; at java.lang.System.arraycopy(Native Method); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHardClipBases(ClippingOp.java:216); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:69); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:142); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipLowQualEnds(ReadClipper.java:251); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:255); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:263); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:132); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:270); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224
https://github.com/broadinstitute/gatk/issues/8224:496,Availability,error,error,496,"### Instructions. Dear GATK team,. I am encountering an error when using the mutect2 function in ""gatk-4.2.6.1"". Whenever I enable the ""--disable-tool-default-read-filters"" option, I receive a java.lang.ArrayIndexOutOfBoundsException error. Since I need to call SNVs for RNA-seq data, I first split the bam file by chromosome, and then perform markduplicate and splitNcigar in two steps. I found that when I use the bam file obtained after using the splitNcigar function to run mutect2, the same error still occurs, even if I don't disable the -read-filters. Therefore, I suspect that the error may be introduced by splitNcigar. Thus, I tried running mutect2 directly on the bam file after markduplicate. If the --disable-tool-default-read-filters option is not set, the command runs successfully. However, once it is set, the same error occurs. ----. ## Bug Report; [February 28, 2023 10:46:30 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2394947584; java.lang.ArrayIndexOutOfBoundsException; at java.lang.System.arraycopy(Native Method); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHardClipBases(ClippingOp.java:216); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:69); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:142); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipLowQualEnds(ReadClipper.java:251); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:255); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:263); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:132); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:270); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224
https://github.com/broadinstitute/gatk/issues/8224:589,Availability,error,error,589,"### Instructions. Dear GATK team,. I am encountering an error when using the mutect2 function in ""gatk-4.2.6.1"". Whenever I enable the ""--disable-tool-default-read-filters"" option, I receive a java.lang.ArrayIndexOutOfBoundsException error. Since I need to call SNVs for RNA-seq data, I first split the bam file by chromosome, and then perform markduplicate and splitNcigar in two steps. I found that when I use the bam file obtained after using the splitNcigar function to run mutect2, the same error still occurs, even if I don't disable the -read-filters. Therefore, I suspect that the error may be introduced by splitNcigar. Thus, I tried running mutect2 directly on the bam file after markduplicate. If the --disable-tool-default-read-filters option is not set, the command runs successfully. However, once it is set, the same error occurs. ----. ## Bug Report; [February 28, 2023 10:46:30 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2394947584; java.lang.ArrayIndexOutOfBoundsException; at java.lang.System.arraycopy(Native Method); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHardClipBases(ClippingOp.java:216); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:69); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:142); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipLowQualEnds(ReadClipper.java:251); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:255); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:263); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:132); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:270); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224
https://github.com/broadinstitute/gatk/issues/8224:832,Availability,error,error,832,"### Instructions. Dear GATK team,. I am encountering an error when using the mutect2 function in ""gatk-4.2.6.1"". Whenever I enable the ""--disable-tool-default-read-filters"" option, I receive a java.lang.ArrayIndexOutOfBoundsException error. Since I need to call SNVs for RNA-seq data, I first split the bam file by chromosome, and then perform markduplicate and splitNcigar in two steps. I found that when I use the bam file obtained after using the splitNcigar function to run mutect2, the same error still occurs, even if I don't disable the -read-filters. Therefore, I suspect that the error may be introduced by splitNcigar. Thus, I tried running mutect2 directly on the bam file after markduplicate. If the --disable-tool-default-read-filters option is not set, the command runs successfully. However, once it is set, the same error occurs. ----. ## Bug Report; [February 28, 2023 10:46:30 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2394947584; java.lang.ArrayIndexOutOfBoundsException; at java.lang.System.arraycopy(Native Method); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHardClipBases(ClippingOp.java:216); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:69); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:142); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipLowQualEnds(ReadClipper.java:251); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:255); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:263); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:132); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:270); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224
https://github.com/broadinstitute/gatk/issues/8224:336,Performance,perform,perform,336,"### Instructions. Dear GATK team,. I am encountering an error when using the mutect2 function in ""gatk-4.2.6.1"". Whenever I enable the ""--disable-tool-default-read-filters"" option, I receive a java.lang.ArrayIndexOutOfBoundsException error. Since I need to call SNVs for RNA-seq data, I first split the bam file by chromosome, and then perform markduplicate and splitNcigar in two steps. I found that when I use the bam file obtained after using the splitNcigar function to run mutect2, the same error still occurs, even if I don't disable the -read-filters. Therefore, I suspect that the error may be introduced by splitNcigar. Thus, I tried running mutect2 directly on the bam file after markduplicate. If the --disable-tool-default-read-filters option is not set, the command runs successfully. However, once it is set, the same error occurs. ----. ## Bug Report; [February 28, 2023 10:46:30 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2394947584; java.lang.ArrayIndexOutOfBoundsException; at java.lang.System.arraycopy(Native Method); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHardClipBases(ClippingOp.java:216); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:69); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:142); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipLowQualEnds(ReadClipper.java:251); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:255); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:263); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:132); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:270); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224
https://github.com/broadinstitute/gatk/pull/8227:242,Testability,log,logger,242,Modified VariantRecalibrator to only use a deduped version of the annotations list provided by the user once the list has been deduped. Previously it was still sometimes using the list that could have duplicates. Fixes #8226 . Also fixed the logger warning that prints the duplicate annotations.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8227
https://github.com/broadinstitute/gatk/pull/8228:5,Deployability,update,updated,5,I've updated the image and it seems to be working locally. Now for the true test...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8228
https://github.com/broadinstitute/gatk/pull/8228:76,Testability,test,test,76,I've updated the image and it seems to be working locally. Now for the true test...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8228
https://github.com/broadinstitute/gatk/pull/8229:89,Deployability,Integrat,Integration,89,More prep work for separating GVS code: remove references to GVS code from non-GVS code. Integration test currently running [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/2af93966-3c84-4aec-bc1e-82cb88478852).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8229
https://github.com/broadinstitute/gatk/pull/8229:89,Integrability,Integrat,Integration,89,More prep work for separating GVS code: remove references to GVS code from non-GVS code. Integration test currently running [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/2af93966-3c84-4aec-bc1e-82cb88478852).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8229
https://github.com/broadinstitute/gatk/pull/8229:101,Testability,test,test,101,More prep work for separating GVS code: remove references to GVS code from non-GVS code. Integration test currently running [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/2af93966-3c84-4aec-bc1e-82cb88478852).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8229
https://github.com/broadinstitute/gatk/pull/8230:0,Deployability,Update,Update,0,Update GvsCalculatePrecisionAndSensitivity.wdl to allow for different scale of calibration_sensitivity vs. lod score.; Also retrieving score from JointVcfFiltering and storing that in BQ and in the VCF. (Probably don't need this long term.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8230
https://github.com/broadinstitute/gatk/issues/8232:120,Availability,error,error,120,"I've been trying repeatedly to run SplitNCigarReads on a mapped RNA-seq bam file (from STAR), but receive the following error:. [March 2, 2023 at 8:40:36 AM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 1.23 minutes.; Runtime.totalMemory()=5184159744; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 	at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 	at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 	at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1101); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). I've run the following command:. /cold/gatk-4.3.0.0/./gatk	 --java-options ""-Xmx25g"" SplitNCigarReads \; 	 -R /cold/base/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; 	 --tmp-dir /thing -O thing.bam. I've tried setting --tmp-dir, as well as the system varliable TEMP_DIR, but all to no avail. Any suggestions/work-arounds?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232
https://github.com/broadinstitute/gatk/issues/8232:1910,Availability,avail,avail,1910,"I've been trying repeatedly to run SplitNCigarReads on a mapped RNA-seq bam file (from STAR), but receive the following error:. [March 2, 2023 at 8:40:36 AM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 1.23 minutes.; Runtime.totalMemory()=5184159744; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 	at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 	at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 	at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1101); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). I've run the following command:. /cold/gatk-4.3.0.0/./gatk	 --java-options ""-Xmx25g"" SplitNCigarReads \; 	 -R /cold/base/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; 	 --tmp-dir /thing -O thing.bam. I've tried setting --tmp-dir, as well as the system varliable TEMP_DIR, but all to no avail. Any suggestions/work-arounds?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232
https://github.com/broadinstitute/gatk/issues/8233:327,Availability,error,error,327,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport / GenotypeGVCFs. ### Affected version(s); 4.3.0.0. ### Description ; When creating a GenomicsDB datastore, the created folder has permissions set to 700 (recursivelly).; As such, when trying to jointly calling genotypes using the GenotypeGVCFs, one encounters error:; ERROR: Couldn't create GenomicsDBFeatureReader. #### Steps to reproduce; - Create a datastore using GenomicsDBImport, e.g. ; gatk ... --genomicsdb-workspace-path IWANNAKILLYOU. - Recursively change access permission to the thus created genomicsdb; chmod 700 -R ./IWANNAKILLYOU. - Run the GenotypeGVCFs; gatk ... --variant gendb://IWANNAKILLYOU. #### Expected behavior; GenotypeGVCFs should initialize the engine normally and start processing the intervals as expected. #### Actual behavior; GenotypeGVCFs intializes the engine and throws out and error; ERROR: Couldn't create GenomicsDBFeatureReader. #### Proposed solution; Mention anywhere in the docs the genomicsdb datastore should be made readable to other users, i.e., change permissions to at least 744 if not do a 766.; Or just make sure the ./IWANNAKILLYOU has proper permissions from the get go. Much obliged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233
https://github.com/broadinstitute/gatk/issues/8233:335,Availability,ERROR,ERROR,335,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport / GenotypeGVCFs. ### Affected version(s); 4.3.0.0. ### Description ; When creating a GenomicsDB datastore, the created folder has permissions set to 700 (recursivelly).; As such, when trying to jointly calling genotypes using the GenotypeGVCFs, one encounters error:; ERROR: Couldn't create GenomicsDBFeatureReader. #### Steps to reproduce; - Create a datastore using GenomicsDBImport, e.g. ; gatk ... --genomicsdb-workspace-path IWANNAKILLYOU. - Recursively change access permission to the thus created genomicsdb; chmod 700 -R ./IWANNAKILLYOU. - Run the GenotypeGVCFs; gatk ... --variant gendb://IWANNAKILLYOU. #### Expected behavior; GenotypeGVCFs should initialize the engine normally and start processing the intervals as expected. #### Actual behavior; GenotypeGVCFs intializes the engine and throws out and error; ERROR: Couldn't create GenomicsDBFeatureReader. #### Proposed solution; Mention anywhere in the docs the genomicsdb datastore should be made readable to other users, i.e., change permissions to at least 744 if not do a 766.; Or just make sure the ./IWANNAKILLYOU has proper permissions from the get go. Much obliged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233
https://github.com/broadinstitute/gatk/issues/8233:881,Availability,error,error,881,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport / GenotypeGVCFs. ### Affected version(s); 4.3.0.0. ### Description ; When creating a GenomicsDB datastore, the created folder has permissions set to 700 (recursivelly).; As such, when trying to jointly calling genotypes using the GenotypeGVCFs, one encounters error:; ERROR: Couldn't create GenomicsDBFeatureReader. #### Steps to reproduce; - Create a datastore using GenomicsDBImport, e.g. ; gatk ... --genomicsdb-workspace-path IWANNAKILLYOU. - Recursively change access permission to the thus created genomicsdb; chmod 700 -R ./IWANNAKILLYOU. - Run the GenotypeGVCFs; gatk ... --variant gendb://IWANNAKILLYOU. #### Expected behavior; GenotypeGVCFs should initialize the engine normally and start processing the intervals as expected. #### Actual behavior; GenotypeGVCFs intializes the engine and throws out and error; ERROR: Couldn't create GenomicsDBFeatureReader. #### Proposed solution; Mention anywhere in the docs the genomicsdb datastore should be made readable to other users, i.e., change permissions to at least 744 if not do a 766.; Or just make sure the ./IWANNAKILLYOU has proper permissions from the get go. Much obliged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233
https://github.com/broadinstitute/gatk/issues/8233:888,Availability,ERROR,ERROR,888,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport / GenotypeGVCFs. ### Affected version(s); 4.3.0.0. ### Description ; When creating a GenomicsDB datastore, the created folder has permissions set to 700 (recursivelly).; As such, when trying to jointly calling genotypes using the GenotypeGVCFs, one encounters error:; ERROR: Couldn't create GenomicsDBFeatureReader. #### Steps to reproduce; - Create a datastore using GenomicsDBImport, e.g. ; gatk ... --genomicsdb-workspace-path IWANNAKILLYOU. - Recursively change access permission to the thus created genomicsdb; chmod 700 -R ./IWANNAKILLYOU. - Run the GenotypeGVCFs; gatk ... --variant gendb://IWANNAKILLYOU. #### Expected behavior; GenotypeGVCFs should initialize the engine normally and start processing the intervals as expected. #### Actual behavior; GenotypeGVCFs intializes the engine and throws out and error; ERROR: Couldn't create GenomicsDBFeatureReader. #### Proposed solution; Mention anywhere in the docs the genomicsdb datastore should be made readable to other users, i.e., change permissions to at least 744 if not do a 766.; Or just make sure the ./IWANNAKILLYOU has proper permissions from the get go. Much obliged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233
https://github.com/broadinstitute/gatk/issues/8233:533,Security,access,access,533,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport / GenotypeGVCFs. ### Affected version(s); 4.3.0.0. ### Description ; When creating a GenomicsDB datastore, the created folder has permissions set to 700 (recursivelly).; As such, when trying to jointly calling genotypes using the GenotypeGVCFs, one encounters error:; ERROR: Couldn't create GenomicsDBFeatureReader. #### Steps to reproduce; - Create a datastore using GenomicsDBImport, e.g. ; gatk ... --genomicsdb-workspace-path IWANNAKILLYOU. - Recursively change access permission to the thus created genomicsdb; chmod 700 -R ./IWANNAKILLYOU. - Run the GenotypeGVCFs; gatk ... --variant gendb://IWANNAKILLYOU. #### Expected behavior; GenotypeGVCFs should initialize the engine normally and start processing the intervals as expected. #### Actual behavior; GenotypeGVCFs intializes the engine and throws out and error; ERROR: Couldn't create GenomicsDBFeatureReader. #### Proposed solution; Mention anywhere in the docs the genomicsdb datastore should be made readable to other users, i.e., change permissions to at least 744 if not do a 766.; Or just make sure the ./IWANNAKILLYOU has proper permissions from the get go. Much obliged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233
https://github.com/broadinstitute/gatk/pull/8234:53,Deployability,pipeline,pipeline,53,This matches arguments of various tools used in gCNV pipeline to those used for running large exome cohorts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8234
https://github.com/broadinstitute/gatk/pull/8236:65,Testability,Test,Test,65,Use existing python function to track avro file generation cost. Test run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/8890e4c5-8521-4555-96a1-c96dd54b2fd8.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8236
https://github.com/broadinstitute/gatk/pull/8237:17,Availability,mask,mask,17,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:29,Availability,mask,mask-name,29,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:98,Availability,mask,mask,98,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:205,Availability,mask,mask,205,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:265,Availability,mask,mask,265,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:281,Availability,mask,mask,281,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:449,Availability,mask,mask,449,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:477,Availability,mask,maskName,477,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:536,Availability,Mask,Mask,536,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:575,Availability,Mask,Mask,575,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:595,Availability,mask,mask,595,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:712,Availability,mask,mask,712,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:722,Availability,mask,masks,722,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:734,Availability,mask,maskName,734,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:748,Availability,mask,maskNames,748,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:844,Availability,mask,mask,844,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:896,Availability,mask,mask,896,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:1031,Availability,mask,maskName,1031,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:1060,Availability,mask,mask,1060,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:675,Modifiability,variab,variable,675,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/pull/8237:1135,Performance,perform,performance,1135,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237
https://github.com/broadinstitute/gatk/issues/8238:1552,Availability,error,error,1552,"## Bug Report. ### Affected tool(s) or class(es); GATK Haplotypecaller . ### Affected version(s); (4.3.0.0 and 4.2.6.1). ### Description ; We used Haplotypecaller in GVCF mode (initially in 4.2.6.1, then again with 4.3.0.0) for one of our human samples. These samples were joint-genotyped with ~2.7k exomes. The exact command used was - . ```gatk HaplotypeCaller -R ""$ref_hg38"" -I input.bam -L twist.bed -ERC GVCF -ip 50 -O test_latest.gvcf.gz -bamout test_bamout_latest.bam --tmp-dir /mnt/exome/tmp/```. Below is a screenshot of the bamout file (top track) and the recalibrated BAM file (bottom track). . ![image](https://user-images.githubusercontent.com/32951653/224249117-ab13800f-5b3c-42f1-b349-993ae182620f.png). As shown in screenshot, there are only 2 reads supporting the alternate allele, however the gvcf.gz file has the below entry for the variant - . ```chr5 176530208 . T C,<NON_REF> 2717.64 . BaseQRankSum=0.975;DP=179;ExcessHet=0.0000;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQandDP=644400,179;ReadPosRankSum=10.927 GT:AD:DP:GQ:PGT:PID:PL:PS:SB 0|1:68,102,0:170:99:0|1:176530208_T_C:2725,0,1279,2928,1584,4512:176530208:38,30,37,65```. This variant is called as a heterozygous variant ```0|1``` with read frequency of ```68,102,0```, which would mean 68 reads supporting the ref allele, 102 reads supporting the alt C allele and 0 reads supporting the <NON_REF> allele. After joint-genotyping, the variant was classified as LOW_VQSLOD. #### Steps to reproduce; Let us know how to share the BAM file subset and let us know if the error is reproducible. #### Expected behavior; Ideally, if the variant had been called, its read frequency should have been represented more accurately. #### Actual behavior; The read frequencies are not matching up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8238
https://github.com/broadinstitute/gatk/issues/8242:460,Energy Efficiency,meter,meterics,460,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8242:673,Security,hash,hashing,673,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8242:392,Testability,test,test,392,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8242:635,Testability,test,test,635,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8242:685,Testability,test,test,685,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8242:783,Testability,test,test,783,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8242:798,Testability,assert,asserts,798,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8242:883,Testability,test,test,883,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8242:609,Usability,clear,clear,609,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242
https://github.com/broadinstitute/gatk/issues/8243:61,Deployability,update,updated,61,"Version 3.0.0 of the base image is still on 18.04 (albeit an updated version of 18.04). Although 18.04 is still supported, we should try to move to the latest LTS release, 22.04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8243
https://github.com/broadinstitute/gatk/issues/8243:163,Deployability,release,release,163,"Version 3.0.0 of the base image is still on 18.04 (albeit an updated version of 18.04). Although 18.04 is still supported, we should try to move to the latest LTS release, 22.04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8243
https://github.com/broadinstitute/gatk/issues/8246:168,Deployability,toggle,toggle,168,As part of #8083 we have added yet another heterogenous way of spitting out detailed debugging logs from yet another part of the HaplotypeCaller (namely in this case a toggle that prints to Stdout). This is in contrast to the existing (mostly assembly region position information) `--verbosity DEBUG` debug arguments and the various debug output stream arguments like `--debug-assembly-region-state`. These debug modes have proliferated and it has become difficult/confusing to know which ones are relevant if you are developing the code. . At some point it might be worth creating some sort of static debug manager class that manages the various specific output streams and saves us from having to pass debugger state to all of the various subclasses/utility methods of the HaplotypeCallerEngine. Importantly using `DEBUG` to stderr is not entirely useful as it jumbles all of the various debugger outputs into one output which quickly becomes large and difficult to read. We must support optionally splitting the output streams by functionality to files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8246
https://github.com/broadinstitute/gatk/issues/8246:95,Testability,log,logs,95,As part of #8083 we have added yet another heterogenous way of spitting out detailed debugging logs from yet another part of the HaplotypeCaller (namely in this case a toggle that prints to Stdout). This is in contrast to the existing (mostly assembly region position information) `--verbosity DEBUG` debug arguments and the various debug output stream arguments like `--debug-assembly-region-state`. These debug modes have proliferated and it has become difficult/confusing to know which ones are relevant if you are developing the code. . At some point it might be worth creating some sort of static debug manager class that manages the various specific output streams and saves us from having to pass debugger state to all of the various subclasses/utility methods of the HaplotypeCallerEngine. Importantly using `DEBUG` to stderr is not entirely useful as it jumbles all of the various debugger outputs into one output which quickly becomes large and difficult to read. We must support optionally splitting the output streams by functionality to files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8246
https://github.com/broadinstitute/gatk/pull/8248:5264,Availability,error,error,5264,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5317,Availability,error,error,5317,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11236,Availability,error,error,11236," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11289,Availability,error,error,11289," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:14776,Availability,reliab,reliability,14776,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:15255,Availability,reliab,reliability,15255,"Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16119,Availability,down,download,16119,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:19018,Availability,error,error,19018,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:21407,Availability,failure,failure,21407,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24019,Availability,Reliab,Reliability,24019,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24380,Availability,Error,Error,24380,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:24573,Availability,error,error,24573,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26303,Availability,error,errors,26303," - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change bac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26691,Availability,down,download,26691," [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:27342,Availability,error,error,27342,"#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guide",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:27807,Availability,error,error,27807,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:29252,Availability,failure,failure,29252,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:656,Deployability,update,update,656,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:899,Deployability,update,update,899,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:1137,Deployability,upgrade,upgraded,1137,"ory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for ac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:1400,Deployability,update,update,1400," major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:2302,Deployability,update,update,2302,"); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:2545,Deployability,update,update,2545,"ariants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:2739,Deployability,upgrade,upgraded,2739,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:3412,Deployability,update,update,3412,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:3442,Deployability,update,update,3442,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:3458,Deployability,update,update,3458,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:4710,Deployability,update,updated,4710,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:4783,Deployability,pipeline,pipeline,4783,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5200,Deployability,integrat,integration,5200,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5289,Deployability,Update,Update,5289,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:5553,Deployability,update,updates,5553,,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:6628,Deployability,update,update,6628,"ract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:6871,Deployability,update,update,6871,"g to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:7109,Deployability,upgrade,upgraded,7109,"ory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for ac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:7372,Deployability,update,update,7372," major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:8274,Deployability,update,update,8274,"); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:8517,Deployability,update,update,8517,"ariants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:8711,Deployability,upgrade,upgraded,8711,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:9384,Deployability,update,update,9384,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:9414,Deployability,update,update,9414,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:9430,Deployability,update,update,9430,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:10682,Deployability,update,updated,10682,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:10755,Deployability,pipeline,pipeline,10755," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11172,Deployability,integrat,integration,11172," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11261,Deployability,Update,Update,11261," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:11525,Deployability,update,updates,11525," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:12506,Deployability,Update,Update,12506," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:13089,Deployability,update,updated,13089," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:13995,Deployability,Update,Update,13995," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:14515,Deployability,upgrade,upgrade,14515,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16328,Deployability,upgrade,upgraded,16328,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:16382,Deployability,Update,Update,16382,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:17094,Deployability,update,update,17094,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:17456,Deployability,update,update,17456,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:17820,Deployability,Update,Update,17820,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:17903,Deployability,Update,Update,17903,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:18211,Deployability,update,updates,18211,"es to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (#7426); - Added additional workflow and README updates for Quickstart [VS-183] (#7463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:18393,Deployability,Update,Update,18393,"es to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (#7426); - Added additional workflow and README updates for Quickstart [VS-183] (#7463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:19072,Deployability,update,updates,19072,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:19519,Deployability,pipeline,pipeline,19519,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:19999,Deployability,update,update,19999,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:20052,Deployability,Update,Update,20052,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:20601,Deployability,upgrade,upgrade,20601,"rom Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:20661,Deployability,upgrade,upgrade,20661,"rom Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:21072,Deployability,Update,Update,21072,"filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to Ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:21297,Deployability,Update,Update,21297,"take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:22317,Deployability,update,update,22317,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23100,Deployability,Update,Update,23100,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:23797,Deployability,integrat,integration,23797,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25172,Deployability,Update,Update,25172,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25190,Deployability,Integrat,Integration,25190,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25419,Deployability,integrat,integration,25419,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25608,Deployability,update,update,25608,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25709,Deployability,Update,Update,25709,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:25729,Deployability,Integrat,Integration,25729,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26250,Deployability,release,release,26250," - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change bac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26729,Deployability,Update,Update,26729,"v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail im",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:26897,Deployability,integrat,integration,26897,"x Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:27422,Deployability,update,update,27422,"from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:27558,Deployability,Update,Update,27558,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
https://github.com/broadinstitute/gatk/pull/8248:28978,Deployability,update,updates,28978,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248
